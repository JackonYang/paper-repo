{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152951058"
                        ],
                        "name": "Drew A. Hudson",
                        "slug": "Drew-A.-Hudson",
                        "structuredName": {
                            "firstName": "Drew",
                            "lastName": "Hudson",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Drew A. Hudson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 277,
                                "start": 265
                            }
                        ],
                        "text": "Others have argued for the importance of incorporating strong inductive biases into neural architectures [14, 10, 6, 8], and indeed, there is a growing body of research that seeks to introduce different forms of structural priors inspired by computer architectures [29, 80, 40] or theory of computation [4, 23], aiming to bridge the gap between the symbolic and neural paradigms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 56
                            }
                        ],
                        "text": "At the same time, the Neural State Machine differs from MAC in two crucial respects: First, we reason over graph structures rather than directly over spatial maps of visual features, traversing the graph by successively shifting attention across its nodes and edges."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 128
                            }
                        ],
                        "text": "We explore our model in the context of VQA, a challenging multimodal task that has gained substantial attention in recent years [27, 79, 40]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 116
                            }
                        ],
                        "text": "Finally, we process the normalized question words with an attention-based encoder-decoder, drawing inspiration from [40]: Given a question of P normalized words V P\u00d7d = {vi}i=1, we first pass it through an LSTM encoder, obtaining the final state q to represent the question."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 47
                            }
                        ],
                        "text": "However, the Neural State Machine differs from MAC in two crucial respects: First, we reason over graph structures rather than directly over spatial maps of visual features, traversing the graph by successively shifting attention across its nodes and edges."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 68
                            }
                        ],
                        "text": "In fact, our new model follows the high-level structure proposed by MAC, where the new decoder (section 3.3) is analogous to the control unit, and the model simulation (section 3.4) is parallel to the read-write operation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "Closest to the NSM is a model called MAC [40] we have developed in prior work, a recurrent network that applies attention-based operations to perform sequential reasoning."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 512,
                                "start": 504
                            }
                        ],
                        "text": "Indeed, humans are particularly adept at making abstractions of various kinds: We make analogies and form concepts to generalize from given instances to unseen examples [70]; we see things in context, and build compositional world models to represent objects and understand their interactions and subtle relations, turning raw sensory signals into high-level semantic knowledge [64]; and we deductively draw inferences via conceptual rules and statements to proceed from known facts to novel conclusions [32, 40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 271,
                                "start": 268
                            }
                        ],
                        "text": "Both our model and implemented baselines are trained to minimize the cross-entropy loss of the predicted candidate answer (out of the top 2000 possibilities), using a hidden state size of d = 300 and, unless otherwise stated, length of N = 8 computation steps for the MAC and NSM models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 38
                            }
                        ],
                        "text": "Closest to our work is a model called MAC [40], a recurrent network that applies attention-based operations to perform sequential reasoning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 100
                            }
                        ],
                        "text": "As our findings suggest, these ideas contribute significantly to the model\u2019s performance compared to MAC and enhance its compositionality, transparency and generalization skills."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3728944,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "289fb3709475f5c87df8d97f129af54029d27fee",
            "isKey": false,
            "numCitedBy": 401,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We present the MAC network, a novel fully differentiable neural network architecture, designed to facilitate explicit and expressive reasoning. MAC moves away from monolithic black-box neural architectures towards a design that encourages both transparency and versatility. The model approaches problems by decomposing them into a series of attention-based reasoning steps, each performed by a novel recurrent Memory, Attention, and Composition (MAC) cell that maintains a separation between control and memory. By stringing the cells together and imposing structural constraints that regulate their interaction, MAC effectively learns to perform iterative reasoning processes that are directly inferred from the data in an end-to-end approach. We demonstrate the model's strength, robustness and interpretability on the challenging CLEVR dataset for visual reasoning, achieving a new state-of-the-art 98.9% accuracy, halving the error rate of the previous best model. More importantly, we show that the model is computationally-efficient and data-efficient, in particular requiring 5x less data than existing models to achieve strong results."
            },
            "slug": "Compositional-Attention-Networks-for-Machine-Hudson-Manning",
            "title": {
                "fragments": [],
                "text": "Compositional Attention Networks for Machine Reasoning"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "The MAC network is presented, a novel fully differentiable neural network architecture, designed to facilitate explicit and expressive reasoning that is computationally-efficient and data-efficient, in particular requiring 5x less data than existing models to achieve strong results."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39051054"
                        ],
                        "name": "I. Higgins",
                        "slug": "I.-Higgins",
                        "structuredName": {
                            "firstName": "Irina",
                            "lastName": "Higgins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Higgins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2873921"
                        ],
                        "name": "Nicolas Sonnerat",
                        "slug": "Nicolas-Sonnerat",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Sonnerat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Sonnerat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2367480"
                        ],
                        "name": "L. Matthey",
                        "slug": "L.-Matthey",
                        "structuredName": {
                            "firstName": "Lo\u00efc",
                            "lastName": "Matthey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Matthey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422676"
                        ],
                        "name": "Arka Pal",
                        "slug": "Arka-Pal",
                        "structuredName": {
                            "firstName": "Arka",
                            "lastName": "Pal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Arka Pal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145463968"
                        ],
                        "name": "Christopher P. Burgess",
                        "slug": "Christopher-P.-Burgess",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burgess",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher P. Burgess"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46378362"
                        ],
                        "name": "M. Botvinick",
                        "slug": "M.-Botvinick",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Botvinick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Botvinick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48987704"
                        ],
                        "name": "D. Hassabis",
                        "slug": "D.-Hassabis",
                        "structuredName": {
                            "firstName": "Demis",
                            "lastName": "Hassabis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hassabis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2289726"
                        ],
                        "name": "Alexander Lerchner",
                        "slug": "Alexander-Lerchner",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Lerchner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Lerchner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 195346124,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8a79a876d65cdaf10244e0def2f5a36f34aecdb7",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "The natural world is infinitely diverse, yet this diversity arises from a relatively small set of coherent properties and rules, such as the laws of physics or chemistry. We conjecture that biological intelligent systems are able to survive within their diverse environments by discovering the regularities that arise from these rules primarily through unsupervised experiences, and representing this knowledge as abstract concepts. Such representations possess useful properties of compositionality and hierarchical organisation, which allow intelligent agents to recombine a finite set of conceptual building blocks into an exponentially large set of useful new concepts. This paper describes SCAN (Symbol-Concept Association Network), a new framework for learning such concepts in the visual domain. We first use the previously published beta-VAE (Higgins et al., 2017a) architecture to learn a disentangled representation of the latent structure of the visual world, before training SCAN to extract abstract concepts grounded in such disentangled visual primitives through fast symbol association. Our approach requires very few pairings between symbols and images and makes no assumptions about the choice of symbol representations. Once trained, SCAN is capable of multimodal bi-directional inference, generating a diverse set of image samples from symbolic descriptions and vice versa. It also allows for traversal and manipulation of the implicit hierarchy of compositional visual concepts through symbolic instructions and learnt logical recombination operations. Such manipulations enable SCAN to invent and learn novel visual concepts through recombination of the few learnt concepts."
            },
            "slug": "SCAN:-Learning-Abstract-Hierarchical-Compositional-Higgins-Sonnerat",
            "title": {
                "fragments": [],
                "text": "SCAN: Learning Abstract Hierarchical Compositional Visual Concepts"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "SCAN (Symbol-Concept Association Network), a new framework for learning concepts in the visual domain capable of multimodal bi-directional inference and traversal and manipulation of the implicit hierarchy of compositional visual concepts through symbolic instructions and learnt logical recombination operations, is described."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2019153"
                        ],
                        "name": "P. Battaglia",
                        "slug": "P.-Battaglia",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Battaglia",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Battaglia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158860"
                        ],
                        "name": "Jessica B. Hamrick",
                        "slug": "Jessica-B.-Hamrick",
                        "structuredName": {
                            "firstName": "Jessica",
                            "lastName": "Hamrick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jessica B. Hamrick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2603033"
                        ],
                        "name": "V. Bapst",
                        "slug": "V.-Bapst",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Bapst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Bapst"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398105826"
                        ],
                        "name": "Alvaro Sanchez-Gonzalez",
                        "slug": "Alvaro-Sanchez-Gonzalez",
                        "structuredName": {
                            "firstName": "Alvaro",
                            "lastName": "Sanchez-Gonzalez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alvaro Sanchez-Gonzalez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3133079"
                        ],
                        "name": "V. Zambaldi",
                        "slug": "V.-Zambaldi",
                        "structuredName": {
                            "firstName": "Vin\u00edcius",
                            "lastName": "Zambaldi",
                            "middleNames": [
                                "Flores"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Zambaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145478807"
                        ],
                        "name": "Mateusz Malinowski",
                        "slug": "Mateusz-Malinowski",
                        "structuredName": {
                            "firstName": "Mateusz",
                            "lastName": "Malinowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mateusz Malinowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2844530"
                        ],
                        "name": "A. Tacchetti",
                        "slug": "A.-Tacchetti",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Tacchetti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tacchetti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143724694"
                        ],
                        "name": "David Raposo",
                        "slug": "David-Raposo",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Raposo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Raposo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35030998"
                        ],
                        "name": "Adam Santoro",
                        "slug": "Adam-Santoro",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Santoro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Santoro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48627702"
                        ],
                        "name": "R. Faulkner",
                        "slug": "R.-Faulkner",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Faulkner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Faulkner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1854385"
                        ],
                        "name": "\u00c7aglar G\u00fcl\u00e7ehre",
                        "slug": "\u00c7aglar-G\u00fcl\u00e7ehre",
                        "structuredName": {
                            "firstName": "\u00c7aglar",
                            "lastName": "G\u00fcl\u00e7ehre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00c7aglar G\u00fcl\u00e7ehre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107148568"
                        ],
                        "name": "H. F. Song",
                        "slug": "H.-F.-Song",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Song",
                            "middleNames": [
                                "Francis"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. F. Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5055381"
                        ],
                        "name": "A. J. Ballard",
                        "slug": "A.-J.-Ballard",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Ballard",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Ballard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058362"
                        ],
                        "name": "J. Gilmer",
                        "slug": "J.-Gilmer",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Gilmer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gilmer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35188630"
                        ],
                        "name": "George E. Dahl",
                        "slug": "George-E.-Dahl",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Dahl",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George E. Dahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40348417"
                        ],
                        "name": "Ashish Vaswani",
                        "slug": "Ashish-Vaswani",
                        "structuredName": {
                            "firstName": "Ashish",
                            "lastName": "Vaswani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashish Vaswani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145254624"
                        ],
                        "name": "Kelsey R. Allen",
                        "slug": "Kelsey-R.-Allen",
                        "structuredName": {
                            "firstName": "Kelsey",
                            "lastName": "Allen",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kelsey R. Allen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36942233"
                        ],
                        "name": "Charlie Nash",
                        "slug": "Charlie-Nash",
                        "structuredName": {
                            "firstName": "Charlie",
                            "lastName": "Nash",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charlie Nash"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066201331"
                        ],
                        "name": "Victoria Langston",
                        "slug": "Victoria-Langston",
                        "structuredName": {
                            "firstName": "Victoria",
                            "lastName": "Langston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victoria Langston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745899"
                        ],
                        "name": "Chris Dyer",
                        "slug": "Chris-Dyer",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Dyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Dyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2801204"
                        ],
                        "name": "N. Heess",
                        "slug": "N.-Heess",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Heess",
                            "middleNames": [
                                "Manfred",
                                "Otto"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Heess"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688276"
                        ],
                        "name": "Daan Wierstra",
                        "slug": "Daan-Wierstra",
                        "structuredName": {
                            "firstName": "Daan",
                            "lastName": "Wierstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daan Wierstra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143967473"
                        ],
                        "name": "Pushmeet Kohli",
                        "slug": "Pushmeet-Kohli",
                        "structuredName": {
                            "firstName": "Pushmeet",
                            "lastName": "Kohli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pushmeet Kohli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46378362"
                        ],
                        "name": "M. Botvinick",
                        "slug": "M.-Botvinick",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Botvinick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Botvinick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47002813"
                        ],
                        "name": "Yujia Li",
                        "slug": "Yujia-Li",
                        "structuredName": {
                            "firstName": "Yujia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yujia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1996134"
                        ],
                        "name": "Razvan Pascanu",
                        "slug": "Razvan-Pascanu",
                        "structuredName": {
                            "firstName": "Razvan",
                            "lastName": "Pascanu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Razvan Pascanu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 130
                            }
                        ],
                        "text": "several prior works have argued for the great potential of abstractions and compositionality in enhancing models of deep learning [8, 10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 105
                            }
                        ],
                        "text": "Others have argued for the importance of incorporating strong inductive biases into neural architectures [14, 10, 6, 8], and indeed, there is a growing body of research that seeks to introduce different forms of structural priors inspired by computer architectures [29, 80, 40] or theory of computation [4, 23], aiming to bridge the gap between the symbolic and neural paradigms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 46935302,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "19b7769dab4e6092aa4b7eeb8aa078a7b725c9b4",
            "isKey": false,
            "numCitedBy": 1548,
            "numCiting": 196,
            "paperAbstract": {
                "fragments": [],
                "text": "Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. \nThe following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between \"hand-engineering\" and \"end-to-end\" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice."
            },
            "slug": "Relational-inductive-biases,-deep-learning,-and-Battaglia-Hamrick",
            "title": {
                "fragments": [],
                "text": "Relational inductive biases, deep learning, and graph networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is argued that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35030998"
                        ],
                        "name": "Adam Santoro",
                        "slug": "Adam-Santoro",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Santoro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Santoro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145783676"
                        ],
                        "name": "Felix Hill",
                        "slug": "Felix-Hill",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Hill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Felix Hill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50181861"
                        ],
                        "name": "D. Barrett",
                        "slug": "D.-Barrett",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Barrett",
                            "middleNames": [
                                "G.",
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Barrett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4690624"
                        ],
                        "name": "Ari S. Morcos",
                        "slug": "Ari-S.-Morcos",
                        "structuredName": {
                            "firstName": "Ari",
                            "lastName": "Morcos",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ari S. Morcos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2542999"
                        ],
                        "name": "T. Lillicrap",
                        "slug": "T.-Lillicrap",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Lillicrap",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Lillicrap"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 49665167,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "acc43abe319bca7652a91f7d4ca6187049fb82e4",
            "isKey": false,
            "numCitedBy": 205,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Whether neural networks can learn abstract reasoning or whether they merely rely on superficial statistics is a topic of recent debate. Here, we propose a dataset and challenge designed to probe abstract reasoning, inspired by a well-known human IQ test. To succeed at this challenge, models must cope with various generalisation `regimes' in which the training and test data differ in clearly-defined ways. We show that popular models such as ResNets perform poorly, even when the training and test sets differ only minimally, and we present a novel architecture, with a structure designed to encourage reasoning, that does significantly better. When we vary the way in which the test questions and training data differ, we find that our model is notably proficient at certain forms of generalisation, but notably weak at others. We further show that the model's ability to generalise improves markedly if it is trained to predict symbolic explanations for its answers. Altogether, we introduce and explore ways to both measure and induce stronger abstract reasoning in neural networks. Our freely-available dataset should motivate further progress in this direction."
            },
            "slug": "Measuring-abstract-reasoning-in-neural-networks-Santoro-Hill",
            "title": {
                "fragments": [],
                "text": "Measuring abstract reasoning in neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A dataset and challenge designed to probe abstract reasoning, inspired by a well-known human IQ test, is proposed and ways to both measure and induce stronger abstract reasoning in neural networks are introduced."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40879119"
                        ],
                        "name": "Kexin Yi",
                        "slug": "Kexin-Yi",
                        "structuredName": {
                            "firstName": "Kexin",
                            "lastName": "Yi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kexin Yi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3045089"
                        ],
                        "name": "Jiajun Wu",
                        "slug": "Jiajun-Wu",
                        "structuredName": {
                            "firstName": "Jiajun",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiajun Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144158271"
                        ],
                        "name": "Chuang Gan",
                        "slug": "Chuang-Gan",
                        "structuredName": {
                            "firstName": "Chuang",
                            "lastName": "Gan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chuang Gan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143967473"
                        ],
                        "name": "Pushmeet Kohli",
                        "slug": "Pushmeet-Kohli",
                        "structuredName": {
                            "firstName": "Pushmeet",
                            "lastName": "Kohli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pushmeet Kohli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 52919654,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d15ebe3f5aaf32a9f835f88703241461324c35b",
            "isKey": false,
            "numCitedBy": 301,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We marry two powerful ideas: deep representation learning for visual recognition and language understanding, and symbolic program execution for reasoning. Our neural-symbolic visual question answering (NS-VQA) system first recovers a structural scene representation from the image and a program trace from the question. It then executes the program on the scene representation to obtain an answer. Incorporating symbolic structure as prior knowledge offers three unique advantages. First, executing programs on a symbolic space is more robust to long program traces; our model can solve complex reasoning tasks better, achieving an accuracy of 99.8% on the CLEVR dataset. Second, the model is more data- and memory-efficient: it performs well after learning on a small number of training data; it can also encode an image into a compact representation, requiring less storage than existing methods for offline question answering. Third, symbolic program execution offers full transparency to the reasoning process; we are thus able to interpret and diagnose each execution step."
            },
            "slug": "Neural-Symbolic-VQA:-Disentangling-Reasoning-from-Yi-Wu",
            "title": {
                "fragments": [],
                "text": "Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work proposes a neural-symbolic visual question answering system that first recovers a structural scene representation from the image and a program trace from the question, then executes the program on the scene representation to obtain an answer."
            },
            "venue": {
                "fragments": [],
                "text": "NeurIPS"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39051054"
                        ],
                        "name": "I. Higgins",
                        "slug": "I.-Higgins",
                        "structuredName": {
                            "firstName": "Irina",
                            "lastName": "Higgins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Higgins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2873921"
                        ],
                        "name": "Nicolas Sonnerat",
                        "slug": "Nicolas-Sonnerat",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Sonnerat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Sonnerat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2367480"
                        ],
                        "name": "L. Matthey",
                        "slug": "L.-Matthey",
                        "structuredName": {
                            "firstName": "Lo\u00efc",
                            "lastName": "Matthey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Matthey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422676"
                        ],
                        "name": "Arka Pal",
                        "slug": "Arka-Pal",
                        "structuredName": {
                            "firstName": "Arka",
                            "lastName": "Pal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Arka Pal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145463968"
                        ],
                        "name": "Christopher P. Burgess",
                        "slug": "Christopher-P.-Burgess",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burgess",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher P. Burgess"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2709802"
                        ],
                        "name": "Matko Bosnjak",
                        "slug": "Matko-Bosnjak",
                        "structuredName": {
                            "firstName": "Matko",
                            "lastName": "Bosnjak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matko Bosnjak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757629"
                        ],
                        "name": "M. Shanahan",
                        "slug": "M.-Shanahan",
                        "structuredName": {
                            "firstName": "Murray",
                            "lastName": "Shanahan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shanahan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46378362"
                        ],
                        "name": "M. Botvinick",
                        "slug": "M.-Botvinick",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Botvinick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Botvinick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48987704"
                        ],
                        "name": "D. Hassabis",
                        "slug": "D.-Hassabis",
                        "structuredName": {
                            "firstName": "Demis",
                            "lastName": "Hassabis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hassabis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2289726"
                        ],
                        "name": "Alexander Lerchner",
                        "slug": "Alexander-Lerchner",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Lerchner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Lerchner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 119
                            }
                        ],
                        "text": "Our model connects to multiple lines of research, including works about compositionality [14, 38], concept acquisition [36, 81], and neural computation [28, 62, 7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7121887,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b22b4817757778bdca5b792277128a7db8206d08",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "The seemingly infinite diversity of the natural world arises from a relatively small set of coherent rules, such as the laws of physics or chemistry. We conjecture that these rules give rise to regularities that can be discovered through primarily unsupervised experiences and represented as abstract concepts. If such representations are compositional and hierarchical, they can be recombined into an exponentially large set of new concepts. This paper describes SCAN (Symbol-Concept Association Network), a new framework for learning such abstractions in the visual domain. SCAN learns concepts through fast symbol association, grounding them in disentangled visual primitives that are discovered in an unsupervised manner. Unlike state of the art multimodal generative model baselines, our approach requires very few pairings between symbols and images and makes no assumptions about the form of symbol representations. Once trained, SCAN is capable of multimodal bi-directional inference, generating a diverse set of image samples from symbolic descriptions and vice versa. It also allows for traversal and manipulation of the implicit hierarchy of visual concepts through symbolic instructions and learnt logical recombination operations. Such manipulations enable SCAN to break away from its training data distribution and imagine novel visual concepts through symbolically instructed recombination of previously learnt concepts."
            },
            "slug": "SCAN:-Learning-Hierarchical-Compositional-Visual-Higgins-Sonnerat",
            "title": {
                "fragments": [],
                "text": "SCAN: Learning Hierarchical Compositional Visual Concepts"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "SCAN (Symbol-Concept Association Network), a new framework for learning such abstractions in the visual domain that allows for traversal and manipulation of the implicit hierarchy of visual concepts through symbolic instructions and learnt logical recombination operations, is described."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112400"
                        ],
                        "name": "Jacob Andreas",
                        "slug": "Jacob-Andreas",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Andreas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacob Andreas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38666915"
                        ],
                        "name": "D. Klein",
                        "slug": "D.-Klein",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Klein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736651"
                        ],
                        "name": "S. Levine",
                        "slug": "S.-Levine",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Levine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Levine"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "r view, these results point to the strongest quality of our approach. several prior works have argued for the great potential of abstractions and compositionality in enhancing models of deep learning [8, 10]. Our results suggest that incorporating these notions may indeed be highly bene\ufb01cial to creating models that are more capable in coping with changing conditions and can better generalize to novel sit"
                    },
                    "intents": []
                }
            ],
            "corpusId": 37390552,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e854977a9c4c2effc42f2e24064726fb6307b6f5",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "The named concepts and compositional operators present in natural language provide a rich source of information about the abstractions humans use to navigate the world. Can this linguistic background knowledge improve the generality and efficiency of learned classifiers and control policies? This paper aims to show that using the space of natural language strings as a parameter space is an effective way to capture natural task structure. In a pretraining phase, we learn a language interpretation model that transforms inputs (e.g. images) into outputs (e.g. labels) given natural language descriptions. To learn a new concept (e.g. a classifier), we search directly in the space of descriptions to minimize the interpreter\u2019s loss on training examples. Crucially, our models do not require language data to learn these concepts: language is used only in pretraining to impose structure on subsequent learning. Results on image classification, text editing, and reinforcement learning show that, in all settings, models with a linguistic parameterization outperform those without."
            },
            "slug": "Learning-with-Latent-Language-Andreas-Klein",
            "title": {
                "fragments": [],
                "text": "Learning with Latent Language"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper aims to show that using the space of natural language strings as a parameter space is an effective way to capture natural task structure and shows that, in all settings, models with a linguistic parameterization outperform those without."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7535126"
                        ],
                        "name": "R\u00e9mi Cad\u00e8ne",
                        "slug": "R\u00e9mi-Cad\u00e8ne",
                        "structuredName": {
                            "firstName": "R\u00e9mi",
                            "lastName": "Cad\u00e8ne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R\u00e9mi Cad\u00e8ne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1405301761"
                        ],
                        "name": "H. Ben-younes",
                        "slug": "H.-Ben-younes",
                        "structuredName": {
                            "firstName": "Hedi",
                            "lastName": "Ben-younes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ben-younes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51021910"
                        ],
                        "name": "M. Cord",
                        "slug": "M.-Cord",
                        "structuredName": {
                            "firstName": "Matthieu",
                            "lastName": "Cord",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Cord"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728523"
                        ],
                        "name": "Nicolas Thome",
                        "slug": "Nicolas-Thome",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Thome",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Thome"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 130
                            }
                        ],
                        "text": "We move further in this direction, performing iterative reasoning over the inferred scene graphs, and in contrast to prior models [50, 15], incorporate higher-level semantic concepts to represent both the visual and linguistic modalities in a shared and sparser manner to facilitate their interaction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 67856593,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a9f1a1321958df7dfb2efce3e9d1e99b9f5ccb3",
            "isKey": false,
            "numCitedBy": 168,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Multimodal attentional networks are currently state-of-the-art models for Visual Question Answering (VQA) tasks involving real images. Although attention allows to focus on the visual content relevant to the question, this simple mechanism is arguably insufficient to model complex reasoning features required for VQA or other high-level tasks. In this paper, we propose MuRel, a multimodal relational network which is learned end-to-end to reason over real images. Our first contribution is the introduction of the MuRel cell, an atomic reasoning primitive representing interactions between question and image regions by a rich vectorial representation, and modeling region relations with pairwise combinations. Secondly, we incorporate the cell into a full MuRel network, which progressively refines visual and question interactions, and can be leveraged to define visualization schemes finer than mere attention maps. We validate the relevance of our approach with various ablation studies, and show its superiority to attention-based methods on three datasets: VQA 2.0, VQA-CP v2 and TDIUC. Our final MuRel network is competitive to or outperforms state-of-the-art results in this challenging context. Our code is available: github.com/Cadene/murel.bootstrap.pytorch"
            },
            "slug": "MUREL:-Multimodal-Relational-Reasoning-for-Visual-Cad\u00e8ne-Ben-younes",
            "title": {
                "fragments": [],
                "text": "MUREL: Multimodal Relational Reasoning for Visual Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper proposes MuRel, a multimodal relational network which is learned end-to-end to reason over real images, and introduces the introduction of the MuRel cell, an atomic reasoning primitive representing interactions between question and image regions by a rich vectorial representation, and modeling region relations with pairwise combinations."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3242613"
                        ],
                        "name": "E. Choi",
                        "slug": "E.-Choi",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2672644"
                        ],
                        "name": "Angeliki Lazaridou",
                        "slug": "Angeliki-Lazaridou",
                        "structuredName": {
                            "firstName": "Angeliki",
                            "lastName": "Lazaridou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Angeliki Lazaridou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737568"
                        ],
                        "name": "N. D. Freitas",
                        "slug": "N.-D.-Freitas",
                        "structuredName": {
                            "firstName": "Nando",
                            "lastName": "Freitas",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. D. Freitas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 127
                            }
                        ],
                        "text": "Several works have explored the discovery and use of visual concepts in the contexts of reinforcement or unsupervised learning [35, 17] as well as in classical computer vision [21, 77]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3457087,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4814c10f84863e016d75e6af42e790f60759b9f4",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the distinguishing aspects of human language is its compositionality, which allows us to describe complex environments with limited vocabulary. Previously, it has been shown that neural network agents can learn to communicate in a highly structured, possibly compositional language based on disentangled input (e.g. hand- engineered features). Humans, however, do not learn to communicate based on well-summarized features. In this work, we train neural agents to simultaneously develop visual perception from raw image pixels, and learn to communicate with a sequence of discrete symbols. The agents play an image description game where the image contains factors such as colors and shapes. We train the agents using the obverter technique where an agent introspects to generate messages that maximize its own understanding. Through qualitative analysis, visualization and a zero-shot test, we show that the agents can develop, out of raw image pixels, a language with compositional properties, given a proper pressure from the environment."
            },
            "slug": "Compositional-Obverter-Communication-Learning-From-Choi-Lazaridou",
            "title": {
                "fragments": [],
                "text": "Compositional Obverter Communication Learning From Raw Visual Input"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work trains neural agents to simultaneously develop visual perception from raw image pixels, and learn to communicate with a sequence of discrete symbols, and shows that the agents can develop a language with compositional properties given a proper pressure from the environment."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2373318"
                        ],
                        "name": "B. Lake",
                        "slug": "B.-Lake",
                        "structuredName": {
                            "firstName": "Brenden",
                            "lastName": "Lake",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Lake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37774552"
                        ],
                        "name": "T. Ullman",
                        "slug": "T.-Ullman",
                        "structuredName": {
                            "firstName": "Tomer",
                            "lastName": "Ullman",
                            "middleNames": [
                                "David"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ullman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1831199"
                        ],
                        "name": "S. Gershman",
                        "slug": "S.-Gershman",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Gershman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gershman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 399,
                                "start": 391
                            }
                        ],
                        "text": "Yet, even though neural networks are undoubtedly powerful, flexible and robust, recent work has repeatedly demonstrated their flaws, showing how they struggle to generalize in a systematic manner [50], overly adhere to superficial and potentially misleading statistical associations instead of learning true causal relations [1, 42], strongly depend on large amounts of data and supervision [25, 51], and sometimes behave in surprising and"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 243
                            }
                        ],
                        "text": "Such settings reduce the extent to which models can circumvent the need for genuine scene understanding skills by exploiting dataset biases and superficial statistics [1, 44, 27], and are known to be particularly difficult for neural networks [51]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 365,
                                "start": 361
                            }
                        ],
                        "text": "According to Jerry Fodor\u2019s Language of Thought hypothesis [22, 72], thinking itself posses a language-like compositional structure, where elementary concepts combine in systematic ways to create compound new ideas or thoughts, allowing us to make \u201cinfinite use of finite means\u201d [18] and fostering human\u2019s remarkable capacities of abstraction and generalization [51]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 196200552,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7260c0692f8d265e11c4e9c4c8ef4c185bd587ad",
            "isKey": false,
            "numCitedBy": 1573,
            "numCiting": 524,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Recent progress in artificial intelligence has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats that of humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn and how they learn it. Specifically, we argue that these machines should (1) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (2) ground learning in intuitive theories of physics and psychology to support and enrich the knowledge that is learned; and (3) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes toward these goals that can combine the strengths of recent neural network advances with more structured cognitive models."
            },
            "slug": "Building-machines-that-learn-and-think-like-people-Lake-Ullman",
            "title": {
                "fragments": [],
                "text": "Building machines that learn and think like people"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is argued that truly human-like learning and thinking machines should build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems, and harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations."
            },
            "venue": {
                "fragments": [],
                "text": "Behavioral and Brain Sciences"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13589371"
                        ],
                        "name": "Jiayuan Mao",
                        "slug": "Jiayuan-Mao",
                        "structuredName": {
                            "firstName": "Jiayuan",
                            "lastName": "Mao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiayuan Mao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144158271"
                        ],
                        "name": "Chuang Gan",
                        "slug": "Chuang-Gan",
                        "structuredName": {
                            "firstName": "Chuang",
                            "lastName": "Gan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chuang Gan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143967473"
                        ],
                        "name": "Pushmeet Kohli",
                        "slug": "Pushmeet-Kohli",
                        "structuredName": {
                            "firstName": "Pushmeet",
                            "lastName": "Kohli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pushmeet Kohli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3045089"
                        ],
                        "name": "Jiajun Wu",
                        "slug": "Jiajun-Wu",
                        "structuredName": {
                            "firstName": "Jiajun",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiajun Wu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 108296442,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ec9b27d019fefadb5e97c8174ac889e831f483d7",
            "isKey": false,
            "numCitedBy": 337,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval."
            },
            "slug": "The-Neuro-Symbolic-Concept-Learner:-Interpreting-Mao-Gan",
            "title": {
                "fragments": [],
                "text": "The Neuro-Symbolic Concept Learner: Interpreting Scenes Words and Sentences from Natural Supervision"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "The Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, the model learns by simply looking at images and reading paired questions and answers."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112400"
                        ],
                        "name": "Jacob Andreas",
                        "slug": "Jacob-Andreas",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Andreas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacob Andreas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34849128"
                        ],
                        "name": "Marcus Rohrbach",
                        "slug": "Marcus-Rohrbach",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Rohrbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus Rohrbach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38666915"
                        ],
                        "name": "D. Klein",
                        "slug": "D.-Klein",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Klein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 152
                            }
                        ],
                        "text": "Our model connects to multiple lines of research, including works about compositionality [14, 38], concept acquisition [36, 81], and neural computation [28, 62, 7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5276660,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "21c99706bb26e9012bfb4d8d48009a3d45af59b2",
            "isKey": false,
            "numCitedBy": 733,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual question answering is fundamentally compositional in nature-a question like where is the dog? shares substructure with questions like what color is the dog? and where is the cat? This paper seeks to simultaneously exploit the representational capacity of deep networks and the compositional linguistic structure of questions. We describe a procedure for constructing and learning neural module networks, which compose collections of jointly-trained neural \"modules\" into deep networks for question answering. Our approach decomposes questions into their linguistic substructures, and uses these structures to dynamically instantiate modular networks (with reusable components for recognizing dogs, classifying colors, etc.). The resulting compound networks are jointly trained. We evaluate our approach on two challenging datasets for visual question answering, achieving state-of-the-art results on both the VQA natural image dataset and a new dataset of complex questions about abstract shapes."
            },
            "slug": "Neural-Module-Networks-Andreas-Rohrbach",
            "title": {
                "fragments": [],
                "text": "Neural Module Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A procedure for constructing and learning neural module networks, which compose collections of jointly-trained neural \"modules\" into deep networks for question answering, and uses these structures to dynamically instantiate modular networks (with reusable components for recognizing dogs, classifying colors, etc.)."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3296335"
                        ],
                        "name": "D. Mascharka",
                        "slug": "D.-Mascharka",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mascharka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mascharka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46450184"
                        ],
                        "name": "Philip Tran",
                        "slug": "Philip-Tran",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Tran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip Tran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49902902"
                        ],
                        "name": "R. Soklaski",
                        "slug": "R.-Soklaski",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Soklaski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Soklaski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2905057"
                        ],
                        "name": "Arjun Majumdar",
                        "slug": "Arjun-Majumdar",
                        "structuredName": {
                            "firstName": "Arjun",
                            "lastName": "Majumdar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Arjun Majumdar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3863856,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd0a7c58964905ccfddbad1614165320ccc56393",
            "isKey": false,
            "numCitedBy": 152,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual question answering requires high-order reasoning about an image, which is a fundamental capability needed by machine systems to follow complex directives. Recently, modular networks have been shown to be an effective framework for performing visual reasoning tasks. While modular networks were initially designed with a degree of model transparency, their performance on complex visual reasoning benchmarks was lacking. Current state-of-the-art approaches do not provide an effective mechanism for understanding the reasoning process. In this paper, we close the performance gap between interpretable models and state-of-the-art visual reasoning methods. We propose a set of visual-reasoning primitives which, when composed, manifest as a model capable of performing complex reasoning tasks in an explicitly-interpretable manner. The fidelity and interpretability of the primitives' outputs enable an unparalleled ability to diagnose the strengths and weaknesses of the resulting model. Critically, we show that these primitives are highly performant, achieving state-of-the-art accuracy of 99.1% on the CLEVR dataset. We also show that our model is able to effectively learn generalized representations when provided a small amount of data containing novel object attributes. Using the CoGenT generalization task, we show more than a 20 percentage point improvement over the current state of the art."
            },
            "slug": "Transparency-by-Design:-Closing-the-Gap-Between-and-Mascharka-Tran",
            "title": {
                "fragments": [],
                "text": "Transparency by Design: Closing the Gap Between Performance and Interpretability in Visual Reasoning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes a set of visual-reasoning primitives which, when composed, manifest as a model capable of performing complex reasoning tasks in an explicitly-interpretable manner, and shows that these primitives are highly performant, achieving state-of-the-art accuracy on the CLEVR dataset."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3468254"
                        ],
                        "name": "M. Garnelo",
                        "slug": "M.-Garnelo",
                        "structuredName": {
                            "firstName": "Marta",
                            "lastName": "Garnelo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Garnelo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "68972911"
                        ],
                        "name": "Kai Arulkumaran",
                        "slug": "Kai-Arulkumaran",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Arulkumaran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Arulkumaran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757629"
                        ],
                        "name": "M. Shanahan",
                        "slug": "M.-Shanahan",
                        "structuredName": {
                            "firstName": "Murray",
                            "lastName": "Shanahan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shanahan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 399,
                                "start": 391
                            }
                        ],
                        "text": "Yet, even though neural networks are undoubtedly powerful, flexible and robust, recent work has repeatedly demonstrated their flaws, showing how they struggle to generalize in a systematic manner [50], overly adhere to superficial and potentially misleading statistical associations instead of learning true causal relations [1, 42], strongly depend on large amounts of data and supervision [25, 51], and sometimes behave in surprising and"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10335455,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "376f23cce537235122fdce5524d084e3a869c403",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep reinforcement learning (DRL) brings the power of deep neural networks to bear on the generic task of trial-and-error learning, and its effectiveness has been convincingly demonstrated on tasks such as Atari video games and the game of Go. However, contemporary DRL systems inherit a number of shortcomings from the current generation of deep learning techniques. For example, they require very large datasets to work effectively, entailing that they are slow to learn even when such datasets are available. Moreover, they lack the ability to reason on an abstract level, which makes it difficult to implement high-level cognitive functions such as transfer learning, analogical reasoning, and hypothesis-based reasoning. Finally, their operation is largely opaque to humans, rendering them unsuitable for domains in which verifiability is important. In this paper, we propose an end-to-end reinforcement learning architecture comprising a neural back end and a symbolic front end with the potential to overcome each of these shortcomings. As proof-of-concept, we present a preliminary implementation of the architecture and apply it to several variants of a simple video game. We show that the resulting system -- though just a prototype -- learns effectively, and, by acquiring a set of symbolic rules that are easily comprehensible to humans, dramatically outperforms a conventional, fully neural DRL system on a stochastic variant of the game."
            },
            "slug": "Towards-Deep-Symbolic-Reinforcement-Learning-Garnelo-Arulkumaran",
            "title": {
                "fragments": [],
                "text": "Towards Deep Symbolic Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown that the resulting system -- though just a prototype -- learns effectively, and, by acquiring a set of symbolic rules that are easily comprehensible to humans, dramatically outperforms a conventional, fully neural DRL system on a stochastic variant of the game."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "89504302"
                        ],
                        "name": "Greg Wayne",
                        "slug": "Greg-Wayne",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Wayne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Wayne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47447264"
                        ],
                        "name": "Malcolm Reynolds",
                        "slug": "Malcolm-Reynolds",
                        "structuredName": {
                            "firstName": "Malcolm",
                            "lastName": "Reynolds",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Malcolm Reynolds"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3367786"
                        ],
                        "name": "Tim Harley",
                        "slug": "Tim-Harley",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Harley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Harley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1841008"
                        ],
                        "name": "Ivo Danihelka",
                        "slug": "Ivo-Danihelka",
                        "structuredName": {
                            "firstName": "Ivo",
                            "lastName": "Danihelka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ivo Danihelka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398898827"
                        ],
                        "name": "Agnieszka Grabska-Barwinska",
                        "slug": "Agnieszka-Grabska-Barwinska",
                        "structuredName": {
                            "firstName": "Agnieszka",
                            "lastName": "Grabska-Barwinska",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Agnieszka Grabska-Barwinska"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2016840"
                        ],
                        "name": "Sergio Gomez Colmenarejo",
                        "slug": "Sergio-Gomez-Colmenarejo",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Colmenarejo",
                            "middleNames": [
                                "Gomez"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sergio Gomez Colmenarejo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1864353"
                        ],
                        "name": "Edward Grefenstette",
                        "slug": "Edward-Grefenstette",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Grefenstette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward Grefenstette"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34505275"
                        ],
                        "name": "Tiago Ramalho",
                        "slug": "Tiago-Ramalho",
                        "structuredName": {
                            "firstName": "Tiago",
                            "lastName": "Ramalho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tiago Ramalho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70495322"
                        ],
                        "name": "J. Agapiou",
                        "slug": "J.-Agapiou",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Agapiou",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Agapiou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36045539"
                        ],
                        "name": "Adri\u00e0 Puigdom\u00e8nech Badia",
                        "slug": "Adri\u00e0-Puigdom\u00e8nech-Badia",
                        "structuredName": {
                            "firstName": "Adri\u00e0",
                            "lastName": "Badia",
                            "middleNames": [
                                "Puigdom\u00e8nech"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adri\u00e0 Puigdom\u00e8nech Badia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2910877"
                        ],
                        "name": "K. Hermann",
                        "slug": "K.-Hermann",
                        "structuredName": {
                            "firstName": "Karl",
                            "lastName": "Hermann",
                            "middleNames": [
                                "Moritz"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3185820"
                        ],
                        "name": "Yori Zwols",
                        "slug": "Yori-Zwols",
                        "structuredName": {
                            "firstName": "Yori",
                            "lastName": "Zwols",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yori Zwols"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2273072"
                        ],
                        "name": "Georg Ostrovski",
                        "slug": "Georg-Ostrovski",
                        "structuredName": {
                            "firstName": "Georg",
                            "lastName": "Ostrovski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Georg Ostrovski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055913310"
                        ],
                        "name": "Adam Cain",
                        "slug": "Adam-Cain",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Cain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Cain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143776287"
                        ],
                        "name": "Helen King",
                        "slug": "Helen-King",
                        "structuredName": {
                            "firstName": "Helen",
                            "lastName": "King",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Helen King"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2372244"
                        ],
                        "name": "C. Summerfield",
                        "slug": "C.-Summerfield",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Summerfield",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Summerfield"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685771"
                        ],
                        "name": "P. Blunsom",
                        "slug": "P.-Blunsom",
                        "structuredName": {
                            "firstName": "Phil",
                            "lastName": "Blunsom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Blunsom"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48987704"
                        ],
                        "name": "D. Hassabis",
                        "slug": "D.-Hassabis",
                        "structuredName": {
                            "firstName": "Demis",
                            "lastName": "Hassabis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hassabis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 277,
                                "start": 265
                            }
                        ],
                        "text": "Others have argued for the importance of incorporating strong inductive biases into neural architectures [14, 10, 6, 8], and indeed, there is a growing body of research that seeks to introduce different forms of structural priors inspired by computer architectures [29, 80, 40] or theory of computation [4, 23], aiming to bridge the gap between the symbolic and neural paradigms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 205251479,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "784ee73d5363c711118f784428d1ab89f019daa5",
            "isKey": false,
            "numCitedBy": 1209,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Artificial neural networks are remarkably adept at sensory processing, sequence learning and reinforcement learning, but are limited in their ability to represent variables and data structures and to store data over long timescales, owing to the lack of an external memory. Here we introduce a machine learning model called a differentiable neural computer (DNC), which consists of a neural network that can read from and write to an external memory matrix, analogous to the random-access memory in a conventional computer. Like a conventional computer, it can use its memory to represent and manipulate complex data structures, but, like a neural network, it can learn to do so from data. When trained with supervised learning, we demonstrate that a DNC can successfully answer synthetic questions designed to emulate reasoning and inference problems in natural language. We show that it can learn tasks such as finding the shortest path between specified points and inferring the missing links in randomly generated graphs, and then generalize these tasks to specific graphs such as transport networks and family trees. When trained with reinforcement learning, a DNC can complete a moving blocks puzzle in which changing goals are specified by sequences of symbols. Taken together, our results demonstrate that DNCs have the capacity to solve complex, structured tasks that are inaccessible to neural networks without external read\u2013write memory."
            },
            "slug": "Hybrid-computing-using-a-neural-network-with-memory-Graves-Wayne",
            "title": {
                "fragments": [],
                "text": "Hybrid computing using a neural network with dynamic external memory"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A machine learning model called a differentiable neural computer (DNC), which consists of a neural network that can read from and write to an external memory matrix, analogous to the random-access memory in a conventional computer."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39051054"
                        ],
                        "name": "I. Higgins",
                        "slug": "I.-Higgins",
                        "structuredName": {
                            "firstName": "Irina",
                            "lastName": "Higgins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Higgins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2367480"
                        ],
                        "name": "L. Matthey",
                        "slug": "L.-Matthey",
                        "structuredName": {
                            "firstName": "Lo\u00efc",
                            "lastName": "Matthey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Matthey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3119801"
                        ],
                        "name": "Xavier Glorot",
                        "slug": "Xavier-Glorot",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Glorot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xavier Glorot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422676"
                        ],
                        "name": "Arka Pal",
                        "slug": "Arka-Pal",
                        "structuredName": {
                            "firstName": "Arka",
                            "lastName": "Pal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Arka Pal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2825051"
                        ],
                        "name": "B. Uria",
                        "slug": "B.-Uria",
                        "structuredName": {
                            "firstName": "Benigno",
                            "lastName": "Uria",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Uria"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723876"
                        ],
                        "name": "C. Blundell",
                        "slug": "C.-Blundell",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Blundell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Blundell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14594344"
                        ],
                        "name": "S. Mohamed",
                        "slug": "S.-Mohamed",
                        "structuredName": {
                            "firstName": "Shakir",
                            "lastName": "Mohamed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mohamed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2289726"
                        ],
                        "name": "Alexander Lerchner",
                        "slug": "Alexander-Lerchner",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Lerchner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Lerchner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 127
                            }
                        ],
                        "text": "Several works have explored the discovery and use of visual concepts in the contexts of reinforcement or unsupervised learning [35, 17] as well as in classical computer vision [21, 77]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5878150,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6af4b651b4835780fe4aa0dda94315c7127921fb",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Automated discovery of early visual concepts from raw image data is a major open challenge in AI research. Addressing this problem, we propose an unsupervised approach for learning disentangled representations of the underlying factors of variation. We draw inspiration from neuroscience, and show how this can be achieved in an unsupervised generative model by applying the same learning pressures as have been suggested to act in the ventral visual stream in the brain. By enforcing redundancy reduction, encouraging statistical independence, and exposure to data with transform continuities analogous to those to which human infants are exposed, we obtain a variational autoencoder (VAE) framework capable of learning disentangled factors. Our approach makes few assumptions and works well across a wide variety of datasets. Furthermore, our solution has useful emergent properties, such as zero-shot inference and an intuitive understanding of \"objectness\"."
            },
            "slug": "Early-Visual-Concept-Learning-with-Unsupervised-Higgins-Matthey",
            "title": {
                "fragments": [],
                "text": "Early Visual Concept Learning with Unsupervised Deep Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An unsupervised approach for learning disentangled representations of the underlying factors of variation by applying the same learning pressures as have been suggested to act in the ventral visual stream in the brain is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 89
                            }
                        ],
                        "text": "Our model connects to multiple lines of research, including works about compositionality [14, 38], concept acquisition [36, 81], and neural computation [28, 62, 7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 105
                            }
                        ],
                        "text": "Others have argued for the importance of incorporating strong inductive biases into neural architectures [14, 10, 6, 8], and indeed, there is a growing body of research that seeks to introduce different forms of structural priors inspired by computer architectures [29, 80, 40] or theory of computation [4, 23], aiming to bridge the gap between the symbolic and neural paradigms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1067591,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cb7853c5d609081ea12cd4db3863a87da2d51808",
            "isKey": false,
            "numCitedBy": 209,
            "numCiting": 80,
            "paperAbstract": {
                "fragments": [],
                "text": "A\u00a0plausible definition of \u201creasoning\u201d could be \u201calgebraically manipulating previously acquired knowledge in order to answer a new question\u201d. This definition covers first-order logical inference or probabilistic inference. It also includes much simpler manipulations commonly used to build large learning systems. For instance, we can build an optical character recognition system by first training a character segmenter, an isolated character recognizer, and a language model, using appropriate labelled training sets. Adequately concatenating these modules and fine tuning the resulting system can be viewed as an algebraic operation in a space of models. The resulting model answers a new question, that is, converting the image of a text page into a computer readable text.This observation suggests a conceptual continuity between algebraically rich inference systems, such as logical or probabilistic inference, and simple manipulations, such as the mere concatenation of trainable learning systems. Therefore, instead of trying to bridge the gap between machine learning systems and sophisticated \u201call-purpose\u201d inference mechanisms, we can instead algebraically enrich the set of manipulations applicable to training systems, and build reasoning capabilities from the ground up."
            },
            "slug": "From-machine-learning-to-machine-reasoning-Bottou",
            "title": {
                "fragments": [],
                "text": "From machine learning to machine reasoning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Instead of trying to bridge the gap between machine learning systems and sophisticated \u201call-purpose\u201d inference mechanisms, the set of manipulations applicable to training systems can be algebraically enriched, and reasoning capabilities from the ground up are built."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112400"
                        ],
                        "name": "Jacob Andreas",
                        "slug": "Jacob-Andreas",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Andreas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacob Andreas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 105
                            }
                        ],
                        "text": "Others have argued for the importance of incorporating strong inductive biases into neural architectures [14, 10, 6, 8], and indeed, there is a growing body of research that seeks to introduce different forms of structural priors inspired by computer architectures [29, 80, 40] or theory of computation [4, 23], aiming to bridge the gap between the symbolic and neural paradigms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 67749672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba4cf6046d420af0ae86e2f4b587a8d50d219be3",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Many machine learning algorithms represent input data with vector embeddings or discrete codes. When inputs exhibit compositional structure (e.g. objects built from parts or procedures from subroutines), it is natural to ask whether this compositional structure is reflected in the the inputs' learned representations. While the assessment of compositionality in languages has received significant attention in linguistics and adjacent fields, the machine learning literature lacks general-purpose tools for producing graded measurements of compositional structure in more general (e.g. vector-valued) representation spaces. We describe a procedure for evaluating compositionality by measuring how well the true representation-producing model can be approximated by a model that explicitly composes a collection of inferred representational primitives. We use the procedure to provide formal and empirical characterizations of compositional structure in a variety of settings, exploring the relationship between compositionality and learning dynamics, human judgments, representational similarity, and generalization."
            },
            "slug": "Measuring-Compositionality-in-Representation-Andreas",
            "title": {
                "fragments": [],
                "text": "Measuring Compositionality in Representation Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work describes a procedure for evaluating compositionality by measuring how well the true representation-producing model can be approximated by a model that explicitly composes a collection of inferred representational primitives."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2406263"
                        ],
                        "name": "Damien Teney",
                        "slug": "Damien-Teney",
                        "structuredName": {
                            "firstName": "Damien",
                            "lastName": "Teney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Damien Teney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2161037"
                        ],
                        "name": "Lingqiao Liu",
                        "slug": "Lingqiao-Liu",
                        "structuredName": {
                            "firstName": "Lingqiao",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lingqiao Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5546141"
                        ],
                        "name": "A. V. Hengel",
                        "slug": "A.-V.-Hengel",
                        "structuredName": {
                            "firstName": "Anton",
                            "lastName": "Hengel",
                            "middleNames": [
                                "van",
                                "den"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. V. Hengel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 262
                            }
                        ],
                        "text": "Prior work commonly relied on dense visual features produced by either CNNs [83, 86] or object detectors [5], with a few recent models that use the relationships among objects to augment those features with contextual information from each object\u2019s surroundings [52, 75, 66]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206595534,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c7d007ba376faddf0046930ea7375ed59600cee9",
            "isKey": false,
            "numCitedBy": 259,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes to improve visual question answering (VQA) with structured representations of both scene contents and questions. A key challenge in VQA is to require joint reasoning over the visual and text domains. The predominant CNN/LSTM-based approach to VQA is limited by monolithic vector representations that largely ignore structure in the scene and in the question. CNN feature vectors cannot effectively capture situations as simple as multiple object instances, and LSTMs process questions as series of words, which do not reflect the true complexity of language structure. We instead propose to build graphs over the scene objects and over the question words, and we describe a deep neural network that exploits the structure in these representations. We show that this approach achieves significant improvements over the state-of-the-art, increasing accuracy from 71.2% to 74.4% in accuracy on the abstract scenes multiple-choice benchmark, and from 34.7% to 39.1% in accuracy over pairs of balanced scenes, i.e. images with fine-grained differences and opposite yes/no answers to a same question."
            },
            "slug": "Graph-Structured-Representations-for-Visual-Teney-Liu",
            "title": {
                "fragments": [],
                "text": "Graph-Structured Representations for Visual Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This paper proposes to build graphs over the scene objects and over the question words, and describes a deep neural network that exploits the structure in these representations, and achieves significant improvements over the state-of-the-art."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152951058"
                        ],
                        "name": "Drew A. Hudson",
                        "slug": "Drew-A.-Hudson",
                        "structuredName": {
                            "firstName": "Drew",
                            "lastName": "Hudson",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Drew A. Hudson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "We compare our performance both with baselines, as appear in [41], as well as with the top-5 single and top-10 ensemble submissions to the GQA challenge."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 126
                            }
                        ],
                        "text": "As table 1 shows, we achieve state-of-the-art performance for a single-model across the dataset\u2019s various metrics (defined in [41]) such as accuracy and consistency."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "We begin by testing the model on the GQA task [41], a recent dataset that features challenging compositional questions that involve diverse reasoning skills in real-world settings, including spatial reasoning, relational reasoning, logic and comparisons."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 104
                            }
                        ],
                        "text": "Table 4 summarizes the results for both settings, comparing our model to the baselines released for GQA [41], all using the same training scheme and input features."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "We evaluate our model (NSM) on two recent VQA datasets: (1) The GQA dataset [41] which focuses on real-world visual reasoning and compositional question answering, and (2) VQA-CP (version 2) [3], a split of the VQA dataset [27] that has been particularly designed to test generalization skills across changes in the answer distribution between the training and the test sets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 80
                            }
                        ],
                        "text": "Model Binary Open Consistency Validity Plausibility Distribution Accuracy Human [41] 91."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 129
                            }
                        ],
                        "text": "We demonstrate the value and performance of the Neural State Machine on two recent Visual Question Answering (VQA) datasets: GQA [41] which focuses on real-world visual reasoning and multi-step question answering, as well as VQA-CP [3], a recent split of the popular VQA dataset [2, 27] that has been designed particularly to evaluate generalization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 152282269,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ab7f7c1d328589f25c79515b9a5d824d7ffbbd1",
            "isKey": false,
            "numCitedBy": 448,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce GQA, a new dataset for real-world visual reasoning and compositional question answering, seeking to address key shortcomings of previous VQA datasets. We have developed a strong and robust question engine that leverages Visual Genome scene graph structures to create 22M diverse reasoning questions, which all come with functional programs that represent their semantics. We use the programs to gain tight control over the answer distribution and present a new tunable smoothing technique to mitigate question biases. Accompanying the dataset is a suite of new metrics that evaluate essential qualities such as consistency, grounding and plausibility. A careful analysis is performed for baselines as well as state-of-the-art models, providing fine-grained results for different question types and topologies. Whereas a blind LSTM obtains a mere 42.1%, and strong VQA models achieve 54.1%, human performance tops at 89.3%, offering ample opportunity for new research to explore. We hope GQA will provide an enabling resource for the next generation of models with enhanced robustness, improved consistency, and deeper semantic understanding of vision and language."
            },
            "slug": "GQA:-A-New-Dataset-for-Real-World-Visual-Reasoning-Hudson-Manning",
            "title": {
                "fragments": [],
                "text": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "GQA is introduced, a new dataset for real-world visual reasoning and compositional question answering, seeking to address key shortcomings of previous VQA datasets, and a strong and robust question engine is developed that leverages Visual Genome scene graph structures to create 22M diverse reasoning questions."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2373318"
                        ],
                        "name": "B. Lake",
                        "slug": "B.-Lake",
                        "structuredName": {
                            "firstName": "Brenden",
                            "lastName": "Lake",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Lake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145283199"
                        ],
                        "name": "Marco Baroni",
                        "slug": "Marco-Baroni",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Baroni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Baroni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 196
                            }
                        ],
                        "text": "Yet, even though neural networks are undoubtedly powerful, flexible and robust, recent work has repeatedly demonstrated their flaws, showing how they struggle to generalize in a systematic manner [50], overly adhere to superficial and potentially misleading statistical associations instead of learning true causal relations [1, 42], strongly depend on large amounts of data and supervision [25, 51], and sometimes behave in surprising and"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 46761158,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "856fe866bcce5e7a540655bea6ecc7406bdcfcba",
            "isKey": false,
            "numCitedBy": 403,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb \"dax,\" he or she can immediately understand the meaning of \"dax twice\" or \"sing and dax.\" In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can make successful zero-shot generalizations when the differences between training and test commands are small, so that they can apply \"mix-and-match\" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the \"dax\" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks' notorious training data thirst."
            },
            "slug": "Generalization-without-Systematicity:-On-the-Skills-Lake-Baroni",
            "title": {
                "fragments": [],
                "text": "Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper introduces the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences, and tests the zero-shot generalization capabilities of a variety of recurrent neural networks trained on SCAN with sequence-to-sequence methods."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2874347"
                        ],
                        "name": "Ronghang Hu",
                        "slug": "Ronghang-Hu",
                        "structuredName": {
                            "firstName": "Ronghang",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronghang Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112400"
                        ],
                        "name": "Jacob Andreas",
                        "slug": "Jacob-Andreas",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Andreas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacob Andreas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34849128"
                        ],
                        "name": "Marcus Rohrbach",
                        "slug": "Marcus-Rohrbach",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Rohrbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus Rohrbach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2903226"
                        ],
                        "name": "Kate Saenko",
                        "slug": "Kate-Saenko",
                        "structuredName": {
                            "firstName": "Kate",
                            "lastName": "Saenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kate Saenko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 89
                            }
                        ],
                        "text": "Our model connects to multiple lines of research, including works about compositionality [14, 38], concept acquisition [36, 81], and neural computation [28, 62, 7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18682,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a396a6febdacb84340d139096455e67049ac1e22",
            "isKey": false,
            "numCitedBy": 430,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural language questions are inherently compositional, and many are most easily answered by reasoning about their decomposition into modular sub-problems. For example, to answer \u201cis there an equal number of balls and boxes?\u201d we can look for balls, look for boxes, count them, and compare the results. The recently proposed Neural Module Network (NMN) architecture [3, 2] implements this approach to question answering by parsing questions into linguistic substructures and assembling question-specific deep networks from smaller modules that each solve one subtask. However, existing NMN implementations rely on brittle off-the-shelf parsers, and are restricted to the module configurations proposed by these parsers rather than learning them from data. In this paper, we propose End-to-End Module Networks (N2NMNs), which learn to reason by directly predicting instance-specific network layouts without the aid of a parser. Our model learns to generate network structures (by imitating expert demonstrations) while simultaneously learning network parameters (using the downstream task loss). Experimental results on the new CLEVR dataset targeted at compositional question answering show that N2NMNs achieve an error reduction of nearly 50% relative to state-of-theart attentional approaches, while discovering interpretable network architectures specialized for each question."
            },
            "slug": "Learning-to-Reason:-End-to-End-Module-Networks-for-Hu-Andreas",
            "title": {
                "fragments": [],
                "text": "Learning to Reason: End-to-End Module Networks for Visual Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "End-to-End Module Networks are proposed, which learn to reason by directly predicting instance-specific network layouts without the aid of a parser, and achieve an error reduction of nearly 50% relative to state-of-theart attentional approaches."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1509240145"
                        ],
                        "name": "Qi Wu",
                        "slug": "Qi-Wu",
                        "structuredName": {
                            "firstName": "Qi",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qi Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12459603"
                        ],
                        "name": "Chunhua Shen",
                        "slug": "Chunhua-Shen",
                        "structuredName": {
                            "firstName": "Chunhua",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunhua Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2161037"
                        ],
                        "name": "Lingqiao Liu",
                        "slug": "Lingqiao-Liu",
                        "structuredName": {
                            "firstName": "Lingqiao",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lingqiao Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2699095"
                        ],
                        "name": "A. Dick",
                        "slug": "A.-Dick",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Dick",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5546141"
                        ],
                        "name": "A. V. Hengel",
                        "slug": "A.-V.-Hengel",
                        "structuredName": {
                            "firstName": "Anton",
                            "lastName": "Hengel",
                            "middleNames": [
                                "van",
                                "den"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. V. Hengel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 119
                            }
                        ],
                        "text": "Our model connects to multiple lines of research, including works about compositionality [14, 38], concept acquisition [36, 81], and neural computation [28, 62, 7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206593820,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "00fe3d95d0fd5f1433d81405bee772c4fe9af9c6",
            "isKey": false,
            "numCitedBy": 369,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "Much recent progress in Vision-to-Language (V2L) problems has been achieved through a combination of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). This approach does not explicitly represent high-level semantic concepts, but rather seeks to progress directly from image features to text. In this paper we investigate whether this direct approach succeeds due to, or despite, the fact that it avoids the explicit representation of high-level information. We propose a method of incorporating high-level concepts into the successful CNN-RNN approach, and show that it achieves a significant improvement on the state-of-the-art in both image captioning and visual question answering. We also show that the same mechanism can be used to introduce external semantic information and that doing so further improves performance. We achieve the best reported results on both image captioning and VQA on several benchmark datasets, and provide an analysis of the value of explicit high-level concepts in V2L problems."
            },
            "slug": "What-Value-Do-Explicit-High-Level-Concepts-Have-in-Wu-Shen",
            "title": {
                "fragments": [],
                "text": "What Value Do Explicit High Level Concepts Have in Vision to Language Problems?"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A method of incorporating high-level concepts into the successful CNN-RNN approach is proposed, and it is shown that it achieves a significant improvement on the state-of-the-art in both image captioning and visual question answering."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1410126033"
                        ],
                        "name": "Will Norcliffe-Brown",
                        "slug": "Will-Norcliffe-Brown",
                        "structuredName": {
                            "firstName": "Will",
                            "lastName": "Norcliffe-Brown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Will Norcliffe-Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2019087"
                        ],
                        "name": "Efstathios Vafeias",
                        "slug": "Efstathios-Vafeias",
                        "structuredName": {
                            "firstName": "Efstathios",
                            "lastName": "Vafeias",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Efstathios Vafeias"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3349497"
                        ],
                        "name": "Sarah Parisot",
                        "slug": "Sarah-Parisot",
                        "structuredName": {
                            "firstName": "Sarah",
                            "lastName": "Parisot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sarah Parisot"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 262
                            }
                        ],
                        "text": "Prior work commonly relied on dense visual features produced by either CNNs [83, 86] or object detectors [5], with a few recent models that use the relationships among objects to augment those features with contextual information from each object\u2019s surroundings [52, 75, 66]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 49317766,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6ac33d3dcecbed17580509a34bccdff2425f7ed8",
            "isKey": false,
            "numCitedBy": 149,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual Question answering is a challenging problem requiring a combination of concepts from Computer Vision and Natural Language Processing. Most existing approaches use a two streams strategy, computing image and question features that are consequently merged using a variety of techniques. Nonetheless, very few rely on higher level image representations, which allow to capture semantic and spatial relationships. In this paper, we propose a novel graph-based approach for Visual Question Answering. Our method combines a graph learner module, which learns a question specific graph representation of the input image, with the recent concept of graph convolutions, aiming to learn image representations that capture question specific interactions. We test our approach on the VQA v2 dataset using a simple baseline architecture enhanced by the proposed graph learner module. We obtain state of the art results with 66.18% accuracy and demonstrate the interpretability of the proposed method."
            },
            "slug": "Learning-Conditioned-Graph-Structures-for-Visual-Norcliffe-Brown-Vafeias",
            "title": {
                "fragments": [],
                "text": "Learning Conditioned Graph Structures for Interpretable Visual Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes a novel graph-based approach for Visual Question Answering that combines a graph learner module, which learns a question specific graph representation of the input image, with the recent concept of graph convolutions, aiming to learn image representations that capture question specific interactions."
            },
            "venue": {
                "fragments": [],
                "text": "NeurIPS"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115231104"
                        ],
                        "name": "Justin Johnson",
                        "slug": "Justin-Johnson",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Justin Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "73710317"
                        ],
                        "name": "B. Hariharan",
                        "slug": "B.-Hariharan",
                        "structuredName": {
                            "firstName": "Bharath",
                            "lastName": "Hariharan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Hariharan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803520"
                        ],
                        "name": "L. V. D. Maaten",
                        "slug": "L.-V.-D.-Maaten",
                        "structuredName": {
                            "firstName": "Laurens",
                            "lastName": "Maaten",
                            "middleNames": [
                                "van",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. V. D. Maaten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 167
                            }
                        ],
                        "text": "Such settings reduce the extent to which models can circumvent the need for genuine scene understanding skills by exploiting dataset biases and superficial statistics [1, 44, 27], and are known to be particularly difficult for neural networks [51]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15458100,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03eb382e04cca8cca743f7799070869954f1402a",
            "isKey": false,
            "numCitedBy": 1223,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "When building artificial intelligence systems that can reason and answer questions about visual data, we need diagnostic tests to analyze our progress and discover short-comings. Existing benchmarks for visual question answering can help, but have strong biases that models can exploit to correctly answer questions without reasoning. They also conflate multiple sources of error, making it hard to pinpoint model weaknesses. We present a diagnostic dataset that tests a range of visual reasoning abilities. It contains minimal biases and has detailed annotations describing the kind of reasoning each question requires. We use this dataset to analyze a variety of modern visual reasoning systems, providing novel insights into their abilities and limitations."
            },
            "slug": "CLEVR:-A-Diagnostic-Dataset-for-Compositional-and-Johnson-Hariharan",
            "title": {
                "fragments": [],
                "text": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work presents a diagnostic dataset that tests a range of visual reasoning abilities and uses this dataset to analyze a variety of modern visual reasoning systems, providing novel insights into their abilities and limitations."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50703697"
                        ],
                        "name": "Linjie Li",
                        "slug": "Linjie-Li",
                        "structuredName": {
                            "firstName": "Linjie",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Linjie Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144702900"
                        ],
                        "name": "Zhe Gan",
                        "slug": "Zhe-Gan",
                        "structuredName": {
                            "firstName": "Zhe",
                            "lastName": "Gan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhe Gan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145215470"
                        ],
                        "name": "Yu Cheng",
                        "slug": "Yu-Cheng",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Cheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu Cheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46700348"
                        ],
                        "name": "Jingjing Liu",
                        "slug": "Jingjing-Liu",
                        "structuredName": {
                            "firstName": "Jingjing",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jingjing Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 262
                            }
                        ],
                        "text": "Prior work commonly relied on dense visual features produced by either CNNs [83, 86] or object detectors [5], with a few recent models that use the relationships among objects to augment those features with contextual information from each object\u2019s surroundings [52, 75, 66]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 88523817,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d379ba96b8f400b23b2cd72c428af67e578959ea",
            "isKey": false,
            "numCitedBy": 146,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "In order to answer semantically-complicated questions about an image, a Visual Question Answering (VQA) model needs to fully understand the visual scene in the image, especially the interactive dynamics between different objects. We propose a Relation-aware Graph Attention Network (ReGAT), which encodes each image into a graph and models multi-type inter-object relations via a graph attention mechanism, to learn question-adaptive relation representations. Two types of visual object relations are explored: (i) Explicit Relations that represent geometric positions and semantic interactions between objects; and (ii) Implicit Relations that capture the hidden dynamics between image regions. Experiments demonstrate that ReGAT outperforms prior state-of-the-art approaches on both VQA 2.0 and VQA-CP v2 datasets. We further show that ReGAT is compatible to existing VQA architectures, and can be used as a generic relation encoder to boost the model performance for VQA."
            },
            "slug": "Relation-Aware-Graph-Attention-Network-for-Visual-Li-Gan",
            "title": {
                "fragments": [],
                "text": "Relation-Aware Graph Attention Network for Visual Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A Relation-aware Graph Attention Network (ReGAT), which encodes each image into a graph and models multi-type inter-object relations via a graph attention mechanism, to learn question-adaptive relation representations."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2545335"
                        ],
                        "name": "Rowan Zellers",
                        "slug": "Rowan-Zellers",
                        "structuredName": {
                            "firstName": "Rowan",
                            "lastName": "Zellers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rowan Zellers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064210"
                        ],
                        "name": "Mark Yatskar",
                        "slug": "Mark-Yatskar",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Yatskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Yatskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38094552"
                        ],
                        "name": "Sam Thomson",
                        "slug": "Sam-Thomson",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Thomson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sam Thomson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699545"
                        ],
                        "name": "Yejin Choi",
                        "slug": "Yejin-Choi",
                        "structuredName": {
                            "firstName": "Yejin",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yejin Choi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 74
                            }
                        ],
                        "text": "Multiple models have been proposed for the task of scene graph generation [85, 86, 16, 89]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4379400,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0da8af8d81e84381ffe656a0bbf2f3937ffac618",
            "isKey": false,
            "numCitedBy": 457,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the problem of producing structured graph representations of visual scenes. Our work analyzes the role of motifs: regularly appearing substructures in scene graphs. We present new quantitative insights on such repeated structures in the Visual Genome dataset. Our analysis shows that object labels are highly predictive of relation labels but not vice-versa. We also find that there are recurring patterns even in larger subgraphs: more than 50% of graphs contain motifs involving at least two relations. Our analysis motivates a new baseline: given object detections, predict the most frequent relation between object pairs with the given labels, as seen in the training set. This baseline improves on the previous state-of-the-art by an average of 3.6% relative improvement across evaluation settings. We then introduce Stacked Motif Networks, a new architecture designed to capture higher order motifs in scene graphs that further improves over our strong baseline by an average 7.1% relative gain. Our code is available at github.com/rowanz/neural-motifs."
            },
            "slug": "Neural-Motifs:-Scene-Graph-Parsing-with-Global-Zellers-Yatskar",
            "title": {
                "fragments": [],
                "text": "Neural Motifs: Scene Graph Parsing with Global Context"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This work analyzes the role of motifs: regularly appearing substructures in scene graphs and introduces Stacked Motif Networks, a new architecture designed to capture higher order motifs in scene graph graphs that improves on the previous state-of-the-art by an average of 3.6% relative improvement across evaluation settings."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2228109"
                        ],
                        "name": "Caiming Xiong",
                        "slug": "Caiming-Xiong",
                        "structuredName": {
                            "firstName": "Caiming",
                            "lastName": "Xiong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Caiming Xiong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3375440"
                        ],
                        "name": "Stephen Merity",
                        "slug": "Stephen-Merity",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Merity",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen Merity"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 76
                            }
                        ],
                        "text": "Prior work commonly relied on dense visual features produced by either CNNs [83, 86] or object detectors [5], with a few recent models that use the relationships among objects to augment those features with contextual information from each object\u2019s surroundings [52, 75, 66]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "Typically, most approaches use either CNNs [83, 86] or object detectors [5] to derive visual features which are then compared to a fixed-size question embedding obtained by an LSTM."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14294589,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f96898d15a1bf1fa8925b1280d0e07a7a8e72194",
            "isKey": false,
            "numCitedBy": 659,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural network architectures with memory and attention mechanisms exhibit certain reasoning capabilities required for question answering. One such architecture, the dynamic memory network (DMN), obtained high accuracy on a variety of language tasks. However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images. Based on an analysis of the DMN, we propose several improvements to its memory and input modules. Together with these changes we introduce a novel input module for images in order to be able to answer visual questions. Our new DMN+ model improves the state of the art on both the Visual Question Answering dataset and the \\babi-10k text question-answering dataset without supporting fact supervision."
            },
            "slug": "Dynamic-Memory-Networks-for-Visual-and-Textual-Xiong-Merity",
            "title": {
                "fragments": [],
                "text": "Dynamic Memory Networks for Visual and Textual Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The new DMN+ model improves the state of the art on both the Visual Question Answering dataset and the \\babi-10k text question-answering dataset without supporting fact supervision."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3444569"
                        ],
                        "name": "Petar Velickovic",
                        "slug": "Petar-Velickovic",
                        "structuredName": {
                            "firstName": "Petar",
                            "lastName": "Velickovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Petar Velickovic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7153363"
                        ],
                        "name": "Guillem Cucurull",
                        "slug": "Guillem-Cucurull",
                        "structuredName": {
                            "firstName": "Guillem",
                            "lastName": "Cucurull",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guillem Cucurull"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8742492"
                        ],
                        "name": "Arantxa Casanova",
                        "slug": "Arantxa-Casanova",
                        "structuredName": {
                            "firstName": "Arantxa",
                            "lastName": "Casanova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Arantxa Casanova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144290131"
                        ],
                        "name": "Adriana Romero",
                        "slug": "Adriana-Romero",
                        "structuredName": {
                            "firstName": "Adriana",
                            "lastName": "Romero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adriana Romero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144269589"
                        ],
                        "name": "P. Lio\u2019",
                        "slug": "P.-Lio\u2019",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Lio\u2019",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Lio\u2019"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3292002,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "33998aff64ce51df8dee45989cdca4b6b1329ec4",
            "isKey": false,
            "numCitedBy": 5524,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training)."
            },
            "slug": "Graph-Attention-Networks-Velickovic-Cucurull",
            "title": {
                "fragments": [],
                "text": "Graph Attention Networks"
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748557"
                        ],
                        "name": "P. Smolensky",
                        "slug": "P.-Smolensky",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Smolensky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Smolensky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 120
                            }
                        ],
                        "text": "Ideas about compositionality, abstraction and reasoning greatly inspired the classical views of artificial intelligence [74, 65], but have lately been overshadowed by the astounding success of deep learning over a wide spectrum of real-world tasks [33, 63, 82]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 27428025,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "03f026f794776a489f15d305677367bcca999a37",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Connectionist AI systems are large networks of extremely simple numerical processors, massively interconnected and running in parallel. There has been great progress in the connectionist approach, and while it is still unclear whether the approach will succeed, it is also unclear exactly what the implications for cognitive science would be if it did succeed. In this paper I present a view of the connectionist approach that implies that the level of analysis at which uniform formal principles of cognition can be found is the subsymbolic level, intermediate between the neural and symbolic levels. Notions such as logical inference, sequential firing of production rules, spreading activation between conceptual units, mental categories, and frames or schemata turn out to provide approximate descriptions of the coarse-grained behaviour of connectionist systems. The implication is that symbol-level structures provide only approximate accounts of cognition, useful for description but not necessarily for constructing detailed formal models."
            },
            "slug": "Connectionist-AI,-symbolic-AI,-and-the-brain-Smolensky",
            "title": {
                "fragments": [],
                "text": "Connectionist AI, symbolic AI, and the brain"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A view of the connectionist approach that implies that the level of analysis at which uniform formal principles of cognition can be found is the subsymbolic level, intermediate between the neural and symbolic levels is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Artificial Intelligence Review"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47002813"
                        ],
                        "name": "Yujia Li",
                        "slug": "Yujia-Li",
                        "structuredName": {
                            "firstName": "Yujia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yujia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725299"
                        ],
                        "name": "Daniel Tarlow",
                        "slug": "Daniel-Tarlow",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Tarlow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Tarlow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107692"
                        ],
                        "name": "Marc Brockschmidt",
                        "slug": "Marc-Brockschmidt",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Brockschmidt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc Brockschmidt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 332,
                                "start": 320
                            }
                        ],
                        "text": "Recent research about scene graphs [43, 85] and graph networks [10] is also relevant to our work, where we propose a novel method for neural graph traversal that is more suitable than prior approaches to our goal of performing sequential reasoning, as it eliminates the need in this case for costly state updates, as in [56, 48, 77]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8393918,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "492f57ee9ceb61fb5a47ad7aebfec1121887a175",
            "isKey": false,
            "numCitedBy": 1968,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract: Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures."
            },
            "slug": "Gated-Graph-Sequence-Neural-Networks-Li-Tarlow",
            "title": {
                "fragments": [],
                "text": "Gated Graph Sequence Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work studies feature learning techniques for graph-structured inputs and achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144282676"
                        ],
                        "name": "Peng Wang",
                        "slug": "Peng-Wang",
                        "structuredName": {
                            "firstName": "Peng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peng Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144663765"
                        ],
                        "name": "Qi Wu",
                        "slug": "Qi-Wu",
                        "structuredName": {
                            "firstName": "Qi",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qi Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780381"
                        ],
                        "name": "Chunhua Shen",
                        "slug": "Chunhua-Shen",
                        "structuredName": {
                            "firstName": "Chunhua",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunhua Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5546141"
                        ],
                        "name": "A. V. Hengel",
                        "slug": "A.-V.-Hengel",
                        "structuredName": {
                            "firstName": "Anton",
                            "lastName": "Hengel",
                            "middleNames": [
                                "van",
                                "den"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. V. Hengel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6088794,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d52fa1a9021d3596930ad2d5121e9d125113ab2",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the most intriguing features of the Visual Question Answering (VQA) challenge is the unpredictability of the questions. Extracting the information required to answer them demands a variety of image operations from detection and counting, to segmentation and reconstruction. To train a method to perform even one of these operations accurately from {image, question, answer} tuples would be challenging, but to aim to achieve them all with a limited set of such training data seems ambitious at best. Our method thus learns how to exploit a set of external off-the-shelf algorithms to achieve its goal, an approach that has something in common with the Neural Turing Machine [10]. The core of our proposed method is a new co-attention model. In addition, the proposed approach generates human-readable reasons for its decision, and can still be trained end-to-end without ground truth reasons being given. We demonstrate the effectiveness on two publicly available datasets, Visual Genome and VQA, and show that it produces the state-of-the-art results in both cases."
            },
            "slug": "The-VQA-Machine:-Learning-How-to-Use-Existing-to-Wang-Wu",
            "title": {
                "fragments": [],
                "text": "The VQA-Machine: Learning How to Use Existing Vision Algorithms to Answer New Questions"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The core of the proposed method is a new co-attention model that learns how to exploit a set of external off-the-shelf algorithms to achieve its goal, an approach that has something in common with the Neural Turing Machine."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2801949"
                        ],
                        "name": "Aishwarya Agrawal",
                        "slug": "Aishwarya-Agrawal",
                        "structuredName": {
                            "firstName": "Aishwarya",
                            "lastName": "Agrawal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aishwarya Agrawal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746610"
                        ],
                        "name": "Dhruv Batra",
                        "slug": "Dhruv-Batra",
                        "structuredName": {
                            "firstName": "Dhruv",
                            "lastName": "Batra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dhruv Batra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153432684"
                        ],
                        "name": "Devi Parikh",
                        "slug": "Devi-Parikh",
                        "structuredName": {
                            "firstName": "Devi",
                            "lastName": "Parikh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Devi Parikh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 332,
                                "start": 325
                            }
                        ],
                        "text": "Yet, even though neural networks are undoubtedly powerful, flexible and robust, recent work has repeatedly demonstrated their flaws, showing how they struggle to generalize in a systematic manner [50], overly adhere to superficial and potentially misleading statistical associations instead of learning true causal relations [1, 42], strongly depend on large amounts of data and supervision [25, 51], and sometimes behave in surprising and"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 167
                            }
                        ],
                        "text": "Such settings reduce the extent to which models can circumvent the need for genuine scene understanding skills by exploiting dataset biases and superficial statistics [1, 44, 27], and are known to be particularly difficult for neural networks [51]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12304778,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e759195eb4b4f0f480a8a2cf1c629bfd881d4e5",
            "isKey": false,
            "numCitedBy": 222,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, a number of deep-learning based models have been proposed for the task of Visual Question Answering (VQA). The performance of most models is clustered around 60-70%. In this paper we propose systematic methods to analyze the behavior of these models as a first step towards recognizing their strengths and weaknesses, and identifying the most fruitful directions for progress. We analyze two models, one each from two major classes of VQA models -- with-attention and without-attention and show the similarities and differences in the behavior of these models. We also analyze the winning entry of the VQA Challenge 2016. \nOur behavior analysis reveals that despite recent progress, today's VQA models are \"myopic\" (tend to fail on sufficiently novel instances), often \"jump to conclusions\" (converge on a predicted answer after 'listening' to just half the question), and are \"stubborn\" (do not change their answers across images)."
            },
            "slug": "Analyzing-the-Behavior-of-Visual-Question-Answering-Agrawal-Batra",
            "title": {
                "fragments": [],
                "text": "Analyzing the Behavior of Visual Question Answering Models"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Today's VQA models are \"myopic\" (tend to fail on sufficiently novel instances), often \"jump to conclusions\" (converge on a predicted answer after 'listening' to just half the question), and are \"stubborn\" (do not change their answers across images)."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115231104"
                        ],
                        "name": "Justin Johnson",
                        "slug": "Justin-Johnson",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Justin Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "25445698"
                        ],
                        "name": "Agrim Gupta",
                        "slug": "Agrim-Gupta",
                        "structuredName": {
                            "firstName": "Agrim",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Agrim Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4593810,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46b5d408d950287637dd21ce04772d9b2bacfd14",
            "isKey": false,
            "numCitedBy": 469,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "To truly understand the visual world our models should be able not only to recognize images but also generate them. To this end, there has been exciting recent progress on generating images from natural language descriptions. These methods give stunning results on limited domains such as descriptions of birds or flowers, but struggle to faithfully reproduce complex sentences with many objects and relationships. To overcome this limitation we propose a method for generating images from scene graphs, enabling explicitly reasoning about objects and their relationships. Our model uses graph convolution to process input graphs, computes a scene layout by predicting bounding boxes and segmentation masks for objects, and converts the layout to an image with a cascaded refinement network. The network is trained adversarially against a pair of discriminators to ensure realistic outputs. We validate our approach on Visual Genome and COCO-Stuff, where qualitative results, ablations, and user studies demonstrate our method's ability to generate complex images with multiple objects."
            },
            "slug": "Image-Generation-from-Scene-Graphs-Johnson-Gupta",
            "title": {
                "fragments": [],
                "text": "Image Generation from Scene Graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes a method for generating images from scene graphs, enabling explicitly reasoning about objects and their relationships, and validates this approach on Visual Genome and COCO-Stuff."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8553015"
                        ],
                        "name": "Jiasen Lu",
                        "slug": "Jiasen-Lu",
                        "structuredName": {
                            "firstName": "Jiasen",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiasen Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120157163"
                        ],
                        "name": "Jianwei Yang",
                        "slug": "Jianwei-Yang",
                        "structuredName": {
                            "firstName": "Jianwei",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianwei Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746610"
                        ],
                        "name": "Dhruv Batra",
                        "slug": "Dhruv-Batra",
                        "structuredName": {
                            "firstName": "Dhruv",
                            "lastName": "Batra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dhruv Batra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153432684"
                        ],
                        "name": "Devi Parikh",
                        "slug": "Devi-Parikh",
                        "structuredName": {
                            "firstName": "Devi",
                            "lastName": "Parikh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Devi Parikh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4406645,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3bf09b2e2639add154a9fe6ff98cc373d3e90e4e",
            "isKey": false,
            "numCitedBy": 306,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a novel framework for image captioning that can produce natural language explicitly grounded in entities that object detectors find in the image. Our approach reconciles classical slot filling approaches (that are generally better grounded in images) with modern neural captioning approaches (that are generally more natural sounding and accurate). Our approach first generates a sentence 'template' with slot locations explicitly tied to specific image regions. These slots are then filled in by visual concepts identified in the regions by object detectors. The entire architecture (sentence template generation and slot filling with object detectors) is end-to-end differentiable. We verify the effectiveness of our proposed model on different image captioning tasks. On standard image captioning and novel object captioning, our model reaches state-of-the-art on both COCO and Flickr30k datasets. We also demonstrate that our model has unique advantages when the train and test distributions of scene compositions - and hence language priors of associated captions - are different. Code has been made available at: https://github.com/jiasenlu/NeuralBabyTalk."
            },
            "slug": "Neural-Baby-Talk-Lu-Yang",
            "title": {
                "fragments": [],
                "text": "Neural Baby Talk"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A novel framework for image captioning that can produce natural language explicitly grounded in entities that object detectors find in the image is introduced and reaches state-of-the-art on both COCO and Flickr30k datasets."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110468365"
                        ],
                        "name": "Yikang Li",
                        "slug": "Yikang-Li",
                        "structuredName": {
                            "firstName": "Yikang",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yikang Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3001348"
                        ],
                        "name": "Wanli Ouyang",
                        "slug": "Wanli-Ouyang",
                        "structuredName": {
                            "firstName": "Wanli",
                            "lastName": "Ouyang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wanli Ouyang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145291669"
                        ],
                        "name": "Bolei Zhou",
                        "slug": "Bolei-Zhou",
                        "structuredName": {
                            "firstName": "Bolei",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bolei Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2119043802"
                        ],
                        "name": "Kun Wang",
                        "slug": "Kun-Wang",
                        "structuredName": {
                            "firstName": "Kun",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kun Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31843833"
                        ],
                        "name": "Xiaogang Wang",
                        "slug": "Xiaogang-Wang",
                        "structuredName": {
                            "firstName": "Xiaogang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaogang Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 21277943,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cf2de559e5a6235783e0762862f6e42192f142a8",
            "isKey": false,
            "numCitedBy": 307,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Object detection, scene graph generation and region captioning, which are three scene understanding tasks at different semantic levels, are tied together: scene graphs are generated on top of objects detected in an image with their pairwise relationship predicted, while region captioning gives a language description of the objects, their attributes, relations and other context information. In this work, to leverage the mutual connections across semantic levels, we propose a novel neural network model, termed as Multi-level Scene Description Network (denoted as MSDN), to solve the three vision tasks jointly in an end-to-end manner. Object, phrase, and caption regions are first aligned with a dynamic graph based on their spatial and semantic connections. Then a feature refining structure is used to pass messages across the three levels of semantic tasks through the graph. We benchmark the learned model on three tasks, and show the joint learning across three tasks with our proposed method can bring mutual improvements over previous models. Particularly, on the scene graph generation task, our proposed method outperforms the stateof- art method with more than 3% margin. Code has been made publicly available."
            },
            "slug": "Scene-Graph-Generation-from-Objects,-Phrases-and-Li-Ouyang",
            "title": {
                "fragments": [],
                "text": "Scene Graph Generation from Objects, Phrases and Region Captions"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work proposes a novel neural network model, termed as Multi-level Scene Description Network (denoted as MSDN), to solve the three vision tasks jointly in an end-to-end manner and shows the joint learning across three tasks with the proposed method can bring mutual improvements over previous models."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765674"
                        ],
                        "name": "Tianshui Chen",
                        "slug": "Tianshui-Chen",
                        "structuredName": {
                            "firstName": "Tianshui",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tianshui Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "23476952"
                        ],
                        "name": "Weihao Yu",
                        "slug": "Weihao-Yu",
                        "structuredName": {
                            "firstName": "Weihao",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weihao Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108279486"
                        ],
                        "name": "Riquan Chen",
                        "slug": "Riquan-Chen",
                        "structuredName": {
                            "firstName": "Riquan",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Riquan Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737218"
                        ],
                        "name": "Liang Lin",
                        "slug": "Liang-Lin",
                        "structuredName": {
                            "firstName": "Liang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liang Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 74
                            }
                        ],
                        "text": "Multiple models have been proposed for the task of scene graph generation [84, 85, 16, 88]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[16] in conjunction with a variant of the Mask R-CNN object detector [34] proposed by Hu et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 72941015,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "beb335e43da1f518ed7df3a851784925254a6a94",
            "isKey": false,
            "numCitedBy": 157,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "To understand a scene in depth not only involves locating/recognizing individual objects, but also requires to infer the relationships and interactions among them. However, since the distribution of real-world relationships is seriously unbalanced, existing methods perform quite poorly for the less frequent relationships. In this work, we find that the statistical correlations between object pairs and their relationships can effectively regularize semantic space and make prediction less ambiguous, and thus well address the unbalanced distribution issue. To achieve this, we incorporate these statistical correlations into deep neural networks to facilitate scene graph generation by developing a Knowledge-Embedded Routing Network. More specifically, we show that the statistical correlations between objects appearing in images and their relationships, can be explicitly represented by a structured knowledge graph, and a routing mechanism is learned to propagate messages through the graph to explore their interactions. Extensive experiments on the large-scale Visual Genome dataset demonstrate the superiority of the proposed method over current state-of-the-art competitors."
            },
            "slug": "Knowledge-Embedded-Routing-Network-for-Scene-Graph-Chen-Yu",
            "title": {
                "fragments": [],
                "text": "Knowledge-Embedded Routing Network for Scene Graph Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work finds that the statistical correlations between object pairs and their relationships can effectively regularize semantic space and make prediction less ambiguous, and thus well address the unbalanced distribution issue."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145237361"
                        ],
                        "name": "Ranjay Krishna",
                        "slug": "Ranjay-Krishna",
                        "structuredName": {
                            "firstName": "Ranjay",
                            "lastName": "Krishna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ranjay Krishna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117748"
                        ],
                        "name": "Yuke Zhu",
                        "slug": "Yuke-Zhu",
                        "structuredName": {
                            "firstName": "Yuke",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuke Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50499889"
                        ],
                        "name": "O. Groth",
                        "slug": "O.-Groth",
                        "structuredName": {
                            "firstName": "Oliver",
                            "lastName": "Groth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Groth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115231104"
                        ],
                        "name": "Justin Johnson",
                        "slug": "Justin-Johnson",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Justin Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1382195702"
                        ],
                        "name": "K. Hata",
                        "slug": "K.-Hata",
                        "structuredName": {
                            "firstName": "Kenji",
                            "lastName": "Hata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40591424"
                        ],
                        "name": "J. Kravitz",
                        "slug": "J.-Kravitz",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Kravitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kravitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110910215"
                        ],
                        "name": "Stephanie Chen",
                        "slug": "Stephanie-Chen",
                        "structuredName": {
                            "firstName": "Stephanie",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephanie Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1944225"
                        ],
                        "name": "Yannis Kalantidis",
                        "slug": "Yannis-Kalantidis",
                        "structuredName": {
                            "firstName": "Yannis",
                            "lastName": "Kalantidis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yannis Kalantidis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2040091191"
                        ],
                        "name": "Li-Jia Li",
                        "slug": "Li-Jia-Li",
                        "structuredName": {
                            "firstName": "Li-Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li-Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760364"
                        ],
                        "name": "David A. Shamma",
                        "slug": "David-A.-Shamma",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Shamma",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. Shamma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145879842"
                        ],
                        "name": "Michael S. Bernstein",
                        "slug": "Michael-S.-Bernstein",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Bernstein",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael S. Bernstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 70
                            }
                        ],
                        "text": "Starting from an image, we first generate a probabilistic scene graph [43, 49] that captures its underlying semantic knowledge in a compact form."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "holding, behind), all derived from the Visual Genome dataset [49] (see section 7."
                    },
                    "intents": []
                }
            ],
            "corpusId": 4492210,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d",
            "isKey": false,
            "numCitedBy": 2772,
            "numCiting": 142,
            "paperAbstract": {
                "fragments": [],
                "text": "Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. When asked \u201cWhat vehicle is the person riding?\u201d, computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) to answer correctly that \u201cthe person is riding a horse-drawn carriage.\u201d In this paper, we present the Visual Genome dataset to enable the modeling of such relationships. We collect dense annotations of objects, attributes, and relationships within each image to learn these models. Specifically, our dataset contains over 108K images where each image has an average of $$35$$35 objects, $$26$$26 attributes, and $$21$$21 pairwise relationships between objects. We canonicalize the objects, attributes, relationships, and noun phrases in region descriptions and questions answer pairs to WordNet synsets. Together, these annotations represent the densest and largest dataset of image descriptions, objects, attributes, relationships, and question answer pairs."
            },
            "slug": "Visual-Genome:-Connecting-Language-and-Vision-Using-Krishna-Zhu",
            "title": {
                "fragments": [],
                "text": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The Visual Genome dataset is presented, which contains over 108K images where each image has an average of $$35$$35 objects, $$26$$26 attributes, and $$21$$21 pairwise relationships between objects, and represents the densest and largest dataset of image descriptions, objects, attributes, relationships, and question answer pairs."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3255983"
                        ],
                        "name": "Volodymyr Mnih",
                        "slug": "Volodymyr-Mnih",
                        "structuredName": {
                            "firstName": "Volodymyr",
                            "lastName": "Mnih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Volodymyr Mnih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145824029"
                        ],
                        "name": "David Silver",
                        "slug": "David-Silver",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Silver",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Silver"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2228824"
                        ],
                        "name": "Andrei A. Rusu",
                        "slug": "Andrei-A.-Rusu",
                        "structuredName": {
                            "firstName": "Andrei",
                            "lastName": "Rusu",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrei A. Rusu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144056327"
                        ],
                        "name": "J. Veness",
                        "slug": "J.-Veness",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Veness",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Veness"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792298"
                        ],
                        "name": "Marc G. Bellemare",
                        "slug": "Marc-G.-Bellemare",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Bellemare",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc G. Bellemare"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3137672"
                        ],
                        "name": "Martin A. Riedmiller",
                        "slug": "Martin-A.-Riedmiller",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Riedmiller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin A. Riedmiller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145600108"
                        ],
                        "name": "A. Fidjeland",
                        "slug": "A.-Fidjeland",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Fidjeland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fidjeland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2273072"
                        ],
                        "name": "Georg Ostrovski",
                        "slug": "Georg-Ostrovski",
                        "structuredName": {
                            "firstName": "Georg",
                            "lastName": "Ostrovski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Georg Ostrovski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48348688"
                        ],
                        "name": "Stig Petersen",
                        "slug": "Stig-Petersen",
                        "structuredName": {
                            "firstName": "Stig",
                            "lastName": "Petersen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stig Petersen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50388928"
                        ],
                        "name": "Charlie Beattie",
                        "slug": "Charlie-Beattie",
                        "structuredName": {
                            "firstName": "Charlie",
                            "lastName": "Beattie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charlie Beattie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49813280"
                        ],
                        "name": "A. Sadik",
                        "slug": "A.-Sadik",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Sadik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sadik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2460849"
                        ],
                        "name": "Ioannis Antonoglou",
                        "slug": "Ioannis-Antonoglou",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Antonoglou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ioannis Antonoglou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143776287"
                        ],
                        "name": "Helen King",
                        "slug": "Helen-King",
                        "structuredName": {
                            "firstName": "Helen",
                            "lastName": "King",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Helen King"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2106164"
                        ],
                        "name": "D. Kumaran",
                        "slug": "D.-Kumaran",
                        "structuredName": {
                            "firstName": "Dharshan",
                            "lastName": "Kumaran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kumaran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688276"
                        ],
                        "name": "Daan Wierstra",
                        "slug": "Daan-Wierstra",
                        "structuredName": {
                            "firstName": "Daan",
                            "lastName": "Wierstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daan Wierstra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34313265"
                        ],
                        "name": "S. Legg",
                        "slug": "S.-Legg",
                        "structuredName": {
                            "firstName": "Shane",
                            "lastName": "Legg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Legg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48987704"
                        ],
                        "name": "D. Hassabis",
                        "slug": "D.-Hassabis",
                        "structuredName": {
                            "firstName": "Demis",
                            "lastName": "Hassabis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hassabis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 248
                            }
                        ],
                        "text": "Ideas about compositionality, abstraction and reasoning greatly inspired the classical views of artificial intelligence [74, 65], but have lately been overshadowed by the astounding success of deep learning over a wide spectrum of real-world tasks [33, 63, 82]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 205242740,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d",
            "isKey": false,
            "numCitedBy": 16186,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks."
            },
            "slug": "Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu",
            "title": {
                "fragments": [],
                "text": "Human-level control through deep reinforcement learning"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8387085"
                        ],
                        "name": "Zichao Yang",
                        "slug": "Zichao-Yang",
                        "structuredName": {
                            "firstName": "Zichao",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zichao Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144137069"
                        ],
                        "name": "Xiaodong He",
                        "slug": "Xiaodong-He",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800422"
                        ],
                        "name": "Jianfeng Gao",
                        "slug": "Jianfeng-Gao",
                        "structuredName": {
                            "firstName": "Jianfeng",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianfeng Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 76
                            }
                        ],
                        "text": "Prior work commonly relied on dense visual features produced by either CNNs [83, 86] or object detectors [5], with a few recent models that use the relationships among objects to augment those features with contextual information from each object\u2019s surroundings [52, 75, 66]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "Typically, most approaches use either CNNs [83, 86] or object detectors [5] to derive visual features which are then compared to a fixed-size question embedding obtained by an LSTM."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8849206,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c1890864c1c2b750f48316dc8b650ba4772adc5",
            "isKey": false,
            "numCitedBy": 1474,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents stacked attention networks (SANs) that learn to answer natural language questions from images. SANs use semantic representation of a question as query to search for the regions in an image that are related to the answer. We argue that image question answering (QA) often requires multiple steps of reasoning. Thus, we develop a multiple-layer SAN in which we query an image multiple times to infer the answer progressively. Experiments conducted on four image QA data sets demonstrate that the proposed SANs significantly outperform previous state-of-the-art approaches. The visualization of the attention layers illustrates the progress that the SAN locates the relevant visual clues that lead to the answer of the question layer-by-layer."
            },
            "slug": "Stacked-Attention-Networks-for-Image-Question-Yang-He",
            "title": {
                "fragments": [],
                "text": "Stacked Attention Networks for Image Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A multiple-layer SAN is developed in which an image is queried multiple times to infer the answer progressively, and the progress that the SAN locates the relevant visual clues that lead to the answer of the question layer-by-layer."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "Drawing inspiration from Bengio\u2019s consciousness prior [12], we further define a set of semantic embedded concepts that describe different entities and aspects of the domain, such as various kinds of objects, attributes and relations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 26694990,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5187414e477d8a93dd0ee8358f26af85d0f9e0f2",
            "isKey": false,
            "numCitedBy": 146,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "A new prior is proposed for learning representations of high-level concepts of the kind we manipulate with language. This prior can be combined with other priors in order to help disentangling abstract factors from each other. It is inspired by cognitive neuroscience theories of consciousness, seen as a bottleneck through which just a few elements, after having been selected by attention from a broader pool, are then broadcast and condition further processing, both in perception and decision-making. The set of recently selected elements one becomes aware of is seen as forming a low-dimensional conscious state. This conscious state is combining the few concepts constituting a conscious thought, i.e., what one is immediately conscious of at a particular moment. We claim that this architectural and information-processing constraint corresponds to assumptions about the joint distribution between high-level concepts. To the extent that these assumptions are generally true (and the form of natural language seems consistent with them), they can form a useful prior for representation learning. A low-dimensional thought or conscious state is analogous to a sentence: it involves only a few variables and yet can make a statement with very high probability of being true. This is consistent with a joint distribution (over high-level concepts) which has the form of a sparse factor graph, i.e., where the dependencies captured by each factor of the factor graph involve only very few variables while creating a strong dip in the overall energy function. The consciousness prior also makes it natural to map conscious states to natural language utterances or to express classical AI knowledge in a form similar to facts and rules, albeit capturing uncertainty as well as efficient search mechanisms implemented by attention mechanisms."
            },
            "slug": "The-Consciousness-Prior-Bengio",
            "title": {
                "fragments": [],
                "text": "The Consciousness Prior"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A new prior is proposed for learning representations of high-level concepts of the kind the authors manipulate with language, inspired by cognitive neuroscience theories of consciousness, that makes it natural to map conscious states to natural language utterances or to express classical AI knowledge in a form similar to facts and rules."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2801949"
                        ],
                        "name": "Aishwarya Agrawal",
                        "slug": "Aishwarya-Agrawal",
                        "structuredName": {
                            "firstName": "Aishwarya",
                            "lastName": "Agrawal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aishwarya Agrawal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746610"
                        ],
                        "name": "Dhruv Batra",
                        "slug": "Dhruv-Batra",
                        "structuredName": {
                            "firstName": "Dhruv",
                            "lastName": "Batra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dhruv Batra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153432684"
                        ],
                        "name": "Devi Parikh",
                        "slug": "Devi-Parikh",
                        "structuredName": {
                            "firstName": "Devi",
                            "lastName": "Parikh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Devi Parikh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2684226"
                        ],
                        "name": "Aniruddha Kembhavi",
                        "slug": "Aniruddha-Kembhavi",
                        "structuredName": {
                            "firstName": "Aniruddha",
                            "lastName": "Kembhavi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aniruddha Kembhavi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 49
                            }
                        ],
                        "text": "We achieve state-of-the-art performance both for VQA-CP, and, under single-model settings, for GQA."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "First, we measure the performance on VQA-CP [3], which provides a new split of the VQA2 dataset [27], where the answer distribution is kept different between the training and the test sets (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 138
                            }
                        ],
                        "text": "To measure the confidence of the results, we have performed additional 5 runs of our best-performing model over both the GQA Test-Dev and VQA-CP, getting standard deviations of 0.22 and 0.31 respectively."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 4
                            }
                        ],
                        "text": "For VQA-CP, we use version v2 which consists of 438k/220k questions for training/test respectively."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 191
                            }
                        ],
                        "text": "We evaluate our model (NSM) on two recent VQA datasets: (1) The GQA dataset [41] which focuses on real-world visual reasoning and compositional question answering, and (2) VQA-CP (version 2) [3], a split of the VQA dataset [27] that has been particularly designed to test generalization skills across changes in the answer distribution between the training and the test sets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 31
                            }
                        ],
                        "text": "Table 2: GQA ensemble\nTable 3: VQA-CPv2\nTable 4: GQA generalization\nFirst, we measure the performance on VQA-CP [3], which provides a new split of the VQA2 dataset [27], where the answer distribution is kept different between the training and the test sets (e.g. in the training set, the most common color answer is white, whereas in the test set, it is black)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 83
                            }
                        ],
                        "text": "Here, we follow the standard VQA1/2 [27] accuracy metric for this task (defined in [3])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 232
                            }
                        ],
                        "text": "We demonstrate the value and performance of the Neural State Machine on two recent Visual Question Answering (VQA) datasets: GQA [41] which focuses on real-world visual reasoning and multi-step question answering, as well as VQA-CP [3], a recent split of the popular VQA dataset [2, 27] that has been designed particularly to evaluate generalization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 19298149,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "90873a97aa9a43775e5aeea01b03aea54b28bfbd",
            "isKey": true,
            "numCitedBy": 335,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "A number of studies have found that today's Visual Question Answering (VQA) models are heavily driven by superficial correlations in the training data and lack sufficient image grounding. To encourage development of models geared towards the latter, we propose a new setting for VQA where for every question type, train and test sets have different prior distributions of answers. Specifically, we present new splits of the VQA v1 and VQA v2 datasets, which we call Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2 respectively). First, we evaluate several existing VQA models under this new setting and show that their performance degrades significantly compared to the original VQA setting. Second, we propose a novel Grounded Visual Question Answering model (GVQA) that contains inductive biases and restrictions in the architecture specifically designed to prevent the model from 'cheating' by primarily relying on priors in the training data. Specifically, GVQA explicitly disentangles the recognition of visual concepts present in the image from the identification of plausible answer space for a given question, enabling the model to more robustly generalize across different distributions of answers. GVQA is built off an existing VQA model - Stacked Attention Networks (SAN). Our experiments demonstrate that GVQA significantly outperforms SAN on both VQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more powerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in several cases. GVQA offers strengths complementary to SAN when trained and evaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more transparent and interpretable than existing VQA models."
            },
            "slug": "Don't-Just-Assume;-Look-and-Answer:-Overcoming-for-Agrawal-Batra",
            "title": {
                "fragments": [],
                "text": "Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "GVQA explicitly disentangles the recognition of visual concepts present in the image from the identification of plausible answer space for a given question, enabling the model to more robustly generalize across different distributions of answers."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37226164"
                        ],
                        "name": "Yash Goyal",
                        "slug": "Yash-Goyal",
                        "structuredName": {
                            "firstName": "Yash",
                            "lastName": "Goyal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yash Goyal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7595427"
                        ],
                        "name": "Tejas Khot",
                        "slug": "Tejas-Khot",
                        "structuredName": {
                            "firstName": "Tejas",
                            "lastName": "Khot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tejas Khot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403432120"
                        ],
                        "name": "Douglas Summers-Stay",
                        "slug": "Douglas-Summers-Stay",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Summers-Stay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Douglas Summers-Stay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746610"
                        ],
                        "name": "Dhruv Batra",
                        "slug": "Dhruv-Batra",
                        "structuredName": {
                            "firstName": "Dhruv",
                            "lastName": "Batra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dhruv Batra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153432684"
                        ],
                        "name": "Devi Parikh",
                        "slug": "Devi-Parikh",
                        "structuredName": {
                            "firstName": "Devi",
                            "lastName": "Parikh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Devi Parikh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 128
                            }
                        ],
                        "text": "We explore our model in the context of VQA, a challenging multimodal task that has gained substantial attention in recent years [27, 79, 40]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 96
                            }
                        ],
                        "text": "First, we measure the performance on VQA-CP [3], which provides a new split of the VQA2 dataset [27], where the answer distribution is kept different between the training and the test sets (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 223
                            }
                        ],
                        "text": "We evaluate our model (NSM) on two recent VQA datasets: (1) The GQA dataset [41] which focuses on real-world visual reasoning and compositional question answering, and (2) VQA-CP (version 2) [3], a split of the VQA dataset [27] that has been particularly designed to test generalization skills across changes in the answer distribution between the training and the test sets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "Here, we follow the standard VQA1/2 [27] accuracy metric for this task (defined in [3])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 167
                            }
                        ],
                        "text": "Such settings reduce the extent to which models can circumvent the need for genuine scene understanding skills by exploiting dataset biases and superficial statistics [1, 44, 27], and are known to be particularly difficult for neural networks [51]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 286,
                                "start": 279
                            }
                        ],
                        "text": "We demonstrate the value and performance of the Neural State Machine on two recent Visual Question Answering (VQA) datasets: GQA [41] which focuses on real-world visual reasoning and multi-step question answering, as well as VQA-CP [3], a recent split of the popular VQA dataset [2, 27] that has been designed particularly to evaluate generalization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8081284,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a27ed1310b9c0832bde8f906e0fcd23d1f3aa79c",
            "isKey": true,
            "numCitedBy": 1162,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of visual question answering (VQA) is of significant importance both as a challenging research question and for the rich set of applications it enables. In this context, however, inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities, resulting in VQA models that ignore visual information, leading to an inflated sense of their capability. We propose to counter these language priors for the task of VQA and make vision (the V in VQA) matter! Specifically, we balance the popular VQA dataset (Antol et al., in: ICCV, 2015) by collecting complementary images such that every question in our balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question. Our dataset is by construction more balanced than the original VQA dataset and has approximately twice the number of image-question pairs. Our complete balanced dataset is publicly available at http://visualqa.org/ as part of the 2nd iteration of the VQA Dataset and Challenge (VQA v2.0). We further benchmark a number of state-of-art VQA models on our balanced dataset. All models perform significantly worse on our balanced dataset, suggesting that these models have indeed learned to exploit language priors. This finding provides the first concrete empirical evidence for what seems to be a qualitative sense among practitioners. We also present interesting insights from analysis of the participant entries in VQA Challenge 2017, organized by us on the proposed VQA v2.0 dataset. The results of the challenge were announced in the 2nd VQA Challenge Workshop at the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017. Finally, our data collection protocol for identifying complementary images enables us to develop a novel interpretable model, which in addition to providing an answer to the given (image, question) pair, also provides a counter-example based explanation. Specifically, it identifies an image that is similar to the original image, but it believes has a different answer to the same question. This can help in building trust for machines among their users."
            },
            "slug": "Making-the-V-in-VQA-Matter:-Elevating-the-Role-of-Goyal-Khot",
            "title": {
                "fragments": [],
                "text": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work balances the popular VQA dataset by collecting complementary images such that every question in the authors' balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068265"
                        ],
                        "name": "Danfei Xu",
                        "slug": "Danfei-Xu",
                        "structuredName": {
                            "firstName": "Danfei",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danfei Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117748"
                        ],
                        "name": "Yuke Zhu",
                        "slug": "Yuke-Zhu",
                        "structuredName": {
                            "firstName": "Yuke",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuke Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144103"
                        ],
                        "name": "C. Choy",
                        "slug": "C.-Choy",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Choy",
                            "middleNames": [
                                "Bongsoo"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Choy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 74
                            }
                        ],
                        "text": "Multiple models have been proposed for the task of scene graph generation [84, 85, 16, 88]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1780254,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "34b73c1aa158b892bbe41705b4ae5bf01ecaea86",
            "isKey": false,
            "numCitedBy": 638,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Understanding a visual scene goes beyond recognizing individual objects in isolation. Relationships between objects also constitute rich semantic information about the scene. In this work, we explicitly model the objects and their relationships using scene graphs, a visually-grounded graphical structure of an image. We propose a novel end-to-end model that generates such structured scene representation from an input image. Our key insight is that the graph generation problem can be formulated as message passing between the primal node graph and its dual edge graph. Our joint inference model can take advantage of contextual cues to make better predictions on objects and their relationships. The experiments show that our model significantly outperforms previous methods on the Visual Genome dataset as well as support relation inference in NYU Depth V2 dataset."
            },
            "slug": "Scene-Graph-Generation-by-Iterative-Message-Passing-Xu-Zhu",
            "title": {
                "fragments": [],
                "text": "Scene Graph Generation by Iterative Message Passing"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work explicitly model the objects and their relationships using scene graphs, a visually-grounded graphical structure of an image, and proposes a novel end-to-end model that generates such structured scene representation from an input image."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2647414"
                        ],
                        "name": "T. Rogers",
                        "slug": "T.-Rogers",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Rogers",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Rogers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 169
                            }
                        ],
                        "text": "Indeed, humans are particularly adept at making abstractions of various kinds: We make analogies and form concepts to generalize from given instances to unseen examples [70]; we see things in context, and build compositional world models to represent objects and understand their interactions and subtle relations, turning raw sensory signals into high-level semantic knowledge [64]; and we deductively draw inferences via conceptual rules and statements to proceed from known facts to novel conclusions [32, 40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2979028,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "c84f76770b820d69a6a1f3914a1c84e7c20a8271",
            "isKey": false,
            "numCitedBy": 1006,
            "numCiting": 357,
            "paperAbstract": {
                "fragments": [],
                "text": "This groundbreaking monograph offers a mechanistic theory of the representation and use of semantic knowledge, integrating the strengths and overcoming many of the weaknesses of hierarchical, categorization-based approaches, similarity-based approaches, and the approach often called \"theory theory.\" Building on earlier models by Geoffrey Hinton in the 1980s and David Rumelhart in the early 1990s, the authors propose that performance in semantic tasks arises through the propagation of graded signals in a system of interconnected processing units. The representations used in performing these tasks are patterns of activation across units, governed by weighted connections among them. Semantic knowledge is acquired through the gradual adjustment of the strengths of these connections in the course of day-to-day experience. The authors show how a simple computational model proposed by Rumelhart exhibits a progressive differentiation of conceptual knowledge, paralleling aspects of cognitive development seen in the work of Frank Keil and Jean Mandler. The authors extend the model to address aspects of conceptual knowledge acquisition in infancy, disintegration of conceptual knowledge in dementia, \"basic-level\" effects and their interaction with expertise, and many findings introduced to support the idea that semantic cognition is guided by naive, domain-specific theories."
            },
            "slug": "Semantic-Cognition:-A-Parallel-Distributed-Approach-Rogers-McClelland",
            "title": {
                "fragments": [],
                "text": "Semantic Cognition: A Parallel Distributed Processing Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The authors propose that performance in semantic tasks arises through the propagation of graded signals in a system of interconnected processing units, and show how a simple computational model proposed by Rumelhart exhibits a progressive differentiation of conceptual knowledge, paralleling aspects of cognitive development seen in the work of Frank Keil and Jean Mandler."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153677280"
                        ],
                        "name": "Robik Shrestha",
                        "slug": "Robik-Shrestha",
                        "structuredName": {
                            "firstName": "Robik",
                            "lastName": "Shrestha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robik Shrestha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33315685"
                        ],
                        "name": "Kushal Kafle",
                        "slug": "Kushal-Kafle",
                        "structuredName": {
                            "firstName": "Kushal",
                            "lastName": "Kafle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kushal Kafle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3290098"
                        ],
                        "name": "Christopher Kanan",
                        "slug": "Christopher-Kanan",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Kanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Kanan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 67855410,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d9344534ab39544a3a3c173b27628e0d9c5d4dc5",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual Question Answering (VQA) research is split into two camps: the first focuses on VQA datasets that require natural image understanding and the second focuses on synthetic datasets that test reasoning. A good VQA algorithm should be capable of both, but only a few VQA algorithms are tested in this manner. We compare five state-of-the-art VQA algorithms across eight VQA datasets covering both domains. To make the comparison fair, all of the models are standardized as much as possible, e.g., they use the same visual features, answer vocabularies, etc. We find that methods do not generalize across the two domains. To address this problem, we propose a new VQA algorithm that rivals or exceeds the state-of-the-art for both domains."
            },
            "slug": "Answer-Them-All!-Toward-Universal-Visual-Question-Shrestha-Kafle",
            "title": {
                "fragments": [],
                "text": "Answer Them All! Toward Universal Visual Question Answering Models"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new VQA algorithm is proposed that rivals or exceeds the state-of-the-art for both domains and uses the same visual features, answer vocabularies, etc."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114531657"
                        ],
                        "name": "Noam Chomsky",
                        "slug": "Noam-Chomsky",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Chomsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noam Chomsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 256
                            }
                        ],
                        "text": "The emergence of a compositional system of symbols that can distill and convey from rich sensory experiences to creative new ideas has been a major turning point in the evolution of intelligence, and made a profound impact on the nature of human cognition [19, 78, 13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1775772,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "eea61d076bb155b822dcbc7077563ccce719a0c6",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "There is substantial evidence that the human language capacity (LC) is a species-specific biological property, essentially unique to humans, invariant among human groups, and dissociated from other cognitive systems. Each language, an instantiation of LC, consists of a generative procedure that yields a discrete infinity of hierarchically structured expressions with semantic interpretations, hence a kind of \u201clanguage of thought\u201d (LOT), along with an operation of externalization (EXT) to some sensory-motor system, typically sound. There is mounting evidence that generation of LOT observes language-independent principles of computational efficiency and is based on the simplest computational operations, and that EXT is an ancillary process not entering into the core semantic properties of LOT and is the primary locus of the apparent complexity, diversity, and mutability of language. These conclusions are not surprising, since the internal system is acquired virtually without evidence in fundamental respects, and EXT relates it to sensory-motor systems that are unrelated to it. Even such properties as the linear order of words appear to be reflexes of the sensory motor system, not available to generation of LOT. The limited evidence from the evolutionary record lends support to these conclusions, suggesting that LC emerged with Homo sapiens or not long after, and has not evolved since human groups dispersed."
            },
            "slug": "The-language-capacity:-architecture-and-evolution-Chomsky",
            "title": {
                "fragments": [],
                "text": "The language capacity: architecture and evolution"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The limited evidence from the evolutionary record lends support to these conclusions, suggesting that LC emerged with Homo sapiens or not long after, and has not evolved since human groups dispersed."
            },
            "venue": {
                "fragments": [],
                "text": "Psychonomic bulletin & review"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2801949"
                        ],
                        "name": "Aishwarya Agrawal",
                        "slug": "Aishwarya-Agrawal",
                        "structuredName": {
                            "firstName": "Aishwarya",
                            "lastName": "Agrawal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aishwarya Agrawal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8553015"
                        ],
                        "name": "Jiasen Lu",
                        "slug": "Jiasen-Lu",
                        "structuredName": {
                            "firstName": "Jiasen",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiasen Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1963421"
                        ],
                        "name": "Stanislaw Antol",
                        "slug": "Stanislaw-Antol",
                        "structuredName": {
                            "firstName": "Stanislaw",
                            "lastName": "Antol",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stanislaw Antol"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067793408"
                        ],
                        "name": "Margaret Mitchell",
                        "slug": "Margaret-Mitchell",
                        "structuredName": {
                            "firstName": "Margaret",
                            "lastName": "Mitchell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Margaret Mitchell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153432684"
                        ],
                        "name": "Devi Parikh",
                        "slug": "Devi-Parikh",
                        "structuredName": {
                            "firstName": "Devi",
                            "lastName": "Parikh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Devi Parikh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746610"
                        ],
                        "name": "Dhruv Batra",
                        "slug": "Dhruv-Batra",
                        "structuredName": {
                            "firstName": "Dhruv",
                            "lastName": "Batra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dhruv Batra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 286,
                                "start": 279
                            }
                        ],
                        "text": "We demonstrate the value and performance of the Neural State Machine on two recent Visual Question Answering (VQA) datasets: GQA [41] which focuses on real-world visual reasoning and multi-step question answering, as well as VQA-CP [3], a recent split of the popular VQA dataset [2, 27] that has been designed particularly to evaluate generalization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3180429,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db",
            "isKey": false,
            "numCitedBy": 2887,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing $$\\sim $$\u223c0.25\u00a0M images, $$\\sim $$\u223c0.76\u00a0M questions, and $$\\sim $$\u223c10\u00a0M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines and methods for VQA are provided and compared with human performance. Our VQA demo is available on CloudCV (http://cloudcv.org/vqa)."
            },
            "slug": "VQA:-Visual-Question-Answering-Agrawal-Lu",
            "title": {
                "fragments": [],
                "text": "VQA: Visual Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "The task of free-form and open-ended Visual Question Answering (VQA) is proposed, given an image and a natural language question about the image, the task is to provide an accurate natural language answer."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8553015"
                        ],
                        "name": "Jiasen Lu",
                        "slug": "Jiasen-Lu",
                        "structuredName": {
                            "firstName": "Jiasen",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiasen Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2228109"
                        ],
                        "name": "Caiming Xiong",
                        "slug": "Caiming-Xiong",
                        "structuredName": {
                            "firstName": "Caiming",
                            "lastName": "Xiong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Caiming Xiong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153432684"
                        ],
                        "name": "Devi Parikh",
                        "slug": "Devi-Parikh",
                        "structuredName": {
                            "firstName": "Devi",
                            "lastName": "Parikh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Devi Parikh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18347865,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f4d7d622d1f7319cc511bfef661cd973e881a4c",
            "isKey": false,
            "numCitedBy": 961,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Attention-based neural encoder-decoder frameworks have been widely adopted for image captioning. Most methods force visual attention to be active for every generated word. However, the decoder likely requires little to no visual information from the image to predict non-visual words such as the and of. Other words that may seem visual can often be predicted reliably just from the language model e.g., sign after behind a red stop or phone following talking on a cell. In this paper, we propose a novel adaptive attention model with a visual sentinel. At each time step, our model decides whether to attend to the image (and if so, to which regions) or to the visual sentinel. The model decides whether to attend to the image and where, in order to extract meaningful information for sequential word generation. We test our method on the COCO image captioning 2015 challenge dataset and Flickr30K. Our approach sets the new state-of-the-art by a significant margin."
            },
            "slug": "Knowing-When-to-Look:-Adaptive-Attention-via-a-for-Lu-Xiong",
            "title": {
                "fragments": [],
                "text": "Knowing When to Look: Adaptive Attention via a Visual Sentinel for Image Captioning"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper proposes a novel adaptive attention model with a visual sentinel that sets the new state-of-the-art by a significant margin on image captioning."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48607963"
                        ],
                        "name": "Yonghui Wu",
                        "slug": "Yonghui-Wu",
                        "structuredName": {
                            "firstName": "Yonghui",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yonghui Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144927151"
                        ],
                        "name": "M. Schuster",
                        "slug": "M.-Schuster",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Schuster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schuster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2545358"
                        ],
                        "name": "Z. Chen",
                        "slug": "Z.-Chen",
                        "structuredName": {
                            "firstName": "Z.",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739074"
                        ],
                        "name": "Mohammad Norouzi",
                        "slug": "Mohammad-Norouzi",
                        "structuredName": {
                            "firstName": "Mohammad",
                            "lastName": "Norouzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohammad Norouzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3153147"
                        ],
                        "name": "Wolfgang Macherey",
                        "slug": "Wolfgang-Macherey",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Macherey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wolfgang Macherey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2048712"
                        ],
                        "name": "M. Krikun",
                        "slug": "M.-Krikun",
                        "structuredName": {
                            "firstName": "Maxim",
                            "lastName": "Krikun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Krikun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145144022"
                        ],
                        "name": "Yuan Cao",
                        "slug": "Yuan-Cao",
                        "structuredName": {
                            "firstName": "Yuan",
                            "lastName": "Cao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuan Cao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145312180"
                        ],
                        "name": "Qin Gao",
                        "slug": "Qin-Gao",
                        "structuredName": {
                            "firstName": "Qin",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qin Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113439369"
                        ],
                        "name": "Klaus Macherey",
                        "slug": "Klaus-Macherey",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Macherey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Klaus Macherey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2367620"
                        ],
                        "name": "J. Klingner",
                        "slug": "J.-Klingner",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Klingner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Klingner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145825976"
                        ],
                        "name": "Apurva Shah",
                        "slug": "Apurva-Shah",
                        "structuredName": {
                            "firstName": "Apurva",
                            "lastName": "Shah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Apurva Shah"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145657834"
                        ],
                        "name": "Melvin Johnson",
                        "slug": "Melvin-Johnson",
                        "structuredName": {
                            "firstName": "Melvin",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Melvin Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109059862"
                        ],
                        "name": "Xiaobing Liu",
                        "slug": "Xiaobing-Liu",
                        "structuredName": {
                            "firstName": "Xiaobing",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaobing Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40527594"
                        ],
                        "name": "Lukasz Kaiser",
                        "slug": "Lukasz-Kaiser",
                        "structuredName": {
                            "firstName": "Lukasz",
                            "lastName": "Kaiser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lukasz Kaiser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2776283"
                        ],
                        "name": "Stephan Gouws",
                        "slug": "Stephan-Gouws",
                        "structuredName": {
                            "firstName": "Stephan",
                            "lastName": "Gouws",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephan Gouws"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2739610"
                        ],
                        "name": "Y. Kato",
                        "slug": "Y.-Kato",
                        "structuredName": {
                            "firstName": "Yoshikiyo",
                            "lastName": "Kato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Kato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765329"
                        ],
                        "name": "Taku Kudo",
                        "slug": "Taku-Kudo",
                        "structuredName": {
                            "firstName": "Taku",
                            "lastName": "Kudo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Taku Kudo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754386"
                        ],
                        "name": "H. Kazawa",
                        "slug": "H.-Kazawa",
                        "structuredName": {
                            "firstName": "Hideto",
                            "lastName": "Kazawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kazawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144077726"
                        ],
                        "name": "K. Stevens",
                        "slug": "K.-Stevens",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Stevens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Stevens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753079661"
                        ],
                        "name": "George Kurian",
                        "slug": "George-Kurian",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Kurian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George Kurian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056800684"
                        ],
                        "name": "Nishant Patil",
                        "slug": "Nishant-Patil",
                        "structuredName": {
                            "firstName": "Nishant",
                            "lastName": "Patil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nishant Patil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49337181"
                        ],
                        "name": "Wei Wang",
                        "slug": "Wei-Wang",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39660914"
                        ],
                        "name": "C. Young",
                        "slug": "C.-Young",
                        "structuredName": {
                            "firstName": "Cliff",
                            "lastName": "Young",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Young"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2119125158"
                        ],
                        "name": "Jason R. Smith",
                        "slug": "Jason-R.-Smith",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Smith",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason R. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2909504"
                        ],
                        "name": "Jason Riesa",
                        "slug": "Jason-Riesa",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Riesa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason Riesa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29951847"
                        ],
                        "name": "Alex Rudnick",
                        "slug": "Alex-Rudnick",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Rudnick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Rudnick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32131713"
                        ],
                        "name": "G. Corrado",
                        "slug": "G.-Corrado",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Corrado",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Corrado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48342565"
                        ],
                        "name": "Macduff Hughes",
                        "slug": "Macduff-Hughes",
                        "structuredName": {
                            "firstName": "Macduff",
                            "lastName": "Hughes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Macduff Hughes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49959210"
                        ],
                        "name": "J. Dean",
                        "slug": "J.-Dean",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Dean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dean"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 248
                            }
                        ],
                        "text": "Ideas about compositionality, abstraction and reasoning greatly inspired the classical views of artificial intelligence [74, 65], but have lately been overshadowed by the astounding success of deep learning over a wide spectrum of real-world tasks [33, 63, 82]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3603249,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd",
            "isKey": false,
            "numCitedBy": 4645,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (\"wordpieces\") for both input and output. This method provides a good balance between the flexibility of \"character\"-delimited models and the efficiency of \"word\"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system."
            },
            "slug": "Google's-Neural-Machine-Translation-System:-the-Gap-Wu-Schuster",
            "title": {
                "fragments": [],
                "text": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "GNMT, Google's Neural Machine Translation system, is presented, which attempts to address many of the weaknesses of conventional phrase-based translation systems and provides a good balance between the flexibility of \"character\"-delimited models and the efficiency of \"word\"-delicited models."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116515859"
                        ],
                        "name": "Jin-Hwa Kim",
                        "slug": "Jin-Hwa-Kim",
                        "structuredName": {
                            "firstName": "Jin-Hwa",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jin-Hwa Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29818400"
                        ],
                        "name": "Jaehyun Jun",
                        "slug": "Jaehyun-Jun",
                        "structuredName": {
                            "firstName": "Jaehyun",
                            "lastName": "Jun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jaehyun Jun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692756"
                        ],
                        "name": "Byoung-Tak Zhang",
                        "slug": "Byoung-Tak-Zhang",
                        "structuredName": {
                            "firstName": "Byoung-Tak",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Byoung-Tak Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "21 BAN [46] 39."
                    },
                    "intents": []
                }
            ],
            "corpusId": 29150617,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5d10341717c0519cf63151b496a6d2ed67aa05f",
            "isKey": false,
            "numCitedBy": 408,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Attention networks in multimodal learning provide an efficient way to utilize given visual information selectively. However, the computational cost to learn attention distributions for every pair of multimodal input channels is prohibitively expensive. To solve this problem, co-attention builds two separate attention distributions for each modality neglecting the interaction between multimodal inputs. In this paper, we propose bilinear attention networks (BAN) that find bilinear attention distributions to utilize given vision-language information seamlessly. BAN considers bilinear interactions among two groups of input channels, while low-rank bilinear pooling extracts the joint representations for each pair of channels. Furthermore, we propose a variant of multimodal residual networks to exploit eight-attention maps of the BAN efficiently. We quantitatively and qualitatively evaluate our model on visual question answering (VQA 2.0) and Flickr30k Entities datasets, showing that BAN significantly outperforms previous methods and achieves new state-of-the-arts on both datasets."
            },
            "slug": "Bilinear-Attention-Networks-Kim-Jun",
            "title": {
                "fragments": [],
                "text": "Bilinear Attention Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "BAN is proposed that find bilinear attention distributions to utilize given vision-language information seamlessly and quantitatively and qualitatively evaluates the model on visual question answering and Flickr30k Entities datasets, showing that BAN significantly outperforms previous methods and achieves new state-of-the-arts on both datasets."
            },
            "venue": {
                "fragments": [],
                "text": "NeurIPS"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145478807"
                        ],
                        "name": "Mateusz Malinowski",
                        "slug": "Mateusz-Malinowski",
                        "structuredName": {
                            "firstName": "Mateusz",
                            "lastName": "Malinowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mateusz Malinowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2786693"
                        ],
                        "name": "Carl Doersch",
                        "slug": "Carl-Doersch",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Doersch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carl Doersch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35030998"
                        ],
                        "name": "Adam Santoro",
                        "slug": "Adam-Santoro",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Santoro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Santoro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2019153"
                        ],
                        "name": "P. Battaglia",
                        "slug": "P.-Battaglia",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Battaglia",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Battaglia"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "e 2: GQA ensemble Model Accuracy Kakao* 73.33 270 70.23 NSM67.25 LXRT 62.71 GRN 61.22 MSM 61.09 DREAM 60.93 SK T-Brain* 60.87 PKU 60.79 Musan 59.93 Table 3: VQA-CPv2 Model Accuracy SAN [86] 24.96 HAN [59] 28.65 GVQA [3] 31.30 RAMEN [73] 39.21 BAN [46] 39.31 MuRel [15] 39.54 ReGAT [52] 40.42 NSM 45.80 Table 4: GQA generalization Model Content Structure Global Prior 8.51 14.64 Local Prior 12.14 18.21 Vi"
                    },
                    "intents": []
                }
            ],
            "corpusId": 51895329,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "afe3a0d463e2f099305c745ddbf943844583795d",
            "isKey": true,
            "numCitedBy": 74,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "Attention mechanisms in biological perception are thought to select subsets of perceptual information for more sophisticated processing which would be prohibitive to perform on all sensory inputs. In computer vision, however, there has been relatively little exploration of hard attention, where some information is selectively ignored, in spite of the success of soft attention, where information is re-weighted and aggregated, but never filtered out. Here, we introduce a new approach for hard attention and find it achieves very competitive performance on a recently-released visual question answering datasets, equalling and in some cases surpassing similar soft attention architectures while entirely ignoring some features. Even though the hard attention mechanism is thought to be non-differentiable, we found that the feature magnitudes correlate with semantic relevance, and provide a useful signal for our mechanism\u2019s attentional selection criterion. Because hard attention selects important features of the input information, it can also be more efficient than analogous soft attention mechanisms. This is especially important for recent approaches that use non-local pairwise operations, whereby computational and memory costs are quadratic in the size of the set of features."
            },
            "slug": "Learning-Visual-Question-Answering-by-Bootstrapping-Malinowski-Doersch",
            "title": {
                "fragments": [],
                "text": "Learning Visual Question Answering by Bootstrapping Hard Attention"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work introduces a new approach for hard attention and finds it achieves very competitive performance on a recently-released visual question answering datasets, equalling and in some cases surpassing similar soft attention architectures while entirely ignoring some features."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144702900"
                        ],
                        "name": "Zhe Gan",
                        "slug": "Zhe-Gan",
                        "structuredName": {
                            "firstName": "Zhe",
                            "lastName": "Gan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhe Gan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144158271"
                        ],
                        "name": "Chuang Gan",
                        "slug": "Chuang-Gan",
                        "structuredName": {
                            "firstName": "Chuang",
                            "lastName": "Gan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chuang Gan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144137069"
                        ],
                        "name": "Xiaodong He",
                        "slug": "Xiaodong-He",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2750075"
                        ],
                        "name": "Yunchen Pu",
                        "slug": "Yunchen-Pu",
                        "structuredName": {
                            "firstName": "Yunchen",
                            "lastName": "Pu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yunchen Pu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069509852"
                        ],
                        "name": "Kenneth Tran",
                        "slug": "Kenneth-Tran",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Tran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Tran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800422"
                        ],
                        "name": "Jianfeng Gao",
                        "slug": "Jianfeng-Gao",
                        "structuredName": {
                            "firstName": "Jianfeng",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianfeng Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145006560"
                        ],
                        "name": "L. Carin",
                        "slug": "L.-Carin",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Carin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Carin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11045175,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "778ce81457383bd5e3fdb11b145ded202ebb4970",
            "isKey": false,
            "numCitedBy": 308,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "A Semantic Compositional Network (SCN) is developed for image captioning, in which semantic concepts (i.e., tags) are detected from the image, and the probability of each tag is used to compose the parameters in a long short-term memory (LSTM) network. The SCN extends each weight matrix of the LSTM to an ensemble of tag-dependent weight matrices. The degree to which each member of the ensemble is used to generate an image caption is tied to the image-dependent probability of the corresponding tag. In addition to captioning images, we also extend the SCN to generate captions for video clips. We qualitatively analyze semantic composition in SCNs, and quantitatively evaluate the algorithm on three benchmark datasets: COCO, Flickr30k, and Youtube2Text. Experimental results show that the proposed method significantly outperforms prior state-of-the-art approaches, across multiple evaluation metrics."
            },
            "slug": "Semantic-Compositional-Networks-for-Visual-Gan-Gan",
            "title": {
                "fragments": [],
                "text": "Semantic Compositional Networks for Visual Captioning"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Experimental results show that the proposed method significantly outperforms prior state-of-the-art approaches, across multiple evaluation metrics."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2313517"
                        ],
                        "name": "Abhishek Das",
                        "slug": "Abhishek-Das",
                        "structuredName": {
                            "firstName": "Abhishek",
                            "lastName": "Das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abhishek Das"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37825612"
                        ],
                        "name": "Harsh Agrawal",
                        "slug": "Harsh-Agrawal",
                        "structuredName": {
                            "firstName": "Harsh",
                            "lastName": "Agrawal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Harsh Agrawal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153432684"
                        ],
                        "name": "Devi Parikh",
                        "slug": "Devi-Parikh",
                        "structuredName": {
                            "firstName": "Devi",
                            "lastName": "Parikh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Devi Parikh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746610"
                        ],
                        "name": "Dhruv Batra",
                        "slug": "Dhruv-Batra",
                        "structuredName": {
                            "firstName": "Dhruv",
                            "lastName": "Batra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dhruv Batra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 463,
                                "start": 455
                            }
                        ],
                        "text": "Yet, even though neural networks are undoubtedly powerful, flexible and robust, recent work has repeatedly demonstrated their flaws, showing how they struggle to generalize in a systematic manner [48], overly adhere to superficial and potentially misleading statistical associations instead of learning true causal relations [1, 40], strongly depend on large amounts of data and supervision [23, 49], and sometimes behave in surprising and worrisome ways [24, 19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 220553,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "58cb0c24c936b8a14ca7b2d56ba80de733c545b3",
            "isKey": false,
            "numCitedBy": 338,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "We conduct large-scale studies on `human attention' in Visual Question Answering (VQA) to understand where humans choose to look to answer questions about images. We design and test multiple game-inspired novel attention-annotation interfaces that require the subject to sharpen regions of a blurred image to answer a question. Thus, we introduce the VQA-HAT (Human ATtention) dataset. We evaluate attention maps generated by state-of-the-art VQA models against human attention both qualitatively (via visualizations) and quantitatively (via rank-order correlation). Overall, our experiments show that current attention models in VQA do not seem to be looking at the same regions as humans."
            },
            "slug": "Human-Attention-in-Visual-Question-Answering:-Do-at-Das-Agrawal",
            "title": {
                "fragments": [],
                "text": "Human Attention in Visual Question Answering: Do Humans and Deep Networks look at the same regions?"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The VQA-HAT (Human ATtention) dataset is introduced and attention maps generated by state-of-the-art V QA models are evaluated against human attention both qualitatively and quantitatively."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586725"
                        ],
                        "name": "M. Forcada",
                        "slug": "M.-Forcada",
                        "structuredName": {
                            "firstName": "Mikel",
                            "lastName": "Forcada",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Forcada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798578"
                        ],
                        "name": "Rafael C. Carrasco",
                        "slug": "Rafael-C.-Carrasco",
                        "structuredName": {
                            "firstName": "Rafael",
                            "lastName": "Carrasco",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rafael C. Carrasco"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11029273,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b76f51afcd423e5ee9b7ee0519153331c5b68f8",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Finite-state machines are the most pervasive models of computation, not only in theoretical computer science, but also in all of its applications to real-life problems, and constitute the best characterized computational model. On the other hand, neural networks --proposed almost sixty years ago by McCulloch and Pitts as a simplified model of nervous activity in living beings-- have evolved into a great variety of so-called artificial neural networks. Artificial neural networks have become a very successful tool for modelling and problem solving because of their built-in learning capability, but most of the progress in this field has occurred with models that are very removed from the behaviour of real, i.e., biological neural networks. This paper surveys the work that has established a connection between finite-state machines and (mainly discrete-time recurrent) neural networks, and suggests possible ways to construct finite-state models in biologically plausible neural networks."
            },
            "slug": "Finite-State-Computation-in-Analog-Neural-Networks:-Forcada-Carrasco",
            "title": {
                "fragments": [],
                "text": "Finite-State Computation in Analog Neural Networks: Steps towards Biologically Plausible Models?"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The work that has established a connection between finite-state machines and (mainly discrete-time recurrent) neural networks is surveyed, and possible ways to construct finite- state models in biologically plausible neural networks are suggested."
            },
            "venue": {
                "fragments": [],
                "text": "Emergent Neural Computational Architectures Based on Neuroscience"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143787583"
                        ],
                        "name": "Ali Farhadi",
                        "slug": "Ali-Farhadi",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Farhadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ali Farhadi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2831988"
                        ],
                        "name": "Ian Endres",
                        "slug": "Ian-Endres",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Endres",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian Endres"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2433269"
                        ],
                        "name": "Derek Hoiem",
                        "slug": "Derek-Hoiem",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Hoiem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Derek Hoiem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 176
                            }
                        ],
                        "text": "Several works have explored the discovery and use of visual concepts in the contexts of reinforcement or unsupervised learning [35, 17] as well as in classical computer vision [21, 77]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14940757,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6a8aef1bf134294482d8088f982d5643347d2ff",
            "isKey": false,
            "numCitedBy": 1665,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose to shift the goal of recognition from naming to describing. Doing so allows us not only to name familiar objects, but also: to report unusual aspects of a familiar object (\u201cspotty dog\u201d, not just \u201cdog\u201d); to say something about unfamiliar objects (\u201chairy and four-legged\u201d, not just \u201cunknown\u201d); and to learn how to recognize new objects with few or no visual examples. Rather than focusing on identity assignment, we make inferring attributes the core problem of recognition. These attributes can be semantic (\u201cspotty\u201d) or discriminative (\u201cdogs have it but sheep do not\u201d). Learning attributes presents a major new challenge: generalization across object categories, not just across instances within a category. In this paper, we also introduce a novel feature selection method for learning attributes that generalize well across categories. We support our claims by thorough evaluation that provides insights into the limitations of the standard recognition paradigm of naming and demonstrates the new abilities provided by our attribute-based framework."
            },
            "slug": "Describing-objects-by-their-attributes-Farhadi-Endres",
            "title": {
                "fragments": [],
                "text": "Describing objects by their attributes"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This paper proposes to shift the goal of recognition from naming to describing, and introduces a novel feature selection method for learning attributes that generalize well across categories."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143845796"
                        ],
                        "name": "Jeffrey Pennington",
                        "slug": "Jeffrey-Pennington",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Pennington",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey Pennington"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 101
                            }
                        ],
                        "text": "Both hidden states and word vectors have a dimension size of 300, the latter being initialized using GloVe [68]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 51
                            }
                        ],
                        "text": "We begin by embedding all the question words using GloVe (dimension d = 300)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 228
                            }
                        ],
                        "text": "We define a vocabulary C of 1335 embedded concepts about object types C0 (e.g. cat, shirt), attributes, grouped into L types {Ci}Li=1 (e.g. colors, materials), and relations CL+1 (e.g. holding, on top of ), all initialized with GloVe [68]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 214
                            }
                        ],
                        "text": "Next, we translate each word into a concept-based representation:\nvi = Pi(c \u2032)wi + \u2211 c\u2208C\\{c\u2032} Pi(c)c\nIntuitively, a content word such as apples will be considered mostly similar to the concept apple (by comparing their GloVe embeddings), and thus will be replaced by the embedding of that term, whereas function words such as who, are, how will be deemed less similar to the semantic concepts and hence will stay close to their original embedding."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 241
                            }
                        ],
                        "text": "All hyperparameters were tuned manually (from the following ranges: learning rate [5\u00b710\u22125, 10\u22124, 5\u00b710\u22124, 10\u22123], batch size [32, 64, 128], dropout [0.08, 0.1, 0.12, 0.15, 0.18, 0.2] and hidden and word dimensions [50, 100, 200, 300] to comply with GloVe provided sizes)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 83
                            }
                        ],
                        "text": "We create an embedded concept vocabulary C for the machine (initialized with GloVe [68]), that will be used to capture and represent the semantic content of input images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1957433,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f37e1b62a767a307c046404ca96bc140b3e68cb5",
            "isKey": false,
            "numCitedBy": 22534,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition."
            },
            "slug": "GloVe:-Global-Vectors-for-Word-Representation-Pennington-Socher",
            "title": {
                "fragments": [],
                "text": "GloVe: Global Vectors for Word Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods and produces a vector space with meaningful substructure."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143622869"
                        ],
                        "name": "Alexander H. Miller",
                        "slug": "Alexander-H.-Miller",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Miller",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander H. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064150446"
                        ],
                        "name": "Adam Fisch",
                        "slug": "Adam-Fisch",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Fisch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Fisch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34176020"
                        ],
                        "name": "Jesse Dodge",
                        "slug": "Jesse-Dodge",
                        "structuredName": {
                            "firstName": "Jesse",
                            "lastName": "Dodge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jesse Dodge"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145926563"
                        ],
                        "name": "Amir-Hossein Karimi",
                        "slug": "Amir-Hossein-Karimi",
                        "structuredName": {
                            "firstName": "Amir-Hossein",
                            "lastName": "Karimi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amir-Hossein Karimi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713934"
                        ],
                        "name": "Antoine Bordes",
                        "slug": "Antoine-Bordes",
                        "structuredName": {
                            "firstName": "Antoine",
                            "lastName": "Bordes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antoine Bordes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 152
                            }
                        ],
                        "text": "Our model connects to multiple lines of research, including works about compositionality [14, 38], concept acquisition [36, 81], and neural computation [28, 62, 7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2711679,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bba5f2852b1db8a18004eb7328efa5e1d57cc62a",
            "isKey": false,
            "numCitedBy": 648,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Directly reading documents and being able to answer questions from them is an unsolved challenge. To avoid its inherent difficulty, question answering (QA) has been directed towards using Knowledge Bases (KBs) instead, which has proven effective. Unfortunately KBs often suffer from being too restrictive, as the schema cannot support certain types of answers, and too sparse, e.g. Wikipedia contains much more information than Freebase. In this work we introduce a new method, Key-Value Memory Networks, that makes reading documents more viable by utilizing different encodings in the addressing and output stages of the memory read operation. To compare using KBs, information extraction or Wikipedia documents directly in a single framework we construct an analysis tool, WikiMovies, a QA dataset that contains raw text alongside a preprocessed KB, in the domain of movies. Our method reduces the gap between all three settings. It also achieves state-of-the-art results on the existing WikiQA benchmark."
            },
            "slug": "Key-Value-Memory-Networks-for-Directly-Reading-Miller-Fisch",
            "title": {
                "fragments": [],
                "text": "Key-Value Memory Networks for Directly Reading Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work introduces a new method, Key-Value Memory Networks, that makes reading documents more viable by utilizing different encodings in the addressing and output stages of the memory read operation."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110468365"
                        ],
                        "name": "Yikang Li",
                        "slug": "Yikang-Li",
                        "structuredName": {
                            "firstName": "Yikang",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yikang Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3001348"
                        ],
                        "name": "Wanli Ouyang",
                        "slug": "Wanli-Ouyang",
                        "structuredName": {
                            "firstName": "Wanli",
                            "lastName": "Ouyang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wanli Ouyang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145291669"
                        ],
                        "name": "Bolei Zhou",
                        "slug": "Bolei-Zhou",
                        "structuredName": {
                            "firstName": "Bolei",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bolei Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9349527"
                        ],
                        "name": "Yawen Cui",
                        "slug": "Yawen-Cui",
                        "structuredName": {
                            "firstName": "Yawen",
                            "lastName": "Cui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yawen Cui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788070"
                        ],
                        "name": "Jianping Shi",
                        "slug": "Jianping-Shi",
                        "structuredName": {
                            "firstName": "Jianping",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianping Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31843833"
                        ],
                        "name": "Xiaogang Wang",
                        "slug": "Xiaogang-Wang",
                        "structuredName": {
                            "firstName": "Xiaogang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaogang Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 49554331,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "acfe5b5c99be70fa3120d410e7be55b9fe299f40",
            "isKey": false,
            "numCitedBy": 184,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "Generating scene graph to describe the object interactions inside an image gains increasing interests these years. However, most of the previous methods use complicated structures with slow inference speed or rely on the external data, which limits the usage of the model in real-life scenarios. To improve the efficiency of scene graph generation, we propose a subgraph-based connection graph to concisely represent the scene graph during the inference. A bottom-up clustering method is first used to factorize the entire graph into subgraphs, where each subgraph contains several objects and a subset of their relationships. By replacing the numerous relationship representations of the scene graph with fewer subgraph and object features, the computation in the intermediate stage is significantly reduced. In addition, spatial information is maintained by the subgraph features, which is leveraged by our proposed Spatial-weighted Message Passing (SMP) structure and Spatial-sensitive Relation Inference (SRI) module to facilitate the relationship recognition. On the recent Visual Relationship Detection and Visual Genome datasets, our method outperforms the state-of-the-art method in both accuracy and speed. Code has been made publicly available (https://github.com/yikang-li/FactorizableNet)."
            },
            "slug": "Factorizable-Net:-An-Efficient-Subgraph-based-for-Li-Ouyang",
            "title": {
                "fragments": [],
                "text": "Factorizable Net: An Efficient Subgraph-based Framework for Scene Graph Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A subgraph-based connection graph is proposed to concisely represent the scene graph during the inference to improve the efficiency of scene graph generation and outperforms the state-of-the-art method in both accuracy and speed."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50178628"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Akshay",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 65
                            }
                        ],
                        "text": "We explore our model in the context of Visual Question Answering [29], a challenging multi-modal task that has gained substantial attention over the last years."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 83
                            }
                        ],
                        "text": "We demonstrate the value and performance of the Neural State Machine on two recent Visual Question Answering (VQA) datasets: GQA [41] which focuses on real-world visual reasoning and multi-step question answering, as well as VQA-CP [3], a recent split of the popular VQA dataset [2, 27] that has been designed particularly to evaluate generalization."
                    },
                    "intents": []
                }
            ],
            "corpusId": 28465771,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "637648198f9e91654ce27eaaa40512f2dc870fc1",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual question answering (or VQA) is a new and exciting problem that combines natural language processing and computer vision techniques. We present a survey of the various datasets and models that have been used to tackle this task. The first part of the survey details the various datasets for VQA and compares them along some common factors. The second part of this survey details the different approaches for VQA, classified into four types: non-deep learning models, deep learning models without attention, deep learning models with attention, and other models which do not fit into the first three. Finally, we compare the performances of these approaches and provide some directions for future work."
            },
            "slug": "Survey-of-Visual-Question-Answering:-Datasets-and-Gupta",
            "title": {
                "fragments": [],
                "text": "Survey of Visual Question Answering: Datasets and Techniques"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A survey of the various datasets and models that have been used to tackle visual question answering, classified into four types: non-deep learning models, deep learning models without attention,Deep learning models with attention, and other models which do not fit into the first three."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153440022"
                        ],
                        "name": "Ian J. Goodfellow",
                        "slug": "Ian-J.-Goodfellow",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Goodfellow",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian J. Goodfellow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789737"
                        ],
                        "name": "Jonathon Shlens",
                        "slug": "Jonathon-Shlens",
                        "structuredName": {
                            "firstName": "Jonathon",
                            "lastName": "Shlens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathon Shlens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574060"
                        ],
                        "name": "Christian Szegedy",
                        "slug": "Christian-Szegedy",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Szegedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Szegedy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 463,
                                "start": 455
                            }
                        ],
                        "text": "Yet, even though neural networks are undoubtedly powerful, flexible and robust, recent work has repeatedly demonstrated their flaws, showing how they struggle to generalize in a systematic manner [48], overly adhere to superficial and potentially misleading statistical associations instead of learning true causal relations [1, 40], strongly depend on large amounts of data and supervision [23, 49], and sometimes behave in surprising and worrisome ways [24, 19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6706414,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bee044c8e8903fb67523c1f8c105ab4718600cdb",
            "isKey": false,
            "numCitedBy": 10104,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset."
            },
            "slug": "Explaining-and-Harnessing-Adversarial-Examples-Goodfellow-Shlens",
            "title": {
                "fragments": [],
                "text": "Explaining and Harnessing Adversarial Examples"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is argued that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature, supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771551"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 248
                            }
                        ],
                        "text": "Ideas about compositionality, abstraction and reasoning greatly inspired the classical views of artificial intelligence [74, 65], but have lately been overshadowed by the astounding success of deep learning over a wide spectrum of real-world tasks [33, 63, 82]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206594692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "isKey": false,
            "numCitedBy": 95314,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
            },
            "slug": "Deep-Residual-Learning-for-Image-Recognition-He-Zhang",
            "title": {
                "fragments": [],
                "text": "Deep Residual Learning for Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work presents a residual learning framework to ease the training of networks that are substantially deeper than those used previously, and provides comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33493200"
                        ],
                        "name": "Tsung-Yi Lin",
                        "slug": "Tsung-Yi-Lin",
                        "structuredName": {
                            "firstName": "Tsung-Yi",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tsung-Yi Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790580"
                        ],
                        "name": "Bharath Hariharan",
                        "slug": "Bharath-Hariharan",
                        "structuredName": {
                            "firstName": "Bharath",
                            "lastName": "Hariharan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bharath Hariharan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10716717,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9b4e05faa194e5022edd9eb9dd07e3d675c2b36",
            "isKey": false,
            "numCitedBy": 9352,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But pyramid representations have been avoided in recent object detectors that are based on deep convolutional networks, partially because they are slow to compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available."
            },
            "slug": "Feature-Pyramid-Networks-for-Object-Detection-Lin-Doll\u00e1r",
            "title": {
                "fragments": [],
                "text": "Feature Pyramid Networks for Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper exploits the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost and achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6965856"
                        ],
                        "name": "Peter Anderson",
                        "slug": "Peter-Anderson",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Anderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Anderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144137069"
                        ],
                        "name": "Xiaodong He",
                        "slug": "Xiaodong-He",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31790073"
                        ],
                        "name": "Chris Buehler",
                        "slug": "Chris-Buehler",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Buehler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Buehler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2406263"
                        ],
                        "name": "Damien Teney",
                        "slug": "Damien-Teney",
                        "structuredName": {
                            "firstName": "Damien",
                            "lastName": "Teney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Damien Teney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145177220"
                        ],
                        "name": "Mark Johnson",
                        "slug": "Mark-Johnson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145273587"
                        ],
                        "name": "Stephen Gould",
                        "slug": "Stephen-Gould",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Gould",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen Gould"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39089563"
                        ],
                        "name": "Lei Zhang",
                        "slug": "Lei-Zhang",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3753452,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8",
            "isKey": false,
            "numCitedBy": 2275,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge."
            },
            "slug": "Bottom-Up-and-Top-Down-Attention-for-Image-and-Anderson-He",
            "title": {
                "fragments": [],
                "text": "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions is proposed, demonstrating the broad applicability of this approach to VQA."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726807"
                        ],
                        "name": "Diederik P. Kingma",
                        "slug": "Diederik-P.-Kingma",
                        "structuredName": {
                            "firstName": "Diederik",
                            "lastName": "Kingma",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diederik P. Kingma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2503659"
                        ],
                        "name": "Jimmy Ba",
                        "slug": "Jimmy-Ba",
                        "structuredName": {
                            "firstName": "Jimmy",
                            "lastName": "Ba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jimmy Ba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6628106,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "isKey": false,
            "numCitedBy": 90052,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."
            },
            "slug": "Adam:-A-Method-for-Stochastic-Optimization-Kingma-Ba",
            "title": {
                "fragments": [],
                "text": "Adam: A Method for Stochastic Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This work introduces Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments, and provides a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "89504302"
                        ],
                        "name": "Greg Wayne",
                        "slug": "Greg-Wayne",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Wayne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Wayne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1841008"
                        ],
                        "name": "Ivo Danihelka",
                        "slug": "Ivo-Danihelka",
                        "structuredName": {
                            "firstName": "Ivo",
                            "lastName": "Danihelka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ivo Danihelka"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15299054,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3823aacea60bc1f2cabb9283144690a3d015db5",
            "isKey": false,
            "numCitedBy": 1634,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-toend, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples."
            },
            "slug": "Neural-Turing-Machines-Graves-Wayne",
            "title": {
                "fragments": [],
                "text": "Neural Turing Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-toend, allowing it to be efficiently trained with gradient descent."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2471378"
                        ],
                        "name": "L. Barsalou",
                        "slug": "L.-Barsalou",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Barsalou",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Barsalou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145632326"
                        ],
                        "name": "W. K. Simmons",
                        "slug": "W.-K.-Simmons",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Simmons",
                            "middleNames": [
                                "Kyle"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. K. Simmons"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2896717"
                        ],
                        "name": "A. Barbey",
                        "slug": "A.-Barbey",
                        "structuredName": {
                            "firstName": "Aron",
                            "lastName": "Barbey",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barbey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48515412"
                        ],
                        "name": "Christine D. Wilson",
                        "slug": "Christine-D.-Wilson",
                        "structuredName": {
                            "firstName": "Christine",
                            "lastName": "Wilson",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christine D. Wilson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 193
                            }
                        ],
                        "text": "In using the notion of concepts, we draw a lot of inspiration from humans, who are known for their ability to learn concepts and use them for tasks that involve abstract thinking and reasoning [11, 9, 30, 67]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 805674,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "3f5e922ce20efe7da64c115e5346481cfc618b79",
            "isKey": false,
            "numCitedBy": 1068,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Grounding-conceptual-knowledge-in-modality-specific-Barsalou-Simmons",
            "title": {
                "fragments": [],
                "text": "Grounding conceptual knowledge in modality-specific systems"
            },
            "venue": {
                "fragments": [],
                "text": "Trends in Cognitive Sciences"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6965856"
                        ],
                        "name": "Peter Anderson",
                        "slug": "Peter-Anderson",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Anderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Anderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722627"
                        ],
                        "name": "Xiaodong He",
                        "slug": "Xiaodong-He",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31790073"
                        ],
                        "name": "Chris Buehler",
                        "slug": "Chris-Buehler",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Buehler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Buehler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2406263"
                        ],
                        "name": "Damien Teney",
                        "slug": "Damien-Teney",
                        "structuredName": {
                            "firstName": "Damien",
                            "lastName": "Teney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Damien Teney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145177145"
                        ],
                        "name": "Mark Johnson",
                        "slug": "Mark-Johnson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145273587"
                        ],
                        "name": "Stephen Gould",
                        "slug": "Stephen-Gould",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Gould",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen Gould"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39089563"
                        ],
                        "name": "Lei Zhang",
                        "slug": "Lei-Zhang",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Zhang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 105
                            }
                        ],
                        "text": "Prior work commonly relied on dense visual features produced by either CNNs [83, 86] or object detectors [5], with a few recent models that use the relationships among objects to augment those features with contextual information from each object\u2019s surroundings [52, 75, 66]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 195347831,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a79b694bd4ef51207787da1948ed473903b751ef",
            "isKey": false,
            "numCitedBy": 292,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and topdown attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, improving the best published result in terms of CIDEr score from 114.7 to 117.9 and BLEU-4 from 35.2 to 36.9. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain a new state-of-the-art on the VQA v2.0 dataset with 70.2% overall accuracy."
            },
            "slug": "Bottom-Up-and-Top-Down-Attention-for-Image-and-VQA-Anderson-He",
            "title": {
                "fragments": [],
                "text": "Bottom-Up and Top-Down Attention for Image Captioning and VQA"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A combined bottom-up and topdown attention mechanism that enables attention to be calculated at the level of objects and other salient image regions is proposed, demonstrating the broad applicability of the method to VQA."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41016725"
                        ],
                        "name": "Thomas Kipf",
                        "slug": "Thomas-Kipf",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Kipf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Kipf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 328,
                                "start": 320
                            }
                        ],
                        "text": "Recent research about scene graphs [41, 81] and graph networks [10] is also relevant to our work, where we propose a novel method for neural graph traversal that is more suitable than prior approaches to our goal of performing sequential reasoning, as it eliminates the need in this case for costly state updates, as in [46, 73]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3144218,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "36eff562f65125511b5dfab68ce7f7a943c27478",
            "isKey": false,
            "numCitedBy": 11719,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin."
            },
            "slug": "Semi-Supervised-Classification-with-Graph-Networks-Kipf-Welling",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Classification with Graph Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "A scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs which outperforms related methods by a significant margin."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120157163"
                        ],
                        "name": "Jianwei Yang",
                        "slug": "Jianwei-Yang",
                        "structuredName": {
                            "firstName": "Jianwei",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianwei Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8553015"
                        ],
                        "name": "Jiasen Lu",
                        "slug": "Jiasen-Lu",
                        "structuredName": {
                            "firstName": "Jiasen",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiasen Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2297229"
                        ],
                        "name": "Stefan Lee",
                        "slug": "Stefan-Lee",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefan Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746610"
                        ],
                        "name": "Dhruv Batra",
                        "slug": "Dhruv-Batra",
                        "structuredName": {
                            "firstName": "Dhruv",
                            "lastName": "Batra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dhruv Batra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153432684"
                        ],
                        "name": "Devi Parikh",
                        "slug": "Devi-Parikh",
                        "structuredName": {
                            "firstName": "Devi",
                            "lastName": "Parikh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Devi Parikh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 74
                            }
                        ],
                        "text": "Multiple models have been proposed for the task of scene graph generation [84, 85, 16, 88]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 51894526,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1fad7fe0a7a90a8470a0688ad26bab6ceb8a85b7",
            "isKey": false,
            "numCitedBy": 452,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel scene graph generation model called Graph R-CNN, that is both effective and efficient at detecting objects and their relations in images. Our model contains a Relation Proposal Network (RePN) that efficiently deals with the quadratic number of potential relations between objects in an image. We also propose an attentional Graph Convolutional Network (aGCN) that effectively captures contextual information between objects and relations. Finally, we introduce a new evaluation metric that is more holistic and realistic than existing metrics. We report state-of-the-art performance on scene graph generation as evaluated using both existing and our proposed metrics."
            },
            "slug": "Graph-R-CNN-for-Scene-Graph-Generation-Yang-Lu",
            "title": {
                "fragments": [],
                "text": "Graph R-CNN for Scene Graph Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A novel scene graph generation model called Graph R-CNN, that is both effective and efficient at detecting objects and their relations in images, is proposed and a new evaluation metric is introduced that is more holistic and realistic than existing metrics."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422908"
                        ],
                        "name": "Robin Jia",
                        "slug": "Robin-Jia",
                        "structuredName": {
                            "firstName": "Robin",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robin Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145419642"
                        ],
                        "name": "Percy Liang",
                        "slug": "Percy-Liang",
                        "structuredName": {
                            "firstName": "Percy",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Percy Liang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 332,
                                "start": 325
                            }
                        ],
                        "text": "Yet, even though neural networks are undoubtedly powerful, flexible and robust, recent work has repeatedly demonstrated their flaws, showing how they struggle to generalize in a systematic manner [50], overly adhere to superficial and potentially misleading statistical associations instead of learning true causal relations [1, 42], strongly depend on large amounts of data and supervision [25, 51], and sometimes behave in surprising and"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7228830,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ffb949d3493c3b2f3c9acf9c75cb03938933ddf0",
            "isKey": false,
            "numCitedBy": 1075,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of 75% F1 score to 36%; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7%. We hope our insights will motivate the development of new models that understand language more precisely."
            },
            "slug": "Adversarial-Examples-for-Evaluating-Reading-Systems-Jia-Liang",
            "title": {
                "fragments": [],
                "text": "Adversarial Examples for Evaluating Reading Comprehension Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work proposes an adversarial evaluation scheme for the Stanford Question Answering Dataset that tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences without changing the correct answer or misleading humans."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48603437"
                        ],
                        "name": "A. Newell",
                        "slug": "A.-Newell",
                        "structuredName": {
                            "firstName": "Allen",
                            "lastName": "Newell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Newell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 120
                            }
                        ],
                        "text": "Ideas about compositionality, abstraction and reasoning greatly inspired the classical views of artificial intelligence [74, 65], but have lately been overshadowed by the astounding success of deep learning over a wide spectrum of real-world tasks [33, 63, 82]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4642944,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a0fc0ffed960258d91db19e6c3a7c34997a9b205",
            "isKey": false,
            "numCitedBy": 1395,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "On the occasion of a first conference on Cognitive Science, it seems appropriate to review the basis of common understanding between the various disciplines. In my estimate, the most fundamental contribution so far of artificial intelligence and computer science to the joint enterprise of cognitive science has been the notion of a physical symbol system, i.e., the concept of a broad class of systems capable of having and manipulating symbols, yet realizable in the physical universe. The notion of symbol so defined is internal to this concept, so it becomes a hypothesis that this notion of symbols includes the symbols that we humans use every day of our lives. In this paper we attempt systematically, but plainly, to lay out the nature of physical symbol systems. Such a review is in ways familiar, but not thereby useless. Restatement of fundamentals is an important exercise.The views and conclusions contained in this document are those of the author and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency, or the U.S. Government.Herb Simon would be a co-author of this paper, except that he is giving his own paper at this conference. The key ideas are entirely joint, as the references indicate."
            },
            "slug": "Physical-Symbol-Systems-Newell",
            "title": {
                "fragments": [],
                "text": "Physical Symbol Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "In this paper, the nature of physical symbol systems is laid out in ways familiar, but not thereby useless, to review the basis of common understanding between the various disciplines."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699999"
                        ],
                        "name": "I. Aleksander",
                        "slug": "I.-Aleksander",
                        "structuredName": {
                            "firstName": "Igor",
                            "lastName": "Aleksander",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Aleksander"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 310,
                                "start": 303
                            }
                        ],
                        "text": "Others have argued for the importance of incorporating strong inductive biases into neural architectures [14, 10, 6, 8], and indeed, there is a growing body of research that seeks to introduce different forms of structural priors inspired by computer architectures [29, 80, 40] or theory of computation [4, 23], aiming to bridge the gap between the symbolic and neural paradigms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60711774,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "02af52ae02d32a41639e7c53471a218846d473ae",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "There has been a recent renewal of interest in the debate as to whether an explanation of consciousness can (Dennett, 1991) or cannot (penrose, 1989) be captured by some formal theory. The aim of this paper is to side with the former and to present a theory testable through the concept of a neural state machine. This is a development of a theme first announced in Aleksander (1992). The theory is developed from the point of view of the conditions necessary to synthesize \u201cconsciousness\u201d in a manufactured artefact. This is given the name \u201cartificial consciousness\u201d so as to create grounds for a discussion of the difference between the synthetic product and that which is normally thought to be possessed by human beings. During the last 40 years, under the heading of \u201cartificial intelligence\u201d, there has been an effort to program computers so as to make them perform functions which, if done by humans, would be said to require intelligence. This endeavour consists of programs that endow the machine with the ability to follow logical rules developed by a programmer. It is not uncommon to point to the poverty of this approach as logical rules do not capture a sense of intentional subjectivity (Searle, 1992) or a sense of \u201cbeing\u201d (Winograd & Flores, 1986). In this paper it is argued that as a result of novel neural approaches to computation, such properties are not outside the scope of formal theory. A system of postulates is shown to contain the concept of \u201cself awareness\u201d and to support corollaries regarding that special part of human consciousness: natural language."
            },
            "slug": "The-Consciousness-of-a-Neural-State-Machine-Aleksander",
            "title": {
                "fragments": [],
                "text": "The Consciousness of a Neural State Machine"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is argued that as a result of novel neural approaches to computation, such properties are not outside the scope of formal theory and are argued to support corollaries regarding that special part of human consciousness: natural language."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2874347"
                        ],
                        "name": "Ronghang Hu",
                        "slug": "Ronghang-Hu",
                        "structuredName": {
                            "firstName": "Ronghang",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronghang Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "generation [84, 85, 16, 88]. Here, we largely follow the approaches of Yang et al. [85] and Chen et al. [16] in conjunction with a variant of the Mask R-CNN object detector [34] proposed by Hu et al. [39]. Further details regarding the graph generation can be found in section 7.4. By using such a graph generation model, we can infer a scene graph that consists of: (1) A set of object nodes Sfrom the i"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4308965,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ccd99008d942b890cecd308a31ba61240eac9e54",
            "isKey": true,
            "numCitedBy": 229,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Most methods for object instance segmentation require all training examples to be labeled with segmentation masks. This requirement makes it expensive to annotate new categories and has restricted instance segmentation models to ~100 well-annotated classes. The goal of this paper is to propose a new partially supervised training paradigm, together with a novel weight transfer function, that enables training instance segmentation models on a large set of categories all of which have box annotations, but only a small fraction of which have mask annotations. These contributions allow us to train Mask R-CNN to detect and segment 3000 visual concepts using box annotations from the Visual Genome dataset and mask annotations from the 80 classes in the COCO dataset. We evaluate our approach in a controlled study on the COCO dataset. This work is a first step towards instance segmentation models that have broad comprehension of the visual world."
            },
            "slug": "Learning-to-Segment-Every-Thing-Hu-Doll\u00e1r",
            "title": {
                "fragments": [],
                "text": "Learning to Segment Every Thing"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A new partially supervised training paradigm is proposed, together with a novel weight transfer function, that enables training instance segmentation models on a large set of categories all of which have box annotations, but only a small fraction ofWhich have mask annotations."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40538579"
                        ],
                        "name": "J. Vogel",
                        "slug": "J.-Vogel",
                        "structuredName": {
                            "firstName": "Julia",
                            "lastName": "Vogel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Vogel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 176
                            }
                        ],
                        "text": "Several works have explored the discovery and use of visual concepts in the contexts of reinforcement or unsupervised learning [35, 17] as well as in classical computer vision [21, 77]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12322757,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e264e1e55433f158bf8aa8b260bf430d76d5fa28",
            "isKey": false,
            "numCitedBy": 429,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a novel image representation that renders it possible to access natural scenes by local semantic description. Our work is motivated by the continuing effort in content-based image retrieval to extract and to model the semantic content of images. The basic idea of the semantic modeling is to classify local image regions into semantic concept classes such as water, rocks, or foliage. Images are represented through the frequency of occurrence of these local concepts. Through extensive experiments, we demonstrate that the image representation is well suited for modeling the semantic content of heterogenous scene categories, and thus for categorization and retrieval.The image representation also allows us to rank natural scenes according to their semantic similarity relative to certain scene categories. Based on human ranking data, we learn a perceptually plausible distance measure that leads to a high correlation between the human and the automatically obtained typicality ranking. This result is especially valuable for content-based image retrieval where the goal is to present retrieval results in descending semantic similarity from the query."
            },
            "slug": "Semantic-Modeling-of-Natural-Scenes-for-Image-Vogel-Schiele",
            "title": {
                "fragments": [],
                "text": "Semantic Modeling of Natural Scenes for Content-Based Image Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A novel image representation is presented that renders it possible to access natural scenes by local semantic description by using a perceptually plausible distance measure that leads to a high correlation between the human and the automatically obtained typicality ranking."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30708169"
                        ],
                        "name": "D. Navon",
                        "slug": "D.-Navon",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Navon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Navon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "see things in context, and build compositional world models to represent objects and understand their interactions and subtle relations, turning raw sensory signals into high-level semantic knowledge [64]; and we deductively draw inferences via conceptual rules and statements to proceed from known facts to novel conclusions [32, 40]. Not only are humans capable of learning, but we are also talented at"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14119789,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "9f7d9abb2277e924a291033d5c3d1195989e80ff",
            "isKey": false,
            "numCitedBy": 3635,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Forest-before-trees:-The-precedence-of-global-in-Navon",
            "title": {
                "fragments": [],
                "text": "Forest before trees: The precedence of global features in visual perception"
            },
            "venue": {
                "fragments": [],
                "text": "Cognitive Psychology"
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115231104"
                        ],
                        "name": "Justin Johnson",
                        "slug": "Justin-Johnson",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Justin Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145237361"
                        ],
                        "name": "Ranjay Krishna",
                        "slug": "Ranjay-Krishna",
                        "structuredName": {
                            "firstName": "Ranjay",
                            "lastName": "Krishna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ranjay Krishna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144421225"
                        ],
                        "name": "Michael Stark",
                        "slug": "Michael-Stark",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Stark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Stark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2040091191"
                        ],
                        "name": "Li-Jia Li",
                        "slug": "Li-Jia-Li",
                        "structuredName": {
                            "firstName": "Li-Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li-Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760364"
                        ],
                        "name": "David A. Shamma",
                        "slug": "David-A.-Shamma",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Shamma",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. Shamma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145879842"
                        ],
                        "name": "Michael S. Bernstein",
                        "slug": "Michael-S.-Bernstein",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Bernstein",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael S. Bernstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 70
                            }
                        ],
                        "text": "Starting from an image, we first generate a probabilistic scene graph [43, 49] that captures its underlying semantic knowledge in a compact form."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16414666,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "85ae705ef4353c6854f5be4a4664269d6317c66b",
            "isKey": false,
            "numCitedBy": 634,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper develops a novel framework for semantic image retrieval based on the notion of a scene graph. Our scene graphs represent objects (\u201cman\u201d, \u201cboat\u201d), attributes of objects (\u201cboat is white\u201d) and relationships between objects (\u201cman standing on boat\u201d). We use these scene graphs as queries to retrieve semantically related images. To this end, we design a conditional random field model that reasons about possible groundings of scene graphs to test images. The likelihoods of these groundings are used as ranking scores for retrieval. We introduce a novel dataset of 5,000 human-generated scene graphs grounded to images and use this dataset to evaluate our method for image retrieval. In particular, we evaluate retrieval using full scene graphs and small scene subgraphs, and show that our method outperforms retrieval methods that use only objects or low-level image features. In addition, we show that our full model can be used to improve object localization compared to baseline methods."
            },
            "slug": "Image-retrieval-using-scene-graphs-Johnson-Krishna",
            "title": {
                "fragments": [],
                "text": "Image retrieval using scene graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A conditional random field model that reasons about possible groundings of scene graphs to test images and shows that the full model can be used to improve object localization compared to baseline methods and outperforms retrieval methods that use only objects or low-level image features."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32264337"
                        ],
                        "name": "S. Schneider",
                        "slug": "S.-Schneider",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Schneider",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Schneider"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 58
                            }
                        ],
                        "text": "According to Jerry Fodor\u2019s Language of Thought hypothesis [22, 72], thinking itself posses a language-like compositional structure, where elementary concepts combine in systematic ways to create compound new ideas or thoughts, allowing us to make \u201cinfinite use of finite means\u201d [18] and fostering human\u2019s remarkable capacities of abstraction and generalization [51]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 141589011,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "b846dc8c9377776012668b325d1992e09a1ff074",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "The language of thought (LOT) approach to the nature of mind has been highly influential in cognitive science and the philosophy of mind; and yet, as Susan Schneider argues, its philosophical foundations are weak. In this philosophical refashioning of LOT and the related computational theory of mind (CTM), Schneider offers a different framework than has been developed by LOT and CTM's main architect, Jerry Fodor: one that seeks integration with neuroscience, repudiates Fodor's pessimism about the capacity of cognitive science to explain cognition, embraces pragmatism, and advances a different approach to the nature of concepts, mental symbols, and modes of presentation. According to the LOT approach, conceptual thought is determined by the manipulation of mental symbols according to algorithms. Schneider tackles three key problems that have plagued the LOT approach for decades: the computational nature of the central system (the system responsible for higher cognitive function); the nature of symbols; and Frege cases. To address these problems,] Schneider develops a computational theory that is based on the Global Workspace approach; develops a theory of symbols, \"the algorithmic view\"; and brings her theory of symbols to bear on LOT's account of the causation of thought and behavior. In the course of solving these problems, Schneider shows that LOT must make peace with both computationalism and pragmatism; indeed, the new conception of symbols renders LOT a pragmatist theory. And LOT must turn its focus to cognitive and computational neuroscience for its naturalism to succeed."
            },
            "slug": "The-Language-of-Thought:-A-New-Philosophical-Schneider",
            "title": {
                "fragments": [],
                "text": "The Language of Thought: A New Philosophical Direction"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2842071"
                        ],
                        "name": "D. Pecher",
                        "slug": "D.-Pecher",
                        "structuredName": {
                            "firstName": "Diane",
                            "lastName": "Pecher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pecher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1824414"
                        ],
                        "name": "Rolf A. Zwaan",
                        "slug": "Rolf-A.-Zwaan",
                        "structuredName": {
                            "firstName": "Rolf",
                            "lastName": "Zwaan",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rolf A. Zwaan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 193
                            }
                        ],
                        "text": "In using the notion of concepts, we draw a lot of inspiration from humans, who are known for their ability to learn concepts and use them for tasks that involve abstract thinking and reasoning [11, 9, 30, 67]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 141996838,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "5ef44aae0700cdd9a5f5bb062b3bf546295661c1",
            "isKey": false,
            "numCitedBy": 356,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Fifty years of research in cognitive science have demonstrated that the study of cognition is essential for a scientific understanding of human behavior. A growing number of researchers in the field are proposing that mental processes such as remembering, thinking, and understanding language are based on the physical interactions that people have with their environment. Rather than viewing the body as a support system for amind that needs to be fueled and transported, they view the mind as a support system that facilitates the functioning of the body. By shifting the basis for mental behavior toward the body, these researchers assume that mental processes are supported by the same processes that are used for physical interactions, that is, forperceptionandaction.Cognitive structuresdevelop from perception and action. To fully understand why this idea is so exciting, we need to look at the history of cognitive science. One of the major ideas propelling the cognitive revolution was the computer metaphor, in which cognitive processes are likened to software computations (Turing, 1950). Just like software can run on different hardware systems, so can cognitive processes run independently from the hardware in which they happened to be implemented, the human brain and body. Furthermore, just as computer programs, the human mind was thought to manipulate abstract symbols in a rule-based manner. These symbols were abstract because they were not derived from interactions with the environment by way of sensory organs and effectors. Traditional cognitive theories assume that the meaning of a concept consists of the links between the abstract symbol for that concept and the abstract symbols for other concepts or for semantic features. However, this view has fundamental problems, as has been demonstrated in an increasingnumber of contributions to the literature (e.g., Barsalou, 1999; Glenberg, 1997; Pulvermuller, 1999). Two of these problems are the transduction problem (Barsalou, 1999) and the grounding problem (Harnad, 1990). The transduction problem is the problem of how perceptual experiences are"
            },
            "slug": "Grounding-Cognition:-Introduction-to-Grounding-The-Pecher-Zwaan",
            "title": {
                "fragments": [],
                "text": "Grounding Cognition: Introduction to Grounding Cognition: The Role of Perception and Action in Memory, Language, and Thinking"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40497777"
                        ],
                        "name": "Joseph Redmon",
                        "slug": "Joseph-Redmon",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Redmon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph Redmon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143787583"
                        ],
                        "name": "Ali Farhadi",
                        "slug": "Ali-Farhadi",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Farhadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ali Farhadi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 786357,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d39d69b23424446f0400ef603b2e3e22d0309d6",
            "isKey": false,
            "numCitedBy": 7933,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that dont have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. YOLO9000 predicts detections for more than 9000 different object categories, all in real-time."
            },
            "slug": "YOLO9000:-Better,-Faster,-Stronger-Redmon-Farhadi",
            "title": {
                "fragments": [],
                "text": "YOLO9000: Better, Faster, Stronger"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories, is introduced and a method to jointly train on object detection and classification is proposed, both novel and drawn from prior work."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145313556"
                        ],
                        "name": "D. McDermott",
                        "slug": "D.-McDermott",
                        "structuredName": {
                            "firstName": "Drew",
                            "lastName": "McDermott",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. McDermott"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 26801195,
            "fieldsOfStudy": [
                "Linguistics",
                "Psychology"
            ],
            "id": "15cad06c606ce6c4f88e186d4e3bbb78aca648e3",
            "isKey": false,
            "numCitedBy": 1244,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction The Language of Thought Hypothesis (LOTH) is a concept in cognitive science which describes mental activity in the brain as a form of language. The hypothesis was developed by Jerry Fodor in his book [1]. It states that the mind works with a language that is similar to regular languages, where an array of \u201cwords\u201d together with syntactic and semantic rules make up the meaning of sentences and that these constructs are processed much like in a computer[2]. The origins for this hypothesis are older than Fodor\u2019s book however. Leibnitz already postulated the existence of an interpretable language of thought, which he called lingua mentis [3]."
            },
            "slug": "LANGUAGE-OF-THOUGHT-McDermott",
            "title": {
                "fragments": [],
                "text": "LANGUAGE OF THOUGHT"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114531657"
                        ],
                        "name": "Noam Chomsky",
                        "slug": "Noam-Chomsky",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Chomsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noam Chomsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12867884,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "16c762445f11fa2020994918dc4f93e76264df17",
            "isKey": false,
            "numCitedBy": 14181,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Contents: Methodological preliminaries: Generative grammars as theories of linguistic competence; theory of performance; organization of a generative grammar; justification of grammars; formal and substantive grammars; descriptive and explanatory theories; evaluation procedures; linguistic theory and language learning; generative capacity and its linguistic relevance Categories and relations in syntactic theory: Scope of the base; aspects of deep structure; illustrative fragment of the base component; types of base rules Deep structures and grammatical transformations Residual problems: Boundaries of syntax and semantics; structure of the lexicon"
            },
            "slug": "\u0935\u093e\u0915\u094d\u092f\u0935\u093f\u0928\u094d\u092f\u093e\u0938-\u0915\u093e-\u0938\u0948\u0926\u094d\u0927\u093e\u0928\u094d\u0924\u093f\u0915-\u092a\u0915\u094d\u0937-=-Aspects-of-the-Chomsky",
            "title": {
                "fragments": [],
                "text": "Aspects of the Theory of Syntax"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "Methodological preliminaries of generative grammars as theories of linguistic competence; theory of performance; organization of a generative grammar; justification of grammar; descriptive and explanatory theories; evaluation procedures; linguistic theory and language learning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1392848204"
                        ],
                        "name": "H. Johansen-Berg",
                        "slug": "H.-Johansen-Berg",
                        "structuredName": {
                            "firstName": "Heidi",
                            "lastName": "Johansen-Berg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Johansen-Berg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 256
                            }
                        ],
                        "text": "The emergence of a compositional system of symbols that can distill and convey from rich sensory experiences to creative new ideas has been a major turning point in the evolution of intelligence, and made a profound impact on the nature of human cognition [19, 78, 13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29580601,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "d6c8902054ff71aed336cbe8acb2e4f0d14cd9f3",
            "isKey": false,
            "numCitedBy": 99,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Language-shapes-thought-Johansen-Berg",
            "title": {
                "fragments": [],
                "text": "Language shapes thought"
            },
            "venue": {
                "fragments": [],
                "text": "Trends in Cognitive Sciences"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1989746"
                        ],
                        "name": "G. Bealer",
                        "slug": "G.-Bealer",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Bealer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Bealer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "y to learn concepts and use them for tasks that involve abstract thinking and reasoning 3 Figure 2: A visualization of object masks from the inferred scene graphs, which form the basis for our model. [11, 9, 28, 65]. In the following sections, rather than using raw and dense sensory input features directly, we represent both the visual and linguistic inputs in terms of our vocabulary, \ufb01nding the most relevant co"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 143461813,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "0b22522852b2804f933ae18dc87696fc748cffee",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "A theory of concepts should answer three questions: (1) Do concepts exist and, if so, what is their modal status (post rem, in rebus, ante rem)? (2) What are concepts (assuming that they exist)? (3) What is it to possess a concept? My starting point is the truism that the concept of being F is a concept. This canonical gerundive form identifies the primary sense of the term 'concept' in ordinary English and serves to anchor usage in philosophical discussions. I hold that concepts, in this primary sense, are sui generis irreducible entities comprising the ontological category in terms of which propositions (thoughts, in Frege's sense) are to be analyzed. Some people believe that one must invoke psychology to justify the ontology of concepts. Even if this style of justification succeeds, I believe that the existence of concepts and propositions is more convincingly established by certain considerations in logic -specifically, modal logic and the logic of logical truthwhere one deals with 'that'-clauses, gerundives, and other canonical intensional terms. Moreover, unlike the psychological approach, the logical approach is able to settle the question of modal status; specifically, it implies the ante rem view of propositions and concepts (i.e., the view that they are mind-independent"
            },
            "slug": "A-theory-of-concepts-and-concepts-possession-Bealer",
            "title": {
                "fragments": [],
                "text": "A theory of concepts and concepts possession"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4566647"
                        ],
                        "name": "D. Laplane",
                        "slug": "D.-Laplane",
                        "structuredName": {
                            "firstName": "Dominique",
                            "lastName": "Laplane",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Laplane"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9579357,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "f2bb477868cf41a71db56ef45039bd660f351efa",
            "isKey": false,
            "numCitedBy": 7584,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "From aphasics' self records, common experience, changes in signification of sentences according to a verbal or non-verbal context, animals and non speaking children performances, it seems possible to get some evidence that thought is distinct from language even though there is a permanent interaction between both in normal adult human beings. Some considerations on formalisation of language suggests that the more formalised it is, the less information it contains. If it is true, it is not reasonable to hope that a formalised language like that used by computers may be a model for thought. Finally, the lack of status of thought, as far as it is a subjective experience and the impossibility of giving it a definition as far as it exceeds language, make it clear that in spite of progress in scientific psychology, thought, per se, is not an object for science."
            },
            "slug": "Thought-and-language.-Laplane",
            "title": {
                "fragments": [],
                "text": "Thought and language."
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "From aphasics' self records, common experience, changes in signification of sentences according to a verbal or non-verbal context, animals and non speaking children performances, it seems possible to get some evidence that thought is distinct from language even though there is a permanent interaction between both in normal adult human beings."
            },
            "venue": {
                "fragments": [],
                "text": "Behavioural neurology"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706504"
                        ],
                        "name": "J. Hopcroft",
                        "slug": "J.-Hopcroft",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopcroft",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopcroft"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742391"
                        ],
                        "name": "J. Ullman",
                        "slug": "J.-Ullman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Ullman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ullman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 103
                            }
                        ],
                        "text": "The Neural State Machine is a graph-based network that simulates the computation of a finite automaton [37], and is explored here in the context of VQA, where we are given an image and a question and asked to provide an answer."
                    },
                    "intents": []
                }
            ],
            "corpusId": 31901407,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41a88a490d7ba9e383ecb16c4290083413a08258",
            "isKey": false,
            "numCitedBy": 13820,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Introduction-to-Automata-Theory,-Languages-and-Hopcroft-Ullman",
            "title": {
                "fragments": [],
                "text": "Introduction to Automata Theory, Languages and Computation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2573070"
                        ],
                        "name": "J. Greeno",
                        "slug": "J.-Greeno",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Greeno",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Greeno"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 193
                            }
                        ],
                        "text": "In using the notion of concepts, we draw a lot of inspiration from humans, who are known for their ability to learn concepts and use them for tasks that involve abstract thinking and reasoning [11, 9, 30, 67]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15481169,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "8161b5b6cd925fff18a836987ce140d633031439",
            "isKey": false,
            "numCitedBy": 364,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Research on general thinking abilities-productive, higher order, critical and creative thinking-has progressed slowly compared with the rapid progress that has been made in the study of cognitive structures and procedures. As alternatives to currently prevailing assumptions, three framing assumptions for the study o f thinking are proposed, involving situated cognition, personal and social epistemologies, and conceptual competence. Evidence consistent with these assumptions is outlined, and topics in the psychology of thinking are discussed in relation to the assumptions. The psychological study of thinking has two parts. One part is concerned with performance on specific tasks. The other part is concerned with broader capabilities of productive thinking, higher order thinking skills, critical thinking, and creativity. In the past 20 years, there has been major scientific progress in the psychology of thinking concerned with performance on specific tasks, and much less in the psychology of critical, productive, higher order, and creative thinking. The study of complex information processing, pioneered by Newell and Simon (1972), has provided a progressive framework for analyzing the cognitive structures and processes in many tasks, including tasks used in Piagetian research (Siegler, 1976), puzzles such as cryptarithmetic and the Towers of Hanoi (Anzai & Simon, 1979), items used in intelligence tests (Pellcgrino & Glaser, 1982; Simon & Kotovsky, 1963; Sternberg, 1977), various kinds of syllogisms (Johnson-Laird, 1983), concept identification (Gregg & Simon, 1967), and tasks used in school instruction, especially in science and mathematics (Anderson, 1982; Greeno, 1978; Resnick, in this issue; Riley & Greeno, 1988). Research on the topics of productive, higher order, critical, and creative thinking has not been an integral part of the major success of cognitive and developmental psychology during the past 20 years or so. In contrast to the remarkable progress achieved in analyses of specific tasks, research on general thinking capabilities has not led to significant advances in theoretical understanding or a systematic body of empirical knowledge. Many efforts to enhance children's thinking abilities have been made, and some of these have been evaluated in research studies. Excellent reviews are available by Chipman, Segal, and Glaser (1985); Nickerson, Perkins, and Smith (1985); Resnick (1987); and Segal, Chipman, and Glaser (1985). The dominant psychological idea in these development efforts is that thinking can be viewed as a skill. Successful thinking in specific subj~-'t-matter domains uses knowledge that is specific to the domains, as much research has shown. At the same time, it is reasonable to expect that there are aspects of thinking skill that are common across domains, and this possibility has motivated the development of many programs in the attempt to enhance general thinking skills. Evaluations of several of these programs have shown positive results, which encourage the idea that there are teachable components of general thinking ability. At the same time, there has been no perceptible progress toward a coherent account of what makes some of the programs succeed; nor has the research contributed significantly toward a set of principles that would constitute an articulate theory of the characteristics of thinking abilities. Nickerson, Perkins, and Smith (1985) summed up the situation, at least regarding programs emphasizing cognitive operations, as follows: Reviewing the various programs discussed, we are impressed by how easy it is to make up a list of fundamental operations, and also by the fact that the lists produced by different programs differ considerably from one another. Each of these lists can be seen as a theory of the components of intelligence or at least of determinants of intellectual performance. And this is the problem. There are too many of these theories for comfort. To be sure, some themes recur frequently but the differences are substantial. (p. 188) Some of the causes of this relatively slow progress may be implicit in the theories and framing assumptions that have dominated our scientific inquiry. There are three framing assumptions about thinking and learning that may be responsible for our apparent inability to develop a more adequate theory of thinking. First, the locus of thinking is assumed to be in an individual's mind, rather than in interaction between an agent and a physical and social situation. Second, processes of thinking and learning are assumed to be uniform across persons and situations. Different individuals are more or less capable of critical or creative thinking, and different situations are more or less conducive to learning and thinking, but the activities of thinking and learning are assumed to have approximately the same character wherever and in whomever they occur. Third, resources for thinking are assumed to be knowledge and skills that are built up from simple components, especially through instruction in school, rather than general conceptual capabilities that children may have as a result of their everyday experience or native endowment. 134 February 1989 \u2022 American Psychologist Copyright 1989 by the American Psychological Association, Inc. 0003-066X/89/$00.75 Vol. 44, No, 2, 134-141 These framing assumptions are reflected in the research and discussions of thinking, including discussions of creative thinking, where Kogan (1983) has summarized a large body of research about whether individuals think divergently or convergenfly and where Weisberg (1986) recently debunked several accounts in terms of genius, divergent thinking, mysterious insight, and inspiration and proposed replacing these with an account in terms of information-processing, problem-solving capabilities; motivation; and concentration. Resnick's (1987) discussion characterized higher order thinking as nonalgorithmic, complex, self-regulative, meaningful, effortful, and providing multiple solutions, nuanced judgments, multiple criteria, and uncertainty, all defined in terms of cognitive traits and processes of individuals. Chipman (1986) characterized a consensus among cognitive scientists that successful thinking depends on organization of cognitive activity with a hierarchy of goals and operations. Alternative Framing Assumptions A different set of framing assumptions may be needed if we are to make significant headway toward an adequate understanding of thinking and creativity. The three assumptions that I propose are the following. 1. Situated cognition. Thinking is situated in physical and social contexts. Cognition, including thinking, knowing, and learning, can be considered as a relation involving an agent in a situation, rather than as an activity in an individual's mind. 2. Personal and social epistemologies. Thinking and learning are situated in contexts of beliefs and understandings about cognition that differ between individuals and social groups, and fundamental properties of thinking and learning are determined by these contexts. 3. Conceptual competence. Children have strong potential capabilities for cognitive growth that enable complex and subtle processes of construction of knowledge and thinking skills. Thinking~ learning, and cognitive growth are activities in which children elaborate and reorganize their knowledge and understanding, rather than simply applying and acquiring cognitive structures and procedures. These assumptions are too general to allow direct empirical tests. Their acceptance will depend on the theoretical analyses and systematic empirical work that result from their use. At the same time, there are at least three kinds of empirical evidence that are consistent with the assumptions and encourage their further use and development. This research was supported by the Office of Naval Research, Contract N00014-88-K-0152, Project 1142CS, and the Institute for Research on Learning. I am grateful for conversations with John Seely Brown, Andy diSessa, Brigitte Jordan, Jean Lave, George Fake, Alan Sehoenfeid, and Susan Stucky in the development of these ideas. Correspondence concerning this article should be addressed to James G. Greeno, School of Education, Stanford University, Stanford, CA 94305. Situated Cognition We have thought of thinking as a process within an individual's mind, perhaps influenced by a context provided by the situation. Recent ethnographic research suggests a different view, in which thinking is an interaction between an individual and a physical and social situation. One example is in observations by Seribner (1984) of young men whose job is to fill drivers' orders in a dairy. The task involves different products that come in different-sized containers. Containers are packed in cases; the number of containers per case is different for the different container sizes. The worker gets a form failed out for each driver, in a special notation that shows, for each product, a number of cases and a number of containers that should be added or subtracted. The task could be done algorithmically; for example, the loader could locate the number of full cases that are specified and either remove the number of containers specified with a negative integer or add a partial case with the number of containers specified with a positive integer. The loaders did better, using partial cases to obtain more efficient solutions. The workers used optimal solutions in more than 90% of the cases that were observed. These workers' performance was not based on mathematical knowledge of the kind that we might expect individuals to obtain in school. We can imagine giving a word problem in arithmetic about the situation, involving the number of cases and containers needed for the order, the number of containers in a partially filled case, and the question \"How many more containers are needed for the order to be filled correctly?\" We would expect some pencil-an"
            },
            "slug": "A-perspective-on-thinking.-Greeno",
            "title": {
                "fragments": [],
                "text": "A perspective on thinking."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143724728"
                        ],
                        "name": "H. Price",
                        "slug": "H.-Price",
                        "structuredName": {
                            "firstName": "Huw",
                            "lastName": "Price",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Price"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 512,
                                "start": 504
                            }
                        ],
                        "text": "Indeed, humans are particularly adept at making abstractions of various kinds: We make analogies and form concepts to generalize from given instances to unseen examples [70]; we see things in context, and build compositional world models to represent objects and understand their interactions and subtle relations, turning raw sensory signals into high-level semantic knowledge [64]; and we deductively draw inferences via conceptual rules and statements to proceed from known facts to novel conclusions [32, 40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 143274304,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "4c609f5e6696642996705d07b1f0cd9cc012a49a",
            "isKey": false,
            "numCitedBy": 324,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Change-in-View:-Principles-of-Reasoning-Price",
            "title": {
                "fragments": [],
                "text": "Change in View: Principles of Reasoning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082991"
                        ],
                        "name": "Georgia Gkioxari",
                        "slug": "Georgia-Gkioxari",
                        "structuredName": {
                            "firstName": "Georgia",
                            "lastName": "Gkioxari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Georgia Gkioxari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": "[16] in conjunction with a variant of the Mask R-CNN object detector [34] proposed by Hu et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 54465873,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "022dd244f2e25525eb37e9dda51abb9cd8ca8c30",
            "isKey": false,
            "numCitedBy": 9771,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron."
            },
            "slug": "Mask-R-CNN-He-Gkioxari",
            "title": {
                "fragments": [],
                "text": "Mask R-CNN"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This work presents a conceptually simple, flexible, and general framework for object instance segmentation that outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2020
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 58
                            }
                        ],
                        "text": "According to Jerry Fodor\u2019s Language of Thought hypothesis [22, 72], thinking itself posses a language-like compositional structure, where elementary concepts combine in systematic ways to create compound new ideas or thoughts, allowing us to make \u201cinfinite use of finite means\u201d [18] and fostering human\u2019s remarkable capacities of abstraction and generalization [51]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The language of thought, volume 5"
            },
            "venue": {
                "fragments": [],
                "text": "Harvard university press,"
            },
            "year": 1975
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 40,
            "methodology": 28,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 90,
        "totalPages": 9
    },
    "page_url": "https://www.semanticscholar.org/paper/Learning-by-Abstraction:-The-Neural-State-Machine-Hudson-Manning/136c05cb8dd359fb8e0dc7947172a9ecb74ccbec?sort=total-citations"
}