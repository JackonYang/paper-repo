{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112096491"
                        ],
                        "name": "Xiao Yang",
                        "slug": "Xiao-Yang",
                        "structuredName": {
                            "firstName": "Xiao",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiao Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8020964"
                        ],
                        "name": "Ersin Yumer",
                        "slug": "Ersin-Yumer",
                        "structuredName": {
                            "firstName": "Ersin",
                            "lastName": "Yumer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ersin Yumer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2934421"
                        ],
                        "name": "Paul Asente",
                        "slug": "Paul-Asente",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Asente",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul Asente"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389971134"
                        ],
                        "name": "Mike Kraley",
                        "slug": "Mike-Kraley",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Kraley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mike Kraley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1852261"
                        ],
                        "name": "Daniel Kifer",
                        "slug": "Daniel-Kifer",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Kifer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Kifer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ctures into lines and then employs heuristics like, perpendicular distance and angle between lines to combine them into text blocks. While deep learning approaches to LLA also exist, these approaches [12] [3] require vast amounts of training data and only learn a \ufb01xed set of labels and are thus not useful for few-shot tasks with a wide variety of different labels. We explore an object detection based "
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "of understanding general documents as well as digitizing historical documents. With the onset of deep learning and data driven approaches, the problem was approached as a pixel-wise segmentation task [12], Both authors contributed equally yWork done during an internship at Paralleldots, Inc. where each pixel is assigned a class based on its surrounding pixels. In this paper, we explore a new tangent, "
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2272015,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9baae0bdc2884bcf0aa4063914b87d60952cb678",
            "isKey": false,
            "numCitedBy": 145,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an end-to-end, multimodal, fully convolutional network for extracting semantic structures from document images. We consider document semantic structure extraction as a pixel-wise segmentation task, and propose a unified model that classifies pixels based not only on their visual appearance, as in the traditional page segmentation task, but also on the content of underlying text. Moreover, we propose an efficient synthetic document generation process that we use to generate pretraining data for our network. Once the network is trained on a large set of synthetic documents, we fine-tune the network on unlabeled real documents using a semi-supervised approach. We systematically study the optimum network architecture and show that both our multimodal approach and the synthetic data pretraining significantly boost the performance."
            },
            "slug": "Learning-to-Extract-Semantic-Structure-from-Using-Yang-Yumer",
            "title": {
                "fragments": [],
                "text": "Learning to Extract Semantic Structure from Documents Using Multimodal Fully Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "An end-to-end, multimodal, fully convolutional network for extracting semantic structures from document images using a unified model that classifies pixels based not only on their visual appearance, as in the traditional page segmentation task, but also on the content of underlying text."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50688939"
                        ],
                        "name": "Hao Chen",
                        "slug": "Hao-Chen",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47903936"
                        ],
                        "name": "Yali Wang",
                        "slug": "Yali-Wang",
                        "structuredName": {
                            "firstName": "Yali",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yali Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50248653"
                        ],
                        "name": "Guoyou Wang",
                        "slug": "Guoyou-Wang",
                        "structuredName": {
                            "firstName": "Guoyou",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guoyou Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143970608"
                        ],
                        "name": "Y. Qiao",
                        "slug": "Y.-Qiao",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Qiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Qiao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[2] introduce a Low-shot Object Detector (LSTD) model which is pretrained on a huge Source Dataset and fine-tuned on a small (low-shot) target dataset."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "There are two regularizations introduced by [2], Background Regularization (BGR) and Tk-Regularization (Tk-R) which helps them in learning from just few examples in the target dataset."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 51
                            }
                        ],
                        "text": "The Source dataset in our case is more basic while [2] assume the Source dataset to be very huge and comprehensive."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "There are two regularizations introduced by [2], Background Regularization (BGR) and Tk-Regularization (TkR)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3707436,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d44872844fe4aec3677304c6165bb33e1243e641",
            "isKey": true,
            "numCitedBy": 146,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "\n \n Recent advances in object detection are mainly driven by deep learning with large-scale detection benchmarks. However, the fully-annotated training set is often limited for a target detection task, which may deteriorate the performance of deep detectors. To address this challenge, we propose a novel low-shot transfer detector (LSTD) in this paper, where we leverage rich source-domain knowledge to construct an effective target-domain detector with very few training examples. The main contributions are described as follows. First, we design a flexible deep architecture of LSTD to alleviate transfer difficulties in low-shot detection. This architecture can integrate the advantages of both SSD and Faster RCNN in a unified deep framework. Second, we introduce a novel regularized transfer learning framework for low-shot detection, where the transfer knowledge (TK) and background depression (BD) regularizations are proposed to leverage object knowledge respectively from source and target domains, in order to further enhance fine-tuning with a few target images. Finally, we examine our LSTD on a number of challenging low-shot detection experiments, where LSTD outperforms other state-of-the-art approaches. The results demonstrate that LSTD is a preferable deep detector for low-shot scenarios.\n \n"
            },
            "slug": "LSTD:-A-Low-Shot-Transfer-Detector-for-Object-Chen-Wang",
            "title": {
                "fragments": [],
                "text": "LSTD: A Low-Shot Transfer Detector for Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A novel low- shot transfer detector (LSTD) is proposed, where rich source-domain knowledge is leveraged to construct an effective target-domain detector with very few training examples, and a novel regularized transfer learning framework for low-shot detection is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46641573"
                        ],
                        "name": "W. Liu",
                        "slug": "W.-Liu",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1838674"
                        ],
                        "name": "Dragomir Anguelov",
                        "slug": "Dragomir-Anguelov",
                        "structuredName": {
                            "firstName": "Dragomir",
                            "lastName": "Anguelov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dragomir Anguelov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761978"
                        ],
                        "name": "D. Erhan",
                        "slug": "D.-Erhan",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Erhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Erhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574060"
                        ],
                        "name": "Christian Szegedy",
                        "slug": "Christian-Szegedy",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Szegedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Szegedy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144828948"
                        ],
                        "name": "Scott E. Reed",
                        "slug": "Scott-E.-Reed",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Reed",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott E. Reed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2084646762"
                        ],
                        "name": "Cheng-Yang Fu",
                        "slug": "Cheng-Yang-Fu",
                        "structuredName": {
                            "firstName": "Cheng-Yang",
                            "lastName": "Fu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheng-Yang Fu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ntroduce a Low-shot Object Detector (LSTD) model which is pretrained on a huge Source Dataset and \ufb01ne-tuned on a small (low-shot) target dataset. The LSTD model is based on Single Shot Detector (SSD) [5] and Faster-RCNN (FRCNN) [10]. Broadly, they use the SSD network to detect foreground segments and a classi\ufb01er which takes ROIPooled features from the SSD feature maps to classify the detected regions"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2141740,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d7a9197433acbfb24ef0e9d0f33ed1699e4a5b0",
            "isKey": false,
            "numCitedBy": 15424,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For $300\\times 300$ input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for $500\\times 500$ input, SSD achieves 75.1% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at this https URL ."
            },
            "slug": "SSD:-Single-Shot-MultiBox-Detector-Liu-Anguelov",
            "title": {
                "fragments": [],
                "text": "SSD: Single Shot MultiBox Detector"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "The approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location, which makes SSD easy to train and straightforward to integrate into systems that require a detection component."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33493200"
                        ],
                        "name": "Tsung-Yi Lin",
                        "slug": "Tsung-Yi-Lin",
                        "structuredName": {
                            "firstName": "Tsung-Yi",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tsung-Yi Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790580"
                        ],
                        "name": "Bharath Hariharan",
                        "slug": "Bharath-Hariharan",
                        "structuredName": {
                            "firstName": "Bharath",
                            "lastName": "Hariharan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bharath Hariharan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": " second step is the ML classi\ufb01er which predicts the domain-speci\ufb01c layout class. For the \ufb01rst step, we leverage a better feature extractor for the object detector. We use the Feature Pyramid Networks [4] as our feature extractor. This (FPN based SSD) achieves state-of-the-art performance for a single model on PASCAL VOC dataset (object detection) as shown here 1. On the Target dataset, many of the ta"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10716717,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9b4e05faa194e5022edd9eb9dd07e3d675c2b36",
            "isKey": false,
            "numCitedBy": 9354,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But pyramid representations have been avoided in recent object detectors that are based on deep convolutional networks, partially because they are slow to compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available."
            },
            "slug": "Feature-Pyramid-Networks-for-Object-Detection-Lin-Doll\u00e1r",
            "title": {
                "fragments": [],
                "text": "Feature Pyramid Networks for Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper exploits the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost and achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34939798"
                        ],
                        "name": "Adam W. Harley",
                        "slug": "Adam-W.-Harley",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Harley",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam W. Harley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2079687415"
                        ],
                        "name": "Alex Ufkes",
                        "slug": "Alex-Ufkes",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Ufkes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Ufkes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3150825"
                        ],
                        "name": "K. Derpanis",
                        "slug": "K.-Derpanis",
                        "structuredName": {
                            "firstName": "Konstantinos",
                            "lastName": "Derpanis",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Derpanis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 72
                            }
                        ],
                        "text": "While deep learning approaches to LLA also exist, these approaches [12] [3] require vast amounts of training data and only learn a fixed set of labels and are thus not useful for few-shot tasks with a wide variety of different labels."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2760893,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd86b4b551b9d3fb498f62008b037e7599365018",
            "isKey": false,
            "numCitedBy": 174,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new state-of-the-art for document image classification and retrieval, using features learned by deep convolutional neural networks (CNNs). In object and scene analysis, deep neural nets are capable of learning a hierarchical chain of abstraction from pixel inputs to concise and descriptive representations. The current work explores this capacity in the realm of document analysis, and confirms that this representation strategy is superior to a variety of popular handcrafted alternatives. Extensive experiments show that (i) features extracted from CNNs are robust to compression, (ii) CNNs trained on non-document images transfer well to document analysis tasks, and (iii) enforcing region-specific feature-learning is unnecessary given sufficient training data. This work also makes available a new labelled subset of the IIT-CDIP collection, containing 400,000 document images across 16 categories."
            },
            "slug": "Evaluation-of-deep-convolutional-nets-for-document-Harley-Ufkes",
            "title": {
                "fragments": [],
                "text": "Evaluation of deep convolutional nets for document image classification and retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A new state-of-the-art for document image classification and retrieval, using features learned by deep convolutional neural networks (CNNs), and makes available a new labelled subset of the IIT-CDIP collection, containing 400,000 document images across 16 categories."
            },
            "venue": {
                "fragments": [],
                "text": "2015 13th International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2032184078"
                        ],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 68
                            }
                        ],
                        "text": "Hence we resorted to using a separate classifier (as opposed to the FRCNN based LSTD classifier) for the detected boxes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 36
                            }
                        ],
                        "text": "Tk-R was not useful as the default (FRCNN based) classifier does not perform well due to the reasons mentioned earlier."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "The LSTD model is based on Single Shot Detector (SSD) [5] and Faster-RCNN (FRCNN) [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10328909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "424561d8585ff8ebce7d5d07de8dbf7aae5e7270",
            "isKey": true,
            "numCitedBy": 32566,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available"
            },
            "slug": "Faster-R-CNN:-Towards-Real-Time-Object-Detection-Ren-He",
            "title": {
                "fragments": [],
                "text": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work introduces a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals and further merge RPN and Fast R-CNN into a single network by sharing their convolutionAL features."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3185334"
                        ],
                        "name": "A. Namboodiri",
                        "slug": "A.-Namboodiri",
                        "structuredName": {
                            "firstName": "Anoop",
                            "lastName": "Namboodiri",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Namboodiri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 58712706,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42b2bc874b1b080cde919c2d9220f32a0023ac66",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "A document image is composed of a variety of physical entities or regions such as text blocks, lines, words, figures, tables, and background. We could also assign functional or logical labels such as sentences, titles, captions, author names, and addresses to some of these regions. The process of document structure and layout analysis tries to decompose a given document image into its component regions and understand their functional roles and relationships. The processing is carried out in multiple steps, such as preprocessing, page decomposition, structure understanding, etc. We will look into each of these steps in detail in the following sections. Document images are often generated from physical documents by digitization using scanners or digital cameras. Many documents, such as newspapers, magazines and brochures, contain very complex layout due to the placement of figures, titles, and captions, complex backgrounds, artistic text formatting, etc. (see Figure 1). A human reader uses a variety of additional cues such as context, conventions and information about language/script, along with a complex reasoning process to decipher the contents of a document. Automatic analysis of an arbitrary document with complex layout is an extremely difficult task and is beyond the capabilities of the state-of-the-art document structure and layout analysis systems. This is interesting since documents are designed to be effective and clear to human interpretation unlike natural images."
            },
            "slug": "Document-Structure-and-Layout-Analysis-Namboodiri-Jain",
            "title": {
                "fragments": [],
                "text": "Document Structure and Layout Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Automatic analysis of an arbitrary document with complex layout is an extremely difficult task and is beyond the capabilities of the state-of-the-art document structure and layout analysis systems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145695526"
                        ],
                        "name": "S. Mao",
                        "slug": "S.-Mao",
                        "structuredName": {
                            "firstName": "Song",
                            "lastName": "Mao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143626870"
                        ],
                        "name": "T. Kanungo",
                        "slug": "T.-Kanungo",
                        "structuredName": {
                            "firstName": "Tapas",
                            "lastName": "Kanungo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanungo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ut involve tuning a lot of heuristics. They are also not scalable to document layouts which are different from those the algorithm is tuned on. Comparisons of such approaches are also covered by [11] [6]. The most popular and widely used of these approaches is the Docstrum [8] algorithm. It uses KNN to aggregate the minute structures into lines and then employs heuristics like, perpendicular distance"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5453548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82328a31d7366c4ecfd339deeae41c3152b43c98",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "While numerous page segmentation algorithms have been proposed in the literature, there is lack of comparative evaluation of these algorithms. In the existing performance evaluation methods, two crucial components are usually missing: 1) automatic training of algorithms with free parameters and 2) statistical and error analysis of experimental results. We use the following five-step methodology to quantitatively compare the performance of page segmentation algorithms: 1) first, we create mutually exclusive training and test data sets with groundtruth, 2) we then select a meaningful and computable performance metric, 3) an optimization procedure is then used to search automatically for the optimal parameter values of the segmentation algorithms on the training data set, 4) the segmentation algorithms are then evaluated on the test data set, and, finally, 5) a statistical and error analysis is performed to give the statistical significance of the experimental results. In particular, instead of the ad hoc and manual approach typically used in the literature for training algorithms, we pose the automatic training of algorithms as an optimization problem and use the simplex algorithm to search for the optimal parameter value. A paired-model statistical analysis and an error analysis are then conducted to provide confidence intervals for the experimental results of the algorithms. This methodology is applied to the evaluation of live page segmentation algorithms of which, three are representative research algorithms and the other two are well-known commercial products, on 978 images from the University of Washington III data set. It is found that the performance indices of the Voronoi, Docstrum, and Caere segmentation algorithms are not significantly different from each other, but they are significantly better than that of ScanSoft's segmentation algorithm, which, in turn, is significantly better than that of X-Y cut."
            },
            "slug": "Empirical-Performance-Evaluation-Methodology-and-to-Mao-Kanungo",
            "title": {
                "fragments": [],
                "text": "Empirical Performance Evaluation Methodology and Its Application to Page Segmentation Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The performance indices of the Voronoi, Docstrum, and Caere segmentation algorithms are not significantly different from each other, but they are significantly better than that of ScanSoft's segmentation algorithm, which, in turn, is significantly betterthan that of X-Y cut."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688013"
                        ],
                        "name": "F. Shafait",
                        "slug": "F.-Shafait",
                        "structuredName": {
                            "firstName": "Faisal",
                            "lastName": "Shafait",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shafait"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51027911"
                        ],
                        "name": "Daniel Keysers",
                        "slug": "Daniel-Keysers",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Keysers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Keysers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733858"
                        ],
                        "name": "T. Breuel",
                        "slug": "T.-Breuel",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Breuel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Breuel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "sed but involve tuning a lot of heuristics. They are also not scalable to document layouts which are different from those the algorithm is tuned on. Comparisons of such approaches are also covered by [11] [6]. The most popular and widely used of these approaches is the Docstrum [8] algorithm. It uses KNN to aggregate the minute structures into lines and then employs heuristics like, perpendicular dist"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5611293,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c8d1ce168a9f82234d81e4210c710e3499aa4af2",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a quantitative comparison of six algorithms for page segmentation: X-Y cut, smearing, whitespace analysis, constrained text-line finding, Docstrum, and Voronoi-diagram-based. The evaluation is performed using a subset of the UW-III collection commonly used for evaluation, with a separate training set for parameter optimization. We compare the results using both default parameters and optimized parameters. In the course of the evaluation, the strengths and weaknesses of each algorithm are analyzed, and it is shown that no single algorithm outperforms all other algorithms. However, we observe that the three best-performing algorithms are those based on constrained text-line finding, Docstrum, and the Voronoi-diagram."
            },
            "slug": "Performance-Comparison-of-Six-Algorithms-for-Page-Shafait-Keysers",
            "title": {
                "fragments": [],
                "text": "Performance Comparison of Six Algorithms for Page Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "It is shown that no single algorithm outperforms all other algorithms, however, the three best-performing algorithms are those based on constrained text-line finding, Docstrum, and the Voronoi-diagram."
            },
            "venue": {
                "fragments": [],
                "text": "Document Analysis Systems"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3297552"
                        ],
                        "name": "R. S. Olson",
                        "slug": "R.-S.-Olson",
                        "structuredName": {
                            "firstName": "Randal",
                            "lastName": "Olson",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. S. Olson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39248319"
                        ],
                        "name": "Nathan Bartley",
                        "slug": "Nathan-Bartley",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Bartley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathan Bartley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800213"
                        ],
                        "name": "R. Urbanowicz",
                        "slug": "R.-Urbanowicz",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Urbanowicz",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urbanowicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152512193"
                        ],
                        "name": "J. Moore",
                        "slug": "J.-Moore",
                        "structuredName": {
                            "firstName": "Jason H.",
                            "lastName": "Moore",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Moore"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 45
                            }
                        ],
                        "text": "We get our classifier using the tpot toolkit [9], which uses genetic programming to optimize machine learning pipelines."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7142590,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3335c340c20609b4e6de481c9eaf67ecd6c960dc",
            "isKey": false,
            "numCitedBy": 319,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "As the field of data science continues to grow, there will be an ever-increasing demand for tools that make machine learning accessible to non-experts. In this paper, we introduce the concept of tree-based pipeline optimization for automating one of the most tedious parts of machine learning--pipeline design. We implement an open source Tree-based Pipeline Optimization Tool (TPOT) in Python and demonstrate its effectiveness on a series of simulated and real-world benchmark data sets. In particular, we show that TPOT can design machine learning pipelines that provide a significant improvement over a basic machine learning analysis while requiring little to no input nor prior knowledge from the user. We also address the tendency for TPOT to design overly complex pipelines by integrating Pareto optimization, which produces compact pipelines without sacrificing classification accuracy. As such, this work represents an important step toward fully automating machine learning pipeline design."
            },
            "slug": "Evaluation-of-a-Tree-based-Pipeline-Optimization-Olson-Bartley",
            "title": {
                "fragments": [],
                "text": "Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper implements an open source Tree-based Pipeline Optimization Tool (TPOT) in Python and shows that TPOT can design machine learning pipelines that provide a significant improvement over a basic machine learning analysis while requiring little to no input nor prior knowledge from the user."
            },
            "venue": {
                "fragments": [],
                "text": "GECCO"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40078651"
                        ],
                        "name": "Mudit Agrawal",
                        "slug": "Mudit-Agrawal",
                        "structuredName": {
                            "firstName": "Mudit",
                            "lastName": "Agrawal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mudit Agrawal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ackground and images, but the task has evolved to not only segregating these basic structures but also derived structures like paragraphs, lists and tables. Various image processing methodologies [8] [1] [7] have approached the problem of understanding general documents as well as digitizing historical documents. With the onset of deep learning and data driven approaches, the problem was approached a"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "by \ufb01nding the smallest entities like words or characters and attempt to aggregate them using a distance metric and an aggregation algorithm like K-Nearest Neighbors or K-D Trees. These approaches [8] [1] [7] have the advantage of being mostly unsupervised but involve tuning a lot of heuristics. They are also not scalable to document layouts which are different from those the algorithm is tuned on. Co"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3355513,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6dc458247092edc982718af4481c6400134b0852",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a dynamic approach to document page segmentation. Current page segmentation algorithms lack the ability to dynamically adapt local variations in the size, orientation and distance of components within a page. Our approach builds upon one of the best algorithms, Kise et. al. work based on Area Voronoi Diagrams, which adapts globally to page content to determine algorithm parameters. In our approach, local thresholds are determined dynamically based on parabolic relations between components, and Docstrum based angular and neighborhood features are integrated to improve accuracy. Zone-based evaluation was performed on four sets of printed and handwritten documents in English and Arabic scripts and an increase of 33% in accuracy is reported."
            },
            "slug": "Voronoi++:-A-Dynamic-Page-Segmentation-Approach-on-Agrawal-Doermann",
            "title": {
                "fragments": [],
                "text": "Voronoi++: A Dynamic Page Segmentation Approach Based on Voronoi and Docstrum Features"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "In this approach, local thresholds are determined dynamically based on parabolic relations between components, and Docstrum based angular and neighborhood features are integrated to improve accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "2009 10th International Conference on Document Analysis and Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398550688"
                        ],
                        "name": "L. O'Gorman",
                        "slug": "L.-O'Gorman",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "O'Gorman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. O'Gorman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 69
                            }
                        ],
                        "text": "The most popular and widely used of these approaches is the Docstrum [8] algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 7
                            }
                        ],
                        "text": "We use Docstrum to construct blocks and then determine the accuracy using the manually annotated ground truth results on both the target datasets ie."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 17
                            }
                        ],
                        "text": "These approaches [8] [1] [7] have the advantage of being mostly unsupervised but involve tuning a lot of heuristics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 39
                            }
                        ],
                        "text": "Various image processing methodologies [8] [1] [7] have approached the problem of understanding general documents as well as digitizing historical documents."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 34
                            }
                        ],
                        "text": "Baselines: The Docstrum algorithm [8] serves as our baseline."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 22995244,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d85097da36118fbccfeb7802abf89bf4b4c63a3e",
            "isKey": true,
            "numCitedBy": 728,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Page layout analysis is a document processing technique used to determine the format of a page. This paper describes the document spectrum (or docstrum), which is a method for structural page layout analysis based on bottom-up, nearest-neighbor clustering of page components. The method yields an accurate measure of skew, within-line, and between-line spacings and locates text lines and text blocks. It is advantageous over many other methods in three main ways: independence from skew angle, independence from different text spacings, and the ability to process local regions of different text orientations within the same image. Results of the method shown for several different page formats and for randomly oriented subpages on the same image illustrate the versatility of the method. We also discuss the differences, advantages, and disadvantages of the docstrum with respect to other lay-out methods. >"
            },
            "slug": "The-Document-Spectrum-for-Page-Layout-Analysis-O'Gorman",
            "title": {
                "fragments": [],
                "text": "The Document Spectrum for Page Layout Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The document spectrum (or docstrum), which is a method for structural page layout analysis based on bottom-up, nearest-neighbor clustering of page components, yields an accurate measure of skew, within-line, and between-line spacings and locates text lines and text blocks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "SSD : single shot multibox detec"
            },
            "venue": {
                "fragments": [],
                "text": "Evaluation of deep convolutional nets for document image classification and retrieval"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 25
                            }
                        ],
                        "text": "These approaches [8] [1] [7] have the advantage of being mostly unsupervised but involve tuning a lot of heuristics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 47
                            }
                        ],
                        "text": "Various image processing methodologies [8] [1] [7] have approached the problem of understanding general documents as well as digitizing historical documents."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Document Structure and Layout Analysis, pages 29\u201348"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 6,
            "methodology": 9
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 14,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Multidomain-Document-Layout-Understanding-using-Few-Singh-Varadarajan/12eede12e8673888e07eb59b922227b2d0f92969?sort=total-citations"
}