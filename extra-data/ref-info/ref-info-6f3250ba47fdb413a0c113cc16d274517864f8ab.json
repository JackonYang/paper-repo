{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680292"
                        ],
                        "name": "P. Resnik",
                        "slug": "P.-Resnik",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Resnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Resnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1752785,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "265be00bf112c6cb2fa3e8176bff8394a114dbde",
            "isKey": false,
            "numCitedBy": 3890,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new measure of semantic similarity in an IS-A taxonomy, based on the notion of information content. Experimental evaluation suggests that the measure performs encouragingly well (a correlation of r = 0.79 with a benchmark set of human similarity judgments, with an upper bound of r = 0.90 for human subjects performing the same task), and significantly better than the traditional edge counting approach (r = 0.66)."
            },
            "slug": "Using-Information-Content-to-Evaluate-Semantic-in-a-Resnik",
            "title": {
                "fragments": [],
                "text": "Using Information Content to Evaluate Semantic Similarity in a Taxonomy"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This paper presents a new measure of semantic similarity in an IS-A taxonomy, based on the notion of information content, which performs encouragingly well and is significantly better than the traditional edge counting approach."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5659557,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc0c3033ea7d4e19e1f5ac71934759507e126162",
            "isKey": false,
            "numCitedBy": 4466,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Similarity is an important and widely used concept. Previous definitions of similarity are tied to a particular application or a form of knowledge representation. We present an informationtheoretic definition of similarity that is applicable as long as there is a probabilistic model. We demonstrate how our definition can be used to measure the similarity in a number of different domains."
            },
            "slug": "An-Information-Theoretic-Definition-of-Similarity-Lin",
            "title": {
                "fragments": [],
                "text": "An Information-Theoretic Definition of Similarity"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This work presents an informationtheoretic definition of similarity that is applicable as long as there is a probabilistic model and demonstrates how this definition can be used to measure the similarity in a number of different domains."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 46
                            }
                        ],
                        "text": "The Dice coefficient (Smadja et al., 1996; D. Lin, 1998a, 1998b) (Kay and Ro\u0308scheisen, 1993) is monotonic in Jaccard\u2019s coefficient (van Rijsbergen, 1979), so its inclusion in our experiments would be redundant."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 134
                            }
                        ],
                        "text": "In the language-modeling domain, there is currently no agreed-upon best similarity metric because there is no agreement on what the \u201csame information\u201d \u2014 the key data that a similarity function should incorporate \u2014 is."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 3
                            }
                        ],
                        "text": "D. Lin (1997; 1998a) takes an axiomatic approach to determining the characteristics of a good similarity measure."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 129
                            }
                        ],
                        "text": "Arguably the most widely used is the mutual information (Hindle, 1990; Church and Hanks, 1990; Dagan et al., 1995; Luk, 1995; D. Lin, 1998a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15698938,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd1901f34cc3673072264104885d70555b1a4cdc",
            "isKey": true,
            "numCitedBy": 1928,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is."
            },
            "slug": "Automatic-Retrieval-and-Clustering-of-Similar-Words-Lin",
            "title": {
                "fragments": [],
                "text": "Automatic Retrieval and Clustering of Similar Words"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A word similarity measure based on the distributional pattern of words allows the automatically constructed thesaurus to be significantly closer to WordNet than Roget Thesaurus is."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 46
                            }
                        ],
                        "text": "The Dice coefficient (Smadja et al., 1996; D. Lin, 1998a, 1998b) (Kay and Ro\u0308scheisen, 1993) is monotonic in Jaccard\u2019s coefficient (van Rijsbergen, 1979), so its inclusion in our experiments would be redundant."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 3
                            }
                        ],
                        "text": "D. Lin (1997; 1998a) takes an axiomatic approach to determining the characteristics of a good similarity measure."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 129
                            }
                        ],
                        "text": "Arguably the most widely used is the mutual information (Hindle, 1990; Church and Hanks, 1990; Dagan et al., 1995; Luk, 1995; D. Lin, 1998a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 219304873,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0527fc6d553fafd894758c44e8ecc7b77dbecc69",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-Retrieval-and-Clustering-of-Similar-Words-Lin",
            "title": {
                "fragments": [],
                "text": "Automatic Retrieval and Clustering of Similar Words"
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21169546"
                        ],
                        "name": "Donald Hindle",
                        "slug": "Donald-Hindle",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Hindle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald Hindle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 57
                            }
                        ],
                        "text": "Arguably the most widely used is the mutual information (Hindle, 1990; Church and Hanks, 1990; Dagan et al., 1995; Luk, 1995; D. Lin, 1998a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15862538,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "f3f3dcfcaa960ec201e0381f4d026e57e64bea76",
            "isKey": false,
            "numCitedBy": 689,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A method of determining the similarity of nouns on the basis of a metric derived from the distribution of subject, verb and object in a large text corpus is described. The resulting quasi-semantic classification of nouns demonstrates the plausibility of the distributional hypothesis, and has potential application to a variety of tasks, including automatic indexing, resolving nominal compounds, and determining the scope of modification."
            },
            "slug": "Noun-Classification-from-Predicate-Argument-Hindle",
            "title": {
                "fragments": [],
                "text": "Noun Classification from Predicate-Argument Structures"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The resulting quasi-semantic classification of nouns demonstrates the plausibility of the distributional hypothesis, and has potential application to a variety of tasks, including automatic indexing, resolving nominal compounds, and determining the scope of modification."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145810617"
                        ],
                        "name": "Lillian Lee",
                        "slug": "Lillian-Lee",
                        "structuredName": {
                            "firstName": "Lillian",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lillian Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2032603,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbe1bf905eacefd9758696d3805f23325d5aa5d2",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 151,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical methods for automatically extracting information about associations between words or documents from large collections of text have the potential to have considerable impact in a number of areas, such as information retrieval and natural-language-based user interfaces. However, even huge bodies of text yield highly unreliable estimates of the probability of relatively common events, and, in fact, perfectly reasonable events may not occur in the training data at all. This is known as the sparse data problem. Traditional approaches to the sparse data problem use crude approximations. We propose a different solution: if we are able to organize the data into classes of similar events, then, if information about an event is lacking, we can estimate its behavior from information about similar events. This thesis presents two such similarity-based approaches, where, in general, we measure similarity by the Kullback-Leibler divergence, an information-theoretic quantity. \nOur first approach is to build soft, hierarchical clusters: soft, because each event belongs to each cluster with some probability; hierarchical, because cluster centroids are iteratively split to model finer distinctions. Our clustering method, which uses the technique of deterministic annealing, represents (to our knowledge) the first application of soft clustering to problems in natural language processing. We use this method to cluster words drawn from 44 million words of Associated Press Newswire and 10 million words from Grolier's encyclopedia, and find that language models built from the clusters have substantial predictive power. Our algorithm also extends with no modification to other domains, such as document clustering. \nOur second approach is a nearest-neighbor approach: instead of calculating a centroid for each class, we in essence build a cluster around each word. We compare several such nearest-neighbor approaches on a word sense disambiguation task and find that as a whole, their performance is far superior to that of standard methods. In another set of experiments, we show that using estimation techniques based on the nearest-neighbor model enables us to achieve perplexity reductions of more than 20 percent over standard techniques in the prediction of low-frequency events, and statistically significant speech recognition error-rate reduction."
            },
            "slug": "Similarity-Based-Approaches-to-Natural-Language-Lee",
            "title": {
                "fragments": [],
                "text": "Similarity-Based Approaches to Natural Language Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The clustering method, which uses the technique of deterministic annealing, represents (to the authors' knowledge) the first application of soft clustering to problems in natural language processing, and is compared to several such nearest-neighbor approaches on a word sense disambiguation task and finds that as a whole, their performance is far superior to that of standard methods."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144908550"
                        ],
                        "name": "Jianhua Lin",
                        "slug": "Jianhua-Lin",
                        "structuredName": {
                            "firstName": "Jianhua",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianhua Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 8
                            }
                        ],
                        "text": "An alternative approach is distance-weighted averaging, which arrives at an estimate for unseen cooccurrences by combining estimates for\ncooccurrences involving similar words:1\nP\u0302 (v|n) =\n\u2211\nm\u2208S(n) sim(n,m)P (v|m) \u2211\nm\u2208S(n) sim(n,m) , (1)\nwhere S(n) is a set of candidate similar words and sim(n,m) is\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 104
                            }
                        ],
                        "text": "We also plan to evaluate skewed versions of the Jensen-Shannon divergence proposed by Rao (1982) and J. Lin (1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 66
                            }
                        ],
                        "text": "Previously, we found the Jensen-Shannon divergence (Rao, 1982; J. Lin, 1991) to be a useful measure of the distance between distributions:\nJS(q, r) = 1\n2\n[ D ( q \u2225 \u2225\n\u2225 avgq,r\n) + D ( r \u2225 \u2225\n\u2225 avgq,r\n)]\n."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12121632,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b586837ac372141eedafd124c57335cbc893bed",
            "isKey": false,
            "numCitedBy": 3505,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel class of information-theoretic divergence measures based on the Shannon entropy is introduced. Unlike the well-known Kullback divergences, the new measures do not require the condition of absolute continuity to be satisfied by the probability distributions involved. More importantly, their close relationship with the variational distance and the probability of misclassification error are established in terms of bounds. These bounds are crucial in many applications of divergence measures. The measures are also well characterized by the properties of nonnegativity, finiteness, semiboundedness, and boundedness. >"
            },
            "slug": "Divergence-measures-based-on-the-Shannon-entropy-Lin",
            "title": {
                "fragments": [],
                "text": "Divergence measures based on the Shannon entropy"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "A novel class of information-theoretic divergence measures based on the Shannon entropy is introduced, which do not require the condition of absolute continuity to be satisfied by the probability distributions involved and are established in terms of bounds."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7465342"
                        ],
                        "name": "Ido Dagan",
                        "slug": "Ido-Dagan",
                        "structuredName": {
                            "firstName": "Ido",
                            "lastName": "Dagan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ido Dagan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055976119"
                        ],
                        "name": "S. Marcus",
                        "slug": "S.-Marcus",
                        "structuredName": {
                            "firstName": "Shaul",
                            "lastName": "Marcus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Marcus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2309269"
                        ],
                        "name": "Shaul Markovitch",
                        "slug": "Shaul-Markovitch",
                        "structuredName": {
                            "firstName": "Shaul",
                            "lastName": "Markovitch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaul Markovitch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 56
                            }
                        ],
                        "text": "Arguably the most widely used is the mutual information (Hindle, 1990; Church and Hanks, 1990; Dagan et al., 1995; Luk, 1995; D. Lin, 1998a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1154960,
            "fieldsOfStudy": [
                "Computer Science",
                "Environmental Science"
            ],
            "id": "8310a3f4ec3d6a37958c1c2aefe80b7b7badbca0",
            "isKey": false,
            "numCitedBy": 140,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract In recent years there is much interest in word co-occurrence relations, such as n-grams, verb\u2013object combinations, or co-occurrence within a limited context. This paper discusses how to estimate the likelihood of co-occurrences that do not occur in the training data. We present a method that makes local analogies between each specific unobserved co-occurrence and other co-occurrences that contain similar words. These analogies are based on the assumption that similar word co-occurrences have similar values of mutual information. Accordingly, the word similarity metric captures similarities between vectors of mutual information values. Our evaluation suggests that this method performs better than existing, frequency-based, smoothing methods, and may provide an alternative to class-based models. A background survey is included, covering issues of lexical co-occurrence, data sparseness and smoothing, word similarity and clustering, and mutual information."
            },
            "slug": "Contextual-Word-Similarity-and-Estimation-From-Data-Dagan-Marcus",
            "title": {
                "fragments": [],
                "text": "Contextual Word Similarity and Estimation From Sparse Data"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The evaluation suggests that this method performs better than existing, frequency-based, smoothing methods, and may provide an alternative to class-based models."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144532256"
                        ],
                        "name": "C. R. Rao",
                        "slug": "C.-R.-Rao",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Rao",
                            "middleNames": [
                                "Radhakrishna"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. R. Rao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 86
                            }
                        ],
                        "text": "We also plan to evaluate skewed versions of the Jensen-Shannon divergence proposed by Rao (1982) and J. Lin (1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 52
                            }
                        ],
                        "text": "Previously, we found the Jensen-Shannon divergence (Rao, 1982; J. Lin, 1991) to be a useful measure of the distance between distributions:\nJS(q, r) = 1\n2\n[ D ( q \u2225 \u2225\n\u2225 avgq,r\n) + D ( r \u2225 \u2225\n\u2225 avgq,r\n)]\n."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 124998305,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "ac81e9d6ae1984e421bdcba141a360a4df086839",
            "isKey": false,
            "numCitedBy": 245,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Two general methods of obtaining measures of diversity within a population are discussed. One is based on an intrinsic notion of dissimilarity between individuals and the other makes use of the concepts of entropy. Some examples are given of the decomposition of diversity within a population in terms of given or conceptual factors. Methods for apportionment of diversity in a hierarchically classified set of populations are discussed. The concept of analysis of diversity as a generalization of analysis of variance is developed for populations classified by combinations of different levels of chosen factors."
            },
            "slug": "Diversity:-its-measurement,-decomposition,-and-Rao",
            "title": {
                "fragments": [],
                "text": "Diversity: its measurement, decomposition, apportionment and analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144329844"
                        ],
                        "name": "A. Luk",
                        "slug": "A.-Luk",
                        "structuredName": {
                            "firstName": "Alpha",
                            "lastName": "Luk",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Luk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 115
                            }
                        ],
                        "text": "Arguably the most widely used is the mutual information (Hindle, 1990; Church and Hanks, 1990; Dagan et al., 1995; Luk, 1995; D. Lin, 1998a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9177224,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3650a346da2656a6d441470c50c2f85ff076532",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Corpus-based sense disambiguation methods, like most other statistical NLP approaches, suffer from the problem of data sparseness. In this paper, we describe an approach which overcomes this problem using dictionary definitions. Using the definition-based conceptual co-occurrence data collected from the relatively small Brown corpus, our sense disambiguation system achieves an average accuracy comparable to human performance given the same contextual information."
            },
            "slug": "Statistical-Sense-Disambiguation-with-Relatively-Luk",
            "title": {
                "fragments": [],
                "text": "Statistical Sense Disambiguation with Relatively Small Corpora Using Dictionary Definitions"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Using the definition-based conceptual co-occurrence data collected from the relatively small Brown corpus, the sense disambiguation system achieves an average accuracy comparable to human performance given the same contextual information."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788050"
                        ],
                        "name": "R. Grishman",
                        "slug": "R.-Grishman",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Grishman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Grishman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144360624"
                        ],
                        "name": "J. Sterling",
                        "slug": "J.-Sterling",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Sterling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sterling"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12462551,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56312d6759bed1f096e083f646b0f44a061f620a",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Frequency information on co-occurrence patterns can be automatically collected from a syntactically analyzed corpus; this information can then serve as the basis for selectional constraints when analyzing new text from the same domain. Better coverage of the domain can be obtained by appropriate generalization of the specific word patterns which are collected. We report here on an approach to automatically make suitable generalizations: using the co-occurrence data to compute a confusion matrix relating individual words, and then using the confusion matrix to smooth the original frequency data."
            },
            "slug": "Smoothing-of-Automatically-Generated-Selectional-Grishman-Sterling",
            "title": {
                "fragments": [],
                "text": "Smoothing of Automatically Generated Selectional Constraints"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An approach to automatically make suitable generalizations is reported on: using the co-occurrence data to compute a confusion matrix relating individual words, and then using the confusion matrix to smooth the original frequency data."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "118049359"
                        ],
                        "name": "W. Jones",
                        "slug": "W.-Jones",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Jones",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737579"
                        ],
                        "name": "G. Furnas",
                        "slug": "G.-Furnas",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Furnas",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Furnas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 129
                            }
                        ],
                        "text": "On the other hand, while there have been many similarity measures proposed and analyzed in the information retrieval literature (Jones and Furnas, 1987), there has been some doubt expressed in that community that the choice of similarity metric has any practical impact:\nSeveral authors have pointed\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9135369,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3a720f42ae2e68d1a6f5fe19f8170532e0caef73",
            "isKey": false,
            "numCitedBy": 293,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We want computer systems that can help us assess the similarity or relevance of existing objects (e.g., documents, functions, commands, etc.) to a statement of our current needs (e.g., the query). Towards this end, a variety of similarity measures have been proposed. However, the relationship between a measure's formula and its performance is not always obvious. A geometric analysis is advanced and its utility demonstrated through its application to six conventional information retrieval similarity measures and a seventh spreading activation measure. All seven similarity measures work with a representational scheme wherein a query and the database objects are represented as vectors of term weights. A geometric analysis characterizes each similarity measure by the nature of its iso\u2010similarity contours in an n\u2010space containing query and object vectors. This analysis reveals important differences among the similarity measures and suggests conditions in which these differences will affect retrieval performance. The cosine coefficient, for example, is shown to be insensitive to between\u2010document differences in the magnitude of term weights while the inner product measure is sometimes overly affected by such differences. The context\u2010sensitive spreading activation measure may overcome both of these limitations and deserves further study. The geometric analysis is intended to complement, and perhaps to guide, the empirical analysis of similarity measures. \u00a9 1987 John Wiley & Sons, Inc."
            },
            "slug": "Pictures-of-relevance:-A-geometric-analysis-of-Jones-Furnas",
            "title": {
                "fragments": [],
                "text": "Pictures of relevance: A geometric analysis of similarity measures"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A geometric analysis is advanced and its utility demonstrated through its application to six conventional information retrieval similarity measures and a seventh spreading activation measure, intended to complement, and perhaps to guide, the empirical analysis of similarity measures."
            },
            "venue": {
                "fragments": [],
                "text": "J. Am. Soc. Inf. Sci."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3316623"
                        ],
                        "name": "Jakub Zavrel",
                        "slug": "Jakub-Zavrel",
                        "structuredName": {
                            "firstName": "Jakub",
                            "lastName": "Zavrel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jakub Zavrel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735272"
                        ],
                        "name": "Walter Daelemans",
                        "slug": "Walter-Daelemans",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Daelemans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Walter Daelemans"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1138221,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41c00f7fbec0ab603836357d020cef7843ef7405",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper analyses the relation between the use of similarity in Memory-Based Learning and the notion of backed-off smoothing in statistical language modeling. We show that the two approaches are closely related, and we argue that feature weighting methods in the Memory-Based paradigm can offer the advantage of automatically specifying a suitable domain-specific hierarchy between most specific and most general conditioning information without the need for a large number of parameters. We report two applications of this approach: PP-attachment and POS-tagging. Our method achieves state-of-the-art performance in both domains, and allows the easy integration of diverse information sources, such as rich lexical representations."
            },
            "slug": "Memory-Based-Learning:-Using-Similarity-for-Zavrel-Daelemans",
            "title": {
                "fragments": [],
                "text": "Memory-Based Learning: Using Similarity for Smoothing"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is argued that feature weighting methods in the Memory-Based paradigm can offer the advantage of automatically specifying a suitable domain-specific hierarchy between most specific and most general conditioning information without the need for a large number of parameters."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2037240"
                        ],
                        "name": "U. Essen",
                        "slug": "U.-Essen",
                        "structuredName": {
                            "firstName": "Ute",
                            "lastName": "Essen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Essen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3214660"
                        ],
                        "name": "Volker Steinbiss",
                        "slug": "Volker-Steinbiss",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "Steinbiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Volker Steinbiss"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 88
                            }
                        ],
                        "text": "These unseen events generally make up a substantial portion of novel data; for example, Essen and Steinbiss (1992) report that 12% of the test-set bigrams in a 75%-25% split of one million words did not occur in the training partition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 125
                            }
                        ],
                        "text": "The confusion probability has been used by several authors to smooth word cooccurrence probabilities (Sugawara et al., 1985; Essen and Steinbiss, 1992; Grishman and Sterling, 1993); it measures the degree to which word m can be substituted into the contexts in which n appears."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62555344,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c80bcd31f077f9a632ce959278d1c2fc095131a8",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Training corpora for stochastic language models are virtually always too small for maximum-likelihood estimation, so smoothing the models is of great importance. The authors derive the cooccurrence smoothing technique for stochastic language modeling and give experimental evidence for its validity. Using word-bigram language models, cooccurrence smoothing improved the test-set perplexity by 14% on a German 100000-word text corpus and by 10% on an English 1-million word corpus.<<ETX>>"
            },
            "slug": "Cooccurrence-smoothing-for-stochastic-language-Essen-Steinbiss",
            "title": {
                "fragments": [],
                "text": "Cooccurrence smoothing for stochastic language modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Using word-bigram language models, cooccurrence smoothing improved the test-set perplexity by 14% on a German 100000-word text corpus and by 10% on an English 1-million word corpus."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings] ICASSP-92: 1992 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47137139"
                        ],
                        "name": "Yael Karov",
                        "slug": "Yael-Karov",
                        "structuredName": {
                            "firstName": "Yael",
                            "lastName": "Karov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yael Karov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2331213"
                        ],
                        "name": "S. Edelman",
                        "slug": "S.-Edelman",
                        "structuredName": {
                            "firstName": "Shimon",
                            "lastName": "Edelman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Edelman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6798775,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0be2200109cc9da076b84b552f623aca476150ae",
            "isKey": false,
            "numCitedBy": 147,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a method for automatic word sense disambiguation using a text corpus and a machine-readble dictionary (MRD). The method is based on word similarity and context similarity measures. Words are considered similar if they appear in similar contexts; contexts are similar if they contain similar words. The circularity of this definition is resolved by an iterative, converging process, in which the system learns from the corpus a set of typical usages for each of the senses of the polysemous word listed in the MRD. A new instance of a polysemous word is assigned the sense associated with the typical usage most similar to its context. Experiments show that this method can learn even from very sparse training data, achieving over 92% correct disambiguation performance."
            },
            "slug": "Similarity-based-Word-Sense-Disambiguation-Karov-Edelman",
            "title": {
                "fragments": [],
                "text": "Similarity-based Word Sense Disambiguation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Experiments show that this method can learn even from very sparse training data, achieving over 92% correct disambiguation performance."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1394805,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "30a8f7577c1ccdbd14dcea85efdfae5420e1ccb6",
            "isKey": false,
            "numCitedBy": 227,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Most previous corpus-based algorithms disambiguate a word with a classifier trained from previous usages of the same word. Separate classifiers have to be trained for different words. We present an algorithm that uses the same knowledge sources to disambiguate different words. The algorithm does not require a sense-tagged corpus and exploits the fact that two different words are likely to have similar meanings if they occur in identical local contexts."
            },
            "slug": "Using-Syntactic-Dependency-as-Local-Context-to-Word-Lin",
            "title": {
                "fragments": [],
                "text": "Using Syntactic Dependency as Local Context to Resolve Word Sense Ambiguity"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "An algorithm is presented that uses the same knowledge sources to disambiguate different words and does not require a sense-tagged corpus and exploits the fact that two different words are likely to have similar meanings if they occur in identical local contexts."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799688"
                        ],
                        "name": "V. Hatzivassiloglou",
                        "slug": "V.-Hatzivassiloglou",
                        "structuredName": {
                            "firstName": "Vasileios",
                            "lastName": "Hatzivassiloglou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Hatzivassiloglou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 78
                            }
                        ],
                        "text": "Finally, Kendall\u2019s \u03c4 , which appears in work on clustering similar adjectives (Hatzivassiloglou and McKeown, 1993; Hatzivassiloglou, 1996), is a nonparametric measure of the association between random variables (Gibbons, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 177
                            }
                        ],
                        "text": "\u2026unigram probabilities as well as the two distributions q and r.\nFinally, Kendall\u2019s \u03c4 , which appears in work on clustering similar adjectives (Hatzivassiloglou and McKeown, 1993; Hatzivassiloglou, 1996), is a nonparametric measure of the association between random variables (Gibbons, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1380096,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "0c72635a82dfd5434f31c409b988aad5a75fef04",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a comparative analysis of the performance of a statistics-based system for the formation of semantic groups of adjectives when various sources of linguistic knowledge are introduced. We identify four different types of slufllow linguistic knowledge that are applicable to this system, and we quantify the performance gained by incorporating each such knowledge module, We perform experiments for different corpus sizes and different inputs (sets of adjectives to group), collect clam on the usaful.ness of each linguistic module, assess the statistical significance of the results, and compare the contributions of the linguistic knowledge sources against each other. We also assess the overall effect linguistic knowledge has in our system. Our results show that linguistic knowledge causes a significant increase in the performance of the system. We conclude by discussing how these positive restdts can be generalized to other problems in statistical NLP."
            },
            "slug": "Do-we-Need-Linguistics-When-We-Have-Statistics-A-of-Hatzivassiloglou",
            "title": {
                "fragments": [],
                "text": "Do we Need Linguistics When We Have Statistics? A Comparative Analysis of the Contributions of Linguistic Cues to a Statistical Word Grouping System"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The results show that linguistic knowledge causes a significant increase in the performance of the system, and it is discussed how these positive restdts can be generalized to other problems in statistical NLP."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69056502"
                        ],
                        "name": "J. Gibbons",
                        "slug": "J.-Gibbons",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Gibbons",
                            "middleNames": [
                                "Dickinson"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gibbons"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 287,
                                "start": 274
                            }
                        ],
                        "text": "\u2026unigram probabilities as well as the two distributions q and r.\nFinally, Kendall\u2019s \u03c4 , which appears in work on clustering similar adjectives (Hatzivassiloglou and McKeown, 1993; Hatzivassiloglou, 1996), is a nonparametric measure of the association between random variables (Gibbons, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "ndall\u2019s \u03c4, which appears in work on clustering similar adjectives (Hatzivassiloglou and McKeown, 1993; Hatzivassiloglou, 1996), is a nonparametric measure of the association between random variables (Gibbons, 1993). In our context, it looks for correlation between the behavior of q and r on pairs of verbs. Three versions exist; we use the simplest, \u03c4a, here: \u03c4(q,r) = X v1,v2 sign[(q(v1)\u2212q(v2))(r(v1)\u2212r(v2))] 2 |"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 120238078,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "650412853f01a9c62774eabe61da537a4acc09f6",
            "isKey": false,
            "numCitedBy": 154,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction Spearman's Rho and Kendall's Tau as Descriptive Measures of Association Inferences Based on Rho and Tau Kendall's Coefficient of Concordance Partial Correlation Measures of Association in Ordered Contingency Tables Summary"
            },
            "slug": "Nonparametric-measures-of-association-Gibbons",
            "title": {
                "fragments": [],
                "text": "Nonparametric measures of association"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34789794"
                        ],
                        "name": "H. Ng",
                        "slug": "H.-Ng",
                        "structuredName": {
                            "firstName": "Hwee",
                            "lastName": "Ng",
                            "middleNames": [
                                "Tou"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3161371"
                        ],
                        "name": "H. Lee",
                        "slug": "H.-Lee",
                        "structuredName": {
                            "firstName": "Hian",
                            "lastName": "Lee",
                            "middleNames": [
                                "Beng"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11202365,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd303a438cef5eb647c63cd1f25bad12a5babba3",
            "isKey": false,
            "numCitedBy": 568,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a new approach for word sense disambiguation (WSD) using an exemplar-based learning algorithm. This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech of neighboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation. We tested our WSD program, named LEXAS, on both a common data set used in previous work, as well as on a large sense-tagged corpus that we separately constructed. LEXAS achieves a higher accuracy on the common data set, and performs better than the most frequent heuristic on the highly ambiguous words in the large corpus tagged with the refined senses of WORDNET."
            },
            "slug": "Integrating-Multiple-Knowledge-Sources-to-Word-An-Ng-Lee",
            "title": {
                "fragments": [],
                "text": "Integrating Multiple Knowledge Sources to Disambiguate Word Sense: An Exemplar-Based Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech of neighboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799688"
                        ],
                        "name": "V. Hatzivassiloglou",
                        "slug": "V.-Hatzivassiloglou",
                        "structuredName": {
                            "firstName": "Vasileios",
                            "lastName": "Hatzivassiloglou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Hatzivassiloglou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145590324"
                        ],
                        "name": "K. McKeown",
                        "slug": "K.-McKeown",
                        "structuredName": {
                            "firstName": "Kathleen",
                            "lastName": "McKeown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. McKeown"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 141
                            }
                        ],
                        "text": "\u2026unigram probabilities as well as the two distributions q and r.\nFinally, Kendall\u2019s \u03c4 , which appears in work on clustering similar adjectives (Hatzivassiloglou and McKeown, 1993; Hatzivassiloglou, 1996), is a nonparametric measure of the association between random variables (Gibbons,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8738143,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4bf9fb7b8aaf072497ce0cfbb046cd7451473e62",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a method to group adjectives according to their meaning, as a first step towards the automatic identification of adjectival scales. We discuss the properties of adjectival scales and of groups of semantically related adjectives and how they imply sources of linguistic knowledge in text corpora. We describe how our system exploits this linguistic knowledge to compute a measure of similarity between two adjectives, using statistical techniques and without having access to any semantic information about the adjectives. We also show how a clustering algorithm can use these similarities to produce the groups of adjectives, and we present results produced by our system for a sample set of adjectives. We conclude by presenting evaluation methods for the task at hand, and analyzing the significance of the results obtained."
            },
            "slug": "Towards-the-Automatic-Identification-of-Adjectival-Hatzivassiloglou-McKeown",
            "title": {
                "fragments": [],
                "text": "Towards the Automatic Identification of Adjectival Scales: Clustering Adjectives According to Meaning"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "This paper presents a method to group adjectives according to their meaning, as a first step towards the automatic identification of adjectival scales, and shows how a clustering algorithm can use similarities to produce the groups of adjectives."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34789794"
                        ],
                        "name": "H. Ng",
                        "slug": "H.-Ng",
                        "structuredName": {
                            "firstName": "Hwee",
                            "lastName": "Ng",
                            "middleNames": [
                                "Tou"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 142
                            }
                        ],
                        "text": "Variations of the value difference metric (Stanfill and Waltz, 1986) have been employed for supervised disambiguation (Ng and H.B. Lee, 1996; Ng, 1997); but it is not reasonable in language modeling to expect training data tagged with correct probabilities."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59986,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f524d92be29a2c969680977994090c0aee31e22",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we report recent improvements to the exemplar-based learning approach for word sense disambiguation that have achieved higher disambiguation accuracy. By using a larger value of k, the number of nearest neighbors to use for determining the class of a test example, and through 10-fold cross validation to automatically determine the best k, we have obtained improved disambiguation accuracy on a large sense-tagged corpus first used in (Ng and Lee, 1996). The accuracy achieved by our improved exemplar-based classifier is comparable to the accuracy on the same data set obtained by the Naive-Bayes algorithm, which was reported in (Mooney, 1996) to have the highest disambiguation accuracy among seven state-of-the-art machine learning algorithms."
            },
            "slug": "Exemplar-Based-Word-Sense-Disambiguation\u201d-Some-Ng",
            "title": {
                "fragments": [],
                "text": "Exemplar-Based Word Sense Disambiguation\u201d Some Recent Improvements"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The accuracy achieved by the improved exemplar-based classifier is comparable to the accuracy on the same data set obtained by the Naive-Bayes algorithm, which was reported to have the highest disambiguation accuracy among seven state-of-the-art machine learning algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145493610"
                        ],
                        "name": "M. Kay",
                        "slug": "M.-Kay",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Kay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2573037"
                        ],
                        "name": "Martin R\u00f6scheisen",
                        "slug": "Martin-R\u00f6scheisen",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "R\u00f6scheisen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin R\u00f6scheisen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 19
                            }
                        ],
                        "text": "Lin, 1998a, 1998b) (Kay and R\u00f6scheisen, 1993) is monotonic in Jaccard\u2019s coefficient (van Rijsbergen, 1979), so its inclusion in our experiments would be redundant."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 66
                            }
                        ],
                        "text": "The Dice coefficient (Smadja et al., 1996; D. Lin, 1998a, 1998b) (Kay and Ro\u0308scheisen, 1993) is monotonic in Jaccard\u2019s coefficient (van Rijsbergen, 1979), so its inclusion in our experiments would be redundant."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14531125,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a0b61625e60a419bd5ea1d892047a65a73d9f0c4",
            "isKey": false,
            "numCitedBy": 410,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an algorithm for aligning texts with their translations that is based only on internal evidence. The relaxation process rests on a notion of which word in one text corresponds to which word in the other text that is essentially based on the similarity of their distributions. It exploits a partial alignment of the word level to induce a maximum likelihood alignment of the sentence level, which is in turn used, in the next iteration, to refine the word level estimate. The algorithm appears to converge to the correct sentence alignment in only a few iterations."
            },
            "slug": "Text-Translation-Alignment-Kay-R\u00f6scheisen",
            "title": {
                "fragments": [],
                "text": "Text-Translation Alignment"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "An algorithm for aligning texts with their translations that is based only on internal evidence and appears to converge to the correct sentence alignment in only a few iterations is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35229948"
                        ],
                        "name": "S. Katz",
                        "slug": "S.-Katz",
                        "structuredName": {
                            "firstName": "Slava",
                            "lastName": "Katz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Katz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6555412,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0130277677e5b915d5cd86b3afafd77fd08eb2e",
            "isKey": false,
            "numCitedBy": 1908,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The description of a novel type of m-gram language model is given. The model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data. This solution compares favorably to other proposed methods. While the method has been developed for and successfully implemented in the IBM Real Time Speech Recognizers, its generality makes it applicable in other areas where the problem of estimating probabilities from sparse data arises."
            },
            "slug": "Estimation-of-probabilities-from-sparse-data-for-of-Katz",
            "title": {
                "fragments": [],
                "text": "Estimation of probabilities from sparse data for the language model component of a speech recognizer"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data, and compares favorably to other proposed methods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145165877"
                        ],
                        "name": "P. Hanks",
                        "slug": "P.-Hanks",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Hanks",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hanks"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 87
                            }
                        ],
                        "text": "In the language-modeling domain, there is currently no agreed-upon best similarity metric because there is no agreement on what the \u201csame information\u201d \u2014 the key data that a similarity function should incorporate \u2014 is."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9558665,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "9e2caa39ac534744a180972a30a320ad0ae41ea3",
            "isKey": false,
            "numCitedBy": 4363,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The term word association is used in a very particular sense in the psycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor. ) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose an objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable.) The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words."
            },
            "slug": "Word-Association-Norms,-Mutual-Information-and-Church-Hanks",
            "title": {
                "fragments": [],
                "text": "Word Association Norms, Mutual Information and Lexicography"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117791658"
                        ],
                        "name": "G. Catlin",
                        "slug": "G.-Catlin",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Catlin",
                            "middleNames": [
                                "Edward",
                                "Gordon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Catlin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4072383,
            "fieldsOfStudy": [
                "Sociology"
            ],
            "id": "58314b0e893e150ec64ea5e5f7e83766821b1019",
            "isKey": false,
            "numCitedBy": 225,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "AN editorial appeared in The Times of March 28 under the heading \u201cSocial Surveys\u201d, accompanied on the same day by a special article entitled \u201cWartime Social Surveys\u201d. It was there suggested that a Social Science Research Council might be set up as a guide to the Government."
            },
            "slug": "The-Social-Sciences-Catlin",
            "title": {
                "fragments": [],
                "text": "The Social Sciences"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1942
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144321599"
                        ],
                        "name": "M. McGill",
                        "slug": "M.-McGill",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "McGill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. McGill"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 114
                            }
                        ],
                        "text": "The cosine metric and Jaccard\u2019s coefficient are commonly used in information retrieval as measures of association (Salton and McGill, 1983)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 59
                            }
                        ],
                        "text": "We consider here the question of how to estimate the conditional cooccurrence probability P (v|n) of an unseen word pair (n, v) drawn from some finite set N \u00d7 V ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 43685115,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "49af3e80343eb80c61e727ae0c27541628c7c5e2",
            "isKey": false,
            "numCitedBy": 12605,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Some people may be laughing when looking at you reading in your spare time. Some may be admired of you. And some may want be like you who have reading hobby. What about your own feel? Have you felt right? Reading is a need and a hobby at once. This condition is the on that will make you feel that you must read. If you know are looking for the book enPDFd introduction to modern information retrieval as the choice of reading, you can find here."
            },
            "slug": "Introduction-to-Modern-Information-Retrieval-Salton-McGill",
            "title": {
                "fragments": [],
                "text": "Introduction to Modern Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Reading is a need and a hobby at once and this condition is the on that will make you feel that you must read."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748501"
                        ],
                        "name": "Claire Cardie",
                        "slug": "Claire-Cardie",
                        "structuredName": {
                            "firstName": "Claire",
                            "lastName": "Cardie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claire Cardie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 774849,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1cf6f1209d29c151b693861e083850f1b385c595",
            "isKey": false,
            "numCitedBy": 99,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a case-based approach to knowledge acquisition for natural language systems that simultaneously learns part of speech, word sense, and concept activation knowledge for all open class words in a corpus. The parser begins with a lexicon of function words and creates a case base of context-sensitive word definitions during a humansupervised training phase. Then, given an unknown word and the context in which it occurs, the parser retrieves definitions from the case base to infer the word's syntactic and semantic features. By encoding context as part of a definition, the meaning of a word can change dynamically in response to surrounding phrases without the need for explicit lexical disambiguation heuristics. Moreover, the approach acquires all three classes of knowledge using the same case representation and requires relatively little training and no hand-coded knowledge acquisition heuristics. We evaluate it in experiments that explore two of many practical applications of the technique and conclude that the case-based method provides a promising approach to automated dictionary construction and knowledge acquisition for sentence analysis in limited domains. In addition, we present a novel case retrieval algorithm that uses decision trees to improve the performance of a k-nearest neighbor similarity metric."
            },
            "slug": "A-Case-Based-Approach-to-Knowledge-Acquisition-for-Cardie",
            "title": {
                "fragments": [],
                "text": "A Case-Based Approach to Knowledge Acquisition for Domain-Specific Sentence Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is concluded that the case-based method provides a promising approach to automated dictionary construction and knowledge acquisition for sentence analysis in limited domains and a novel case retrieval algorithm that uses decision trees to improve the performance of a k-nearest neighbor similarity metric."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801071"
                        ],
                        "name": "Frank Smadja",
                        "slug": "Frank-Smadja",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Smadja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frank Smadja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145590324"
                        ],
                        "name": "K. McKeown",
                        "slug": "K.-McKeown",
                        "structuredName": {
                            "firstName": "Kathleen",
                            "lastName": "McKeown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. McKeown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799688"
                        ],
                        "name": "V. Hatzivassiloglou",
                        "slug": "V.-Hatzivassiloglou",
                        "structuredName": {
                            "firstName": "Vasileios",
                            "lastName": "Hatzivassiloglou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Hatzivassiloglou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6720757,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0285f18f1642c3684e6abb7d5162348278c41abf",
            "isKey": false,
            "numCitedBy": 576,
            "numCiting": 84,
            "paperAbstract": {
                "fragments": [],
                "text": "Collocations are notoriously difficult for non-native speakers to translate, primarily because they are opaque and cannot be translated on a word-by-word basis. We describe a program named Champollion which, given a pair of parallel corpora in two different languages and a list of collocations in one of them, automatically produces their translations. Our goal is to provide a tool for compiling bilingual lexical information above the word level in multiple languages, for different domains. The algorithm we use is based on statistical methods and produces p-word translations of n-word collocations in which n and p need not be the same. For example, Champollion translates make...decision, employment equity, and stock market into prendre...decision, equite en matiere d'emploi, and bourse respectively. Testing Champollion on three years' worth of the Hansards corpus yielded the French translations of 300 collocations for each year, evaluated at 73% accuracy on average. In this paper, we describe the statistical measures used, the algorithm, and the implementation of Champollion, presenting our results and evaluation."
            },
            "slug": "Translating-Collocations-for-Bilingual-Lexicons:-A-Smadja-McKeown",
            "title": {
                "fragments": [],
                "text": "Translating Collocations for Bilingual Lexicons: A Statistical Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A program named Champollion is described which, given a pair of parallel corpora in two different languages and a list of collocations in one of them, automatically produces their translations, to provide a tool for compiling bilingual lexical information above the word level in multiple languages, for different domains."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065261753"
                        ],
                        "name": "K. Sugawara",
                        "slug": "K.-Sugawara",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Sugawara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sugawara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49710297"
                        ],
                        "name": "M. Nishimura",
                        "slug": "M.-Nishimura",
                        "structuredName": {
                            "firstName": "Masafumi",
                            "lastName": "Nishimura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Nishimura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49517873"
                        ],
                        "name": "K. Toshioka",
                        "slug": "K.-Toshioka",
                        "structuredName": {
                            "firstName": "Koichi",
                            "lastName": "Toshioka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Toshioka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145332982"
                        ],
                        "name": "M. Okochi",
                        "slug": "M.-Okochi",
                        "structuredName": {
                            "firstName": "Masaaki",
                            "lastName": "Okochi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Okochi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070623103"
                        ],
                        "name": "T. Kaneko",
                        "slug": "T.-Kaneko",
                        "structuredName": {
                            "firstName": "Toyohisa",
                            "lastName": "Kaneko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kaneko"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 102
                            }
                        ],
                        "text": "The confusion probability has been used by several authors to smooth word cooccurrence probabilities (Sugawara et al., 1985; Essen and Steinbiss, 1992; Grishman and Sterling, 1993); it measures the degree to which word m can be substituted into the contexts in which n appears."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61203024,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fcd8599fa26d7742516cc88baf36a0ca5a96216a",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we investigated smoothing techniques for alleviating the problem due to insufficient amount of training data. Hidden Markov Models (HMM) require a large amount of training data to obtain reliable probability estimates. But for isolated word recognition (100 words or more), we can not expect a user to speak each word more than several times. We found that the confusion matrix between a pair of label prototypes was particularly effective for the problem. We investigated two ways of computing the confusion matrix. One is based on distance among labels, and the other is based on the correspondence of labels in several utterances of the same word. Performance of these techniques was tested by using 100 Japanese city names spoken in an isolated word mode by three speakers. It was found that the smoothing technique reduced recognition errors from 1% to 0.1%. To visualize such performance improvement, we used, together with recognition rate, \"two-dimensional score plot,\" which shows the distribution of the best score of the true word and that of the remaining false ones in the vocabulary."
            },
            "slug": "Isolated-word-recognition-using-hidden-Markov-Sugawara-Nishimura",
            "title": {
                "fragments": [],
                "text": "Isolated word recognition using hidden Markov models"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This paper investigated smoothing techniques for alleviating the problem due to insufficient amount of training data for isolated word recognition and found that the confusion matrix between a pair of label prototypes was particularly effective for the problem."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP '85. IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3166885,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10",
            "isKey": false,
            "numCitedBy": 1058,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A program that tags each word in an input sentence with the most likely part of speech has been written. The program uses a linear-time dynamic programming algorithm to find an assignment of parts of speech to words that optimizes the product of (a) lexical probabilities (probability of observing part of speech i given word i) and (b) contextual probabilities (probability of observing part of speech i given n following parts of speech). Program performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct.<<ETX>>"
            },
            "slug": "A-Stochastic-Parts-Program-and-Noun-Phrase-Parser-Church",
            "title": {
                "fragments": [],
                "text": "A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A program that tags each word in an input sentence with the most likely part of speech has been written and performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115282352"
                        ],
                        "name": "Joy A. Thomas",
                        "slug": "Joy-A.-Thomas",
                        "structuredName": {
                            "firstName": "Joy",
                            "lastName": "Thomas",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joy A. Thomas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 138
                            }
                        ],
                        "text": "The function D is the KL divergence, which measures the (always nonnegative) average inefficiency in using one distribution to code for another (Cover and Thomas, 1991):\nD(p1(V ) \u2016 p2(V )) = \u2211\nv\np1(v) log p1(v)\np2(v) ."
                    },
                    "intents": []
                }
            ],
            "corpusId": 190432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7dbdb4209626fd92d2436a058663206216036e68",
            "isKey": false,
            "numCitedBy": 42796,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index."
            },
            "slug": "Elements-of-Information-Theory-Cover-Thomas",
            "title": {
                "fragments": [],
                "text": "Elements of Information Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author examines the role of entropy, inequality, and randomness in the design of codes and the construction of codes in the rapidly changing environment."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2928927"
                        ],
                        "name": "C. Stanfill",
                        "slug": "C.-Stanfill",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Stanfill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Stanfill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788375"
                        ],
                        "name": "D. Waltz",
                        "slug": "D.-Waltz",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Waltz",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Waltz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 43
                            }
                        ],
                        "text": "Variations of the value difference metric (Stanfill and Waltz, 1986) have been employed for supervised disambiguation (Ng and H.B. Lee, 1996; Ng, 1997); but it is not reasonable in language modeling to expect training data tagged with correct probabilities."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 42
                            }
                        ],
                        "text": "Variations of the value difference metric (Stanfill and Waltz, 1986) have been employed for supervised disambiguation (Ng and H."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16624499,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "123a726f6feb2bce29708b68ab2db5cdf9fcdaf4",
            "isKey": false,
            "numCitedBy": 1436,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "The intensive use of memory to recall specific episodes from the past\u2014rather than rules\u2014should be the foundation of machine reasoning."
            },
            "slug": "Toward-memory-based-reasoning-Stanfill-Waltz",
            "title": {
                "fragments": [],
                "text": "Toward memory-based reasoning"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The intensive use of memory to recall specific episodes from the past\u2014rather than rules\u2014should be the foundation of machine reasoning."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50718271"
                        ],
                        "name": "L. Kaufman",
                        "slug": "L.-Kaufman",
                        "structuredName": {
                            "firstName": "Leonard",
                            "lastName": "Kaufman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kaufman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2218687"
                        ],
                        "name": "P. Rousseeuw",
                        "slug": "P.-Rousseeuw",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Rousseeuw",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Rousseeuw"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 60820748,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "965ea5946708f0b669403a377c48e78e9326be61",
            "isKey": false,
            "numCitedBy": 8265,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "An electrical signal transmission system, applicable to the transmission of signals from trackside hot box detector equipment for railroad locomotives and rolling stock, wherein a basic pulse train is transmitted whereof the pulses are of a selected first amplitude and represent a train axle count, and a spike pulse of greater selected amplitude is transmitted, occurring immediately after the axle count pulse to which it relates, whenever an overheated axle box is detected. To enable the signal receiving equipment to determine on which side of a train the overheated box is located, the spike pulses are of two different amplitudes corresponding, respectively, to opposite sides of the train."
            },
            "slug": "Finding-Groups-in-Data:-An-Introduction-to-Cluster-Kaufman-Rousseeuw",
            "title": {
                "fragments": [],
                "text": "Finding Groups in Data: An Introduction to Cluster Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An electrical signal transmission system, applicable to the transmission of signals from trackside hot box detector equipment for railroad locomotives and rolling stock, wherein a basic pulse train is transmitted whereof the pulses are of a selected first amplitude and represent a train axle count."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 130
                            }
                        ],
                        "text": "The Dice coefficient (Smadja et al., 1996; D. Lin, 1998a, 1998b) (Kay and Ro\u0308scheisen, 1993) is monotonic in Jaccard\u2019s coefficient (van Rijsbergen, 1979), so its inclusion in our experiments would be redundant."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 220
                            }
                        ],
                        "text": "This function yielded the best performance overall: an average error rate reduction of 4% (significant at the .01 level) with respect to the Jensen-Shannon divergence, the best predictor of unseen events in our earlier experiments (Dagan et al., 1999)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "C L\n] 1\n8 Ja\nn 20"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 1
                            }
                        ],
                        "text": "(van Rijsbergen, 1979, pg. 38)\nBut no contradiction arises because, as van Rijsbergen continues, \u201cone would expect this since most measures incorporate the same information\u201d."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information Retrieval . Butterworths"
            },
            "venue": {
                "fragments": [],
                "text": "Information Retrieval . Butterworths"
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 33
                            }
                        ],
                        "text": "We consider here the question of how to estimate the conditional cooccurrence probability P (v|n) of an unseen word pair (n, v) drawn from some finite set N \u00d7 V ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 18
                            }
                        ],
                        "text": "In previous work (Dagan et al., 1999), we compared the performance of three different functions: the Jensen-Shannon divergence (total divergence to the average), the L1 norm, and the confusion probability."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 4
                            }
                        ],
                        "text": "However, we can make a further observation: with the exception of the confusion probability, all the functions we compared are symmetric, that is, f(q, r) = f(r, q)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 279
                            }
                        ],
                        "text": "\u2026been some doubt expressed in that community that the choice of similarity metric has any practical impact:\nSeveral authors have pointed out that the difference in retrieval performance achieved by different measures of association is insignificant, providing that these are appropriately normalised."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 181
                            }
                        ],
                        "text": "We evaluated the similarity functions introduced in the previous section on a binary decision task, using the same experimental framework as in our previous preliminary comparison (Dagan et al., 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 87
                            }
                        ],
                        "text": "If the base language model probabilities obey certain Bayesian consistency conditions (Dagan et al., 1999), as is the case for relative frequencies, then we may write the confusion probability as follows:\nconf(q, r, P (m)) = \u2211\nv\nq(v)r(v) P (m)\nP (v) ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 232
                            }
                        ],
                        "text": "This function yielded the best performance overall: an average error rate reduction of 4% (significant at the .01 level) with respect to the Jensen-Shannon divergence, the best predictor of unseen events in our earlier experiments (Dagan et al., 1999)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Similarity-based models of cooccurrence probabilities"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800862"
                        ],
                        "name": "G. Rand",
                        "slug": "G.-Rand",
                        "structuredName": {
                            "firstName": "Graham",
                            "lastName": "Rand",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Rand"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61584796,
            "fieldsOfStudy": [
                "Sociology"
            ],
            "id": "4fbd642068cc1021a3800eb0a57f928154aacef9",
            "isKey": false,
            "numCitedBy": 3927,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Quantitative-Applications-in-the-Social-Sciences-Rand",
            "title": {
                "fragments": [],
                "text": "Quantitative Applications in the Social Sciences"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61012010,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a923c9f89ed53b6e835b3807c0c1bd8d532687b",
            "isKey": false,
            "numCitedBy": 1037,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Interpolated-estimation-of-Markov-source-parameters-Jelinek",
            "title": {
                "fragments": [],
                "text": "Interpolated estimation of Markov source parameters from sparse data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Noun classification from predicateargument structures"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pictures of relevance"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the American Society for Information Science"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 128
                            }
                        ],
                        "text": "On the other hand, while there have been many similarity measures proposed and analyzed in the information retrieval literature (Jones and Furnas, 1987), there has been some doubt expressed in that community that the choice of similarity metric has any practical impact:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pictures of relevance"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the American Society for Information Science, 38(6):420\u2013442."
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 104
                            }
                        ],
                        "text": "We also plan to evaluate skewed versions of the Jensen-Shannon divergence proposed by Rao (1982) and J. Lin (1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 66
                            }
                        ],
                        "text": "Previously, we found the Jensen-Shannon divergence (Rao, 1982; J. Lin, 1991) to be a useful measure of the distance between distributions:\nJS(q, r) = 1\n2\n[ D ( q \u2225 \u2225\n\u2225 avgq,r\n) + D ( r \u2225 \u2225\n\u2225 avgq,r\n)]\n."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Divergence measures based"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 287,
                                "start": 274
                            }
                        ],
                        "text": "\u2026unigram probabilities as well as the two distributions q and r.\nFinally, Kendall\u2019s \u03c4 , which appears in work on clustering similar adjectives (Hatzivassiloglou and McKeown, 1993; Hatzivassiloglou, 1996), is a nonparametric measure of the association between random variables (Gibbons, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nonparametric Measures of Association. Sage University Paper series on Quantitative Applications in the"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pictures of relevance"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the American Society for Information Science"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Inte - grating multiple knowledge sources to disambiguate word sense : An exemplarbased approach"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 20,
            "methodology": 10,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 44,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Measures-of-Distributional-Similarity-Lee/6f3250ba47fdb413a0c113cc16d274517864f8ab?sort=total-citations"
}