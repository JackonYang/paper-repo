{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157708855"
                        ],
                        "name": "R. Smith",
                        "slug": "R.-Smith",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7038773,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89d9aae7e0c8b6edd56d0d79b277c07b7ab66fda",
            "isKey": false,
            "numCitedBy": 1509,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The Tesseract OCR engine, as was the HP Research Prototype in the UNLV Fourth Annual Test of OCR Accuracy, is described in a comprehensive overview. Emphasis is placed on aspects that are novel or at least unusual in an OCR engine, including in particular the line finding, features/classification methods, and the adaptive classifier."
            },
            "slug": "An-Overview-of-the-Tesseract-OCR-Engine-Smith",
            "title": {
                "fragments": [],
                "text": "An Overview of the Tesseract OCR Engine"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The Tesseract OCR engine, as was the HP Research Prototype in the UNLV Fourth Annual Test of OCR Accuracy, is described in a comprehensive overview."
            },
            "venue": {
                "fragments": [],
                "text": "Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733858"
                        ],
                        "name": "T. Breuel",
                        "slug": "T.-Breuel",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Breuel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Breuel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7565957,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d61ff4b81965d54ec1b9d488e80d0e8bf1c67e8",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Large scale scanning and document conversion efforts have led to a renewed interest in OCR systems and workflows. This paper describes a new format for representing both intermediate and final OCR results, developed in response to the needs of a newly developed OCR system and ground truth data release. The format embeds OCR information invisibly inside the HTML and CSS standards and therefore can represent a wide range of linguistic and typographic phenomena with already well-defined, widely understood markup and can be processed using widely available and known tools. The format is based on a new, multi-level abstraction of OCR results based on logical markup, common typesetting models, and OCR engine-specific markup, making it suitable both for the support of existing workflows and the development of future model-based OCR engines."
            },
            "slug": "The-hOCR-Microformat-for-OCR-Workflow-and-Results-Breuel",
            "title": {
                "fragments": [],
                "text": "The hOCR Microformat for OCR Workflow and Results"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A new format for representing both intermediate and final OCR results is described, developed in response to the needs of a newly developed OCR system and ground truth data release, which embeds OCR information invisibly inside the HTML and CSS standards."
            },
            "venue": {
                "fragments": [],
                "text": "Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145916951"
                        ],
                        "name": "G. Nagy",
                        "slug": "G.-Nagy",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Nagy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Nagy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145062511"
                        ],
                        "name": "S. Seth",
                        "slug": "S.-Seth",
                        "structuredName": {
                            "firstName": "Sharad",
                            "lastName": "Seth",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Seth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145266621"
                        ],
                        "name": "M. Viswanathan",
                        "slug": "M.-Viswanathan",
                        "structuredName": {
                            "firstName": "Mahesh",
                            "lastName": "Viswanathan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Viswanathan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 2530196,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "62a53caea5213ea177298d7b2aff292b1386c37a",
            "isKey": false,
            "numCitedBy": 434,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Gobbledoc, a system providing remote access to stored documents, which is based on syntactic document analysis and optical character recognition (OCR), is discussed. In Gobbledoc, image processing, document analysis, and OCR operations take place in batch mode when the documents are acquired. The document image acquisition process and the knowledge base that must be entered into the system to process a family of page images are described. The process by which the X-Y tree data structure converts a 2-D page-segmentation problem into a series of 1-D string-parsing problems that can be tackled using conventional compiler tools is also described. Syntactic analysis is used in Gobbledoc to divide each page into labeled rectangular blocks. Blocks labeled text are converted by OCR to obtain a secondary (ASCII) document representation. Since such symbolic files are better suited for computerized search than for human access to the document content and because too many visual layout clues are lost in the OCR process (including some special characters), Gobbledoc preserves the original block images for human browsing. Storage, networking, and display issues specific to document images are also discussed.<<ETX>>"
            },
            "slug": "A-prototype-document-image-analysis-system-for-Nagy-Seth",
            "title": {
                "fragments": [],
                "text": "A prototype document image analysis system for technical journals"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The document image acquisition process and the knowledge base that must be entered into the system to process a family of page images are described, and the process by which the X-Y tree data structure converts a 2-D page-segmentation problem into a series of 1-D string-parsing problems that can be tackled using conventional compiler tools."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733858"
                        ],
                        "name": "T. Breuel",
                        "slug": "T.-Breuel",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Breuel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Breuel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 210
                            }
                        ],
                        "text": "Text Line Modeling Text-line finding matches a geometrically precise text line model (Figure 4) against the bounding boxes of the source document; each text line is constrained not to cross any column boundary.(8) Search is currently carried out by a memory-intensive best-first branch-and-bound method; to conserve memory, it will be altered to perform a depth-first search."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14805532,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f414ff60064bf236035656c64ef8c342f7e4f24b",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Many document analysis and OCR systems depend on precise identification of page rotation, as well as the reliable identification of text lines. This paper presents a new algorithm to address both problems. It uses a branch-and-bound approach to globally optimal line finding and simultaneously models the baseline and the descender line under a Gaussian error/robust least square model. Results of applying the algorithm to documents in the University of Washington Database 2 are presented."
            },
            "slug": "Robust-least-square-baseline-finding-using-a-branch-Breuel",
            "title": {
                "fragments": [],
                "text": "Robust least-square-baseline finding using a branch and bound algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new algorithm that uses a branch-and-bound approach to globally optimal line finding and simultaneously models the baseline and the descender line under a Gaussian error/robust least square model is presented."
            },
            "venue": {
                "fragments": [],
                "text": "IS&T/SPIE Electronic Imaging"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733858"
                        ],
                        "name": "T. Breuel",
                        "slug": "T.-Breuel",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Breuel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Breuel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 209
                            }
                        ],
                        "text": "2 RAST-Based Layout Analysis The primary layout analysis method used by OCRopus is currently based on two related algorithms, one for whitespace identification, and the other for constrained text line finding.(12) Both methods operate on bounding boxes computed for the connected components of the scanned input page image."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2655325,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a7809043fe4f50da53163d1fd318754c5f259e6",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, I summarize research in document layout analysis carried out over the last few years in our laboratory. Correct document layout analysis is a key step in document capture conversions into electronic formats, optical character recognition (OCR), information retrieval from scanned documents, appearance-based document retrieval, and reformatting of documents for on-screen display. We have developed a number of novel geometric algorithms and statistical methods. Layout analysis systems built from these algorithms are applicable to a wide variety of languages and layouts, and have proven to be robust to the presence of noise and spurious features in a page image. The system itself consists of reusable and independent software modules that can be reconfigured to be adapted to different languages and applications. Currently, we are using them for electronic book and document capture applications. If there is commercial or government demand, we are interested in adapting these tools to information retrieval and intelligence applications."
            },
            "slug": "High-Performance-Document-Layout-Analysis-Breuel",
            "title": {
                "fragments": [],
                "text": "High Performance Document Layout Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This paper summarize research in document layout analysis carried out over the last few years in the laboratory, which has developed a number of novel geometric algorithms and statistical methods that are applicable to a wide variety of languages and layouts."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694191"
                        ],
                        "name": "J. Hull",
                        "slug": "J.-Hull",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Hull",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hull"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 58317587,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a76e38292960c7fc7576463225938fe74131611",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Several significant sets of labeled samples of image data are surveyed that can be used in the development of algorithms for offline and online handwriting recognition as well as for machine printed text recognition. The method used to gather each data set, the numbers of samples they contain, and the associated truth data are discussed. In the domain of offline handwriting, the CEDAR, NIST, and CENPARMI data sets are pre\u00ad sented. These contain primarily isolated digits and alphabetic characters. The UNIPEN data set of online handwriting was collected from a number of independent sources and it contains individual characters as well as handwritten phrases. The University of Wash\u00ad ington document image databases are also discussed. They contain a large number of English and Japanese document images that were selected from a range of publications."
            },
            "slug": "DATA-SETS-FOR-OCR-AND-DOCUMENT-IMAGE-UNDERSTANDING-Guyon-Haralick",
            "title": {
                "fragments": [],
                "text": "DATA SETS FOR OCR AND DOCUMENT IMAGE UNDERSTANDING RESEARCH"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Several significant sets of labeled samples of image data are surveyed that can be used in the development of algorithms for offline and online handwriting recognition as well as for machine printed text recognition."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145470231"
                        ],
                        "name": "Kwan Y. Wong",
                        "slug": "Kwan-Y.-Wong",
                        "structuredName": {
                            "firstName": "Kwan",
                            "lastName": "Wong",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kwan Y. Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34896449"
                        ],
                        "name": "R. Casey",
                        "slug": "R.-Casey",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Casey",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Casey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1880661"
                        ],
                        "name": "F. Wahl",
                        "slug": "F.-Wahl",
                        "structuredName": {
                            "firstName": "Friedrich",
                            "lastName": "Wahl",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Wahl"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15921038,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7abca302c74d2f5adfd323a28e26d40b019df2b5",
            "isKey": false,
            "numCitedBy": 594,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper outlines the requirements and components for a proposed Document Analysis System, which assists a user in encoding printed documents for computer processing. Several critical functions have been investigated and the technical approaches are discussed. The first is the segmentation and classification of digitized printed documents into regions of text and images. A nonlinear, run-length smoothing algorithm has been used for this purpose. By using the regular features of text lines, a linear adaptive classification scheme discriminates text regions from others. The second technique studied is an adaptive approach to the recognition of the hundreds of font styles and sizes that can occur on printed documents. A preclassifier is constructed during the input process and used to speed up a well-known pattern-matching method for clustering characters from an arbitrary print source into a small sample of prototypes. Experimental results are included."
            },
            "slug": "Document-Analysis-System-Wong-Casey",
            "title": {
                "fragments": [],
                "text": "Document Analysis System"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "The requirements and components for a proposed Document Analysis System, which assists a user in encoding printed documents for computer processing, are outlined and several critical functions have been investigated and the technical approaches are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "IBM J. Res. Dev."
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733858"
                        ],
                        "name": "T. Breuel",
                        "slug": "T.-Breuel",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Breuel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Breuel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 286,
                                "start": 283
                            }
                        ],
                        "text": "The overall architecture of the OCRopus OCR system is a strictly feed-forward architecture (no backtracking) with three major components: (physical) layout analysis, text line recognition, and statistical language modeling; it is similar to a previous handwriting recognition system.(4) The individual steps are (Figure 1):"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59744966,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e8074bd3ede87e6c0d0828598535d2f0c6e25c23",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Keywords: vision Reference EPFL-CONF-82303 Record created on 2006-03-10, modified on 2016-08-08"
            },
            "slug": "Design-and-Implementation-of-a-System-for-the-of-on-Breuel",
            "title": {
                "fragments": [],
                "text": "Design and Implementation of a System for the Recognition of Handwritten Responses on US Census Forms"
            },
            "tldr": {
                "abstractSimilarityScore": 29,
                "text": "The EPFL-CONF-82303 image represents a 3D model of the liquid-phase response of the eye and provides a detailed description of the \u201cna\u00efve eye\u201d as well as three different types of \u201cspatial errors\u201d."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688013"
                        ],
                        "name": "F. Shafait",
                        "slug": "F.-Shafait",
                        "structuredName": {
                            "firstName": "Faisal",
                            "lastName": "Shafait",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shafait"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3342880"
                        ],
                        "name": "J. V. Beusekom",
                        "slug": "J.-V.-Beusekom",
                        "structuredName": {
                            "firstName": "Joost",
                            "lastName": "Beusekom",
                            "middleNames": [
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. V. Beusekom"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51027911"
                        ],
                        "name": "Daniel Keysers",
                        "slug": "Daniel-Keysers",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Keysers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Keysers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733858"
                        ],
                        "name": "T. Breuel",
                        "slug": "T.-Breuel",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Breuel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Breuel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2851577,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f50652fff76bf9b4cb7667e5e72c4970b0a3919f",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe and evaluate a method to robustly detect the page frame in document images, locating the actual page contents area and removing textual and non-textual noise along the page borders. We use a geometric matching algorithm to find the optimal page frame, which has the advantages of not assuming the existence of whitespace between noisy borders and actual page contents, and of giving a practical solution to the page frame detection problem without the need for parameter tuning. We define suitable performance measures and evaluate the algorithm on the UW-III database. The results show that the error rates are below 4% for each of the performance measures used. In addition, we demonstrate that the use of page frame detection reduces the optical character recognition (OCR) error rate by removing textual noise. Experiments using a commercial OCR system show that the error rate due to elements outside the page frame is reduced from 4.3% to 1.7% on the UW-III dataset."
            },
            "slug": "Page-Frame-Detection-for-Marginal-Noise-Removal-Shafait-Beusekom",
            "title": {
                "fragments": [],
                "text": "Page Frame Detection for Marginal Noise Removal from Scanned Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A geometric matching algorithm is used to find the optimal page frame, which has the advantages of not assuming the existence of whitespace between noisy borders and actual page contents, and of giving a practical solution to the page frame detection problem without the need for parameter tuning."
            },
            "venue": {
                "fragments": [],
                "text": "SCIA"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688013"
                        ],
                        "name": "F. Shafait",
                        "slug": "F.-Shafait",
                        "structuredName": {
                            "firstName": "Faisal",
                            "lastName": "Shafait",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shafait"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51027911"
                        ],
                        "name": "Daniel Keysers",
                        "slug": "Daniel-Keysers",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Keysers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Keysers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733858"
                        ],
                        "name": "T. Breuel",
                        "slug": "T.-Breuel",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Breuel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Breuel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9284084,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a7d94a2b04d949ed54c8b009658cca67fe16ff6",
            "isKey": false,
            "numCitedBy": 328,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Adaptive binarization is an important first step in many document analysis and OCR processes. This paper describes a fast adaptive binarization algorithm that yields the same quality of binarization as the Sauvola method,1 but runs in time close to that of global thresholding methods (like Otsu's method2), independent of the window size. The algorithm combines the statistical constraints of Sauvola's method with integral images.3 Testing on the UW-1 dataset demonstrates a 20-fold speedup compared to the original Sauvola algorithm."
            },
            "slug": "Efficient-implementation-of-local-adaptive-using-Shafait-Keysers",
            "title": {
                "fragments": [],
                "text": "Efficient implementation of local adaptive thresholding techniques using integral images"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A fast adaptive binarization algorithm that yields the same quality of Binarization as the Sauvola method but runs in time close to that of global thresholding methods (like Otsu's method), independent of the window size."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757030"
                        ],
                        "name": "Berrin A. Yanikoglu",
                        "slug": "Berrin-A.-Yanikoglu",
                        "structuredName": {
                            "firstName": "Berrin",
                            "lastName": "Yanikoglu",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Berrin A. Yanikoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1820595"
                        ],
                        "name": "L. Vincent",
                        "slug": "L.-Vincent",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Vincent",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Vincent"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16994816,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23b63ccfe44badde2b162d26cd836839f2133a6f",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a new approach for evaluating page segmentation algorithms. Unlike techniques that rely on OCR output, our method is region-based: the segmentation output, described as a set of regions together with their types, output order etc., is matched against the pre-stored set of ground-truth regions. Misclassifications, splitting, and merging of regions are among the errors that are detected by the system. Each error is weighted individually for a particular application and a global estimate of segmentation quality is derived. The system can be customized to benchmark specific aspects of segmentation (e.g., headline detection) and according to the type of error correction that might follow (e.g., re-typing). Segmentation ground-truth files are quickly and easily generated and edited using GroundsKeeper, an X-Window based tool that allows one to view a document, manually draw regions (arbitrary polygons) on it, and specify information about each region (e.g., type, parent)."
            },
            "slug": "Ground-truthing-and-benchmarking-document-page-Yanikoglu-Vincent",
            "title": {
                "fragments": [],
                "text": "Ground-truthing and benchmarking document page segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "This work describes a new approach for evaluating page segmentation algorithms that is region-based: the segmentation output, described as a set of regions together with their types, output order etc., is matched against the pre-stored set of ground-truth regions."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51027911"
                        ],
                        "name": "Daniel Keysers",
                        "slug": "Daniel-Keysers",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Keysers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Keysers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688013"
                        ],
                        "name": "F. Shafait",
                        "slug": "F.-Shafait",
                        "structuredName": {
                            "firstName": "Faisal",
                            "lastName": "Shafait",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shafait"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733858"
                        ],
                        "name": "T. Breuel",
                        "slug": "T.-Breuel",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Breuel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Breuel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 83
                            }
                        ],
                        "text": "1 Text-Image Segmentation OCRopus contains a simple text-image segmentation system.(10) It operates by first dividing the input image into candidate regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16742704,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56234dd12764334bf79f5d5678da22954961033c",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a simple, fast, and accurate system for document image zone classification \u2014 an important subproblem of document image analysis \u2014 that results from a detailed analysis of different features. Using a novel combination of known algorithms, we achieve a very competitive error rate of 1.46% (n = 13811) in comparison to (Wang et al., 2006) who report an error rate of 1.55% (n = 24177) using more complicated techniques. The experiments were performed on zones extracted from the widely used UW-III database, which is representative of images of scanned journal pages and contains ground-truthed real-world data."
            },
            "slug": "Document-image-zone-classification-a-simple-Keysers-Shafait",
            "title": {
                "fragments": [],
                "text": "Document image zone classification - a simple high-performance approach"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A simple, fast, and accurate system for document image zone classification \u2014 an important subproblem of document image analysis \u2014 that results from a detailed analysis of different features is described."
            },
            "venue": {
                "fragments": [],
                "text": "VISAPP"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723766"
                        ],
                        "name": "H. Baird",
                        "slug": "H.-Baird",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Baird",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Baird"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 26229616,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da44af7e967c0c847e63b9600811a58ee7716f45",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for analyzing the structure of the white background in document images is described, along with applications to the problem of isolating blocks of machine-printed text. The approach is based on computational-geometry algorithms for off-line enumeration of maximal white rectangles and on-line rectangle unification. These support a fast, simple, and general heuristic for geometric layout segmentation, in which white space is covered greedily by rectangles until all text blocks are isolated. Design of the heuristic can be substantially automated by an analysis of the empirical statistical distribution of properties of covering rectangles: for example, the stopping rule can be chosen by Rosenblatt\u2019s perceptron training algorithm. Experimental trials show good behavior on the large and useful class of textual Manhattan layouts. On complex layouts from English-language technical journals of many publishers, the method finds good segmentations in a uniform and nearly parameter-free manner. On a variety of non-Latin texts, some with vertical text lines, the method finds good segmentations without prior knowledge of page and text-line orientation."
            },
            "slug": "Background-Structure-in-Document-Images-Baird",
            "title": {
                "fragments": [],
                "text": "Background Structure in Document Images"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A method for analyzing the structure of the white background in document images is described, along with applications to the problem of isolating blocks of machine-printed text."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Pattern Recognit. Artif. Intell."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733858"
                        ],
                        "name": "T. Breuel",
                        "slug": "T.-Breuel",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Breuel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Breuel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 182
                            }
                        ],
                        "text": "These include an HMM-based recognizer, suitable for recognizing small font sizes, and an alternative shape-based recognizer, based on prior work on shape-based character recognition.(22)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8272050,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "526a28a5f960e5d691ffd44f6487b95b3271888b",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "A system for recognizing handprinted digits using optimal bounded error matching is described. Bounded error matching is already in common use in general-purpose 2D and 3D visual object recognition and can cope with clutter, occlusions, and noise, important issues also in OCR. The results presented demonstrate that the same techniques achieve high recognition rates (up to 99.2%) on a real-world handprinted digit recognition task (the NIST database of hand-printed census forms and the CEDAR database of digits extracted from US mail ZIP codes). As part of the system, a post-processing step for k-nearest neighbor classifiers based on decision trees is described that can be used (in place of the usual heuristic methods) for setting thresholds and that improves recognition rates significantly.<<ETX>>"
            },
            "slug": "Recognition-of-handprinted-digits-using-optimal-Breuel",
            "title": {
                "fragments": [],
                "text": "Recognition of handprinted digits using optimal bounded error matching"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A post-processing step for k-nearest neighbor classifiers based on decision trees is described that can be used (in place of the usual heuristic methods) for setting thresholds and that improves recognition rates significantly."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR '93)"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733858"
                        ],
                        "name": "T. Breuel",
                        "slug": "T.-Breuel",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Breuel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Breuel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 3132858,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff21e5f41eade5d48e30e61081bf17df65cee100",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Segmentation of handwritten input into individual characters is a crucial step in many connected handwriting recognition systems. This paper describes a segmentation algorithm for letters in Roman alphabets, curved pre-stroke cut (CPSC) segmentation. The CPSC algorithm evaluates a large set of curved cuts through the image of the input string using dynamic programming and selects a small \"optimal\" subset of cuts for segmentation. It usually generates pixel accurate segmentations, indistinguishable from characters written in isolation. At four times oversegmentation, segmentation points are missed with an undetectable frequency on real-world databases. The CPSC algorithm has been used as part of a high-performance handwriting recognition system."
            },
            "slug": "Segmentation-of-handprinted-letter-strings-using-a-Breuel",
            "title": {
                "fragments": [],
                "text": "Segmentation of handprinted letter strings using a dynamic programming algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A segmentation algorithm for letters in Roman alphabets, curved pre-stroke cut (CPSC) segmentation, which evaluates a large set of curved cuts through the image of the input string using dynamic programming and selects a small \"optimal\" subset of cuts for segmentation."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Sixth International Conference on Document Analysis and Recognition"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688013"
                        ],
                        "name": "F. Shafait",
                        "slug": "F.-Shafait",
                        "structuredName": {
                            "firstName": "Faisal",
                            "lastName": "Shafait",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shafait"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51027911"
                        ],
                        "name": "Daniel Keysers",
                        "slug": "Daniel-Keysers",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Keysers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Keysers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733858"
                        ],
                        "name": "T. Breuel",
                        "slug": "T.-Breuel",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Breuel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Breuel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5611293,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c8d1ce168a9f82234d81e4210c710e3499aa4af2",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a quantitative comparison of six algorithms for page segmentation: X-Y cut, smearing, whitespace analysis, constrained text-line finding, Docstrum, and Voronoi-diagram-based. The evaluation is performed using a subset of the UW-III collection commonly used for evaluation, with a separate training set for parameter optimization. We compare the results using both default parameters and optimized parameters. In the course of the evaluation, the strengths and weaknesses of each algorithm are analyzed, and it is shown that no single algorithm outperforms all other algorithms. However, we observe that the three best-performing algorithms are those based on constrained text-line finding, Docstrum, and the Voronoi-diagram."
            },
            "slug": "Performance-Comparison-of-Six-Algorithms-for-Page-Shafait-Keysers",
            "title": {
                "fragments": [],
                "text": "Performance Comparison of Six Algorithms for Page Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "It is shown that no single algorithm outperforms all other algorithms, however, the three best-performing algorithms are those based on constrained text-line finding, Docstrum, and the Voronoi-diagram."
            },
            "venue": {
                "fragments": [],
                "text": "Document Analysis Systems"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740276"
                        ],
                        "name": "Cyril Allauzen",
                        "slug": "Cyril-Allauzen",
                        "structuredName": {
                            "firstName": "Cyril",
                            "lastName": "Allauzen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cyril Allauzen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145428168"
                        ],
                        "name": "M. Riley",
                        "slug": "M.-Riley",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Riley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Riley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698491"
                        ],
                        "name": "J. Schalkwyk",
                        "slug": "J.-Schalkwyk",
                        "structuredName": {
                            "firstName": "Johan",
                            "lastName": "Schalkwyk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schalkwyk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718399"
                        ],
                        "name": "Wojciech Skut",
                        "slug": "Wojciech-Skut",
                        "structuredName": {
                            "firstName": "Wojciech",
                            "lastName": "Skut",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wojciech Skut"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "78659204"
                        ],
                        "name": "M. Mohri",
                        "slug": "M.-Mohri",
                        "structuredName": {
                            "firstName": "Mehryar",
                            "lastName": "Mohri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mohri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10869889,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "31e105cac80aa8f1646dfb22c95d035564ea4998",
            "isKey": false,
            "numCitedBy": 656,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe OpenFst, an open-source library for weighted finite-state transducers (WFSTs). OpenFst consists of a C++ template library with efficient WFST representations and over twenty-five operations for constructing, combining, optimizing, and searching them. At the shell-command level, there are corresponding transducer file representations and programs that operate on them. OpenFst is designed to be both very efficient in time and space and to scale to very large problems. \n \nThis library has key applications speech, image, and natural language processing, pattern and string matching, and machine learning. \n \nWe give an overview of the library, examples of its use, details of its design that allow customizing the labels, states, and weights and the lazy evaluation of many of its operations. \n \nFurther information and a download of the OpenFst library can be obtained from http://www.openfst.org."
            },
            "slug": "OpenFst:-A-General-and-Efficient-Weighted-Library-Allauzen-Riley",
            "title": {
                "fragments": [],
                "text": "OpenFst: A General and Efficient Weighted Finite-State Transducer Library"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "OpenFst is an open-source library for weighted finite-state transducers (WFSTs) that consists of a C++ template library with efficient WFST representations and over twenty-five operations for constructing, combining, optimizing, and searching them."
            },
            "venue": {
                "fragments": [],
                "text": "CIAA"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688013"
                        ],
                        "name": "F. Shafait",
                        "slug": "F.-Shafait",
                        "structuredName": {
                            "firstName": "Faisal",
                            "lastName": "Shafait",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shafait"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51027911"
                        ],
                        "name": "Daniel Keysers",
                        "slug": "Daniel-Keysers",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Keysers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Keysers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733858"
                        ],
                        "name": "T. Breuel",
                        "slug": "T.-Breuel",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Breuel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Breuel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6280391,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "6b323f1fc7fa0d01d398e3868440343f3fbcf942",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new representation and evaluation procedure of page segmentation algorithms and analyzes six widely-used layout analysis algorithms using the procedure. The method permits a detailed analysis of the behavior of page segmentation algorithms in terms of over- and undersegmentation at different layout levels, as well as determination of the geometric accuracy of the segmentation. The representation of document layouts relies on labeling each pixel according to its function in the overall segmentation, permitting pixel-accurate representation of layout information of arbitrary layouts and allowing background pixels to be classified as \"don't care\". Our representations can be encoded easily in standard color image formats like PNG, permitting easy interchange of segmentation results and ground truth"
            },
            "slug": "Pixel-Accurate-Representation-and-Evaluation-of-in-Shafait-Keysers",
            "title": {
                "fragments": [],
                "text": "Pixel-Accurate Representation and Evaluation of Page Segmentation in Document Images"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "A new representation and evaluation procedure of page segmentation algorithms and analyzes six widely-used layout analysis algorithms using the procedure, permitting easy interchange of segmentation results and ground truth."
            },
            "venue": {
                "fragments": [],
                "text": "18th International Conference on Pattern Recognition (ICPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733858"
                        ],
                        "name": "T. Breuel",
                        "slug": "T.-Breuel",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Breuel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Breuel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15850446,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "480423285b2d146501cb6a425e2f0eeaa35866cc",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Binary morphology on large images is compute intensive, in particular for large structuring elements. Run-length encoding is a compact and space-saving technique for representing images. This paper describes how to implement binary morphology directly on run-length encoded binary images for rectangular structuring elements. In addition, it describes efficient algorithm for transposing and rotating run-length encoded images. The paper evaluates and compares run length morphologial processing on page images from the UW3 database with an efficient and mature bit blit-based implementation and shows that the run length approach is several times faster than bit blit-based implementations for large images and masks. The experiments also show that complexity decreases for larger mask sizes. The paper also demonstrates running times on a simple morphology-based layout analysis algorithm on the UW3 database and shows that replacing bit blit morphology with run length based morphology speeds up performance approximately two-fold."
            },
            "slug": "Binary-Morphology-and-Related-Operations-on-Breuel",
            "title": {
                "fragments": [],
                "text": "Binary Morphology and Related Operations on Run-Length Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The paper evaluates and compares run length morphologial processing on page images from the UW3 database with an efficient and mature bit blit-based implementation and shows that the run length approach is several times faster than bit blits based implementations for large images and masks."
            },
            "venue": {
                "fragments": [],
                "text": "VISAPP"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145695526"
                        ],
                        "name": "S. Mao",
                        "slug": "S.-Mao",
                        "structuredName": {
                            "firstName": "Song",
                            "lastName": "Mao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143766793"
                        ],
                        "name": "A. Rosenfeld",
                        "slug": "A.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Azriel",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rosenfeld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143626870"
                        ],
                        "name": "T. Kanungo",
                        "slug": "T.-Kanungo",
                        "structuredName": {
                            "firstName": "Tapas",
                            "lastName": "Kanungo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanungo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6128200,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "633fd1e2bd089c2c402244037876e879861d6739",
            "isKey": false,
            "numCitedBy": 254,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Document structure analysis can be regarded as a syntactic analysis problem. The order and containment relations among the physical or logical components of a document page can be described by an ordered tree structure and can be modeled by a tree grammar which describes the page at the component level in terms of regions or blocks. This paper provides a detailed survey of past work on document structure analysis algorithms and summarize the limitations of past approaches. In particular, we survey past work on document physical layout representations and algorithms, document logical structure representations and algorithms, and performance evaluation of document structure analysis algorithms. In the last section, we summarize this work and point out its limitations."
            },
            "slug": "Document-structure-analysis-algorithms:-a-survey-Mao-Rosenfeld",
            "title": {
                "fragments": [],
                "text": "Document structure analysis algorithms: a literature survey"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper provides a detailed survey of past work on document structure analysis algorithms and summarize the limitations of past approaches."
            },
            "venue": {
                "fragments": [],
                "text": "IS&T/SPIE Electronic Imaging"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3277321"
                        ],
                        "name": "K. Kise",
                        "slug": "K.-Kise",
                        "structuredName": {
                            "firstName": "Koichi",
                            "lastName": "Kise",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kise"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064863306"
                        ],
                        "name": "A. Sato",
                        "slug": "A.-Sato",
                        "structuredName": {
                            "firstName": "Akinori",
                            "lastName": "Sato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40411993"
                        ],
                        "name": "M. Iwata",
                        "slug": "M.-Iwata",
                        "structuredName": {
                            "firstName": "Motoi",
                            "lastName": "Iwata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Iwata"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 23399574,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "be80678c0eeedf754647a8b9adccd5d6d9be3e86",
            "isKey": false,
            "numCitedBy": 275,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method of page segmentation based on the approximated area Voronoi diagram. The characteristics of the proposed method are as follows: (1) The Voronoi diagram enables us to obtain the candidates of boundaries of document components from page images with non-Manhattan layout and a skew. (2) The candidates are utilized to estimate the intercharacter and interline gaps without the use of domain-specific parameters to select the boundaries. From the experimental results for 128 images with non-Manhattan layout and the skew of 0\u00b0~45\u00b0 as well as 98 images with Manhattan layout, we have confirmed that the method is effective for extraction of body text regions, and it is as efficient as other methods based on connected component analysis."
            },
            "slug": "Segmentation-of-Page-Images-Using-the-Area-Voronoi-Kise-Sato",
            "title": {
                "fragments": [],
                "text": "Segmentation of Page Images Using the Area Voronoi Diagram"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "It is confirmed that the proposed method of page segmentation based on the approximated area Voronoi diagram is effective for extraction of body text regions, and it is as efficient as other methods based on connected component analysis."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145916951"
                        ],
                        "name": "G. Nagy",
                        "slug": "G.-Nagy",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Nagy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Nagy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145062511"
                        ],
                        "name": "S. Seth",
                        "slug": "S.-Seth",
                        "structuredName": {
                            "firstName": "Sharad",
                            "lastName": "Seth",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Seth"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59683040,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc45263226de157763006aef70b681dbac744dcc",
            "isKey": false,
            "numCitedBy": 316,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "HIERARCHICAL-REPRESENTATION-OF-OPTICALLY-SCANNED-Nagy-Seth",
            "title": {
                "fragments": [],
                "text": "HIERARCHICAL REPRESENTATION OF OPTICALLY SCANNED DOCUMENTS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796273"
                        ],
                        "name": "H. Leung",
                        "slug": "H.-Leung",
                        "structuredName": {
                            "firstName": "Hong",
                            "lastName": "Leung",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Leung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758141"
                        ],
                        "name": "I. L. Hetherington",
                        "slug": "I.-L.-Hetherington",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Hetherington",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. L. Hetherington"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710543"
                        ],
                        "name": "V. Zue",
                        "slug": "V.-Zue",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Zue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Zue"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35778766,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "59fff25d4485d987e541c9422e4d19cd78e0b8c1",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Speech-recognition-using-stochastic-modeling-Leung-Hetherington",
            "title": {
                "fragments": [],
                "text": "Speech recognition using stochastic explicit-segment modeling"
            },
            "venue": {
                "fragments": [],
                "text": "EUROSPEECH"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Tesseract open source ocr system"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Xdoc data format: Technical specification."
            },
            "venue": {
                "fragments": [],
                "text": "Xerox Imaging Systems part no"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Xdoc data format: Technical specification Xerox Imaging Systems part no"
            },
            "venue": {
                "fragments": [],
                "text": "Xdoc data format: Technical specification Xerox Imaging Systems part no"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Illuminator user's manual"
            },
            "venue": {
                "fragments": [],
                "text": "Illuminator user's manual"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "CSS: Under Construction http://www.w3.org/Style/CSS/current-work"
            },
            "venue": {
                "fragments": [],
                "text": "CSS: Under Construction http://www.w3.org/Style/CSS/current-work"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u201c The lua language"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 1,
            "methodology": 4
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 29,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/The-OCRopus-open-source-OCR-system-Breuel/06f92305db8d7432668fe45b04a6c8647c2fcf15?sort=total-citations"
}