{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2460212"
                        ],
                        "name": "Olivier Delalleau",
                        "slug": "Olivier-Delalleau",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Delalleau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olivier Delalleau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7245737"
                        ],
                        "name": "Nicolas Le Roux",
                        "slug": "Nicolas-Le-Roux",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Le Roux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Le Roux"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 253,
                                "start": 233
                            }
                        ],
                        "text": "More specifically, when the number of hidden units becomes very large, and an L2 regularizer is used on the output weights, such a neural net becomes a kernelmachine, whose kernel has a simple form that can be computed analytically [Bengio et al., 2006b]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8924778,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7d471970467a99bec4bce34c7dba5ef6745ad06",
            "isKey": false,
            "numCitedBy": 194,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a series of theoretical arguments supporting the claim that a large class of modern learning algorithms that rely solely on the smoothness prior - with similarity between examples expressed with a local kernel - are sensitive to the curse of dimensionality, or more precisely to the variability of the target. Our discussion covers supervised, semi-supervised and unsupervised learning algorithms. These algorithms are found to be local in the sense that crucial properties of the learned function at x depend mostly on the neighbors of x in the training set. This makes them sensitive to the curse of dimensionality, well studied for classical non-parametric statistical learning. We show in the case of the Gaussian kernel that when the function to be learned has many variations, these algorithms require a number of training examples proportional to the number of variations, which could be large even though there may exist short descriptions of the target function, i.e. their Kolmogorov complexity may be low. This suggests that there exist non-local learning algorithms that at least have the potential to learn about such structured but apparently complex functions (because locally they have many variations), while not using very specific prior domain knowledge."
            },
            "slug": "The-Curse-of-Highly-Variable-Functions-for-Local-Bengio-Delalleau",
            "title": {
                "fragments": [],
                "text": "The Curse of Highly Variable Functions for Local Kernel Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "A series of theoretical arguments are presented supporting the claim that a large class of modern learning algorithms that rely solely on the smoothness prior - with similarity between examples expressed with a local kernel - are sensitive to the curse of dimensionality, or more precisely to the variability of the target."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3472959"
                        ],
                        "name": "C. Rasmussen",
                        "slug": "C.-Rasmussen",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Rasmussen",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rasmussen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 106
                            }
                        ],
                        "text": "In most kernel based machine learning algorithms (Shawe-Taylor and Cristianini, 2004), Gaussian processes (Rasmussen and Williams, 2006), and nonparametric statistics (Izenman, 1991) the key computationally intensive task is to compute a linear combination of local kernel functions centered"
                    },
                    "intents": []
                }
            ],
            "corpusId": 1430472,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82266f6103bade9005ec555ed06ba20b5210ff22",
            "isKey": false,
            "numCitedBy": 17873,
            "numCiting": 231,
            "paperAbstract": {
                "fragments": [],
                "text": "Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received growing attention in the machine learning community over the past decade. The book provides a long-needed, systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises. Code and datasets can be obtained on the web. Appendices provide mathematical background and a discussion of Gaussian Markov processes."
            },
            "slug": "Gaussian-Processes-for-Machine-Learning-Rasmussen-Williams",
            "title": {
                "fragments": [],
                "text": "Gaussian Processes for Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics, and deals with the supervised learning problem for both regression and classification."
            },
            "venue": {
                "fragments": [],
                "text": "Adaptive computation and machine learning"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703070"
                        ],
                        "name": "Alexander G. Gray",
                        "slug": "Alexander-G.-Gray",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Gray",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander G. Gray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760402"
                        ],
                        "name": "A. Moore",
                        "slug": "A.-Moore",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Moore",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Moore"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15852901,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7550b1c4abfd8fb046847f20cac360217057c47a",
            "isKey": false,
            "numCitedBy": 186,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Density estimation is a core operation of virtually all probabilistic learning methods (as opposed to discriminative methods). Approaches to density estimation can be divided into two principal classes, parametric methods, such as Bayesian networks, and nonparametric methods such as kernel density estimation and smoothing splines. While neither choice should be universally preferred for all situations, a well-known benefit of nonparametric methods is their ability to achieve estimation optimality for ANY input distribution as more data are observed, a property that no model with a parametric assumption can have, and one of great importance in exploratory data analysis and mining where the underlying distribution is decidedly unknown. To date, however, despite a wealth of advanced underlying statistical theory, the use of nonparametric methods has been limited by their computational intractibility for all but the smallest datasets. In this paper, we present an algorithm for kernel density estimation, the chief nonparametric approach, which is dramatically faster than previous algorithmic approaches in terms of both dataset size and dimensionality. Furthermore, the algorithm provides arbitrarily tight accuracy guarantees, provides anytime convergence, works for all common kernel choices, and requires no parameter tuning. The algorithm is an instance of a new principle of algorithm design: multi-recursion, or higher-order"
            },
            "slug": "Nonparametric-Density-Estimation:-Toward-Gray-Moore",
            "title": {
                "fragments": [],
                "text": "Nonparametric Density Estimation: Toward Computational Tractability"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper presents an algorithm for kernel density estimation, the chief nonparametric approach, which is dramatically faster than previous algorithmic approaches in terms of both dataset size and dimensionality and is an instance of a new principle of algorithm design: multi-recursion, or higher-order algorithm design."
            },
            "venue": {
                "fragments": [],
                "text": "SDM"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145769639"
                        ],
                        "name": "V. Raykar",
                        "slug": "V.-Raykar",
                        "structuredName": {
                            "firstName": "Vikas",
                            "lastName": "Raykar",
                            "middleNames": [
                                "Chandrakant"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Raykar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152727281"
                        ],
                        "name": "Chan-Qiang Yang",
                        "slug": "Chan-Qiang-Yang",
                        "structuredName": {
                            "firstName": "Chan-Qiang",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chan-Qiang Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719541"
                        ],
                        "name": "R. Duraiswami",
                        "slug": "R.-Duraiswami",
                        "structuredName": {
                            "firstName": "Ramani",
                            "lastName": "Duraiswami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duraiswami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697880"
                        ],
                        "name": "N. Gumerov",
                        "slug": "N.-Gumerov",
                        "structuredName": {
                            "firstName": "Nail",
                            "lastName": "Gumerov",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Gumerov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 132
                            }
                        ],
                        "text": "More details of the algorithms including the automatic choice of the algorithm parameters and a tighter error bound can be found in (Raykar et al., 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6457698,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee78f9cd65b9d680c9de9ab04e98d37bcef33778",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Evaluating sums of multivariate Gaussian kernels is a key computational task in many problems in computational statistics and machine learning. The computational cost of the direct evaluation of such sums scales as the product of the number of kernel functions and the evaluation points. The fast Gauss transform proposed by Greengard and Strain (1991) is a \u2020-exact approximation algorithm that reduces the computational complexity of the evaluation of the sum of N Gaussians at M points in d dimensions from O(MN) to O(M+N). However, the constant factor in O(M+N) grows exponentially with increasing dimensionality d, which makes the algorithm impractical for dimensions greater than three. In this paper we present a new algorithm where the constant factor is reduced to asymptotically polynomial order. The reduction is based on a new multivariate Taylor\u2019s series expansion (which can act both as a local as well as a far fleld expansion) scheme combined with the e\u2010cient space subdivision using the k-center algorithm. The proposed method difiers from the original fast Gauss transform in terms of a difierent factorization, e\u2010cient space subdivision, and the use of point-wise error bounds. Algorithm details, error bounds, procedure to choose the parameters and numerical experiments are presented. As an example we shows how the proposed method can be used for very fast \u2020-exact multivariate kernel density estimation."
            },
            "slug": "Fast-Computation-of-Sums-of-Gaussians-in-High-Raykar-Yang",
            "title": {
                "fragments": [],
                "text": "Fast Computation of Sums of Gaussians in High Dimensions"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new algorithm where the constant factor is reduced to asymptotically polynomial order is presented, based on a new multivariate Taylor\u2019s series expansion scheme combined with the e\u2010cient space subdivision using the k-center algorithm."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2460212"
                        ],
                        "name": "Olivier Delalleau",
                        "slug": "Olivier-Delalleau",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Delalleau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olivier Delalleau"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 222
                            }
                        ],
                        "text": "Note that if the centers of the Gaussians are not restricted anymore to bep ints in the training set (i.e., a Type-3 shallow architecture), it is possible to solve the parity problem with only d + 1 Gaussians and no bias [Bengio et al., 2005]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123887468,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b4c0dee1b1bfe2ba9fa87ebb526df2a288011533",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a series of theoretical arguments supporting the claim that a large class of modern learning algorithms based on local kernels are sensitive to the curse of dimensionality. These include local manifold learning algorithms such as Isomap and LLE, support vector classifiers with Gaussian or other local kernels, and graph-based semisupervised learning algorithms using a local similarity function. These algorithms are shown to be local in the sense that crucial properties of the learned function at x depend mostly on the neighbors of x in the training set. This makes them sensitive to the curse of dimensionality, well studied for classical non-parametric statistical learning. There is a large class of data distributions for which non-local solutions could be expressed compactly and potentially be learned with few examples, but which will require a large number of local bases and therefore a large number of training examples when using a local learning algorithm."
            },
            "slug": "The-Curse-of-Dimensionality-for-Local-Kernel-Bengio-Delalleau",
            "title": {
                "fragments": [],
                "text": "The Curse of Dimensionality for Local Kernel Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A series of theoretical arguments support the claim that a large class of modern learning algorithms based on local kernels are sensitive to the curse of dimensionality, including local manifold learning algorithms such as Isomap and LLE, support vector classifiers with Gaussian or other local kernels, and graph-based semisupervised learning algorithms using a local similarity function."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9094007"
                        ],
                        "name": "Yuh-Jye Lee",
                        "slug": "Yuh-Jye-Lee",
                        "structuredName": {
                            "firstName": "Yuh-Jye",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuh-Jye Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747026"
                        ],
                        "name": "O. Mangasarian",
                        "slug": "O.-Mangasarian",
                        "structuredName": {
                            "firstName": "Olvi",
                            "lastName": "Mangasarian",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Mangasarian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 362,
                                "start": 151
                            }
                        ],
                        "text": "There are many strategies for specific problems which try to reduce this computational complexity by searching for a sparse representation of the data (Williams and Seeger, 2001; Smola and Bartlett, 2001; Fine and Scheinberg, 2001; Lee and Mangasarian, 2001; Lawrence et al., 2003; Csato and Opper, 2002; Tresp, 2000; Tipping, 2001; Snelson and Ghahramani, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14152802,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de4d36bc28e13917297d619f585cd8f7860848f0",
            "isKey": false,
            "numCitedBy": 699,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract An algorithm is proposed which generates a nonlinear kernel-based separating surface that requires as little as 1% of a large dataset for its explicit evaluation. To generate this nonlinear surface, the entire dataset is used as a constraint in an optimization problem with very few variables corresponding to the 1% of the data kept. The remainder of the data can be thrown away after solving the optimization problem. This is achieved by making use of a rectangular m\u00d7m kernel K(A, \u0100) that greatly reduces the size of the quadratic program to be solved and simplifies the characterization of the nonlinear separating surface. Here, the m rows of A represent the original m data points while the m rows of \u0100 represent a greatly reduced m data points. Computational results indicate that test set correctness for the reduced support vector machine (RSVM), with a nonlinear separating surface that depends on a small randomly selected portion of the dataset, is better than that of a conventional support vector machine (SVM) with a nonlinear surface that explicitly depends on the entire dataset, and much better than a conventional SVM using a small random sample of the data. Computational times, as well as memory usage, are much smaller for RSVM than that of a conventional SVM using the entire dataset."
            },
            "slug": "RSVM:-Reduced-Support-Vector-Machines-Lee-Mangasarian",
            "title": {
                "fragments": [],
                "text": "RSVM: Reduced Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Computational results indicate that test set correctness for the reduced support vector machine (RSVM), with a nonlinear separating surface that depends on a small randomly selected portion of the dataset, is better than that of a conventional support vectors machine (SVM) with aNonlinear surface that explicitly depends on the entire dataset, and much better than a conventional SVM using a small random sample of the data."
            },
            "venue": {
                "fragments": [],
                "text": "SDM"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2460212"
                        ],
                        "name": "Olivier Delalleau",
                        "slug": "Olivier-Delalleau",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Delalleau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olivier Delalleau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7245737"
                        ],
                        "name": "Nicolas Le Roux",
                        "slug": "Nicolas-Le-Roux",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Le Roux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Le Roux"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 137
                            }
                        ],
                        "text": "In this section we focus on algorithms of the type described in recent papers [Zhu et al., 2003, Zhou et al., 2004, Belkin et al., 2004, Delalleau et al., 2005], which are graphbased, non-parametric, semi-supervised learning algorithms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7513025,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7b662bb37a4fc10bcaf0f2d6df1b0ccab5c9b6c7",
            "isKey": false,
            "numCitedBy": 199,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "There has been an increase of interest for semi-supervised learning recently, because of the many datasets with large amounts of unlabeled examples and only a few labeled ones. This paper follows up on proposed non-parametric algorithms which provide an estimated continuous label for the given unlabeled examples. It extends them to function induction algorithms that correspond to the minimization of a regularization criterion applied to an out-of-sample example, and happens to have the form of a Parzen windows regressor. The advantage of the extension is that it allows predicting the label for a new example without having to solve again a linear system of dimension 'n' (the number of unlabeled and labeled training examples), which can cost O(n^3). Experiments show that the extension works well, in the sense of predicting a label close to the one that would have been obtained if the test example had been included in the unlabeled set. This relatively efficient function induction procedure can also be used when 'n' is large to approximate the solution by writing it only in terms of a kernel expansion with 'm' Keywords: non-parametric models, classification, regression, semi-supervised learning, modeles non parametriques, classification, regression, apprentissage semi-supervise"
            },
            "slug": "Efficient-Non-Parametric-Function-Induction-in-Delalleau-Bengio",
            "title": {
                "fragments": [],
                "text": "Efficient Non-Parametric Function Induction in Semi-Supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Experiments show that the proposed non-parametric algorithms which provide an estimated continuous label for the given unlabeled examples are extended to function induction algorithms that correspond to the minimization of a regularization criterion applied to an out-of-sample example, and happens to have the form of a Parzen windows regressor."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145306271"
                        ],
                        "name": "Neil D. Lawrence",
                        "slug": "Neil-D.-Lawrence",
                        "structuredName": {
                            "firstName": "Neil",
                            "lastName": "Lawrence",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Neil D. Lawrence"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680574"
                        ],
                        "name": "M. Seeger",
                        "slug": "M.-Seeger",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Seeger",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Seeger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3234984"
                        ],
                        "name": "R. Herbrich",
                        "slug": "R.-Herbrich",
                        "structuredName": {
                            "firstName": "Ralf",
                            "lastName": "Herbrich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Herbrich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12555103,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fde4bf50d48d54ba913d574c849566dc4f3d4fde",
            "isKey": false,
            "numCitedBy": 553,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a framework for sparse Gaussian process (GP) methods which uses forward selection with criteria based on information-theoretic principles, previously suggested for active learning. Our goal is not only to learn d-sparse predictors (which can be evaluated in O(d) rather than O(n), d \u226a n, n the number of training points), but also to perform training under strong restrictions on time and memory requirements. The scaling of our method is at most O(n \u00b7 d2), and in large real-world classification experiments we show that it can match prediction performance of the popular support vector machine (SVM), yet can be significantly faster in training. In contrast to the SVM, our approximation produces estimates of predictive probabilities ('error bars'), allows for Bayesian model selection and is less complex in implementation."
            },
            "slug": "Fast-Sparse-Gaussian-Process-Methods:-The-Vector-Lawrence-Seeger",
            "title": {
                "fragments": [],
                "text": "Fast Sparse Gaussian Process Methods: The Informative Vector Machine"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A framework for sparse Gaussian process (GP) methods which uses forward selection with criteria based on information-theoretic principles, which allows for Bayesian model selection and is less complex in implementation is presented."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737568"
                        ],
                        "name": "N. D. Freitas",
                        "slug": "N.-D.-Freitas",
                        "structuredName": {
                            "firstName": "Nando",
                            "lastName": "Freitas",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. D. Freitas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 12
                            }
                        ],
                        "text": "A criticism (Lang et al., 2005) of the original IFGT (Yang et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1401510,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f08b493c6c7e1d137affad99f22d7e540c636de",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We present results of experiments testing the Fast Gauss Transform, Improved Fast Gauss Transform, and Dual-Tree methods (using kd-tree and Anchors Hierarchy data structures) for fast Kernel Density Estimation (KDE). We examine the performance of these methods with respect to data set size, dimension, allowable error, and data set structure (\u201cclumpiness\u201d), measured in terms of CPU time and memory usage. This is the first multi-method comparison in the literature. The results are striking, challenging several claims that are commonly made about these methods. The results are useful for researchers considering fast methods for KDE problems. Along the way, we provide a corrected error bound and a parameter-selection regime for the IFGT algorithm."
            },
            "slug": "Empirical-Testing-of-Fast-Kernel-Density-Estimation-Freitas",
            "title": {
                "fragments": [],
                "text": "Empirical Testing of Fast Kernel Density Estimation Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This first multi-method comparison in the literature examines the performance of these methods with respect to data set size, dimension, allowable error, and data set structure, measured in terms of CPU time and memory usage."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2094778056"
                        ],
                        "name": "V. De Silva",
                        "slug": "V.-De-Silva",
                        "structuredName": {
                            "firstName": "Vin",
                            "lastName": "De Silva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. De Silva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46657367"
                        ],
                        "name": "J. Langford",
                        "slug": "J.-Langford",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Langford",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Langford"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 72
                            }
                        ],
                        "text": "In the case of graph-based manifold learning algorithms such as LLE and Isomap, the domination of near examples is perfect(i. ., the derivative is strictly in the span of the difference vectors with the neighbors), because the kernel implicit in these algorithms takes value 0 for the non-neighbors."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 41
                            }
                        ],
                        "text": "See Bengio et al. [2004]to see that LLE, Isomap, Laplacian eigenmaps and other spectral manifold learning algorithms such as spectral clustering can be generalized and written in the form of eq."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 125
                            }
                        ],
                        "text": "If data lie on a low-dimensional manifold then geodesic distances in this graph approach geodesic distances on the manifold [Tenenbaum et al., 2000], as the number of examples increases."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 146
                            }
                        ],
                        "text": "\u2026learning algorithms that attempt to capture the manifold structure of the data, such as Locally Linear Embedding [Roweis and Saul, 2000], Isomap [Tenenbaum et al., 2000], kernel PCA [Scho\u0308lkopf et al., 1998], Laplacian Eigenmaps [Belkin and Niyogi, 2003], Manifold Charting [Brand, 2003], and\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 422,
                                "start": 416
                            }
                        ],
                        "text": "This class of learning algorithms includes most instances of the kernel machine algorithms [Scho\u0308lkopf et al., 1999], such as Support Vector Machines (SVMs) [Boser et al., 1992, Cortes and Vapnik, 1995] or Gaussi n processes [Williams and Rasmussen, 1996], but also unsupervised learning algorithms that attempt to capture the manifold structure of the data, such as Locally Linear Embedding [Roweis and Saul, 2000], Isomap [Tenenbaum et al., 2000], kernel PCA [Scho\u0308lkopf et al., 1998], Laplacian Eigenmaps [Belkin and Niyogi, 2003], Manifold Charting [Brand, 2003], and spectral clusteringalgorithms (see Weiss [1999] for a review)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 173
                            }
                        ],
                        "text": "In Bengio et al. [2005] we present similar results that applyto unsupervised learning algorithms such as non-parametric manifold learning algorithms [Roweis and Saul, 2000, Tenenbaum et al., 2000, Scho\u0308lkopf et al., 1998, Belkin and Niyogi, 2003]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 221338160,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3537fcd0ff99a3b3cb3d279012df826358420556",
            "isKey": true,
            "numCitedBy": 12181,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs-30,000 auditory nerve fibers or 10(6) optic nerve fibers-a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure."
            },
            "slug": "A-global-geometric-framework-for-nonlinear-Tenenbaum-Silva",
            "title": {
                "fragments": [],
                "text": "A global geometric framework for nonlinear dimensionality reduction."
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set and efficiently computes a globally optimal solution, and is guaranteed to converge asymptotically to the true structure."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709263"
                        ],
                        "name": "Dongryeol Lee",
                        "slug": "Dongryeol-Lee",
                        "structuredName": {
                            "firstName": "Dongryeol",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dongryeol Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703070"
                        ],
                        "name": "Alexander G. Gray",
                        "slug": "Alexander-G.-Gray",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Gray",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander G. Gray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760402"
                        ],
                        "name": "A. Moore",
                        "slug": "A.-Moore",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Moore",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Moore"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2008990,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "422426ff35c7562f8a549c806adcb88abecc76d3",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "In previous work we presented an efficient approach to computing kernel summations which arise in many machine learning methods such as kernel density estimation. This approach, dual-tree recursion with finite-difference approximation, generalized existing methods for similar problems arising in computational physics in two ways appropriate for statistical problems: toward distribution sensitivity and general dimension, partly by avoiding series expansions. While this proved to be the fastest practical method for multivariate kernel density estimation at the optimal bandwidth, it is much less efficient at larger-than-optimal bandwidths. In this work, we explore the extent to which the dual-tree approach can be integrated with multipole-like Hermite expansions in order to achieve reasonable efficiency across all bandwidth scales, though only for low dimensionalities. In the process, we derive and demonstrate the first truly hierarchical fast Gauss transforms, effectively combining the best tools from discrete algorithms and continuous approximation theory."
            },
            "slug": "Dual-Tree-Fast-Gauss-Transforms-Lee-Gray",
            "title": {
                "fragments": [],
                "text": "Dual-Tree Fast Gauss Transforms"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The extent to which the dual-tree recursion with finite-difference approximation can be integrated with multipole-like Hermite expansions in order to achieve reasonable efficiency across all bandwidth scales is explored, though only for low dimensionalities."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684722"
                        ],
                        "name": "S. Fine",
                        "slug": "S.-Fine",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Fine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2005127"
                        ],
                        "name": "K. Scheinberg",
                        "slug": "K.-Scheinberg",
                        "structuredName": {
                            "firstName": "Katya",
                            "lastName": "Scheinberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Scheinberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 362,
                                "start": 151
                            }
                        ],
                        "text": "There are many strategies for specific problems which try to reduce this computational complexity by searching for a sparse representation of the data (Williams and Seeger, 2001; Smola and Bartlett, 2001; Fine and Scheinberg, 2001; Lee and Mangasarian, 2001; Lawrence et al., 2003; Csato and Opper, 2002; Tresp, 2000; Tipping, 2001; Snelson and Ghahramani, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13899309,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0db7af02be7cbadc029f9104a8c784d02de42df7",
            "isKey": false,
            "numCitedBy": 662,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "SVM training is a convex optimization problem which scales with the training set size rather than the feature space dimension. While this is usually considered to be a desired quality, in large scale problems it may cause training to be impractical. The common techniques to handle this difficulty basically build a solution by solving a sequence of small scale subproblems. Our current effort is concentrated on the rank of the kernel matrix as a source for further enhancement of the training procedure. We first show that for a low rank kernel matrix it is possible to design a better interior point method (IPM) in terms of storage requirements as well as computational complexity. We then suggest an efficient use of a known factorization technique to approximate a given kernel matrix by a low rank matrix, which in turn will be used to feed the optimizer. Finally, we derive an upper bound on the change in the objective function value based on the approximation error and the number of active constraints (support vectors). This bound is general in the sense that it holds regardless of the approximation method."
            },
            "slug": "Efficient-SVM-Training-Using-Low-Rank-Kernel-Fine-Scheinberg",
            "title": {
                "fragments": [],
                "text": "Efficient SVM Training Using Low-Rank Kernel Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work shows that for a low rank kernel matrix it is possible to design a better interior point method (IPM) in terms of storage requirements as well as computational complexity and derives an upper bound on the change in the objective function value based on the approximation error and the number of active constraints (support vectors)."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 76
                            }
                        ],
                        "text": "regularized least squares (Poggio and Smale, 2003), support vector machines (Cristianini and Shawe-Taylor, 2000), kernel regression (Wand and Jones, 1995)) f is the regression/classification function."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14727192,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5c04f8002e24a8c09bfbfedca3c6c346fe1e5d53",
            "isKey": false,
            "numCitedBy": 13352,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "From the publisher: This is the first comprehensive introduction to Support Vector Machines (SVMs), a new generation learning system based on recent advances in statistical learning theory. SVMs deliver state-of-the-art performance in real-world applications such as text categorisation, hand-written character recognition, image classification, biosequences analysis, etc., and are now established as one of the standard tools for machine learning and data mining. Students will find the book both stimulating and accessible, while practitioners will be guided smoothly through the material required for a good grasp of the theory and its applications. The concepts are introduced gradually in accessible and self-contained stages, while the presentation is rigorous and thorough. Pointers to relevant literature and web sites containing software ensure that it forms an ideal starting point for further study. Equally, the book and its associated web site will guide practitioners to updated literature, new applications, and on-line software."
            },
            "slug": "An-Introduction-to-Support-Vector-Machines-and-Cristianini-Shawe-Taylor",
            "title": {
                "fragments": [],
                "text": "An Introduction to Support Vector Machines and Other Kernel-based Learning Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "This is the first comprehensive introduction to Support Vector Machines (SVMs), a new generation learning system based on recent advances in statistical learning theory, and will guide practitioners to updated literature, new applications, and on-line software."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 70
                            }
                        ],
                        "text": "Shallow architectures are best exemplified by modern kernel machines [Scho\u0308lkopf et al., 1999], such as Support Vector Machines (SVMs) [Boser et al., 1992, Cortes and Vapnik, 1995]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 36
                            }
                        ],
                        "text": "Through the famouskernel trick(see [Scho\u0308lkopf et al., 1999]), Type-2 architectures can be seen as a compactway of representing Type1 architectures, including some that may be too large to be practical."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 92
                            }
                        ],
                        "text": "This class of learning algorithms includes most instances of the kernel machine algorithms [Scho\u0308lkopf et al., 1999], such as Support Vector Machines (SVMs) [Boser et al., 1992, Cortes and Vapnik, 1995] or Gaussi n processes [Williams and Rasmussen, 1996], but also unsupervised learning algorithms\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We argue that deep architectures have the potential to generalize in non-local ways, i.e., beyond immediate neighbors, and that this is crucial in order to make progress on the kind of complex tasks required for artificial intelligence."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60502900,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9c4da62e9e89e65ac78ee271e424e8b498053e8c",
            "isKey": true,
            "numCitedBy": 5544,
            "numCiting": 260,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction to support vector learning roadmap. Part 1 Theory: three remarks on the support vector method of function estimation, Vladimir Vapnik generalization performance of support vector machines and other pattern classifiers, Peter Bartlett and John Shawe-Taylor Bayesian voting schemes and large margin classifiers, Nello Cristianini and John Shawe-Taylor support vector machines, reproducing kernel Hilbert spaces, and randomized GACV, Grace Wahba geometry and invariance in kernel based methods, Christopher J.C. Burges on the annealed VC entropy for margin classifiers - a statistical mechanics study, Manfred Opper entropy numbers, operators and support vector kernels, Robert C. Williamson et al. Part 2 Implementations: solving the quadratic programming problem arising in support vector classification, Linda Kaufman making large-scale support vector machine learning practical, Thorsten Joachims fast training of support vector machines using sequential minimal optimization, John C. Platt. Part 3 Applications: support vector machines for dynamic reconstruction of a chaotic system, Davide Mattera and Simon Haykin using support vector machines for time series prediction, Klaus-Robert Muller et al pairwise classification and support vector machines, Ulrich Kressel. Part 4 Extensions of the algorithm: reducing the run-time complexity in support vector machines, Edgar E. Osuna and Federico Girosi support vector regression with ANOVA decomposition kernels, Mark O. Stitson et al support vector density estimation, Jason Weston et al combining support vector and mathematical programming methods for classification, Bernhard Scholkopf et al."
            },
            "slug": "Advances-in-kernel-methods:-support-vector-learning-Sch\u00f6lkopf-Burges",
            "title": {
                "fragments": [],
                "text": "Advances in kernel methods: support vector learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Support vector machines for dynamic reconstruction of a chaotic system, Klaus-Robert Muller et al pairwise classification and support vector machines, Ulrich Kressel."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3087941"
                        ],
                        "name": "Pascal Lamblin",
                        "slug": "Pascal-Lamblin",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Lamblin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Lamblin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32384143"
                        ],
                        "name": "D. Popovici",
                        "slug": "D.-Popovici",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Popovici",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Popovici"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 45
                            }
                        ],
                        "text": "These results are reported and discussed in [Bengio et al., 2007]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 271,
                                "start": 252
                            }
                        ],
                        "text": "\u2026such architectures: simple gradient descent applied to convolutional networks [LeCun et al., 1989, LeCun et al., 1998] (for signals and images), and more recently, layer-by-layer unsupervised learning followedby gradient descent [Hinton et al., 2006, Bengio et al., 2007, Ranzato et al., 2006]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14201947,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "355d44f53428b1ac4fb2ab468d593c720640e5bd",
            "isKey": false,
            "numCitedBy": 3434,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization."
            },
            "slug": "Greedy-Layer-Wise-Training-of-Deep-Networks-Bengio-Lamblin",
            "title": {
                "fragments": [],
                "text": "Greedy Layer-Wise Training of Deep Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "These experiments confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703070"
                        ],
                        "name": "Alexander G. Gray",
                        "slug": "Alexander-G.-Gray",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Gray",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander G. Gray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760402"
                        ],
                        "name": "A. Moore",
                        "slug": "A.-Moore",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Moore",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Moore"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14961407,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93c473358b063cec9bb9ea7cb3036bbce7b76493",
            "isKey": false,
            "numCitedBy": 302,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We present efficient algorithms for all-point-pairs problems, or 'N-body'- like problems, which are ubiquitous in statistical learning. We focus on six examples, including nearest-neighbor classification, kernel density estimation, outlier detection, and the two-point correlation. These include any problem which abstractly requires a comparison of each of the N points in a dataset with each other point and would naively be solved using N2 distance computations. In practice N is often large enough to make this infeasible. We present a suite of new geometric techniques which are applicable in principle to any 'N-body' computation including large-scale mixtures of Gaussians, RBF neural networks, and HMM's. Our algorithms exhibit favorable asymptotic scaling and are empirically several orders of magnitude faster than the naive computation, even for small datasets. We are aware of no exact algorithms for these problems which are more efficient either empirically or theoretically. In addition, our framework yields simple and elegant algorithms. It also permits two important generalizations beyond the standard all-point-pairs problems, which are more difficult. These are represented by our final examples, the multiple two-point correlation and the notorious n-point correlation."
            },
            "slug": "'N-Body'-Problems-in-Statistical-Learning-Gray-Moore",
            "title": {
                "fragments": [],
                "text": "'N-Body' Problems in Statistical Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A suite of new geometric techniques which are applicable in principle to any 'N-body' computation including large-scale mixtures of Gaussians, RBF neural networks, and HMM's are presented."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713934"
                        ],
                        "name": "Antoine Bordes",
                        "slug": "Antoine-Bordes",
                        "structuredName": {
                            "firstName": "Antoine",
                            "lastName": "Bordes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antoine Bordes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769472"
                        ],
                        "name": "S. Ertekin",
                        "slug": "S.-Ertekin",
                        "structuredName": {
                            "firstName": "Seyda",
                            "lastName": "Ertekin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ertekin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 109
                            }
                        ],
                        "text": "This problem is somewhat mitigated by recent progress with online algorithms for kernel machines (e.g., see [Bordes et al., 2005]), but there remains the question of the increase in the number of support vectorsas the number of examples increases."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14227081,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "07b54bac0159028aed116dbdbc2b747f723e585e",
            "isKey": false,
            "numCitedBy": 693,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Very high dimensional learning systems become theoretically possible when training examples are abundant. The computing cost then becomes the limiting factor. Any efficient learning algorithm should at least take a brief look at each example. But should all examples be given equal attention?This contribution proposes an empirical answer. We first present an online SVM algorithm based on this premise. LASVM yields competitive misclassification rates after a single pass over the training examples, outspeeding state-of-the-art SVM solvers. Then we show how active example selection can yield faster training, higher accuracies, and simpler models, using only a fraction of the training example labels."
            },
            "slug": "Fast-Kernel-Classifiers-with-Online-and-Active-Bordes-Ertekin",
            "title": {
                "fragments": [],
                "text": "Fast Kernel Classifiers with Online and Active Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This contribution presents an online SVM algorithm based on the premise that active example selection can yield faster training, higher accuracies, and simpler models, using only a fraction of the training example labels."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2081889"
                        ],
                        "name": "Edward Snelson",
                        "slug": "Edward-Snelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Snelson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward Snelson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 362,
                                "start": 151
                            }
                        ],
                        "text": "There are many strategies for specific problems which try to reduce this computational complexity by searching for a sparse representation of the data (Williams and Seeger, 2001; Smola and Bartlett, 2001; Fine and Scheinberg, 2001; Lee and Mangasarian, 2001; Lawrence et al., 2003; Csato and Opper, 2002; Tresp, 2000; Tipping, 2001; Snelson and Ghahramani, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 394337,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b6a2d80854651a56e0f023543131744f14f20ab4",
            "isKey": false,
            "numCitedBy": 1503,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new Gaussian process (GP) regression model whose co-variance is parameterized by the the locations of M pseudo-input points, which we learn by a gradient based optimization. We take M \u226a N, where N is the number of real data points, and hence obtain a sparse regression method which has O(M2N) training cost and O(M2) prediction cost per test case. We also find hyperparameters of the covariance function in the same joint optimization. The method can be viewed as a Bayesian regression model with particular input dependent noise. The method turns out to be closely related to several other sparse GP approaches, and we discuss the relation in detail. We finally demonstrate its performance on some large data sets, and make a direct comparison to other sparse GP methods. We show that our method can match full GP performance with small M, i.e. very sparse solutions, and it significantly outperforms other approaches in this regime."
            },
            "slug": "Sparse-Gaussian-Processes-using-Pseudo-inputs-Snelson-Ghahramani",
            "title": {
                "fragments": [],
                "text": "Sparse Gaussian Processes using Pseudo-inputs"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "It is shown that this new Gaussian process (GP) regression model can match full GP performance with small M, i.e. very sparse solutions, and it significantly outperforms other approaches in this regime."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3103897"
                        ],
                        "name": "Changjiang Yang",
                        "slug": "Changjiang-Yang",
                        "structuredName": {
                            "firstName": "Changjiang",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Changjiang Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719541"
                        ],
                        "name": "R. Duraiswami",
                        "slug": "R.-Duraiswami",
                        "structuredName": {
                            "firstName": "Ramani",
                            "lastName": "Duraiswami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duraiswami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 52
                            }
                        ],
                        "text": ", 2003) and its use in kernel machines was shown in (Yang et al., 2005)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 29
                            }
                        ],
                        "text": ", 2005) of the original IFGT (Yang et al., 2005) was that the error bound was too pessimistic, and too many computational resources were wasted as a consequence."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 113
                            }
                        ],
                        "text": "For training, the IFGT can also be embedded in a conjugate-gradient or any other suitable optimization procedure (Yang et al., 2005; Shen et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5809817,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "303600138609133e1cf558fca94514d9c19470d2",
            "isKey": false,
            "numCitedBy": 150,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "The computation and memory required for kernel machines with N training samples is at least O(N2). Such a complexity is significant even for moderate size problems and is prohibitive for large datasets. We present an approximation technique based on the improved fast Gauss transform to reduce the computation to O(N). We also give an error bound for the approximation, and provide experimental results on the UCI datasets."
            },
            "slug": "Efficient-Kernel-Machines-Using-the-Improved-Fast-Yang-Duraiswami",
            "title": {
                "fragments": [],
                "text": "Efficient Kernel Machines Using the Improved Fast Gauss Transform"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An approximation technique based on the improved fast Gauss transform to reduce the computation to O(N) is presented and an error bound for the approximation is given."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 281,
                                "start": 262
                            }
                        ],
                        "text": "\u2026kernel (1.4% error) is only marginally betterhan that of a considerably smaller neural net with a single hidden layer of 800 hidden units (1.6% as reported by [Simard et al., 2003]), and similar to the results obtained with a 3-layer neural net as reported in [Hinton et al., 2006] (1.53% error)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 140
                            }
                        ],
                        "text": "Notable exceptions include work on convolutional networks [LeCun et al., 1989, LeCun et al., 1998], and recentwork on Deep Belief Networks [Hinton et al., 2006]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 84
                            }
                        ],
                        "text": "The main objective of this chapter is to discuss fundamentallimitations of certain classes of learning algorithms, and point towards approaches that overcome these limitations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 231
                            }
                        ],
                        "text": "\u2026such architectures: simple gradient descent applied to convolutional networks [LeCun et al., 1989, LeCun et al., 1998] (for signals and images), and more recently, layer-by-layer unsupervised learning followedby gradient descent [Hinton et al., 2006, Bengio et al., 2007, Ranzato et al., 2006]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 89
                            }
                        ],
                        "text": "The best results on the original MNIST set with a knowledge free method was reported in [Hinton et al., 2006] (0.95% error), using a Deep Belief NetworkBy knowledge-free method, we mean a method that has no prior knowledge of the pictorial nature of the signal."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2309950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8978cf7574ceb35f4c3096be768c7547b28a35d0",
            "isKey": true,
            "numCitedBy": 13408,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind."
            },
            "slug": "A-Fast-Learning-Algorithm-for-Deep-Belief-Nets-Hinton-Osindero",
            "title": {
                "fragments": [],
                "text": "A Fast Learning Algorithm for Deep Belief Nets"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A fast, greedy algorithm is derived that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47209015"
                        ],
                        "name": "A. Izenman",
                        "slug": "A.-Izenman",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Izenman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Izenman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 167
                            }
                        ],
                        "text": "In most kernel based machine learning algorithms (Shawe-Taylor and Cristianini, 2004), Gaussian processes (Rasmussen and Williams, 2006), and nonparametric statistics (Izenman, 1991) the key computationally intensive task is to compute a linear combination of local kernel functions centered"
                    },
                    "intents": []
                }
            ],
            "corpusId": 121879377,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "939835a28713b94a2ca0d37d7b1d87b112e5e449",
            "isKey": false,
            "numCitedBy": 525,
            "numCiting": 155,
            "paperAbstract": {
                "fragments": [],
                "text": "Advances in computation and the fast and cheap computational facilities now available to statisticians have had a significant impact upon statistical research, and especially the development of nonparametric data analysis procedures. In particular, theoretical and applied research on nonparametric density estimation has had a noticeable influence on related topics, such as nonparametric regression, nonparametric discrimination, and nonparametric pattern recognition. This article reviews recent developments in nonparametric density estimation and includes topics that have been omitted from review articles and books on the subject. The early density estimation methods, such as the histogram, kernel estimators, and orthogonal series estimators are still very popular, and recent research on them is described. Different types of restricted maximum likelihood density estimators, including order-restricted estimators, maximum penalized likelihood estimators, and sieve estimators, are discussed, where restrictions are imposed upon the class of densities or on the form of the likelihood function. Nonparametric density estimators that are data-adaptive and lead to locally smoothed estimators are also discussed; these include variable partition histograms, estimators based on statistically equivalent blocks, nearest-neighbor estimators, variable kernel estimators, and adaptive kernel estimators. For the multivariate case, extensions of methods of univariate density estimation are usually straightforward but can be computationally expensive. A method of multivariate density estimation that did not spring from a univariate generalization is described, namely, projection pursuit density estimation, in which both dimensionality reduction and density estimation can be pursued at the same time. Finally, some areas of related research are mentioned, such as nonparametric estimation of functionals of a density, robust parametric estimation, semiparametric models, and density estimation for censored and incomplete data, directional and spherical data, and density estimation for dependent sequences of observations."
            },
            "slug": "Recent-Developments-in-Nonparametric-Density-Izenman",
            "title": {
                "fragments": [],
                "text": "Recent Developments in Nonparametric Density Estimation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2460212"
                        ],
                        "name": "Olivier Delalleau",
                        "slug": "Olivier-Delalleau",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Delalleau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olivier Delalleau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7245737"
                        ],
                        "name": "Nicolas Le Roux",
                        "slug": "Nicolas-Le-Roux",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Le Roux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Le Roux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39651651"
                        ],
                        "name": "Jean-Fran\u00e7ois Paiement",
                        "slug": "Jean-Fran\u00e7ois-Paiement",
                        "structuredName": {
                            "firstName": "Jean-Fran\u00e7ois",
                            "lastName": "Paiement",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Fran\u00e7ois Paiement"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467703"
                        ],
                        "name": "Pascal Vincent",
                        "slug": "Pascal-Vincent",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Vincent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2120888"
                        ],
                        "name": "M. Ouimet",
                        "slug": "M.-Ouimet",
                        "structuredName": {
                            "firstName": "Marie",
                            "lastName": "Ouimet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ouimet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1944221,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cfa15801bf4e7bf610d38dc86da62b83e2ddedcb",
            "isKey": false,
            "numCitedBy": 308,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "In this letter, we show a direct relation between spectral embedding methods and kernel principal components analysis and how both are special cases of a more general learning problem: learning the principal eigenfunctions of an operator defined from a kernel and the unknown data-generating density. Whereas spectral embedding methods provided only coordinates for the training points, the analysis justifies a simple extension to out-of-sample examples (the Nystrm formula) for multidimensional scaling (MDS), spectral clustering, Laplacian eigenmaps, locally linear embedding (LLE), and Isomap. The analysis provides, for all such spectral embedding methods, the definition of a loss function, whose empirical average is minimized by the traditional algorithms. The asymptotic expected value of that loss defines a generalization performance and clarifies what these algorithms are trying to learn. Experiments with LLE, Isomap, spectral clustering, and MDS show that this out-of-sample embedding formula generalizes well, with a level of error comparable to the effect of small perturbations of the training set on the embedding."
            },
            "slug": "Learning-Eigenfunctions-Links-Spectral-Embedding-Bengio-Delalleau",
            "title": {
                "fragments": [],
                "text": "Learning Eigenfunctions Links Spectral Embedding and Kernel PCA"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A direct relation is shown between spectral embedding methods and kernel principal components analysis and how both are special cases of a more general learning problem: learning the principal eigenfunctions of an operator defined from a kernel and the unknown data-generating density."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3103897"
                        ],
                        "name": "Changjiang Yang",
                        "slug": "Changjiang-Yang",
                        "structuredName": {
                            "firstName": "Changjiang",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Changjiang Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719541"
                        ],
                        "name": "R. Duraiswami",
                        "slug": "R.-Duraiswami",
                        "structuredName": {
                            "firstName": "Ramani",
                            "lastName": "Duraiswami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duraiswami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697880"
                        ],
                        "name": "N. Gumerov",
                        "slug": "N.-Gumerov",
                        "structuredName": {
                            "firstName": "Nail",
                            "lastName": "Gumerov",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Gumerov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693428"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Davis",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 47
                            }
                        ],
                        "text": "The core IFGT algorithm was first presented in (Yang et al., 2003) and its use in kernel machines was shown in (Yang et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 59
                            }
                        ],
                        "text": "A few applications would include kernel density estimation (Yang et al., 2003), prediction in SVMs, and mean prediction in Gaussian process regression."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7211602,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fac97a6cb182236664fc55914004e0fafbead66e",
            "isKey": false,
            "numCitedBy": 492,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Evaluating sums of multivariate Gaussians is a common computational task in computer vision and pattern recognition, including in the general and powerful kernel density estimation technique. The quadratic computational complexity of the summation is a significant barrier to the scalability of this algorithm to practical applications. The fast Gauss transform (FGT) has successfully accelerated the kernel density estimation to linear running time for low-dimensional problems. Unfortunately, the cost of a direct extension of the FGT to higher-dimensional problems grows exponentially with dimension, making it impractical for dimensions above 3. We develop an improved fast Gauss transform to efficiently estimate sums of Gaussians in higher dimensions, where a new multivariate expansion scheme and an adaptive space subdivision technique dramatically improve the performance. The improved FGT has been applied to the mean shift algorithm achieving linear computational complexity. Experimental results demonstrate the efficiency and effectiveness of our algorithm."
            },
            "slug": "Improved-fast-gauss-transform-and-efficient-kernel-Yang-Duraiswami",
            "title": {
                "fragments": [],
                "text": "Improved fast gauss transform and efficient kernel density estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An improved fast Gauss transform is developed to efficiently estimate sums of Gaussians in higher dimensions, where a new multivariate expansion scheme and an adaptive space subdivision technique dramatically improve the performance."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686671"
                        ],
                        "name": "L. Csat\u00f3",
                        "slug": "L.-Csat\u00f3",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Csat\u00f3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Csat\u00f3"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691022"
                        ],
                        "name": "M. Opper",
                        "slug": "M.-Opper",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Opper",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Opper"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 362,
                                "start": 151
                            }
                        ],
                        "text": "There are many strategies for specific problems which try to reduce this computational complexity by searching for a sparse representation of the data (Williams and Seeger, 2001; Smola and Bartlett, 2001; Fine and Scheinberg, 2001; Lee and Mangasarian, 2001; Lawrence et al., 2003; Csato and Opper, 2002; Tresp, 2000; Tipping, 2001; Snelson and Ghahramani, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11375333,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6df761a4db0be30776366024bf7ecaa60dd4d05b",
            "isKey": false,
            "numCitedBy": 714,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop an approach for sparse representations of gaussian process (GP) models (which are Bayesian types of kernel machines) in order to overcome their limitations for large data sets. The method is based on a combination of a Bayesian on-line algorithm, together with a sequential construction of a relevant subsample of the data that fully specifies the prediction of the GP model. By using an appealing parameterization and projection techniques in a reproducing kernel Hilbert space, recursions for the effective parameters and a sparse gaussian approximation of the posterior process are obtained. This allows for both a propagation of predictions and Bayesian error measures. The significance and robustness of our approach are demonstrated on a variety of experiments."
            },
            "slug": "Sparse-On-Line-Gaussian-Processes-Csat\u00f3-Opper",
            "title": {
                "fragments": [],
                "text": "Sparse On-Line Gaussian Processes"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "An approach for sparse representations of gaussian process (GP) models (which are Bayesian types of kernel machines) in order to overcome their limitations for large data sets is developed based on a combination of a Bayesian on-line algorithm and a sequential construction of a relevant subsample of data that fully specifies the prediction of the GP model."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 505,
                                "start": 486
                            }
                        ],
                        "text": "This class of learning algorithms includes most instances of the kernel machine algorithms [Scho\u0308lkopf et al., 1999], such as Support Vector Machines (SVMs) [Boser et al., 1992, Cortes and Vapnik, 1995] or Gaussi n processes [Williams and Rasmussen, 1996], but also unsupervised learning algorithms that attempt to capture the manifold structure of the data, such as Locally Linear Embedding [Roweis and Saul, 2000], Isomap [Tenenbaum et al., 2000], kernel PCA [Scho\u0308lkopf et al., 1998], Laplacian Eigenmaps [Belkin and Niyogi, 2003], Manifold Charting [Brand, 2003], and spectral clusteringalgorithms (see Weiss [1999] for a review)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 221
                            }
                        ],
                        "text": "In Bengio et al. [2005] we present similar results that applyto unsupervised learning algorithms such as non-parametric manifold learning algorithms [Roweis and Saul, 2000, Tenenbaum et al., 2000, Scho\u0308lkopf et al., 1998, Belkin and Niyogi, 2003]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 172
                            }
                        ],
                        "text": "\u2026structure of the data, such as Locally Linear Embedding [Roweis and Saul, 2000], Isomap [Tenenbaum et al., 2000], kernel PCA [Scho\u0308lkopf et al., 1998], Laplacian Eigenmaps [Belkin and Niyogi, 2003], Manifold Charting [Brand, 2003], and spectral clusteringalgorithms (see Weiss [1999] for a review)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6789724,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "38a49f2d906b48a36ab4baca448298666a9ec259",
            "isKey": false,
            "numCitedBy": 232,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the general problem of utilizing both labeled and unlabeled data to improve classification accuracy. Under the assumption that the data lie on a submanifold in a high dimensional space, we develop an algorithmic framework to classify a partially labeled data set in a principled manner. The central idea of our approach is that classification functions are naturally defined only on the sub-manifold in question rather than the total ambient space. Using the Laplace Beltrami operator one produces a basis for a Hilbert space of square integrable functions on the submanifold. To recover such a basis, only unlabeled examples are required. Once a basis is obtained, training can be performed using the labeled data set. Our algorithm models the manifold using the adjacency graph for the data and approximates the Laplace Beltrami operator by the graph Laplacian. Practical applications to image and text classification are considered."
            },
            "slug": "Using-manifold-structure-for-partially-labelled-Belkin-Niyogi",
            "title": {
                "fragments": [],
                "text": "Using manifold structure for partially labelled classification"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "An algorithmic framework to classify a partially labeled data set in a principled manner under the assumption that the data lie on a submanifold in a high dimensional space is developed."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 2002"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 49
                            }
                        ],
                        "text": "In most kernel based machine learning algorithms (Shawe-Taylor and Cristianini, 2004), Gaussian processes (Rasmussen and Williams, 2006), and nonparametric statistics (Izenman, 1991) the key computationally intensive task is to compute a linear combination of local kernel functions centered"
                    },
                    "intents": []
                }
            ],
            "corpusId": 35730151,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a0f9f3c338dd84b054dabcbd50b725f2b3609ad9",
            "isKey": false,
            "numCitedBy": 4628,
            "numCiting": 184,
            "paperAbstract": {
                "fragments": [],
                "text": "Kernel methods provide a powerful and unified framework for pattern discovery, motivating algorithms that can act on general types of data (e.g. strings, vectors or text) and look for general types of relations (e.g. rankings, classifications, regressions, clusters). The application areas range from neural networks and pattern recognition to machine learning and data mining. This book, developed from lectures and tutorials, fulfils two major roles: firstly it provides practitioners with a large toolkit of algorithms, kernels and solutions ready to use for standard pattern discovery problems in fields such as bioinformatics, text analysis, image analysis. Secondly it provides an easy introduction for students and researchers to the growing field of kernel-based pattern analysis, demonstrating with examples how to handcraft an algorithm or a kernel for a new specific application, and covering all the necessary conceptual and mathematical tools to do so."
            },
            "slug": "Kernel-Methods-for-Pattern-Analysis-Shawe-Taylor-Cristianini",
            "title": {
                "fragments": [],
                "text": "Kernel Methods for Pattern Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This book provides an easy introduction for students and researchers to the growing field of kernel-based pattern analysis, demonstrating with examples how to handcraft an algorithm or a kernel for a new specific application, and covering all the necessary conceptual and mathematical tools to do so."
            },
            "venue": {
                "fragments": [],
                "text": "ICTAI"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9330607"
                        ],
                        "name": "S. Roweis",
                        "slug": "S.-Roweis",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Roweis",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roweis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 390,
                                "start": 366
                            }
                        ],
                        "text": "This class of learning algorithms includes most instances of the kernel machine algorithms [Scho\u0308lkopf et al., 1999], such as Support Vector Machines (SVMs) [Boser et al., 1992, Cortes and Vapnik, 1995] or Gaussi n processes [Williams and Rasmussen, 1996], but also unsupervised learning algorithms that attempt to capture the manifold structure of the data, such as Locally Linear Embedding [Roweis and Saul, 2000], Isomap [Tenenbaum et al., 2000], kernel PCA [Scho\u0308lkopf et al., 1998], Laplacian Eigenmaps [Belkin and Niyogi, 2003], Manifold Charting [Brand, 2003], and spectral clusteringalgorithms (see Weiss [1999] for a review)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 150
                            }
                        ],
                        "text": "In Bengio et al. [2005] we present similar results that applyto unsupervised learning algorithms such as non-parametric manifold learning algorithms [Roweis and Saul, 2000, Tenenbaum et al., 2000, Scho\u0308lkopf et al., 1998, Belkin and Niyogi, 2003]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 143
                            }
                        ],
                        "text": "\u20261996], but also unsupervised learning algorithms that attempt to capture the manifold structure of the data, such as Locally Linear Embedding [Roweis and Saul, 2000], Isomap [Tenenbaum et al., 2000], kernel PCA [Scho\u0308lkopf et al., 1998], Laplacian Eigenmaps [Belkin and Niyogi, 2003],\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5987139,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "afcd6da7637ddeef6715109aca248da7a24b1c65",
            "isKey": false,
            "numCitedBy": 13979,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text."
            },
            "slug": "Nonlinear-dimensionality-reduction-by-locally-Roweis-Saul",
            "title": {
                "fragments": [],
                "text": "Nonlinear dimensionality reduction by locally linear embedding."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Locally linear embedding (LLE) is introduced, an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs that learns the global structure of nonlinear manifolds."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2334304,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "73e93d0346e8eee6c2ab45e46c26eaafb66e12a8",
            "isKey": false,
            "numCitedBy": 154,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a neurally-inspired, unsupervised learning algorithm that builds a non-linear generative model for pairs of face images from the same individual. Individuals are then recognized by finding the highest relative probability pair among all pairs that consist of a test image and an image whose identity is known. Our method compares favorably with other methods in the literature. The generative model consists of a single layer of rate-coded, non-linear feature detectors and it has the property that, given a data vector, the true posterior probability distribution over the feature detector activities can be inferred rapidly without iteration or approximation. The weights of the feature detectors are learned by comparing the correlations of pixel intensities and feature activations in two phases: When the network is observing real data and when it is observing reconstructions of real data generated from the feature activations."
            },
            "slug": "Rate-coded-Restricted-Boltzmann-Machines-for-Face-Teh-Hinton",
            "title": {
                "fragments": [],
                "text": "Rate-coded Restricted Boltzmann Machines for Face Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "A neurally-inspired, unsupervised learning algorithm that builds a non-linear generative model for pairs of face images from the same individual and individuals are then recognized by finding the highest relative probability pair among all pairs that consist of a test image and an image whose identity is known."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700754"
                        ],
                        "name": "Volker Tresp",
                        "slug": "Volker-Tresp",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "Tresp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Volker Tresp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 362,
                                "start": 151
                            }
                        ],
                        "text": "There are many strategies for specific problems which try to reduce this computational complexity by searching for a sparse representation of the data (Williams and Seeger, 2001; Smola and Bartlett, 2001; Fine and Scheinberg, 2001; Lee and Mangasarian, 2001; Lawrence et al., 2003; Csato and Opper, 2002; Tresp, 2000; Tipping, 2001; Snelson and Ghahramani, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 930666,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7eeea351d07ef9cc2d572d857e944ab59ee7c61b",
            "isKey": false,
            "numCitedBy": 384,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "The Bayesian committee machine (BCM) is a novel approach to combining estimators that were trained on different data sets. Although the BCM can be applied to the combination of any kind of estimators, the main foci are gaussian process regression and related systems such as regularization networks and smoothing splines for which the degrees of freedom increase with the number of training data. Somewhat surprisingly, we find that the performance of the BCM improves if several test points are queried at the same time and is optimal if the number of test points is at least as large as the degrees of freedom of the estimator. The BCM also provides a new solution for on-line learning with potential applications to data mining. We apply the BCM to systems with fixed basis functions and discuss its relationship to gaussian process regression. Finally, we show how the ideas behind the BCM can be applied in a non-Bayesian setting to extend the input-dependent combination of estimators."
            },
            "slug": "A-Bayesian-Committee-Machine-Tresp",
            "title": {
                "fragments": [],
                "text": "A Bayesian Committee Machine"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is found that the performance of the BCM improves if several test points are queried at the same time and is optimal if the number of test points is at least as large as the degrees of freedom of the estimator."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8981636,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e597460557d44de07ec570738cd2b42cdcc2580",
            "isKey": false,
            "numCitedBy": 389,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a simple sparse greedy technique to approximate the maximum a posteriori estimate of Gaussian Processes with much improved scaling behaviour in the sample size m. In particular, computational requirements are O(n2m), storage is O(nm), the cost for prediction is O(n) and the cost to compute confidence bounds is O(nm), where n \u226a m. We show how to compute a stopping criterion, give bounds on the approximation error, and show applications to large scale problems."
            },
            "slug": "Sparse-Greedy-Gaussian-Process-Regression-Smola-Bartlett",
            "title": {
                "fragments": [],
                "text": "Sparse Greedy Gaussian Process Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "A simple sparse greedy technique to approximate the maximum a posteriori estimate of Gaussian Processes with much improved scaling behaviour in the sample size m, and shows applications to large scale problems."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809909"
                        ],
                        "name": "L. Greengard",
                        "slug": "L.-Greengard",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Greengard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Greengard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143815327"
                        ],
                        "name": "J. Strain",
                        "slug": "J.-Strain",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Strain",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Strain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 8
                            }
                        ],
                        "text": "The FGT (Greengard and Strain, 1991) is a special case of the more general single level fast multipole method (Greengard and Rokhlin, 1987), adapted to the Gaussian potential."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3145209,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d6d75eb56c0ffd35234137bfd6a759754ac384a",
            "isKey": false,
            "numCitedBy": 527,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Many problems in applied mathematics require the evaluation of the sum of N Gaussians at M points in space. The work required for direct evaluation grows like $NM$ as N and M increase; this makes it very expensive to carry out such calculations on a large scale. In this paper, an algorithm is presented which evaluates the sum of N Gaussians at M arbitrarily distributed points in $C \\cdot (N + M)$ work, where C depends only on the precision required. When $N = M = 100,000$, the algorithm presented here is several thousand times faster than direct evaluation. It is based on a divide-and-conquer strategy, combined with the manipulation of Hermite expansions and Taylor series."
            },
            "slug": "The-Fast-Gauss-Transform-Greengard-Strain",
            "title": {
                "fragments": [],
                "text": "The Fast Gauss Transform"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "An algorithm is presented which evaluates the sum of N Gaussians at M arbitrarily distributed points in $C \\cdot (N + M)$ work, where C depends only on the precision required."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Sci. Comput."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832364"
                        ],
                        "name": "Xiaojin Zhu",
                        "slug": "Xiaojin-Zhu",
                        "structuredName": {
                            "firstName": "Xiaojin",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojin Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 79
                            }
                        ],
                        "text": "In this section we focus on algorithms of the type described in recent papers [Zhu et al., 2003, Zhou et al., 2004, Belkin et al., 2004, Delalleau et al., 2005], which are graphbased, non-parametric, semi-supervised learning algorithms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1052837,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "125842668eab7decac136db8a59d392dc5e4e395",
            "isKey": false,
            "numCitedBy": 3710,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "An approach to semi-supervised learning is proposed that is based on a Gaussian random field model. Labeled and unlabeled data are represented as vertices in a weighted graph, with edge weights encoding the similarity between instances. The learning problem is then formulated in terms of a Gaussian random field on this graph, where the mean of the field is characterized in terms of harmonic functions, and is efficiently obtained using matrix methods or belief propagation. The resulting learning algorithms have intimate connections with random walks, electric networks, and spectral graph theory. We discuss methods to incorporate class priors and the predictions of classifiers obtained by supervised learning. We also propose a method of parameter learning by entropy minimization, and show the algorithm's ability to perform feature selection. Promising experimental results are presented for synthetic data, digit classification, and text classification tasks."
            },
            "slug": "Semi-Supervised-Learning-Using-Gaussian-Fields-and-Zhu-Ghahramani",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "An approach to semi-supervised learning is proposed that is based on a Gaussian random field model, and methods to incorporate class priors and the predictions of classifiers obtained by supervised learning are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144811496"
                        ],
                        "name": "T. Feder",
                        "slug": "T.-Feder",
                        "structuredName": {
                            "firstName": "Tom\u00e1s",
                            "lastName": "Feder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Feder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2422263"
                        ],
                        "name": "D. Greene",
                        "slug": "D.-Greene",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Greene",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Greene"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 60
                            }
                        ],
                        "text": "The farthest point clustering has a running time O(N log K) (Feder and Greene, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 658151,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f6634e7122ea85eaa098c0b609cfe255afa5faf",
            "isKey": false,
            "numCitedBy": 438,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "In a clustering problem, the aim is to partition a given set of <italic>n</italic> points in <italic>d</italic>-dimensional space into <italic>k</italic> groups, called clusters, so that points within each cluster are near each other. Two objective functions frequently used to measure the performance of a clustering algorithm are, for any <italic>L<subscrpt>4</subscrpt></italic> metric, (a) the maximum distance between pairs of points in the same cluster, and (b) the maximum distance between points in each cluster and a chosen cluster center; we refer to either measure as the cluster size.\nWe show that one cannot approximate the optimal cluster size for a fixed number of clusters within a factor close to 2 in polynomial time, for two or more dimensions, unless P=NP. We also present an algorithm that achieves this factor of 2 in time <italic>&Ogr;</italic>(<italic>n</italic> log <italic>k</italic>), and show that this running time is optimal in the algebraic decision tree model. For a fixed cluster size, on the other hand, we give a polynomial time approximation scheme that estimates the optimal number of clusters under the second measure of cluster size within factors arbitrarily close to 1. Our approach is extended to provide approximation algorithms for the restricted centers, suppliers, and weighted suppliers problems that run in optimal <italic>&Ogr;</italic>(<italic>n</italic> log <italic>k</italic>) time and achieve optimal or nearly optimal approximation bounds."
            },
            "slug": "Optimal-algorithms-for-approximate-clustering-Feder-Greene",
            "title": {
                "fragments": [],
                "text": "Optimal algorithms for approximate clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work gives a polynomial time approximation scheme that estimates the optimal number of clusters under the second measure of cluster size within factors arbitrarily close to 1 for a fixed cluster size."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '88"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150220964"
                        ],
                        "name": "Monperrus Martin",
                        "slug": "Monperrus-Martin",
                        "structuredName": {
                            "firstName": "Monperrus",
                            "lastName": "Martin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Monperrus Martin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14850798,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc1d132c79a72dba386dc47750a49b0c29b54568",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We claim and present arguments to the effect that a large class of manifold learning algorithms that are essentially local and can be framed as kernel learning algorithms will suffer from the curse of dimensionality, at the dimension of the true underlying manifold. This observation suggests to explore non-local manifold learning algorithms which attempt to discover shared structure in the tangent planes at different positions. A criterion for such an algorithm is proposed and experiments estimating a tangent plane prediction function are presented, showing its advantages with respect to local manifold learning algorithms: it is able to generalize very far from training data (on learning handwritten character image rotations), where a local non-parametric method fails."
            },
            "slug": "Non-Local-Manifold-Tangent-Learning-Bengio-Martin",
            "title": {
                "fragments": [],
                "text": "Non-Local Manifold Tangent Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "It is claimed and presented arguments that a large class of manifold learning algorithms that are essentially local and can be framed as kernel learning algorithms will suffer from the curse of dimensionality, at the dimension of the true underlying manifold."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 195
                            }
                        ],
                        "text": "\u2026a general-purpose learning algorithm, appe rances can be deceiving: the so-called no-free-lunch theorems [Wolpert, 1996], as well as Vapnik\u2019s necessary and sufficient conditions for consistency [Vapnik, 1998, see], clearly show that there is no such thing as a completely general learning algorithm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 19
                            }
                        ],
                        "text": "The chapter mostly focuses on computational and statistical efficiency."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28637672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "385197d4c02593e2823c71e4f90a0993b703620e",
            "isKey": false,
            "numCitedBy": 26320,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "slug": "Statistical-learning-theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "Statistical learning theory"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7245737"
                        ],
                        "name": "Nicolas Le Roux",
                        "slug": "Nicolas-Le-Roux",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Le Roux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Le Roux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467703"
                        ],
                        "name": "Pascal Vincent",
                        "slug": "Pascal-Vincent",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Vincent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2460212"
                        ],
                        "name": "Olivier Delalleau",
                        "slug": "Olivier-Delalleau",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Delalleau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olivier Delalleau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144683424"
                        ],
                        "name": "P. Marcotte",
                        "slug": "P.-Marcotte",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Marcotte",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Marcotte"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 253,
                                "start": 233
                            }
                        ],
                        "text": "More specifically, when the number of hidden units becomes very large, and an L2 regularizer is used on the output weights, such a neural net becomes a kernelmachine, whose kernel has a simple form that can be computed analytically [Bengio et al., 2006b]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 586725,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "758b1d823ac975720e6e81e375cd4432009e5bca",
            "isKey": false,
            "numCitedBy": 166,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Convexity has recently received a lot of attention in the machine learning community, and the lack of convexity has been seen as a major disadvantage of many learning algorithms, such as multi-layer artificial neural networks. We show that training multi-layer neural networks in which the number of hidden units is learned can be viewed as a convex optimization problem. This problem involves an infinite number of variables, but can be solved by incrementally inserting a hidden unit at a time, each time finding a linear classifier that minimizes a weighted sum of errors."
            },
            "slug": "Convex-Neural-Networks-Bengio-Roux",
            "title": {
                "fragments": [],
                "text": "Convex Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Training multi-layer neural networks in which the number of hidden units is learned can be viewed as a convex optimization problem, which involves an infinite number of variables but can be solved by incrementally inserting a hidden unit at a time."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144509099"
                        ],
                        "name": "M. Kauffman",
                        "slug": "M.-Kauffman",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Kauffman",
                            "middleNames": [
                                "R.",
                                "Hern\u00e1ndez"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kauffman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "81426350"
                        ],
                        "name": "San Mateo",
                        "slug": "San-Mateo",
                        "structuredName": {
                            "firstName": "San",
                            "lastName": "Mateo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "San Mateo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3011435"
                        ],
                        "name": "E. Prescott",
                        "slug": "E.-Prescott",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Prescott",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Prescott"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 148
                            }
                        ],
                        "text": "\u2026sometimes cited as an existence proof of a general-purpose learning algorithm, appe rances can be deceiving: the so-called no-free-lunch theorems [Wolpert, 1996], as well as Vapnik\u2019s necessary and sufficient conditions for consistency [Vapnik, 1998, see], clearly show that there is no such thing\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 134
                            }
                        ],
                        "text": "A crucial question is how efficient are some of the popular learning methods when they are applied to complex perceptual tasks, such a visual pattern recognition with complicated intra-class variability."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 50
                            }
                        ],
                        "text": "The No-Free-Lunchtheorem for learning algorithms [Wolpert, 1996] states that no completely general-purpose learning algorithm can exist,in he sense that for every learning model there is a data distribution on which it will fare poorly (on both training and test, in the case of finite VC dimension)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15897108,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "604954a3600f749b25a9f52317a42d13a8ec0339",
            "isKey": true,
            "numCitedBy": 591,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Plutowski, M., et al. (1994). Cross-validation estimates integrated mean squared error. In Advances in neural information processing systems 6, Cowan et al. some constant set by k and m. It's also true that E(C OTS | \u03c6, d X) is not drastically different if one considers d X 's with a different m'. Accordingly, our summand doesn't vary drastically between d X 's of one m' and d X 's of another. Since n >> m and \u03c0(x) is uniform though, almost all of the terms in the sum have m' = m."
            },
            "slug": "The-Lack-of-A-Priori-Distinctions-Between-Learning-Kauffman-Mateo",
            "title": {
                "fragments": [],
                "text": "The Lack of A Priori Distinctions Between Learning Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It's true that n >> m and \u03c0(x) is uniform though, almost all of the terms in the sum have m' = m, so the summand doesn't vary drastically between d X 's of one m' and d X's of another."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13919023"
                        ],
                        "name": "F. Huang",
                        "slug": "F.-Huang",
                        "structuredName": {
                            "firstName": "Fu",
                            "lastName": "Huang",
                            "middleNames": [
                                "Jie"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 64
                            }
                        ],
                        "text": "Many of the results in ths section were previously reported in [Huang and LeCun, 2006]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 9123239,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cf03fdf52dd6e4249cbbdbd0bffbbbe5ca389feb",
            "isKey": false,
            "numCitedBy": 349,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "The detection and recognition of generic object categories with invariance to viewpoint, illumination, and clutter requires the combination of a feature extractor and a classifier. We show that architectures such as convolutional networks are good at learning invariant features, but not always optimal for classification, while Support Vector Machines are good at producing decision surfaces from wellbehaved feature vectors, but cannot learn complicated invariances. We present a hybrid system where a convolutional network is trained to detect and recognize generic objects, and a Gaussian-kernel SVM is trained from the features learned by the convolutional network. Results are given on a large generic object recognition task with six categories (human figures, four-legged animals, airplanes, trucks, cars, and \"none of the above\"), with multiple instances of each object category under various poses, illuminations, and backgrounds. On the test set, which contains different object instances than the training set, an SVM alone yields a 43.3% error rate, a convolutional net alone yields 7.2% and an SVM on top of features produced by the convolutional net yields 5.9%."
            },
            "slug": "Large-scale-Learning-with-SVM-and-Convolutional-for-Huang-LeCun",
            "title": {
                "fragments": [],
                "text": "Large-scale Learning with SVM and Convolutional for Generic Object Categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that architectures such as convolutional networks are good at learning invariant features, but not always optimal for classification, while Support Vector Machines are good for producing decision surfaces from wellbehaved feature vectors, but cannot learn complicated invariances."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 123
                            }
                        ],
                        "text": "In a DBN, each layer is trained as a Restricted Boltzmann Machine [Teh and Hi ton, 2001] using the Contrastive Divergence [Hinton, 2002] approximation of the log-likelihood gradient."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207596505,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9360e5ce9c98166bb179ad479a9d2919ff13d022",
            "isKey": false,
            "numCitedBy": 4570,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual expert models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called contrastive divergence whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data."
            },
            "slug": "Training-Products-of-Experts-by-Minimizing-Hinton",
            "title": {
                "fragments": [],
                "text": "Training Products of Experts by Minimizing Contrastive Divergence"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A product of experts (PoE) is an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary because it is hard even to approximate the derivatives of the renormalization term in the combination rule."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38767254"
                        ],
                        "name": "David Steinkraus",
                        "slug": "David-Steinkraus",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Steinkraus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Steinkraus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 161
                            }
                        ],
                        "text": "\u2026kernel (1.4% error) is only marginally betterhan that of a considerably smaller neural net with a single hidden layer of 800 hidden units (1.6% as reported by [Simard et al., 2003]), and similar to the results obtained with a 3-layer neural net as reported in [Hinton et al., 2006] (1.53% error)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 125
                            }
                        ],
                        "text": "Results are reported with three convolutional net architectur s: LeNet-5, LeNet-6, and the subsampling convolutional net of [Simard et al., 2003]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 79
                            }
                        ],
                        "text": "A conventional 2-layer neural network with 800 hidden units yelds 0.70% error [Simard\net al., 2003]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 26
                            }
                        ],
                        "text": "The convolutional net in [Simard et al., 2003] is somewhat similar to the original one in [LeCun et al., 1989] in that there is no separate convolution and subsampling layers."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 4659176,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5562a56da3a96dae82add7de705e2bd841eb00fc",
            "isKey": true,
            "numCitedBy": 2432,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural networks are a powerful technology forclassification of visual inputs arising from documents.However, there is a confusing plethora of different neuralnetwork methods that are used in the literature and inindustry. This paper describes a set of concrete bestpractices that document analysis researchers can use toget good results with neural networks. The mostimportant practice is getting a training set as large aspossible: we expand the training set by adding a newform of distorted data. The next most important practiceis that convolutional neural networks are better suited forvisual document tasks than fully connected networks. Wepropose that a simple \"do-it-yourself\" implementation ofconvolution with a flexible architecture is suitable formany visual document problems. This simpleconvolutional neural network does not require complexmethods, such as momentum, weight decay, structure-dependentlearning rates, averaging layers, tangent prop,or even finely-tuning the architecture. The end result is avery simple yet general architecture which can yieldstate-of-the-art performance for document analysis. Weillustrate our claims on the MNIST set of English digitimages."
            },
            "slug": "Best-practices-for-convolutional-neural-networks-to-Simard-Steinkraus",
            "title": {
                "fragments": [],
                "text": "Best practices for convolutional neural networks applied to visual document analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A set of concrete bestpractices that document analysis researchers can use to get good results with neural networks, including a simple \"do-it-yourself\" implementation of convolution with a flexible architecture suitable for many visual document problems."
            },
            "venue": {
                "fragments": [],
                "text": "Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145115014"
                        ],
                        "name": "Corinna Cortes",
                        "slug": "Corinna-Cortes",
                        "structuredName": {
                            "firstName": "Corinna",
                            "lastName": "Cortes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Corinna Cortes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 155
                            }
                        ],
                        "text": "Shallow architectures are best exemplified by modern kernel machines [Scho\u0308lkopf et al., 1999], such as Support Vector Machines (SVMs) [Boser et al., 1992, Cortes and Vapnik, 1995]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We argue that deep architectures have the potential to generalize in non-local ways, i.e., beyond immediate neighbors, and that this is crucial in order to make progress on the kind of complex tasks required for artificial intelligence."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 144
                            }
                        ],
                        "text": "\u2026includes most instances of the kernel machine algorithms [Scho\u0308lkopf et al., 1999], such as Support Vector Machines (SVMs) [Boser et al., 1992, Cortes and Vapnik, 1995] or Gaussi n processes [Williams and Rasmussen, 1996], but also unsupervised learning algorithms that attempt to capture the\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52874011,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "52b7bf3ba59b31f362aa07f957f1543a29a4279e",
            "isKey": false,
            "numCitedBy": 33430,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition."
            },
            "slug": "Support-Vector-Networks-Cortes-Vapnik",
            "title": {
                "fragments": [],
                "text": "Support-Vector Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated and the performance of the support- vector network is compared to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13919023"
                        ],
                        "name": "F. Huang",
                        "slug": "F.-Huang",
                        "structuredName": {
                            "firstName": "Fu",
                            "lastName": "Huang",
                            "middleNames": [
                                "Jie"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 6
                            }
                        ],
                        "text": "NORB [LeCun et al., 2004] is a publicly available dataset of object images from 5 generic categories."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 712708,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f354310098e09c1e1dc88758fca36767fd9d084d",
            "isKey": false,
            "numCitedBy": 1306,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We assess the applicability of several popular learning methods for the problem of recognizing generic visual categories with invariance to pose, lighting, and surrounding clutter. A large dataset comprising stereo image pairs of 50 uniform-colored toys under 36 azimuths, 9 elevations, and 6 lighting conditions was collected (for a total of 194,400 individual images). The objects were 10 instances of 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. Five instances of each category were used for training, and the other five for testing. Low-resolution grayscale images of the objects with various amounts of variability and surrounding clutter were used for training and testing. Nearest neighbor methods, support vector machines, and convolutional networks, operating on raw pixels or on PCA-derived features were tested. Test error rates for unseen object instances placed on uniform backgrounds were around 13% for SVM and 7% for convolutional nets. On a segmentation/recognition task with highly cluttered images, SVM proved impractical, while convolutional nets yielded 16/7% error. A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second."
            },
            "slug": "Learning-methods-for-generic-object-recognition-to-LeCun-Huang",
            "title": {
                "fragments": [],
                "text": "Learning methods for generic object recognition with invariance to pose and lighting"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second and proved impractical, while convolutional nets yielded 16/7% error."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30400079"
                        ],
                        "name": "Yair Weiss",
                        "slug": "Yair-Weiss",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15872360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e9d7f589a3d368a3701832e28d90ca09ec9e5577",
            "isKey": false,
            "numCitedBy": 857,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic grouping and segmentation of images remains a challenging problem in computer vision. Recently, a number of authors have demonstrated good performance on this task using methods that are based on eigenvectors of the affinity matrix. These approaches are extremely attractive in that they are based on simple eigendecomposition algorithms whose stability is well understood. Nevertheless, the use of eigendecompositions in the context of segmentation is far from well understood. In this paper we give a unified treatment of these algorithms, and show the close connections between them while highlighting their distinguishing features. We then prove results on eigenvectors of block matrices that allow us to analyze the performance of these algorithms in simple grouping settings. Finally, we use our analysis to motivate a variation on the existing methods that combines aspects from different eigenvector segmentation algorithms. We illustrate our analysis with results on real and synthetic images."
            },
            "slug": "Segmentation-using-eigenvectors:-a-unifying-view-Weiss",
            "title": {
                "fragments": [],
                "text": "Segmentation using eigenvectors: a unifying view"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A unified treatment of eigenvectors of block matrices based on eigendecompositions in the context of segmentation is given, and close connections between them are shown while highlighting their distinguishing features."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh IEEE International Conference on Computer Vision"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706809"
                        ],
                        "name": "Marc'Aurelio Ranzato",
                        "slug": "Marc'Aurelio-Ranzato",
                        "structuredName": {
                            "firstName": "Marc'Aurelio",
                            "lastName": "Ranzato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc'Aurelio Ranzato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060013"
                        ],
                        "name": "Christopher S. Poultney",
                        "slug": "Christopher-S.-Poultney",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Poultney",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher S. Poultney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3295092"
                        ],
                        "name": "S. Chopra",
                        "slug": "S.-Chopra",
                        "structuredName": {
                            "firstName": "Sumit",
                            "lastName": "Chopra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chopra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 819006,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "932c2a02d462abd75af018125413b1ceaa1ee3f4",
            "isKey": false,
            "numCitedBy": 1182,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a novel unsupervised method for learning sparse, overcomplete features. The model uses a linear encoder, and a linear decoder preceded by a sparsifying non-linearity that turns a code vector into a quasi-binary sparse code vector. Given an input, the optimal code minimizes the distance between the output of the decoder and the input patch while being as similar as possible to the encoder output. Learning proceeds in a two-phase EM-like fashion: (1) compute the minimum-energy code vector, (2) adjust the parameters of the encoder and decoder so as to decrease the energy. The model produces \"stroke detectors\" when trained on handwritten numerals, and Gabor-like filters when trained on natural image patches. Inference and learning are very fast, requiring no preprocessing, and no expensive sampling. Using the proposed unsupervised method to initialize the first layer of a convolutional network, we achieved an error rate slightly lower than the best reported result on the MNIST dataset. Finally, an extension of the method is described to learn topographical filter maps."
            },
            "slug": "Efficient-Learning-of-Sparse-Representations-with-Ranzato-Poultney",
            "title": {
                "fragments": [],
                "text": "Efficient Learning of Sparse Representations with an Energy-Based Model"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A novel unsupervised method for learning sparse, overcomplete features using a linear encoder, and a linear decoder preceded by a sparsifying non-linearity that turns a code vector into a quasi-binary sparse code vector."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809909"
                        ],
                        "name": "L. Greengard",
                        "slug": "L.-Greengard",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Greengard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Greengard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 272,
                                "start": 255
                            }
                        ],
                        "text": "Recently, such nonparametric problems have been collectively referred to as N -body problems in learning by Gray and Moore (2001), in analogy with the coulombic, magnetostatic, and gravitational N -body potential problems arising in computational physics (Greengard, 1994), where all pairwise"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 19084435,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9569dbe0c2e2aaf13aabbc061baa5c3fd11c2366",
            "isKey": false,
            "numCitedBy": 308,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Some of the recently developed fast summation methods that have arisen in scientific computing are described. These methods require an amount of work proportional to N or N log N to evaluate all pairwise interactions in an ensemble of N particles. Traditional methods, by contrast, require an amount of work proportional to N2. As a result, largescale simulations can be carried out using only modest computer resources. In combination with supercomputers, it is possible to address questions that were previously out of reach. Problems from diffusion, gravitation, and wave propagation are considered."
            },
            "slug": "Fast-Algorithms-for-Classical-Physics-Greengard",
            "title": {
                "fragments": [],
                "text": "Fast Algorithms for Classical Physics"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "Some of the recently developed fast summation methods that have arisen in scientific computing are described, and it is shown that in combination with supercomputers, it is possible to address questions that were previously out of reach."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46237323"
                        ],
                        "name": "M. Bianchi",
                        "slug": "M.-Bianchi",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Bianchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bianchi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 115765253,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "df0a69807ef9871fd192f3d9abe4bbd075be9083",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The motivation for density estimation in statistics and data analysis is to realize where observations occur more frequently in a sample. The aim of density estimation is to approximate a \u201ctrue\u201d probability density function f(x) from a sample information {X i }n i=1 of independent and identically distributed observations. The estimated density is constructed by centering around each observation X i a kernel function K h (u) = K(u/h)/h with u = x - X i , and averaging the values of this function at any given x."
            },
            "slug": "Bandwidth-Selection-in-Density-Estimation-Bianchi",
            "title": {
                "fragments": [],
                "text": "Bandwidth Selection in Density Estimation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2195972"
                        ],
                        "name": "P. Utgoff",
                        "slug": "P.-Utgoff",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Utgoff",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Utgoff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2599515"
                        ],
                        "name": "D. Stracuzzi",
                        "slug": "D.-Stracuzzi",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Stracuzzi",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Stracuzzi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1119517,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "398c477f674b228fec7f3f418a8cec047e2dafe5",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore incremental assimilation of new knowledge by sequential learning. Of particular interest is how a network of many knowledge layers can be constructed in an on-line manner, such that the learned units represent building blocks of knowledge that serve to compress the overall representation and facilitate transfer. We motivate the need for many layers of knowledge, and we advocate sequential learning as an avenue for promoting the construction of layered knowledge structures. Finally, our novel STL algorithm demonstrates a method for simultaneously acquiring and organizing a collection of concepts and functions as a network from a stream of unstructured information."
            },
            "slug": "Many-Layered-Learning-Utgoff-Stracuzzi",
            "title": {
                "fragments": [],
                "text": "Many-Layered Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This work explores incremental assimilation of new knowledge by sequential learning, and demonstrates a method for simultaneously acquiring and organizing a collection of concepts and functions as a network from a stream of unstructured information."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24982365"
                        ],
                        "name": "Dengyong Zhou",
                        "slug": "Dengyong-Zhou",
                        "structuredName": {
                            "firstName": "Dengyong",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dengyong Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698617"
                        ],
                        "name": "O. Bousquet",
                        "slug": "O.-Bousquet",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bousquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bousquet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2312432"
                        ],
                        "name": "T. N. Lal",
                        "slug": "T.-N.-Lal",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Lal",
                            "middleNames": [
                                "Navin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. N. Lal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 97
                            }
                        ],
                        "text": "In this section we focus on algorithms of the type described in recent papers [Zhu et al., 2003, Zhou et al., 2004, Belkin et al., 2004, Delalleau et al., 2005], which are graphbased, non-parametric, semi-supervised learning algorithms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 508435,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46770a8e7e2af28f5253e5961f709be74e34c1f6",
            "isKey": false,
            "numCitedBy": 3894,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the general problem of learning from labeled and unlabeled data, which is often called semi-supervised learning or transductive inference. A principled approach to semi-supervised learning is to design a classifying function which is sufficiently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points. We present a simple algorithm to obtain such a smooth solution. Our method yields encouraging experimental results on a number of classification problems and demonstrates effective use of unlabeled data."
            },
            "slug": "Learning-with-Local-and-Global-Consistency-Zhou-Bousquet",
            "title": {
                "fragments": [],
                "text": "Learning with Local and Global Consistency"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A principled approach to semi-supervised learning is to design a classifying function which is sufficiently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 147
                            }
                        ],
                        "text": "\u2026of learning algorithms includes most instances of the kernel machine algorithms [Scho\u0308lkopf et al., 1999], such as Support Vector Machines (SVMs) [Boser et al., 1992, Cortes and Vapnik, 1995] or Gaussi n processes [Williams and Rasmussen, 1996], but also unsupervised learning algorithms that\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 135
                            }
                        ],
                        "text": "Shallow architectures are best exemplified by modern kernel machines [Scho\u0308lkopf et al., 1999], such as Support Vector Machines (SVMs) [Boser et al., 1992, Cortes and Vapnik, 1995]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We argue that deep architectures have the potential to generalize in non-local ways, i.e., beyond immediate neighbors, and that this is crucial in order to make progress on the kind of complex tasks required for artificial intelligence."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207165665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2599131a4bc2fa957338732a37c744cfe3e17b24",
            "isKey": false,
            "numCitedBy": 10840,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms."
            },
            "slug": "A-training-algorithm-for-optimal-margin-classifiers-Boser-Guyon",
            "title": {
                "fragments": [],
                "text": "A training algorithm for optimal margin classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented, applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721248"
                        ],
                        "name": "P. Haffner",
                        "slug": "P.-Haffner",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Haffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haffner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 150
                            }
                        ],
                        "text": "\u2026different approaches have worked well in training such architectures: simple gradient descent applied to convolutional networks [LeCun et al., 1989, LeCun et al., 1998] (for signals and images), and more recently, layer-by-layer unsupervised learning followedby gradient descent [Hinton et al.,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 228
                            }
                        ],
                        "text": "In visual systems, an example of such a broad prior, which is inspired by Nature\u2019s bias towards retinotopic mappings, is the kind of connectivity used in convolutional networks for visual pattern recognition [LeCun et al., 1989, LeCun et al., 1998]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 98
                            }
                        ],
                        "text": "A notable exception to this is the convolutional neural network architecture [LeCun et al., 1989, LeCun et al., 1998] discussed in the next section, that has a sparse connectivity from layer to layer."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 79
                            }
                        ],
                        "text": "Notable exceptions include work on convolutional networks [LeCun et al., 1989, LeCun et al., 1998], and recentwork on Deep Belief Networks [Hinton et al., 2006]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 106
                            }
                        ],
                        "text": "Convolutional nets are being used commercially in several widely-deployed systems for reading bank check [LeCun et al., 1998], recognizing handwriting for tabletPC, and for detecting faces, people, and objects in videos inreal time."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 12
                            }
                        ],
                        "text": "In LeNet-5 [LeCun et al., 1998], the first feature detection layer produces 6 feature maps of size28 \u00d7 28 using 5 \u00d7 5 convolution kernels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 49
                            }
                        ],
                        "text": "The same network was reported to yield 0.95% in [LeCun et al., 1998] with a smaller number of training iterations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14542261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "162d958ff885f1462aeda91cd72582323fd6a1f4",
            "isKey": false,
            "numCitedBy": 35257,
            "numCiting": 248,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day."
            },
            "slug": "Gradient-based-learning-applied-to-document-LeCun-Bottou",
            "title": {
                "fragments": [],
                "text": "Gradient-based learning applied to document recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task, and Convolutional neural networks are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689438"
                        ],
                        "name": "J. H\u00e5stad",
                        "slug": "J.-H\u00e5stad",
                        "structuredName": {
                            "firstName": "Johan",
                            "lastName": "H\u00e5stad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H\u00e5stad"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121681905,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3067cab09b04637260b85716c605ba578dafec54",
            "isKey": false,
            "numCitedBy": 523,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Proving lower bounds on the amount of resources needed to compute specific functions is one of the most active branches of theoretical computer science. Significant progress has been made recently in proving lower bounds in two restricted models of Boolean circuits. One is the model of small depth circuits, and in this book Johan Torkel Hastad has developed very powerful techniques for proving exponential lower bounds on the size of small depth circuits' computing functions.The techniques described in \"Computational Limitations for Small Depth Circuits\" can be used to demonstrate almost optimal lower bounds on the size of small depth circuits computing several different functions, such as parity and majority. The main tool used in the proof of the lower bounds is a lemma, stating that any AND of small fanout OR gates can be converted into an OR of small fanout AND gates with high probability when random values are substituted for the variables.Hastad also applies this tool to relativized complexity, and discusses in great detail the computation of parity and majority in small depth circuits.Contents: Introduction. Small Depth Circuits. Outline of Lower Bound Proofs. Main Lemma. Lower Bounds for Small Depth Circuits. Functions Requiring Depth k to Have Small Circuits. Applications to Relativized Complexity. How Well Can We Compute Parity in Small Depth? Is Majority Harder than Parity? Conclusions.John Hastad is a postdoctoral fellow in the Department of Mathematics at MIT C\"omputational Limitations of Small Depth Circuits\" is a winner of the 1986 ACM Doctoral Dissertation Award."
            },
            "slug": "Computational-limitations-of-small-depth-circuits-H\u00e5stad",
            "title": {
                "fragments": [],
                "text": "Computational limitations of small-depth circuits"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The techniques described in \"Computational Limitations for Small Depth Circuits\" can be used to demonstrate almost optimal lower bounds on the size of small depth circuits computing several different functions, such as parity and majority."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 29
                            }
                        ],
                        "text": "Note that transductive SVMs [Joachims, 1999], which are another class of semi-supervised algorithms, are already subject to the limitations of corollary 2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14591650,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74b1a9e50f18af8a7b9f8dd38f40e0466ad7a8e8",
            "isKey": false,
            "numCitedBy": 3047,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces Transductive Support Vector Machines (TSVMs) for text classi cation. While regular Support Vector Machines (SVMs) try to induce a general decision function for a learning task, Transductive Support Vector Machines take into account a particular test set and try to minimize misclassi cations of just those particular examples. The paper presents an analysis of why TSVMs are well suited for text classi cation. These theoretical ndings are supported by experiments on three test collections. The experiments show substantial improvements over inductive methods, especially for small training sets, cutting the number of labeled training examples down to a twentieth on some tasks. This work also proposes an algorithm for training TSVMs e ciently, handling 10,000 examples and more."
            },
            "slug": "Transductive-Inference-for-Text-Classification-Joachims",
            "title": {
                "fragments": [],
                "text": "Transductive Inference for Text Classification using Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "An analysis of why Transductive Support Vector Machines are well suited for text classi cation is presented, and an algorithm for training TSVMs, handling 10,000 examples and more is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2416039"
                        ],
                        "name": "Yirong Shen",
                        "slug": "Yirong-Shen",
                        "structuredName": {
                            "firstName": "Yirong",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yirong Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680574"
                        ],
                        "name": "M. Seeger",
                        "slug": "M.-Seeger",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Seeger",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Seeger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 113
                            }
                        ],
                        "text": "For training, the IFGT can also be embedded in a conjugate-gradient or any other suitable optimization procedure (Yang et al., 2005; Shen et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1190939,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb00902d67f2fe224721b8a54c816e72883d28d2",
            "isKey": false,
            "numCitedBy": 121,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "The computation required for Gaussian process regression with n training examples is about O(n3) during training and O(n) for each prediction. This makes Gaussian process regression too slow for large datasets. In this paper, we present a fast approximation method, based on kd-trees, that significantly reduces both the prediction and the training times of Gaussian process regression."
            },
            "slug": "Fast-Gaussian-Process-Regression-using-KD-Trees-Shen-Ng",
            "title": {
                "fragments": [],
                "text": "Fast Gaussian Process Regression using KD-Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper presents a fast approximation method, based on kd-trees, that significantly reduces both the prediction and the training times of Gaussian process regression."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737568"
                        ],
                        "name": "N. D. Freitas",
                        "slug": "N.-D.-Freitas",
                        "structuredName": {
                            "firstName": "Nando",
                            "lastName": "Freitas",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. D. Freitas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46396571"
                        ],
                        "name": "Yang Wang",
                        "slug": "Yang-Wang",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3021403"
                        ],
                        "name": "Maryam Mahdaviani",
                        "slug": "Maryam-Mahdaviani",
                        "structuredName": {
                            "firstName": "Maryam",
                            "lastName": "Mahdaviani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Maryam Mahdaviani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064636190"
                        ],
                        "name": "Dustin Lang",
                        "slug": "Dustin-Lang",
                        "structuredName": {
                            "firstName": "Dustin",
                            "lastName": "Lang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dustin Lang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7778508,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c141d07d43ff71197235bc5bd76ec815baba9291",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the issue of numerical computation in machine learning domains based on similarity metrics, such as kernel methods, spectral techniques and Gaussian processes. It presents a general solution strategy based on Krylov subspace iteration and fast N-body learning methods. The experiments show significant gains in computation and storage on datasets arising in image segmentation, object detection and dimensionality reduction. The paper also presents theoretical bounds on the stability of these methods."
            },
            "slug": "Fast-Krylov-Methods-for-N-Body-Learning-Freitas-Wang",
            "title": {
                "fragments": [],
                "text": "Fast Krylov Methods for N-Body Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "This paper presents a general solution strategy based on Krylov subspace iteration and fast N-body learning methods for numerical computation in machine learning domains based on similarity metrics, such as kernel methods, spectral techniques and Gaussian processes."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722243"
                        ],
                        "name": "N. Linial",
                        "slug": "N.-Linial",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Linial",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Linial"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144830983"
                        ],
                        "name": "Y. Mansour",
                        "slug": "Y.-Mansour",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Mansour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Mansour"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689609"
                        ],
                        "name": "N. Nisan",
                        "slug": "N.-Nisan",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Nisan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Nisan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 163
                            }
                        ],
                        "text": "In particular, highly-variable functions (in the sense of having high frequencies in their Fourier spectrum) are difficult to represent with a circuit of depth 2 [Linial et al., 1993]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8753721,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6072c6734cc6911bdf4b260f16a2e6253e64897b",
            "isKey": false,
            "numCitedBy": 460,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Boolean functions in AC/sup O/ are studied using the harmonic analysis of the cube. The main result is that an AC/sup O/ Boolean function has almost all of its power spectrum on the low-order coefficients. This result implies the following properties of functions in AC/sup O/: functions in AC/sup O/ have low average sensitivity; they can be approximated well be a real polynomial of low degree; they cannot be pseudorandom function generators and their correlation with any polylog-wide independent probability distribution is small. An O(n/sup polylog(/ /sup sup)/ /sup (n)/)-time algorithm for learning functions in AC/sup O/ is obtained. The algorithm observed the behavior of an AC/sup O/ function on O(n/sup polylog/ /sup (n)/) randomly chosen inputs and derives a good approximation for the Fourier transform of the function. This allows it to predict with high probability the value of the function on other randomly chosen inputs.<<ETX>>"
            },
            "slug": "Constant-depth-circuits,-Fourier-transform,-and-Linial-Mansour",
            "title": {
                "fragments": [],
                "text": "Constant depth circuits, Fourier transform, and learnability"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An O(n/sup polylog(/ /sup sup)/ /sup (n)/)-time algorithm for learning functions in AC/sup O/ is obtained and derives a good approximation for the Fourier transform of the function."
            },
            "venue": {
                "fragments": [],
                "text": "30th Annual Symposium on Foundations of Computer Science"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 67
                            }
                        ],
                        "text": "To illustratehis point, we will use the example first proposed by [LeCun and Denker, 1992]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 58746468,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4096248dca0986be1a6978945a58028e549403d8",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The interrelated concepts of information. entropy, probability, aiid universal coniyutaliou eiijoy w r y wide applicatioii. I t is wvrth iroting, tliutcgh, t l t d the successful applications and theorems are inostly restricted to systeiits lcaviirg a zwst riuinler of par*licles or syinbols. For smal ler sys tems, it i s not clear how i o quanliffy th.e infom~atioii./eiitropy/probabtlziy precisely. W e uiyue that these are trot jusl. acigue, (icadt nric qtre.si.iotia, b-ut svirset irines (befits it e p 1.0 hlcnrs a I & praciice. Malheiiraiics and computer science al1o.w u s lo coiistrucd nisy nrriinher of itiequitrnlend probability n t m sure.q. E v e s \u201890-called \u201cunii~er.sal\u201d prohability niwnsiires can be quite iiiequivaleiit. Deciding tuhicli of these best deacribea the red world ia a t m k for enrpirical . w i t w e . As illuatratioiss uie coiisider the (1) itijorniation coiit ea l of the genome and (,?) a practical \u201clcariiing front ezamplcs \u201d task. Additional kcywords: ITniversal Turing Afnch,ine, Uti iversnl Data (?oiicpressivri., Viiive w a l P i i o r, A 1goiiih.iiric Complexity, Iiolrnogorotr Complexity, Macliiiie Leariiiirg."
            },
            "slug": "Natural-Versus-\"universal\"-Probability,-Complexity,-Denker-LeCun",
            "title": {
                "fragments": [],
                "text": "Natural Versus \"universal\" Probability, Complexity, And Entropy"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "The interrelated concepts of information, entropy, probability, aiid universal coniyutaliou eiijoy w r y wide applicatioii, and the successful applications and theorems are inostly restricted to systeiits lcaviirg a zwst riuinler of par*licles or syinbols."
            },
            "venue": {
                "fragments": [],
                "text": "Workshop on Physics and Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3472959"
                        ],
                        "name": "C. Rasmussen",
                        "slug": "C.-Rasmussen",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Rasmussen",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rasmussen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 146
                            }
                        ],
                        "text": "\u2026algorithms [Scho\u0308lkopf et al., 1999], such as Support Vector Machines (SVMs) [Boser et al., 1992, Cortes and Vapnik, 1995] or Gaussi n processes [Williams and Rasmussen, 1996], but also unsupervised learning algorithms that attempt to capture the manifold structure of the data, such as Locally\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2877073,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "0e658618c9dad4d70dd7dcd5c519185ec4f845f5",
            "isKey": false,
            "numCitedBy": 1160,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The Bayesian analysis of neural networks is difficult because a simple prior over weights implies a complex prior distribution over functions. In this paper we investigate the use of Gaussian process priors over functions, which permit the predictive Bayesian analysis for fixed values of hyperparameters to be carried out exactly using matrix operations. Two methods, using optimization and averaging (via Hybrid Monte Carlo) over hyperparameters have been tested on a number of challenging problems and have produced excellent results."
            },
            "slug": "Gaussian-Processes-for-Regression-Williams-Rasmussen",
            "title": {
                "fragments": [],
                "text": "Gaussian Processes for Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper investigates the use of Gaussian process priors over functions, which permit the predictive Bayesian analysis for fixed values of hyperparameters to be carried out exactly using matrix operations."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10327263,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "51ff037291582df4c205d4a9cbe6e7dcec8f5973",
            "isKey": false,
            "numCitedBy": 300,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "To-recognize-shapes,-first-learn-to-generate-Hinton",
            "title": {
                "fragments": [],
                "text": "To recognize shapes, first learn to generate images."
            },
            "venue": {
                "fragments": [],
                "text": "Progress in brain research"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152457109"
                        ],
                        "name": "M. Schmitt",
                        "slug": "M.-Schmitt",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Schmitt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schmitt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35434023,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee959f48aeb96c23672e065dcff818227cdd0aad",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We establish versions of Descartes' rule of signs for radial basis function (RBF) neural networks. The RBF rules of signs provide tight bounds for the number of zeros of univariate networks with certain parameter restrictions. Moreover, they can be used to infer that the Vapnik-Chervonenkis (VC) dimension and pseudodimension of these networks are no more than linear. This contrasts with previous work showing that RBF neural networks with two or more input nodes have superlinear VC dimension. The rules also give rise to lower bounds for network sizes, thus demonstrating the relevance of network parameters for the complexity of computing with RBF neural networks."
            },
            "slug": "Descartes'-Rule-of-Signs-for-Radial-Basis-Function-Schmitt",
            "title": {
                "fragments": [],
                "text": "Descartes' Rule of Signs for Radial Basis Function Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "Three versions of Descartes' rule of signs for radial basis function (RBF) neural networks are established, demonstrating the relevance of network parameters for the complexity of computing with RBF neural networks."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809909"
                        ],
                        "name": "L. Greengard",
                        "slug": "L.-Greengard",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Greengard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Greengard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121640573,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "24bdfaa23f8e8cdbcd45342cf6fea4cb2c8a1ea1",
            "isKey": false,
            "numCitedBy": 1234,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The evaluation of Coulombic or gravitational interactions in large-scale ensembles of particles is an integral part of the numerical simulation of a large number of physical processes. Examples include celestial mechanics, plasma physics, the vortex method in fluid dynamics, molecular dynamics, and the solution of the Laplace equation via potential theory. In a typical application, a numerical model follows the trajectories of a number of particles moving in accordance with Newton's second law of motion in a field generated by the whole ensemble. In many situations, in order to be of physical interest, the simulation has to involve thousands of particles (or more), and the fields have to be evaluated for a large number of configurations. Unfortunately, an amount of work of the order $O(N\\sp 2)$ has traditionally been required to evaluate all pairwise interactions in a system of N particles, unless some approximation or truncation method is used. As a result, large-scale simulations have been extremely expensive in some cases, and prohibitive in others. We present an algorithm for the rapid evaluation of the potential and force fields in large-scale systems of particles. In order to evaluate all pairwise Coulombic interactions of N particles to within round-off error, the algorithm requires an amount of work proportional to N, and this estimate does not depend on the statistics of the distribution. Both two and three dimensional versions of the algorithm have been constructed, and we will discuss their applications to several problems in physics, chemistry, biology, and numerical complex analysis."
            },
            "slug": "The-Rapid-Evaluation-of-Potential-Fields-in-Systems-Greengard",
            "title": {
                "fragments": [],
                "text": "The Rapid Evaluation of Potential Fields in Particle Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144549270"
                        ],
                        "name": "M. Brand",
                        "slug": "M.-Brand",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Brand",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Brand"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 550,
                                "start": 533
                            }
                        ],
                        "text": "This class of learning algorithms includes most instances of the kernel machine algorithms [Scho\u0308lkopf et al., 1999], such as Support Vector Machines (SVMs) [Boser et al., 1992, Cortes and Vapnik, 1995] or Gaussi n processes [Williams and Rasmussen, 1996], but also unsupervised learning algorithms that attempt to capture the manifold structure of the data, such as Locally Linear Embedding [Roweis and Saul, 2000], Isomap [Tenenbaum et al., 2000], kernel PCA [Scho\u0308lkopf et al., 1998], Laplacian Eigenmaps [Belkin and Niyogi, 2003], Manifold Charting [Brand, 2003], and spectral clusteringalgorithms (see Weiss [1999] for a review)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 217
                            }
                        ],
                        "text": "\u2026structure of the data, such as Locally Linear Embedding [Roweis and Saul, 2000], Isomap [Tenenbaum et al., 2000], kernel PCA [Scho\u0308lkopf et al., 1998], Laplacian Eigenmaps [Belkin and Niyogi, 2003], Manifold Charting [Brand, 2003], and spectral clusteringalgorithms (see Weiss [1999] for a review)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14688376,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "c291edf0bb5ead2e45ee9fefb1683e2438158121",
            "isKey": false,
            "numCitedBy": 492,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We construct a nonlinear mapping from a high-dimensional sample space to a low-dimensional vector space, effectively recovering a Cartesian coordinate system for the manifold from which the data is sampled. The mapping preserves local geometric relations in the manifold and is pseudo-invertible. We show how to estimate the intrinsic dimensionality of the manifold from samples, decompose the sample data into locally linear low-dimensional patches, merge these patches into a single low-dimensional coordinate system, and compute forward and reverse mappings between the sample and coordinate spaces. The objective functions are convex and their solutions are given in closed form."
            },
            "slug": "Charting-a-Manifold-Brand",
            "title": {
                "fragments": [],
                "text": "Charting a Manifold"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "A nonlinear mapping from a high-dimensional sample space to a low-dimensional vector space is constructed, effectively recovering a Cartesian coordinate system for the manifold from which the data is sampled, and is pseudo-invertible."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3444268"
                        ],
                        "name": "R. Lordo",
                        "slug": "R.-Lordo",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Lordo",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lordo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 5
                            }
                        ],
                        "text": "See [Ha\u0308rdle et al., 2004] for a recent and easily accessible exposition of the curse of dimensionality for classical non-parametric methods."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 133
                            }
                        ],
                        "text": "For a wide class of kernel regression estimators, the unconditional variance and squared bias can be shown to be written as follows [Ha\u0308rdle et al., 2004]:\nexpected error = C1 n\u03c3d + C2\u03c3 4,\nwith C1 andC2 not depending on nor on the dimensiond."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 35612632,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "99dbf84594820af16b394c781688d941237fdc77",
            "isKey": false,
            "numCitedBy": 591,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Weisberg (1985). Also, very little recent literature (after 1984) is covered (with the exception of Sec. 7.3, which covers radial basis functions). Following Cook and Weisberg (1999, p. 432), the most important idea from the recent literature is that MLR is the study of the conditional distribution of the response variable given the predictors, and this distribution can be visualized with a plot of the fitted values versus the response variable. Texts that do not discuss this plot may be obsolete."
            },
            "slug": "Nonparametric-and-Semiparametric-Models-Lordo",
            "title": {
                "fragments": [],
                "text": "Nonparametric and Semiparametric Models"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "Following Cook and Weisberg (1999, p. 432), the most important idea from the recent literature is that MLR is the study of the conditional distribution of the response variable given the predictors, and this distribution can be visualized with a plot of the fitted values versus the responseVariable."
            },
            "venue": {
                "fragments": [],
                "text": "Technometrics"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145520115"
                        ],
                        "name": "Mikhail Belkin",
                        "slug": "Mikhail-Belkin",
                        "structuredName": {
                            "firstName": "Mikhail",
                            "lastName": "Belkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikhail Belkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2458509"
                        ],
                        "name": "Irina Matveeva",
                        "slug": "Irina-Matveeva",
                        "structuredName": {
                            "firstName": "Irina",
                            "lastName": "Matveeva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Irina Matveeva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770745"
                        ],
                        "name": "P. Niyogi",
                        "slug": "P.-Niyogi",
                        "structuredName": {
                            "firstName": "Partha",
                            "lastName": "Niyogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Niyogi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 116
                            }
                        ],
                        "text": "In this section we focus on algorithms of the type described in recent papers [Zhu et al., 2003, Zhou et al., 2004, Belkin et al., 2004, Delalleau et al., 2005], which are graphbased, non-parametric, semi-supervised learning algorithms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 44352521,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c5c6ea2f23fe8d3e986c4c99e83a90c204538619",
            "isKey": false,
            "numCitedBy": 583,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of labeling a partially labeled graph. This setting may arise in a number of situations from survey sampling to information retrieval to pattern recognition in manifold settings. It is also of potential practical importance, when the data is abundant, but labeling is expensive or requires human assistance."
            },
            "slug": "Regularization-and-Semi-supervised-Learning-on-Belkin-Matveeva",
            "title": {
                "fragments": [],
                "text": "Regularization and Semi-supervised Learning on Large Graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "This work considers the problem of labeling a partially labeled graph, which may arise in a number of situations from survey sampling to information retrieval to pattern recognition in manifold settings."
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37274089"
                        ],
                        "name": "D. Henderson",
                        "slug": "D.-Henderson",
                        "structuredName": {
                            "firstName": "Donnie",
                            "lastName": "Henderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Henderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2799635"
                        ],
                        "name": "R. Howard",
                        "slug": "R.-Howard",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Howard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34859193"
                        ],
                        "name": "W. Hubbard",
                        "slug": "W.-Hubbard",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Hubbard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Hubbard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041866"
                        ],
                        "name": "L. Jackel",
                        "slug": "L.-Jackel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Jackel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jackel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 208
                            }
                        ],
                        "text": "In visual systems, an example of such a broad prior, which is inspired by Nature\u2019s bias towards retinotopic mappings, is the kind of connectivity used in convolutional networks for visual pattern recognition [LeCun et al., 1989, LeCun et al., 1998]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 78
                            }
                        ],
                        "text": "A notable exception to this is the convolutional neural network architecture [LeCun et al., 1989, LeCun et al., 1998] discussed in the next section, that has a sparse connectivity from layer to layer."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 143
                            }
                        ],
                        "text": "\u2026at least two different approaches have worked well in training such architectures: simple gradient descent applied to convolutional networks [LeCun et al., 1989, LeCun et al., 1998] (for signals and images), and more recently, layer-by-layer unsupervised learning followedby gradient descent\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 59
                            }
                        ],
                        "text": "Notable exceptions include work on convolutional networks [LeCun et al., 1989, LeCun et al., 1998], and recentwork on Deep Belief Networks [Hinton et al., 2006]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 91
                            }
                        ],
                        "text": "The convolutional net in [Simard et al., 2003] is somewhat similar to the original one in [LeCun et al., 1989] in that there is no separate convolution and subsampling layers."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 41312633,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8e8f3c8d4418c8d62e306538c9c1292635e9d27",
            "isKey": true,
            "numCitedBy": 7830,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification."
            },
            "slug": "Backpropagation-Applied-to-Handwritten-Zip-Code-LeCun-Boser",
            "title": {
                "fragments": [],
                "text": "Backpropagation Applied to Handwritten Zip Code Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This paper demonstrates how constraints from the task domain can be integrated into a backpropagation network through the architecture of the network, successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144306657"
                        ],
                        "name": "T. Gonzalez",
                        "slug": "T.-Gonzalez",
                        "structuredName": {
                            "firstName": "Teofilo",
                            "lastName": "Gonzalez",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Gonzalez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 205092276,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1cdd3c62172b7598cd090e349d38e9644734edfd",
            "isKey": false,
            "numCitedBy": 1547,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Clustering-to-Minimize-the-Maximum-Intercluster-Gonzalez",
            "title": {
                "fragments": [],
                "text": "Clustering to Minimize the Maximum Intercluster Distance"
            },
            "venue": {
                "fragments": [],
                "text": "Theor. Comput. Sci."
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809909"
                        ],
                        "name": "L. Greengard",
                        "slug": "L.-Greengard",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Greengard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Greengard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2487745"
                        ],
                        "name": "V. Rokhlin",
                        "slug": "V.-Rokhlin",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Rokhlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Rokhlin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 110
                            }
                        ],
                        "text": "The FGT (Greengard and Strain, 1991) is a special case of the more general single level fast multipole method (Greengard and Rokhlin, 1987), adapted to the Gaussian potential."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 69
                            }
                        ],
                        "text": "The FGT is a special case of the more general fast multipole methods (Greengard and Rokhlin, 1987), adapted to the Gaussian potential."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 309,
                                "start": 280
                            }
                        ],
                        "text": "Originally this method was developed for the fast summation of the potential fields generated by a large number of sources (charges), such as those arising in gravitational or electrostatic potential problems, that are described by the Laplace equation in two or three dimensions (Greengard and Rokhlin, 1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16415369,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "688384fc5e643445e835435e96b9dfcfb6598d36",
            "isKey": false,
            "numCitedBy": 4759,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-fast-algorithm-for-particle-simulations-Greengard-Rokhlin",
            "title": {
                "fragments": [],
                "text": "A fast algorithm for particle simulations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703049"
                        ],
                        "name": "D. DeCoste",
                        "slug": "D.-DeCoste",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "DeCoste",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. DeCoste"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 85843,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd14aa8bef5afc7d248a04de681242f2e64c6a1e",
            "isKey": false,
            "numCitedBy": 594,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "Practical experience has shown that in order to obtain the best possible performance, prior knowledge about invariances of a classification problem at hand ought to be incorporated into the training procedure. We describe and review all known methods for doing so in support vector machines, provide experimental results, and discuss their respective merits. One of the significant new results reported in this work is our recent achievement of the lowest reported test error on the well-known MNIST digit recognition benchmark task, with SVM training times that are also significantly faster than previous SVM methods."
            },
            "slug": "Training-Invariant-Support-Vector-Machines-DeCoste-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "Training Invariant Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work reports the recent achievement of the lowest reported test error on the well-known MNIST digit recognition benchmark task, with SVM training times that are also significantly faster than previous SVM methods."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760644"
                        ],
                        "name": "E. Allender",
                        "slug": "E.-Allender",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Allender",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Allender"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9335300,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5e1b26ed51ece1f83e00dd18ae2454ef9fd50bb6",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 148,
            "paperAbstract": {
                "fragments": [],
                "text": "The 1980's saw rapid and exciting development of techniques for proving lower bounds in circuit complexity. This pace has slowed recently, and there has even been work indicating that quite different proof techniques must be employed to advance beyond the current frontier of circuit lower bounds. Although this has engendered pessimism in some quarters, there have in fact been many positive developments in the past few years showing that significant progress is possible on many fronts. This paper is a (necessarily incomplete) survey of the state of circuit complexity as we await the dawn of the new millennium."
            },
            "slug": "Circuit-Complexity-before-the-Dawn-of-the-New-Allender",
            "title": {
                "fragments": [],
                "text": "Circuit Complexity before the Dawn of the New Millennium"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A (necessarily incomplete) survey of the state of circuit complexity as the authors await the dawn of the new millennium."
            },
            "venue": {
                "fragments": [],
                "text": "FSTTCS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34911188"
                        ],
                        "name": "S. Smale",
                        "slug": "S.-Smale",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Smale",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Smale"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1002516,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8cc06f9035fd11cf30ecd1b36266cc7b31c4df1",
            "isKey": false,
            "numCitedBy": 542,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Learning is key to developing systems tailored to a broad range of data analysis and information extraction tasks. We outline the mathematical foundations of learning theory and describe a key algorithm of it."
            },
            "slug": "The-Mathematics-of-Learning:-Dealing-with-Data-Poggio-Smale",
            "title": {
                "fragments": [],
                "text": "The Mathematics of Learning: Dealing with Data"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The mathematical foundations of learning theory are outlined and a key algorithm of it is described, which is key to developing systems tailored to a broad range of data analysis and information extraction tasks."
            },
            "venue": {
                "fragments": [],
                "text": "2005 International Conference on Neural Networks and Brain"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 147
                            }
                        ],
                        "text": "\u2026non-parametric statistical lerning algorithms \u2013 such as the k-nearest neighbors and the Parzen windows regression and density stimation algorithms [Duda and Hart, 1973] \u2013 which have been shown to sufferfrom the so-called curse of dimensionality, it is logical to investigate the following\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12946615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b07ce649d6f6eb636872527104b0209d3edc8188",
            "isKey": false,
            "numCitedBy": 16926,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "slug": "Pattern-classification-and-scene-analysis-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Pattern classification and scene analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "venue": {
                "fragments": [],
                "text": "A Wiley-Interscience publication"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 459,
                                "start": 456
                            }
                        ],
                        "text": "This class of learning algorithms includes most instances of the kernel machine algorithms [Scho\u0308lkopf et al., 1999], such as Support Vector Machines (SVMs) [Boser et al., 1992, Cortes and Vapnik, 1995] or Gaussi n processes [Williams and Rasmussen, 1996], but also unsupervised learning algorithms that attempt to capture the manifold structure of the data, such as Locally Linear Embedding [Roweis and Saul, 2000], Isomap [Tenenbaum et al., 2000], kernel PCA [Scho\u0308lkopf et al., 1998], Laplacian Eigenmaps [Belkin and Niyogi, 2003], Manifold Charting [Brand, 2003], and spectral clusteringalgorithms (see Weiss [1999] for a review)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 197
                            }
                        ],
                        "text": "In Bengio et al. [2005] we present similar results that applyto unsupervised learning algorithms such as non-parametric manifold learning algorithms [Roweis and Saul, 2000, Tenenbaum et al., 2000, Scho\u0308lkopf et al., 1998, Belkin and Niyogi, 2003]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 150
                            }
                        ],
                        "text": "\u2026to capture the manifold structure of the data, such as Locally Linear Embedding [Roweis and Saul, 2000], Isomap [Tenenbaum et al., 2000], kernel PCA [Scho\u0308lkopf et al., 1998], Laplacian Eigenmaps [Belkin and Niyogi, 2003], Manifold Charting [Brand, 2003], and spectral clusteringalgorithms (see\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6674407,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3f600e6c6cf93e78c9e6e690443d6d22c4bf18b9",
            "isKey": false,
            "numCitedBy": 7882,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method for performing a nonlinear form of principal component analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in high-dimensional feature spaces, related to input space by some nonlinear mapfor instance, the space of all possible five-pixel products in 16 16 images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition."
            },
            "slug": "Nonlinear-Component-Analysis-as-a-Kernel-Eigenvalue-Sch\u00f6lkopf-Smola",
            "title": {
                "fragments": [],
                "text": "Nonlinear Component Analysis as a Kernel Eigenvalue Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A new method for performing a nonlinear form of principal component analysis by the use of integral operator kernel functions is proposed and experimental results on polynomial feature extraction for pattern recognition are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 70
                            }
                        ],
                        "text": "This is a consequence of the well known classical representer theorem (Wabha, 1990) which states that the solutions of certain risk minimization problems involving an empirical risk term and a quadratic regularizer can be written as expansions in terms of the kernels centered on the training examples."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121858740,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e786caa59202d923ccaae00ae6a4682eec92699b",
            "isKey": false,
            "numCitedBy": 5072,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Foreword 1. Background 2. More splines 3. Equivalence and perpendicularity, or, what's so special about splines? 4. Estimating the smoothing parameter 5. 'Confidence intervals' 6. Partial spline models 7. Finite dimensional approximating subspaces 8. Fredholm integral equations of the first kind 9. Further nonlinear generalizations 10. Additive and interaction splines 11. Numerical methods 12. Special topics Bibliography Author index."
            },
            "slug": "Spline-Models-for-Observational-Data-Wahba",
            "title": {
                "fragments": [],
                "text": "Spline Models for Observational Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50467915"
                        ],
                        "name": "M. Wand",
                        "slug": "M.-Wand",
                        "structuredName": {
                            "firstName": "Matt",
                            "lastName": "Wand",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wand"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123402533"
                        ],
                        "name": "M. C. Jones",
                        "slug": "M.-C.-Jones",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Jones",
                            "middleNames": [
                                "Chris"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. C. Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 132
                            }
                        ],
                        "text": "regularized least squares (Poggio and Smale, 2003), support vector machines (Cristianini and Shawe-Taylor, 2000), kernel regression (Wand and Jones, 1995)) f is the regression/classification function."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 63
                            }
                        ],
                        "text": "In its most general form , the d-dimensional KDE is written as (Wand and Jones, 1995)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 72
                            }
                        ],
                        "text": "For non-parametric density estimation it is the kernel density estimate (Wand and Jones, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 45
                            }
                        ],
                        "text": ", h 2 d) the optimal bandwidths are given by (Wand and Jones, 1995)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 73
                            }
                        ],
                        "text": "This is sometimes referred to as \u2018letting the data speak for themselves\u2019 (Wand and Jones, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 65
                            }
                        ],
                        "text": "The plug-in bandwidths are known to show more stable performance (Wand and Jones, 1995) than the cross-validation methods."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 198189892,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "823d470f417d0365556f7ed18a4b9ea36f2a73f4",
            "isKey": true,
            "numCitedBy": 3004,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Kernel-Smoothing-Wand-Jones",
            "title": {
                "fragments": [],
                "text": "Kernel Smoothing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699108"
                        ],
                        "name": "G. Tesauro",
                        "slug": "G.-Tesauro",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Tesauro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Tesauro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 48
                            }
                        ],
                        "text": "Examples of such complex behaviors are found in visual perception, auditory perception, and natural language processing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 169
                            }
                        ],
                        "text": "Until very recently, empirical studies often found that deep networks generally performed no better, and often worse, than neural networks with one or twohidden layers [Tesauro, 1992]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 269,
                                "start": 256
                            }
                        ],
                        "text": "Deep architectures rarely appear in the machine learning literature; the vast majority of neural network research has focused on shallow architectures with a single hidden layer, because of the difficulty of training networks with more than 2 or 3 layers [Tesauro, 1992]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 996637,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "646ff15fbd38f8c4e2b099ad09e4570179709c73",
            "isKey": true,
            "numCitedBy": 337,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper examines whether temporal difference methods for training connectionist networks, such as Sutton's TD(\u03bb) algorithm, can be successfully applied to complex real-world problems. A number of important practical issues are identified and discussed from a general theoretical perspective. These practical issues are then examined in the context of a case study in which TD(\u03bb) is applied to learning the game of backgammon from the outcome of self-play. This is apparently the first application of this algorithm to a complex non-trivial task. It is found that, with zero knowledge built in, the network is able to learn from scratch to play the entire game at a fairly strong intermediate level of performance, which is clearly better than conventional commercial programs, and which in fact surpasses comparable networks trained on a massive human expert data set. This indicates that TD learning may work better in practice than one would expect based on current theory, and it suggests that further analysis of TD methods, as well as applications in other complex domains, may be worth investigating."
            },
            "slug": "Practical-issues-in-temporal-difference-learning-Tesauro",
            "title": {
                "fragments": [],
                "text": "Practical issues in temporal difference learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is found that, with zero knowledge built in, the network is able to learn from scratch to play the entire game at a fairly strong intermediate level of performance, which is clearly better than conventional commercial programs, and which surpasses comparable networks trained on a massive human expert data set."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49800308"
                        ],
                        "name": "B. Silverman",
                        "slug": "B.-Silverman",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Silverman",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Silverman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 118
                            }
                        ],
                        "text": "One of the earliest methods, especially proposed for univariate fast kernel density estimation was based on this idea (Silverman, 1982)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 222309742,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3e71cf138d0428bb646c660554369afa3dc4b7c7",
            "isKey": false,
            "numCitedBy": 113,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Algorithm-AS-176:-Kernel-Density-Estimation-Using-Silverman",
            "title": {
                "fragments": [],
                "text": "Algorithm AS 176: Kernel Density Estimation Using the Fast Fourier Transform"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704344"
                        ],
                        "name": "M. Ajtai",
                        "slug": "M.-Ajtai",
                        "structuredName": {
                            "firstName": "Mikl\u00f3s",
                            "lastName": "Ajtai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ajtai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 121249363,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8dedaa0168e36600c6ee7091b49913f3007245ba",
            "isKey": false,
            "numCitedBy": 557,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "\u221111-Formulae-on-finite-structures-Ajtai",
            "title": {
                "fragments": [],
                "text": "\u221111-Formulae on finite structures"
            },
            "venue": {
                "fragments": [],
                "text": "Ann. Pure Appl. Log."
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770124"
                        ],
                        "name": "Sunita Sarawagi",
                        "slug": "Sunita-Sarawagi",
                        "structuredName": {
                            "firstName": "Sunita",
                            "lastName": "Sarawagi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sunita Sarawagi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5080176,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5e4a5edf46e65c87072c85e7362e63210849c69a",
            "isKey": false,
            "numCitedBy": 613,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Graphical models provide a powerful framework for probabilistic modelling and reasoning. Although theory behind learning and inference is well understood, most practical applications require approximation to known algorithms. We review learning of thin junction trees\u2013a class of graphical models that permits efficient inference. We discuss particular cases in clique graphs where exact inference is possible in polynomial time and some special cases where good approximation guarantees can be given. We also point out the drawbacks of learning with approximate inference. Finally, a practical application of probabilistic generative model for learning visual attributes from images is discussed."
            },
            "slug": "Learning-with-Graphical-Models-Sarawagi",
            "title": {
                "fragments": [],
                "text": "Learning with Graphical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Learning of thin junction trees is reviewed\u2013a class of graphical models that permits efficient inference and particular cases in clique graphs where exact inference is possible in polynomial time and some special cases where good approximation guarantees can be given."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10871103,
            "fieldsOfStudy": [],
            "id": "29b7fb7444159c5baa06d00e99afe9f8546811d7",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Asymptotic derivation of the finite-sample risk of the k nearest neighbor classifier \u2217 ( Technical Report UVM \u2013 CS \u2013 1998 \u2013 0101 )"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680574"
                        ],
                        "name": "M. Seeger",
                        "slug": "M.-Seeger",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Seeger",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Seeger"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 362,
                                "start": 151
                            }
                        ],
                        "text": "There are many strategies for specific problems which try to reduce this computational complexity by searching for a sparse representation of the data (Williams and Seeger, 2001; Smola and Bartlett, 2001; Fine and Scheinberg, 2001; Lee and Mangasarian, 2001; Lawrence et al., 2003; Csato and Opper, 2002; Tresp, 2000; Tipping, 2001; Snelson and Ghahramani, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 42041158,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b6fff8b8ea77f157913986e7af53951d9fc1128e",
            "isKey": false,
            "numCitedBy": 2168,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "A major problem for kernel-based predictors (such as Support Vector Machines and Gaussian processes) is that the amount of computation required to find the solution scales as O(n3), where n is the number of training examples. We show that an approximation to the eigendecomposition of the Gram matrix can be computed by the Nystrom method (which is used for the numerical solution of eigenproblems). This is achieved by carrying out an eigendecomposition on a smaller system of size m < n, and then expanding the results back up to n dimensions. The computational complexity of a predictor using this approximation is O(m2n). We report experiments on the USPS and abalone data sets and show that we can set m \u226a n without any significant decrease in the accuracy of the solution."
            },
            "slug": "Using-the-Nystr\u00f6m-Method-to-Speed-Up-Kernel-Williams-Seeger",
            "title": {
                "fragments": [],
                "text": "Using the Nystr\u00f6m Method to Speed Up Kernel Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that an approximation to the eigendecomposition of the Gram matrix can be computed by the Nystrom method (which is used for the numerical solution of eigenproblems) and the computational complexity of a predictor using this approximation is O(m2n)."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70345806"
                        ],
                        "name": "George Eastman House",
                        "slug": "George-Eastman-House",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "House",
                            "middleNames": [
                                "Eastman"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George Eastman House"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117676260"
                        ],
                        "name": "Guildhall StreetCambridge",
                        "slug": "Guildhall-StreetCambridge",
                        "structuredName": {
                            "firstName": "Guildhall",
                            "lastName": "StreetCambridge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guildhall StreetCambridge"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 362,
                                "start": 151
                            }
                        ],
                        "text": "There are many strategies for specific problems which try to reduce this computational complexity by searching for a sparse representation of the data (Williams and Seeger, 2001; Smola and Bartlett, 2001; Fine and Scheinberg, 2001; Lee and Mangasarian, 2001; Lawrence et al., 2003; Csato and Opper, 2002; Tresp, 2000; Tipping, 2001; Snelson and Ghahramani, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 217295,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d68725804eadecf83d707d89e12c5132bf376187",
            "isKey": false,
            "numCitedBy": 4410,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Sparse-Bayesian-Learning-and-the-Relevance-Vector-House-StreetCambridge",
            "title": {
                "fragments": [],
                "text": "Sparse Bayesian Learning and the Relevance Vector Machine"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kernel Smoothing. Chapman and Hall"
            },
            "venue": {
                "fragments": [],
                "text": "Kernel Smoothing. Chapman and Hall"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 362,
                                "start": 151
                            }
                        ],
                        "text": "There are many strategies for specific problems which try to reduce this computational complexity by searching for a sparse representation of the data (Williams and Seeger, 2001; Smola and Bartlett, 2001; Fine and Scheinberg, 2001; Lee and Mangasarian, 2001; Lawrence et al., 2003; Csato and Opper, 2002; Tresp, 2000; Tipping, 2001; Snelson and Ghahramani, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sparse greedy Gaussian process regression. In Advances in Neural Information Processing Systems, page 619625"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 54
                            }
                        ],
                        "text": "Howe ver, a promising new method recently proposed by Hinton et al. [2006] is causing a resurgence of interest in the subject."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 672,
                                "start": 54
                            }
                        ],
                        "text": "Howe ver, a promising new method recently proposed by Hinton et al. [2006] is causing a resurgence of interest in the subject. A common explanation for the difficulty of deep network learn ing is the presence of local minima or plateaus in the loss function. Gradient-b ased optimization methods that start from random initial conditions appear to ofte n g t trapped in poor local minima or plateaus. The problem seems particularly dire for nar ow networks (with few hidden units or with a bottleneck) and for networks with m any symmetries (i.e., fully-connected networks in which hidden units are exchang eable). The solution recently introduced by Hinton et al. [2006] for training deep l ayered networks is based on a greedy, layer-wise unsupervised learning phase ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A fast l"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 85
                            }
                        ],
                        "text": "The fast multipole method has been called one of the ten most significant algorithms (Dongarra and Sullivan, 2000) in scientific computation discovered in the 20th century, and won its inventors, Vladimir Rokhlin and Leslie Greengard, the 2001 Steele prize."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The top ten algorithms of the century"
            },
            "venue": {
                "fragments": [],
                "text": "Computing in Science and Engineering,"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The C++ code with MATLAB bindings for the dual-tree algorithms can be downloaded from the website http"
            },
            "venue": {
                "fragments": [],
                "text": "The C++ code with MATLAB bindings for the dual-tree algorithms can be downloaded from the website http"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The C++ code with MATLAB bindings for both the FGT and the IFGT implementation are available at http"
            },
            "venue": {
                "fragments": [],
                "text": "The C++ code with MATLAB bindings for both the FGT and the IFGT implementation are available at http"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": ", and Larry Davis . Improved fast Gauss transform and efficient kernel density estimation"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE International Conference on Computer Vision"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Approximation algorithms for NPhard problems, chapter Approximation algorithms for geometric problems"
            },
            "venue": {
                "fragments": [],
                "text": "Approximation algorithms for NPhard problems, chapter Approximation algorithms for geometric problems"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fortran code for the FGT is available at http://math.berkeley.edu/ ~ strain/Codes/index.html"
            },
            "venue": {
                "fragments": [],
                "text": "Fortran code for the FGT is available at http://math.berkeley.edu/ ~ strain/Codes/index.html"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 45
                            }
                        ],
                        "text": "The k-center problem is known to be NP -hard (Bern and Eppstein, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Approximation algorithms for NPhard problems, chapter Approximation algorithms for geometric problems, pages 296\u2013345"
            },
            "venue": {
                "fragments": [],
                "text": "PWS Publishing Company, Boston,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "MATLAB code for fast kernel density estimation based on N -body algorithms is also available at http"
            },
            "venue": {
                "fragments": [],
                "text": "MATLAB code for fast kernel density estimation based on N -body algorithms is also available at http"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1362,
                                "start": 135
                            }
                        ],
                        "text": "[2005] we present similar results that apply to unsupervised learning algorithms such as non-parametric manifold learning al gorithms [Roweis and Saul, 2000, Tenenbaum et al., 2000, Sch\u00f6lkopf et al., 1998, Belki n and Niyogi, 2003]. We find that when the underlying manifold varies a lot in the sens e of having high curvature in many places, then a large number of examples is requir ed. Note that the tangent plane of the manifold is defined by the derivatives of the kern el machine functionf , for such algorithms. The core result is that the manifold tangen t plane atx is dominated by terms associated with the near neighbors of x in the training set (more precisely it is constrained to be in the span of the vectors x \u2212 xi, with xi a neighbor ofx). This idea is illustrated in figure 3. In the case of graph-based manifol d learning algorithms such as LLE and Isomap, the domination of near examples is perfect (i. ., the derivative is strictly in the span of the difference vectors with the neigh bors), because the kernel implicit in these algorithms takes value 0 for the non-neighbo rs. With such local manifold learning algorithms, one needs to cover the manifold with sm all enough linear patches with at leastd+1 examples per patch (where d is the dimension of the manifold). This argument was previously introduced in Bengio and Monperrus [2005] to describe the limitations of neighborhood-based manifold learning algo rithms."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nonlinear dimensionality red"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 362,
                                "start": 151
                            }
                        ],
                        "text": "There are many strategies for specific problems which try to reduce this computational complexity by searching for a sparse representation of the data (Williams and Seeger, 2001; Smola and Bartlett, 2001; Fine and Scheinberg, 2001; Lee and Mangasarian, 2001; Lawrence et al., 2003; Csato and Opper, 2002; Tresp, 2000; Tipping, 2001; Snelson and Ghahramani, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems 15, chapter Fast Sparse Gaussian Process methods: The Informative Vector Machine, pages 625\u2013632"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tipping . Sparse Bayesian learning and the relevance vector machine"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of machine learning research"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A brief survey 1"
            },
            "venue": {
                "fragments": [],
                "text": "A brief survey 1"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 38,
            "methodology": 24,
            "result": 9
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 95,
        "totalPages": 10
    },
    "page_url": "https://www.semanticscholar.org/paper/Large-scale-kernel-machines-Bottou-Chapelle/63f27307cb028f9341544fff32eceb2c3c652bf2?sort=total-citations"
}