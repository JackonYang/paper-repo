{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39092098"
                        ],
                        "name": "Y. Wu",
                        "slug": "Y.-Wu",
                        "structuredName": {
                            "firstName": "Ying",
                            "lastName": "Wu",
                            "middleNames": [
                                "Nian"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117481816"
                        ],
                        "name": "D. Mumford",
                        "slug": "D.-Mumford",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mumford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mumford"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 68
                            }
                        ],
                        "text": "Thus, if model p(I) reproduces f (\u03b1)(z) for all \u03b1, then p(I) = f (I) (Zhu et al., 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 40
                            }
                        ],
                        "text": "For more discussion on this aspect, see Zhu et al. (1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 61
                            }
                        ],
                        "text": "For detailed analysis of the texture modeling algorithm, see Zhu et al. (1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 153
                            }
                        ],
                        "text": "The computation of the model is complicated by the nature of such isolated peaks, and we proposed an annealing approach for computing 3 (for details see Zhu et al.,\n1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 54
                            }
                        ],
                        "text": "(For a discussion of previous models and methods, see Zhu et al., 1996.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 77
                            }
                        ],
                        "text": "Detailed comparison between the FRAME model and the MRF models is covered in Zhu et al. (1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 93
                            }
                        ],
                        "text": "Notice that the original\n3 The synthetic fur texture in these figures is better than that in Zhu et al. (1996) since the L1 criterion used here for filter pursuit has been replaced by the criterion of equation 3.7.\nobserved texture image is not homogeneous, since the shapes of the blobs vary\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 28032474,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d74786e0cd90e85bdea33c709c39f45cb342d2f6",
            "isKey": false,
            "numCitedBy": 120,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, a minimax entropy principle is studied, based on which a novel theory, called FRAME (Filters, Random fields And Minimax Entropy) is proposed for texture modeling. FRAME combines attractive aspects of two important themes in texture modeling: multi-channel filtering and Markov random field (MRF) modeling. It incorporates the responses of a set of well selected filters into the distribution over a random field and hence has a much stronger descriptive ability than the traditional MRF models. Furthermore, it interprets and clarifies many previous concepts and methods for texture analysis and synthesis from a unified point of view. Algorithms are proposed for probability inference, stochastic simulation and filter selection. Experiments on a variety of textures are described to illustrate our theory and to show the performance of our algorithms. These experiments demonstrate that many textures previously considered as different categories can be modeled and synthesized in a common framework."
            },
            "slug": "FRAME:-filters,-random-fields,-and-minimax-entropy-Zhu-Wu",
            "title": {
                "fragments": [],
                "text": "FRAME: filters, random fields, and minimax entropy towards a unified theory for texture modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "These experiments demonstrate that many textures previously considered as different categories can be modeled and synthesized in a common framework, and interprets and clarifies many previous concepts and methods for texture analysis and synthesis from a unified point of view."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings CVPR IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1869519"
                        ],
                        "name": "G. R. Cross",
                        "slug": "G.-R.-Cross",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Cross",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. R. Cross"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 276
                            }
                        ],
                        "text": "\u2026adopts some parametric Markov random field (MRF) models in the forms of Gibbs distributions\u2014for example, the general smoothness models in image restoration (Geman & Geman, 1984; Mumford & Shah, 1989) and the conditional autoregression models in texture modeling (Besag, 1973; Cross & Jain, 1983)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The first method adopts some parametric Markov random field (MRF) models in the forms of Gibbs distributions\u2014for example, the general smoothness models in image restoration (Geman & Geman, 1984; Mumford & Shah, 1989) and the conditional autoregression models in texture modeling (Besag, 1973;  Cross & Jain, 1983 )."
                    },
                    "intents": []
                }
            ],
            "corpusId": 19038308,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7862fc4099b31f0a21fcf681403c2e594c2dd5bc",
            "isKey": false,
            "numCitedBy": 1501,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider a texture to be a stochastic, possibly periodic, two-dimensional image field. A texture model is a mathematical procedure capable of producing and describing a textured image. We explore the use of Markov random fields as texture models. The binomial model, where each point in the texture has a binomial distribution with parameter controlled by its neighbors and ``number of tries'' equal to the number of gray levels, was taken to be the basic model for the analysis. A method of generating samples from the binomial model is given, followed by a theoretical and practical analysis of the method's convergence. Examples show how the parameters of the Markov random field control the strength and direction of the clustering in the image. The power of the binomial model to produce blurry, sharp, line-like, and blob-like textures is demonstrated. Natural texture samples were digitized and their parameters were estimated under the Markov random field model. A hypothesis test was used for an objective assessment of goodness-of-fit under the Markov random field model. Overall, microtextures fit the model well. The estimated parameters of the natural textures were used as input to the generation procedure. The synthetic microtextures closely resembled their real counterparts, while the regular and inhomogeneous textures did not."
            },
            "slug": "Markov-Random-Field-Texture-Models-Cross-Jain",
            "title": {
                "fragments": [],
                "text": "Markov Random Field Texture Models"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The power of the binomial model to produce blurry, sharp, line-like, and blob-like textures is demonstrated and the synthetic microtextures closely resembled their real counterparts, while the regular and inhomogeneous textures did not."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117481816"
                        ],
                        "name": "D. Mumford",
                        "slug": "D.-Mumford",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mumford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mumford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118654416,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4099a4a1c14f03cfe266f4946c797f428646eb20",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Presented in this thesis are the statistical theories and computational schemes for three fundamental problems in computational vision: image segmentation, texture modeling, and two dimensional object recognition. Though these problems can be studied separately, they, like many other vision problems, depend on each other, therefore if our objective is to build a consistent and sophisticated vision system, the solutions to these problems should obey a common theory. The first chapter of this thesis proposes a general unified theory and a computational framework for solving vision problems. Lying at the core of this unified theory is a pyramidal description scheme, where various visual concepts are described by random variables, continuous or discrete, in a hierarchic structure. Visual computation is then posed as a statistical inference problem according to the Bayesian theory. It suggests that given the observed images, we should be able to infer the random variables of various levels all together. This computational scheme automatically incorporates concepts like the multiple intermediate solutions and the top-down/bottom-up loop. Guided by this unified theory, an algorithm for image segmentation, called region competition, is proposed in chapter 2, This algorithm is derived by minimizing a generalized Bayes/MDL criterion using the variational principle. It combines the most attractive features of the existing algorithms such as snakes/balloons and region growing, and is also related to edge detection using filters. Hence addresses the existing 4 kinds of approaches to image segmentation as different aspect of the same problem. Then in chapter 3, a unified theory called FRAME (filters, random fields and maximum entropy) is proposed for texture modeling, and it combines filtering theory and random field modeling through the maximum entropy principle. It interprets many previous concepts and methods for texture analysis and synthesis from a unified point of view. A novel strategy for filter pursuit and probability approximation is also proposed in chapter 3. Chapter 4 reports a flexible object recognition and modeling system (FORMS) which represents and recognizes objects from their contours. This consists of a forward model for generating the shapes of animate objects which gives a formalism for solving the inverse problem-object recognition in a topdown/bottom-up loop. In FORMS, the forward model includes principal component analysis for shape deformation and stochastic grammar for shape generation. The inverse process includes a novel method for skeleton extraction and part segmentation."
            },
            "slug": "Statistical-and-computational-theories-for-image-Mumford-Zhu",
            "title": {
                "fragments": [],
                "text": "Statistical and computational theories for image segmentation, texture modeling and object recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A unified theory called FRAME (filters, random fields and maximum entropy) is proposed for texture modeling, and it combines filtering theory and random field modeling through the maximum entropy principle."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117481816"
                        ],
                        "name": "D. Mumford",
                        "slug": "D.-Mumford",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mumford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mumford"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 200
                            }
                        ],
                        "text": "As an illustration of the nongaussian property, Figure 1a shows the empirical marginal distribution (or histogram) of the intensity differences of horizontally adjacent pixels of some natural images (Zhu & Mumford, 1997)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 12762065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2e4d819a5934ef018d0894ad530d959f565d357",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel theory for learning generic prior models from a set of observed natural images based on a minimax entropy theory that the authors studied in modeling textures. We start by studying the statistics of natural images including the scale invariant properties, then generic prior models were learnt to duplicate the observed statistics. The learned Gibbs distributions confirm and improve the forms of existing prior models. More interestingly inverted potentials are found to be necessary, and such potentials form patterns and enhance preferred image features. The learned model is compared with existing prior models in experiments of image restoration."
            },
            "slug": "Learning-generic-prior-models-for-visual-Zhu-Mumford",
            "title": {
                "fragments": [],
                "text": "Learning generic prior models for visual computation"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A novel theory for learning generic prior models from a set of observed natural images based on a minimax entropy theory is presented, and inverted potentials are found to be necessary, and such potentials form patterns and enhance preferred image features."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707642"
                        ],
                        "name": "D. Geman",
                        "slug": "D.-Geman",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 120
                            }
                        ],
                        "text": "For a fixed 3, we synthesize some typical images\n{Isyni , i = 1, . . . ,M\u2032} by sampling p(I;3, S)with the Gibbs sampler (Geman & Geman, 1984) or other Markov chain Monte Carlo (MCMC) methods (Winkler, 1995), and approximate Ep(I;3,S)[\u03c6(\u03b1)(I)] by the sample means; that is,\nEp(I;3,S)[\u03c6(\u03b1)(I)] \u2248\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 157
                            }
                        ],
                        "text": "\u2026adopts some parametric Markov random field (MRF) models in the forms of Gibbs distributions\u2014for example, the general smoothness models in image restoration (Geman & Geman, 1984; Mumford & Shah, 1989) and the conditional autoregression models in texture modeling (Besag, 1973; Cross & Jain, 1983)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The first method adopts some parametric Markov random field (MRF) models in the forms of Gibbs distributions\u2014for example, the general smoothness models in image restoration ( Geman & Geman, 1984;  Mumford & Shah, 1989) and the conditional autoregression models in texture modeling (Besag, 1973; Cross & Jain, 1983).,Second, the Monte Carlo Markov chain for model estimation and texture sampling is guaranteed to converge to a stationary process that follows the estimated distribution p.II3; S/ ( Geman & Geman, 1984 ), and the observed histograms can be matched closely."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 180
                            }
                        ],
                        "text": "Second, the Monte Carlo Markov chain for model estimation and texture sampling is guaranteed to converge to a stationary process that follows the estimated distribution p(I;3, S) (Geman & Geman, 1984), and the observed histograms can be matched closely."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5837272,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "459b30a9a960080f3b313e41886b1aa0e51e882c",
            "isKey": true,
            "numCitedBy": 18704,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios."
            },
            "slug": "Stochastic-Relaxation,-Gibbs-Distributions,-and-the-Geman-Geman",
            "title": {
                "fragments": [],
                "text": "Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "The analogy between images and statistical mechanics systems is made and the analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations, creating a highly parallel ``relaxation'' algorithm for MAP estimation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48377731"
                        ],
                        "name": "G. Winkler",
                        "slug": "G.-Winkler",
                        "structuredName": {
                            "firstName": "Gerhard",
                            "lastName": "Winkler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Winkler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "i = 1, , ATI by sampling p(I; A, S) with the Gibbs sampler (Geman & Geman, 1984) or other Markov chain Monte Carlo (MCMC) methods (Winkler, 1995), and approximate Ep(i A."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 150
                            }
                        ],
                        "text": "\u2026images\n{Isyni , i = 1, . . . ,M\u2032} by sampling p(I;3, S)with the Gibbs sampler (Geman & Geman, 1984) or other Markov chain Monte Carlo (MCMC) methods (Winkler, 1995), and approximate Ep(I;3,S)[\u03c6(\u03b1)(I)] by the sample means; that is,\nEp(I;3,S)[\u03c6(\u03b1)(I)] \u2248 \u00b5(\u03b1)obs(3) = 1 M\u2032 M\u2032\u2211 i=1 \u03c6(\u03b1)(Isyni ), \u03b1 = 1,\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 45306254,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "769a3188b73fdd9c5ea10970989827bd6d5c769d",
            "isKey": false,
            "numCitedBy": 697,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The book is mainly concerned with the mathematical foundations of Bayesian image analysis and its algorithms. This amounts to the study of Markov random fields and dynamic Monte Carlo algorithms like sampling, simulated annealing and stochastic gradient algorithms. The approach is introductory and elementary: given basic concepts from linear algebra and real analysis it is self-contained. No previous knowledge from image analysis is required. Knowledge of elementary probability theory and statistics is certainly beneficial but not absolutely necessary. The necessary background from imaging is sketched and illustrated by a number of concrete applications like restoration, texture segmentation and motion analysis."
            },
            "slug": "Image-analysis,-random-fields-and-dynamic-Monte-a-Winkler",
            "title": {
                "fragments": [],
                "text": "Image analysis, random fields and dynamic Monte Carlo methods: a mathematical introduction"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "The book is mainly concerned with the mathematical foundations of Bayesian image analysis and its algorithms, which amounts to the study of Markov random fields and dynamic Monte Carlo algorithms like sampling, simulated annealing and stochastic gradient algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Applications of mathematics"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780112"
                        ],
                        "name": "R. Coifman",
                        "slug": "R.-Coifman",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Coifman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Coifman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709398"
                        ],
                        "name": "M. Wickerhauser",
                        "slug": "M.-Wickerhauser",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Wickerhauser",
                            "middleNames": [
                                "Victor"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wickerhauser"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 222
                            }
                        ],
                        "text": "Examples of filters include the frequency and orientation selective Gabor filters (Daugman, 1985) and some wavelet pyramids based on various coding criteria (Mallat, 1989;\n(a)\nSimoncelli, Freeman, Adelson, & Weeger, 1992; Coifman & Wickerhauser, 1992; Donoho & Johnstone, 1994)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 546882,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5478a91c183c3a460bd4098acb8927bfc671367c",
            "isKey": false,
            "numCitedBy": 3339,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Adapted waveform analysis uses a library of orthonormal bases and an efficiency functional to match a basis to a given signal or family of signals. It permits efficient compression of a variety of signals, such as sound and images. The predefined libraries of modulated waveforms include orthogonal wavelet-packets and localized trigonometric functions, and have reasonably well-controlled time-frequency localization properties. The idea is to build out of the library functions an orthonormal basis relative to which the given signal or collection of signals has the lowest information cost. The method relies heavily on the remarkable orthogonality properties of the new libraries: all expansions in a given library conserve energy and are thus comparable. Several cost functionals are useful; one of the most attractive is Shannon entropy, which has a geometric interpretation in this context. >"
            },
            "slug": "Entropy-based-algorithms-for-best-basis-selection-Coifman-Wickerhauser",
            "title": {
                "fragments": [],
                "text": "Entropy-based algorithms for best basis selection"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "Adapted waveform analysis uses a library of orthonormal bases and an efficiency functional to match a basis to a given signal or family of signals, and relies heavily on the remarkable orthogonality properties of the new libraries."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1828659"
                        ],
                        "name": "A. Watson",
                        "slug": "A.-Watson",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Watson",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Watson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This involves measuring the efficiency of coding schemes in terms of entropy ( Watson, 1987;  Barlow, Kaushal, & Mitchison, 1989), where the computation of the entropy and thus the choice of the optimal coding schemes depend on the estimation of f.I/.,These histograms are used for pattern classification, recognition, and visual coding ( Watson, 1987;  Donoho & Johnstone, 1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 78
                            }
                        ],
                        "text": "This involves measuring the efficiency of coding schemes in terms of entropy (Watson, 1987; Barlow, Kaushal, & Mitchison, 1989), where the computation of the entropy and thus the choice of the optimal coding schemes depend on the estimation of f (I)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 86
                            }
                        ],
                        "text": "These histograms are used for pattern classification, recognition, and visual coding (Watson, 1987; Donoho & Johnstone, 1994)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 39719867,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bfbc502970488202325b1bc01b6b23f70a3ace7f",
            "isKey": false,
            "numCitedBy": 249,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Hypothetical schemes for neural representation of visual information can be expressed as explicit image codes. We may test whether a given code is sufficient, in the sense of retaining all the information that the human perceives, and necessary, in the sense of retaining only that information. The latter is a test of efficiency. Here, we explore a code modeled on the simple cells of the primate striate cortex. The Cortex transform maps a digital image into a set of subimages (layers) that are bandpass in spatial frequency and orientation. The layers are sampled so as to minimize the number of samples and still avoid aliasing. Samples are quantized in a manner that exploits the bandpass contrast-masking properties of human vision. The entropy of the samples is computed to provide a lower bound on the code size. Finally, the image is reconstructed from the code. We devise psychophysical methods for comparing the original and reconstructed images to evaluate the sufficiency of the code. When each resolution is coded at the threshold for detection artifacts, the image-code size is about 1 bit/pixel."
            },
            "slug": "Efficiency-of-a-model-human-image-code.-Watson",
            "title": {
                "fragments": [],
                "text": "Efficiency of a model human image code."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A code modeled on the simple cells of the primate striate cortex is explored, which maps a digital image into a set of subimages (layers) that are bandpass in spatial frequency and orientation and which is reconstructed from the code."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Optical Society of America. A, Optics and image science"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 170
                            }
                        ],
                        "text": "As a comparison, the gaussian distribution with the same mean and variance is plotted as a dashed curve in Figure 1a. Similar nongaussian properties are also observed in Field (1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 76
                            }
                        ],
                        "text": "For example, two kinds of coding schemes are compared in the recent work of Field (1994): the compact coding and the sparse coding."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1650980,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff1152582155acaa0e9d0ccbc900a4641504256d",
            "isKey": false,
            "numCitedBy": 1344,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "A number of recent attempts have been made to describe early sensory coding in terms of a general information processing strategy. In this paper, two strategies are contrasted. Both strategies take advantage of the redundancy in the environment to produce more effective representations. The first is described as a compact coding scheme. A compact code performs a transform that allows the input to be represented with a reduced number of vectors (cells) with minimal RMS error. This approach has recently become popular in the neural network literature and is related to a process called Principal Components Analysis (PCA). A number of recent papers have suggested that the optimal compact code for representing natural scenes will have units with receptive field profiles much like those found in the retina and primary visual cortex. However, in this paper, it is proposed that compact coding schemes are insufficient to account for the receptive field properties of cells in the mammalian visual pathway. In contrast, it is proposed that the visual system is near to optimal in representing natural scenes only if optimality is defined in terms of sparse distributed coding. In a sparse distributed code, all cells in the code have an equal response probability across the class of images but have a low response probability for any single image. In such a code, the dimensionality is not reduced. Rather, the redundancy of the input is transformed into the redundancy of the firing pattern of cells. It is proposed that the signature for a sparse code is found in the fourth moment of the response distribution (i.e., the kurtosis). In measurements with 55 calibrated natural scenes, the kurtosis was found to peak when the bandwidths of the visual code matched those of cells in the mammalian visual cortex. Codes resembling wavelet transforms are proposed to be effective because the response histograms of such codes are sparse (i.e., show high kurtosis) when presented with natural scenes. It is proposed that the structure of the image that allows sparse coding is found in the phase spectrum of the image. It is suggested that natural scenes, to a first approximation, can be considered as a sum of self-similar local functions (the inverse of a wavelet). Possible reasons for why sensory systems would evolve toward sparse coding are presented."
            },
            "slug": "What-Is-the-Goal-of-Sensory-Coding-Field",
            "title": {
                "fragments": [],
                "text": "What Is the Goal of Sensory Coding?"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is proposed that compact coding schemes are insufficient to account for the receptive field properties of cells in the mammalian visual pathway and suggested that natural scenes, to a first approximation, can be considered as a sum of self-similar local functions (the inverse of a wavelet)."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117481816"
                        ],
                        "name": "D. Mumford",
                        "slug": "D.-Mumford",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mumford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mumford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "80271822"
                        ],
                        "name": "J. Shah",
                        "slug": "J.-Shah",
                        "structuredName": {
                            "firstName": "Jayant",
                            "lastName": "Shah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shah"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 178
                            }
                        ],
                        "text": "\u2026adopts some parametric Markov random field (MRF) models in the forms of Gibbs distributions\u2014for example, the general smoothness models in image restoration (Geman & Geman, 1984; Mumford & Shah, 1989) and the conditional autoregression models in texture modeling (Besag, 1973; Cross & Jain, 1983)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 222243846,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a1067e837c7cbbefb30f8324c1eacb1afde39fc6",
            "isKey": false,
            "numCitedBy": 5156,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : This reprint will introduce and study the most basic properties of three new variational problems which are suggested by applications to computer vision. In computer vision, a fundamental problem is to appropriately decompose the domain R of a function g (x,y) of two variables. This problem starts by describing the physical situation which produces images: assume that a three-dimensional world is observed by an eye or camera from some point P and that g1(rho) represents the intensity of the light in this world approaching the point sub 1 from a direction rho. If one has a lens at P focusing this light on a retina or a film-in both cases a plane domain R in which we may introduce coordinates x, y then let g(x,y) be the strength of the light signal striking R at a point with coordinates (x,y); g(x,y) is essentially the same as sub 1 (rho) -possibly after a simple transformation given by the geometry of the imaging syste. The function g(x,y) defined on the plane domain R will be called an image. What sort of function is g? The light reflected off the surfaces Si of various solid objects O sub i visible from P will strike the domain R in various open subsets R sub i. When one object O1 is partially in front of another object O2 as seen from P, but some of object O2 appears as the background to the sides of O1, then the open sets R1 and R2 will have a common boundary (the 'edge' of object O1 in the image defined on R) and one usually expects the image g(x,y) to be discontinuous along this boundary. (JHD)"
            },
            "slug": "Optimal-approximations-by-piecewise-smooth-and-Mumford-Shah",
            "title": {
                "fragments": [],
                "text": "Optimal approximations by piecewise smooth functions and associated variational problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2360881"
                        ],
                        "name": "D. Heeger",
                        "slug": "D.-Heeger",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Heeger",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Heeger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "116003860"
                        ],
                        "name": "J. Bergen",
                        "slug": "J.-Bergen",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bergen",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bergen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 98
                            }
                        ],
                        "text": "Our method for texture modeling was inspired by and bears some similarities to the recent work by Heeger and Bergen (1995) on texture synthesis, where many natural-looking texture images are successfully synthesized by matching the histograms of filter responses organized in the form of a pyramid."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Our method for texture modeling was inspired by and bears some similarities to the recent work by  Heeger and Bergen (1995)  on texture synthesis, where many natural-looking texture images are successfully synthesized by matching the histograms of filter responses organized in the form of a pyramid."
                    },
                    "intents": []
                }
            ],
            "corpusId": 53246434,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6cbfaac6a50abe961f7150a02a8fce691d0218f",
            "isKey": false,
            "numCitedBy": 747,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a method for synthesizing images that match the texture appearanceof a given digitized sample. This synthesis is completely automatic and requires only the \u201ctarget\u201d texture as input. It allows generation of as much texture as desired so that any object can be covered. It can be used to produce solid textures for creating textured 3-d objects without the distortions inherent in texture mapping. It can also be used to synthesize texture mixtures, images that look a bit like each of several digitized samples. The approach is based on a model of human texture perception, and has potential to be a practically useful tool for graphics applications."
            },
            "slug": "Pyramid-based-texture-analysis/synthesis-Heeger-Bergen",
            "title": {
                "fragments": [],
                "text": "Pyramid-based texture analysis/synthesis"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This paper describes a method for synthesizing images that match the texture appearance of a given digitized sample, based on a model of human texture perception, and has potential to be a practically useful tool for graphics applications."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380991"
                        ],
                        "name": "Song-Chun Zhu",
                        "slug": "Song-Chun-Zhu",
                        "structuredName": {
                            "firstName": "Song-Chun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song-Chun Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117481816"
                        ],
                        "name": "D. Mumford",
                        "slug": "D.-Mumford",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mumford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mumford"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 200
                            }
                        ],
                        "text": "As an illustration of the nongaussian property, Figure 1a shows the empirical marginal distribution (or histogram) of the intensity differences of horizontally adjacent pixels of some natural images (Zhu & Mumford, 1997)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7423086,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "adc69aebcee4af29e115f54e3a5f210c5cc7dadc",
            "isKey": false,
            "numCitedBy": 408,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "This article addresses two important themes in early visual computation: it presents a novel theory for learning the universal statistics of natural images, and, it proposes a general framework of designing reaction-diffusion equations for image processing. We studied the statistics of natural images including the scale invariant properties, then generic prior models were learned to duplicate the observed statistics, based on minimax entropy theory. The resulting Gibbs distributions have potentials of the form U(I; /spl Lambda/, S)=/spl Sigma//sub /spl alpha/=1//sup k//spl Sigma//sub x,y//spl lambda//sup (/spl alpha/)/((F/sup (/spl alpha/)/*I)(x,y)) with S={F/sup (1)/, F/sup (2)/,...,F/sup (K)/} being a set of filters and /spl Lambda/={/spl lambda//sup (1)/(),/spl lambda//sup (2)/(),...,/spl lambda//sup (K)/()} the potential functions. The learned Gibbs distributions confirm and improve the form of existing prior models such as line-process, but, in contrast to all previous models, inverted potentials were found to be necessary. We find that the partial differential equations given by gradient descent on U(I; /spl Lambda/, S) are essentially reaction-diffusion equations, where the usual energy terms produce anisotropic diffusion, while the inverted energy terms produce reaction associated with pattern formation, enhancing preferred image features. We illustrate how these models can be used for texture pattern rendering, denoising, image enhancement, and clutter removal by careful choice of both prior and data models of this type, incorporating the appropriate features."
            },
            "slug": "Prior-Learning-and-Gibbs-Reaction-Diffusion-Zhu-Mumford",
            "title": {
                "fragments": [],
                "text": "Prior Learning and Gibbs Reaction-Diffusion"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is found that the partial differential equations given by gradient descent on U(I; /spl Lambda/, S) are essentially reaction-diffusion equations, where the usualEnergy terms produce anisotropic diffusion, while the inverted energy terms produce reaction associated with pattern formation, enhancing preferred image features."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781325"
                        ],
                        "name": "J. Daugman",
                        "slug": "J.-Daugman",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Daugman",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Daugman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 83
                            }
                        ],
                        "text": "Examples of filters include the frequency and orientation selective Gabor filters (Daugman, 1985) and some wavelet pyramids based on various coding criteria (Mallat, 1989;\n(a)\nSimoncelli, Freeman, Adelson, & Weeger, 1992; Coifman & Wickerhauser, 1992; Donoho & Johnstone, 1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Examples of filters include the frequency and orientation selective Gabor filters (Daugman, 1985) and some wavelet pyramids based on various coding criteria (Mallat, 1989;"
                    },
                    "intents": []
                }
            ],
            "corpusId": 9271650,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "02f89cd1fd6f013a1a301a292936ff8fb06aff25",
            "isKey": false,
            "numCitedBy": 3420,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Two-dimensional spatial linear filters are constrained by general uncertainty relations that limit their attainable information resolution for orientation, spatial frequency, and two-dimensional (2D) spatial position. The theoretical lower limit for the joint entropy, or uncertainty, of these variables is achieved by an optimal 2D filter family whose spatial weighting functions are generated by exponentiated bivariate second-order polynomials with complex coefficients, the elliptic generalization of the one-dimensional elementary functions proposed in Gabor's famous theory of communication [J. Inst. Electr. Eng. 93, 429 (1946)]. The set includes filters with various orientation bandwidths, spatial-frequency bandwidths, and spatial dimensions, favoring the extraction of various kinds of information from an image. Each such filter occupies an irreducible quantal volume (corresponding to an independent datum) in a four-dimensional information hyperspace whose axes are interpretable as 2D visual space, orientation, and spatial frequency, and thus such a filter set could subserve an optimally efficient sampling of these variables. Evidence is presented that the 2D receptive-field profiles of simple cells in mammalian visual cortex are well described by members of this optimal 2D filter family, and thus such visual neurons could be said to optimize the general uncertainty relations for joint 2D-spatial-2D-spectral information resolution. The variety of their receptive-field dimensions and orientation and spatial-frequency bandwidths, and the correlations among these, reveal several underlying constraints, particularly in width/length aspect ratio and principal axis organization, suggesting a polar division of labor in occupying the quantal volumes of information hyperspace.(ABSTRACT TRUNCATED AT 250 WORDS)"
            },
            "slug": "Uncertainty-relation-for-resolution-in-space,-and-Daugman",
            "title": {
                "fragments": [],
                "text": "Uncertainty relation for resolution in space, spatial frequency, and orientation optimized by two-dimensional visual cortical filters."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Evidence is presented that the 2D receptive-field profiles of simple cells in mammalian visual cortex are well described by members of this optimal 2D filter family, and thus such visual neurons could be said to optimize the general uncertainty relations for joint 2D-spatial-2D-spectral information resolution."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Optical Society of America. A, Optics and image science"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832423"
                        ],
                        "name": "A. Karni",
                        "slug": "A.-Karni",
                        "structuredName": {
                            "firstName": "Avi",
                            "lastName": "Karni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Karni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144737366"
                        ],
                        "name": "D. Sagi",
                        "slug": "D.-Sagi",
                        "structuredName": {
                            "firstName": "Dov",
                            "lastName": "Sagi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sagi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 180
                            }
                        ],
                        "text": "Evidence that texture pairs that are not preattentively segmentable by naive subjects become segmentable after practice has been reported by many groups, most notably by Karni and Sagi (1991)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16536474,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "cda3358015f0352c556212347ff8bb6542e6668a",
            "isKey": false,
            "numCitedBy": 1036,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In terms of functional anatomy, where does learning occur when, for a basic visual discrimination task, performance improves with practice (perceptual learning)? We report remarkable long-term learning in a simple texture discrimination task where learning is specific for retinal input. This learning is (i) local (in a retinotopic sense), (ii) orientation specific but asymmetric (it is specific for background but not for target-element orientation), and (iii) strongly monocular (there is little interocular transfer of learning). Our results suggest that learning involves experience-dependent changes at a level of the visual system where monocularity and the retinotopic organization of the visual input are still retained and where different orientations are processed separately. These results can be interpreted in terms of local plasticity induced by retinal input in early visual processing in human adults, presumably at the level of orientation-gradient sensitive cells in primary visual cortex."
            },
            "slug": "Where-practice-makes-perfect-in-texture-evidence-Karni-Sagi",
            "title": {
                "fragments": [],
                "text": "Where practice makes perfect in texture discrimination: evidence for primary visual cortex plasticity."
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This work reports remarkable long-term learning in a simple texture discrimination task where learning is specific for retinal input and suggests that learning involves experience-dependent changes at a level of the visual system where monocularity and the retinotopic organization of thevisual input are still retained and where different orientations are processed separately."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719389"
                        ],
                        "name": "Rosalind W. Picard",
                        "slug": "Rosalind-W.-Picard",
                        "structuredName": {
                            "firstName": "Rosalind",
                            "lastName": "Picard",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rosalind W. Picard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 144
                            }
                        ],
                        "text": "\u2026(1) dynamic equations or replacement rules, which simulate specific physical and chemical processes to generate textures (Witkin & Kass, 1991; Picard, 1996), (2) the kth-order statistics model for texture perception, that is, the famous Julesz\u2019s conjecture (Julesz, 1962), and (3) MRF models."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14297666,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bdcd8d93fc7a659b147f95842ca5e124b676f710",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 107,
            "paperAbstract": {
                "fragments": [],
                "text": "The average person with a computer will soon have access to the world's collections of digital video and images. However, unlike text that can be alphabetized or numbers that can be ordered, image and video has no general language to aid in its organization. Tools that can \"see\" and \"understand\" the content of imagery are still in their infancy, but they are now at the point where they can provide substantial assistance to users in navigating through visual media. This paper describes new tools based on \"vision texture\" for modeling image and video. The focus of this research is the use of a society of low-level models for performing relatively high-level tasks, such as retrieval and annotation of image and video libraries. This paper surveys recent and present research in this fast-growing area."
            },
            "slug": "A-Society-of-Models-for-Video-and-Image-Libraries-Picard",
            "title": {
                "fragments": [],
                "text": "A Society of Models for Video and Image Libraries"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The focus of this research is the use of a society of low-level models for performing relatively high-level tasks, such as retrieval and annotation of image and video libraries."
            },
            "venue": {
                "fragments": [],
                "text": "IBM Syst. J."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144246983"
                        ],
                        "name": "H. Barlow",
                        "slug": "H.-Barlow",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Barlow",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Barlow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2315855"
                        ],
                        "name": "T. P. Kaushal",
                        "slug": "T.-P.-Kaushal",
                        "structuredName": {
                            "firstName": "Tej",
                            "lastName": "Kaushal",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. P. Kaushal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145666307"
                        ],
                        "name": "G. Mitchison",
                        "slug": "G.-Mitchison",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Mitchison",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Mitchison"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 28107770,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5074f2219ddac70995f0a5e81ee2e892cb884b56",
            "isKey": false,
            "numCitedBy": 203,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "To determine whether a particular sensory event is a reliable predictor of reward or punishment it is necessary to know the prior probability of that event. If the variables of a sensory representation normally occur independently of each other, then it is possible to derive the prior probability of any logical function of the variables from the prior probabilities of the individual variables, without any additional knowledge; hence such a representation enormously enlarges the scope of definable events that can be searched for reliable predictors. Finding a Minimum Entropy Code is a possible method of forming such a representation, and methods for doing this are explored in this paper. The main results are (1) to show how to find such a code when the probabilities of the input states form a geometric progression, as is shown to be nearly true for keyboard characters in normal text; (2) to show how a Minimum Entropy Code can be approximated by repeatedly recoding pairs, triples, etc. of an original 7-bit code for keyboard characters; (3) to prove that in some cases enlarging the capacity of the output channel can lower the entropy."
            },
            "slug": "Finding-Minimum-Entropy-Codes-Barlow-Kaushal",
            "title": {
                "fragments": [],
                "text": "Finding Minimum Entropy Codes"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Find a Minimum Entropy Code is a possible method of forming such a representation, and methods for doing this are explored, and the main results are to show how to find such a code when the probabilities of the input states form a geometric progression."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 186
                            }
                        ],
                        "text": ", Ep(I; ;S)[ ( )(I)] ( ) syn( ) = 1 M 0 M 0 Xi=1 ( )(Isyn i ); = 1; :::;K: (9) Therefore the iterative equation for computing becomes d ( ) dt = ( )( ) = ( ) syn( ) ( ) obs; = 1; :::;K: (10) For the accuracy of the approximation in equation (9), the sample sizeM 0 should be large enough."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 147
                            }
                        ],
                        "text": "When ( ) syn comes very close to ( ) obs, d0( ( )) become negative, which provides a criterion for stopping the iteration in computing in equation (10)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1890561,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "605402e235bd62437baf3c9ebefe77fb4d92ee95",
            "isKey": false,
            "numCitedBy": 1172,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning. One fruitful approach is to build a parameterized stochastic generative model, independent draws from which are likely to produce the patterns. For all but the simplest generative models, each pattern can be generated in exponentially many ways. It is thus intractable to adjust the parameters to maximize the probability of the observed patterns. We describe a way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations. Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways."
            },
            "slug": "The-Helmholtz-Machine-Dayan-Hinton",
            "title": {
                "fragments": [],
                "text": "The Helmholtz Machine"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations is described, viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689350"
                        ],
                        "name": "Eero P. Simoncelli",
                        "slug": "Eero-P.-Simoncelli",
                        "structuredName": {
                            "firstName": "Eero",
                            "lastName": "Simoncelli",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eero P. Simoncelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358192"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adelson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2360881"
                        ],
                        "name": "D. Heeger",
                        "slug": "D.-Heeger",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Heeger",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Heeger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 43701174,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8515604037444b3f079a9d328b0c560f33da0a19",
            "isKey": false,
            "numCitedBy": 1427,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the major drawbacks of orthogonal wavelet transforms is their lack of translation invariance: the content of wavelet subbands is unstable under translations of the input signal. Wavelet transforms are also unstable with respect to dilations of the input signal and, in two dimensions, rotations of the input signal. The authors formalize these problems by defining a type of translation invariance called shiftability. In the spatial domain, shiftability corresponds to a lack of aliasing; thus, the conditions under which the property holds are specified by the sampling theorem. Shiftability may also be applied in the context of other domains, particularly orientation and scale. Jointly shiftable transforms that are simultaneously shiftable in more than one domain are explored. Two examples of jointly shiftable transforms are designed and implemented: a 1-D transform that is jointly shiftable in position and scale, and a 2-D transform that is jointly shiftable in position and orientation. The usefulness of these image representations for scale-space analysis, stereo disparity measurement, and image enhancement is demonstrated. >"
            },
            "slug": "Shiftable-multiscale-transforms-Simoncelli-Freeman",
            "title": {
                "fragments": [],
                "text": "Shiftable multiscale transforms"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Two examples of jointly shiftable transforms that are simultaneously shiftable in more than one domain are explored and the usefulness of these image representations for scale-space analysis, stereo disparity measurement, and image enhancement is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145762806"
                        ],
                        "name": "M. Sherman",
                        "slug": "M.-Sherman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Sherman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sherman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 56
                            }
                        ],
                        "text": "In particular, we adopt the method recently proposed by Sherman (1996) for the estimation of Varf [\u03c6(\u03b2)(I)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 19
                            }
                        ],
                        "text": "Then, according to Sherman (1996), Varf [\u03c6(\u03b2)(I)] can be estimated by\nm2 N2 Var(\u03b2)obs(Dm)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 116178440,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "208a693d15c06e94bf4e4348206a87ff3b224803",
            "isKey": false,
            "numCitedBy": 106,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY A statistic s( ) is computed on spatially indexed data {X,: i e D}, where D is a finite subset of the integer lattice 22. We propose a method for estimating the variance (and other moments) of s( ) by using only the data. The set D may be irregularly shaped, the statistic s( ) may be arbitrarily complicated and no distributional assumptions (marginal or joint) are necessary. The method uses the statistic computed on overlapping 'subshapes' of D as replicates of s( ). The estimator is simply the sample variance of the (standardized) replicates. We demonstrate C2-consistency of the estimator (under mild conditions on s( ) and the strength of spatial dependence). A rate of convergence is also given, giving guidance into the appropriate choice of subshape size, and the estimator is illustrated in two data examples."
            },
            "slug": "Variance-Estimation-for-Statistics-Computed-from-Sherman",
            "title": {
                "fragments": [],
                "text": "Variance Estimation for Statistics Computed from Spatial Lattice Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746242"
                        ],
                        "name": "S. Mallat",
                        "slug": "S.-Mallat",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Mallat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mallat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 158
                            }
                        ],
                        "text": "Examples of filters include the frequency and orientation selective Gabor filters (Daugman, 1985) and some wavelet pyramids based on various coding criteria (Mallat, 1989;\n(a)\nSimoncelli, Freeman, Adelson, & Weeger, 1992; Coifman & Wickerhauser, 1992; Donoho & Johnstone, 1994)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1136690,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "fd9fb6a24c5d85f52bb44fb1e4b951bea6234df1",
            "isKey": false,
            "numCitedBy": 2572,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "A multiresolution approximation is a sequence of embedded vector spaces \uf8f1 \uf8f3 V j \uf8fc \uf8fejmember Z for approximating L 2 (R) functions. We study the properties of a multiresolution approximation and prove that it is characterized by a 2\u03c0 periodic function which is further described. From any multiresolution approximation, we can derive a function \u03c8(x) called a wavelet such that \uf8f1 \uf8f3 \u221a\uf8e5 \uf8e5 2 j \u03c8(2 j x \u2212k) \uf8fc \uf8fe (k ,j)member Z 2 is an orthonormal basis of L 2 (R). This provides a new approach for understanding and computing wavelet orthonormal bases. Finally, we characterize the asymptotic decay rate of multiresolution approximation errors for functions in a Sobolev space H s ."
            },
            "slug": "Multiresolution-approximations-and-wavelet-bases-of-Mallat",
            "title": {
                "fragments": [],
                "text": "Multiresolution approximations and wavelet orthonormal bases of L^2(R)"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "It is proved that a multiresolution approximation is characterized by a 2\u03c0 periodic function which is further described and provides a new approach for understanding and computing wavelet orthonormal bases."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117481816"
                        ],
                        "name": "D. Mumford",
                        "slug": "D.-Mumford",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mumford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mumford"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 24652416,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "f450f508a3f4ed0fa202e4f9469aaacbdb93a4ce",
            "isKey": false,
            "numCitedBy": 645,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper is a sequel to an earlier paper which proposed an active role for the thalamus, integrating multiple hypotheses formed in the cortex via the thalamo-cortical loop. In this paper, I put forward a hypothesis on the role of the reciprocal, topographic pathways between two cortical areas, one often a 'higher' area dealing with more abstract information about the world, the other 'lower', dealing with more concrete data. The higher area attempts to fit its abstractions to the data it receives from lower areas by sending back to them from its deep pyramidal cells a template reconstruction best fitting the lower level view. The lower area attempts to reconcile the reconstruction of its view that it receives from higher areas with what it knows, sending back from its superficial pyramidal cells the features in its data which are not predicted by the higher area. The whole calculation is done with all areas working simultaneously, but with order imposed by synchronous activity in the various top-down, bottom-up loops. Evidence for this theory is reviewed and experimental tests are proposed. A third part of this paper will deal with extensions of these ideas to the frontal lobe."
            },
            "slug": "On-the-computational-architecture-of-the-neocortex.-Mumford",
            "title": {
                "fragments": [],
                "text": "On the computational architecture of the neocortex. II. The role of cortico-cortical loops."
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A hypothesis on the role of the reciprocal, topographic pathways between two cortical areas, one often a 'higher' area dealing with more abstract information about the world, the other 'lower', deals with more concrete data, is put forward."
            },
            "venue": {
                "fragments": [],
                "text": "Biological cybernetics"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809905"
                        ],
                        "name": "A. Witkin",
                        "slug": "A.-Witkin",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Witkin",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Witkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143602141"
                        ],
                        "name": "M. Kass",
                        "slug": "M.-Kass",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kass"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 146
                            }
                        ],
                        "text": "\u2026into three categories: (1) dynamic equations or replacement rules, which simulate specific physical and chemical processes to generate textures (Witkin & Kass, 1991; Picard, 1996), (2) the kth-order statistics model for texture perception, that is, the famous Julesz\u2019s conjecture (Julesz,\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Existing models for textures can be roughly classified into three categories: (1) dynamic equations or replacement rules, which simulate specific physical and chemical processes to generate textures ( Witkin & Kass, 1991;  Picard, 1996), (2) the kth-order statistics model for texture perception, that is, the famous Julesz\u2019s conjecture (Julesz, 1962), and (3) MRF models."
                    },
                    "intents": []
                }
            ],
            "corpusId": 207162368,
            "fieldsOfStudy": [
                "Computer Science",
                "Materials Science"
            ],
            "id": "8b67b6b9b909b336ca0c2e62c475f363c68fbd38",
            "isKey": false,
            "numCitedBy": 387,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for texture synthesis based on the simulation of a process of local nonlinear interaction, called reaction-diffusion, which has been proposed as a model of biological pattern formation. We extend traditional reaction-diffusion systems by allowing anisotropic and spatially non-uniform diffusion, as well as multiple competing directions of diffusion. We adapt reaction-diffusion system to the needs of computer graphics by presenting a method to synthesize patterns which compensate for the effects of non-uniform surface parameterization. Finally, we develop efficient algorithms for simulating reaction-diffusion systems and display a collection of resulting textures using standard texture- and displacement-mapping techniques."
            },
            "slug": "Reaction-diffusion-textures-Witkin-Kass",
            "title": {
                "fragments": [],
                "text": "Reaction-diffusion textures"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "This work presents a method for texture synthesis based on the simulation of a process of local nonlinear interaction, called reaction-diffusion, which has been proposed as a model of biological pattern formation and adapts it to the needs of computer graphics."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2122942"
                        ],
                        "name": "B. Ripley",
                        "slug": "B.-Ripley",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Ripley",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ripley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "as kernel methods, radial basis functions (Ripley, 1996), and mixture of gaussian models (Jordan & Jacobs, 1994)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10536649,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93cb06180743fa648d844b9e7883b62468921c84",
            "isKey": false,
            "numCitedBy": 2848,
            "numCiting": 113,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nPattern recognition has long been studied in relation to many different (and mainly unrelated) applications, such as remote sensing, computer vision, space research, and medical imaging. In this book Professor Ripley brings together two crucial ideas in pattern recognition; statistical methods and machine learning via neural networks. Unifying principles are brought to the fore, and the author gives an overview of the state of the subject. Many examples are included to illustrate real problems in pattern recognition and how to overcome them.This is a self-contained account, ideal both as an introduction for non-specialists readers, and also as a handbook for the more expert reader."
            },
            "slug": "Pattern-Recognition-and-Neural-Networks-Ripley",
            "title": {
                "fragments": [],
                "text": "Pattern Recognition and Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Professor Ripley brings together two crucial ideas in pattern recognition; statistical methods and machine learning via neural networks in this self-contained account."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3326400"
                        ],
                        "name": "B. Julesz",
                        "slug": "B.-Julesz",
                        "structuredName": {
                            "firstName": "B\u00e9la",
                            "lastName": "Julesz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Julesz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Existing models for textures can be roughly classified into three categories: (1) dynamic equations or replacement rules, which simulate specific physical and chemical processes to generate textures (Witkin & Kass, 1991; Picard, 1996), (2) the kth-order statistics model for texture perception, that is, the famous Julesz\u2019s conjecture ( Julesz, 1962 ), and (3) MRF models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 258
                            }
                        ],
                        "text": "\u2026(1) dynamic equations or replacement rules, which simulate specific physical and chemical processes to generate textures (Witkin & Kass, 1991; Picard, 1996), (2) the kth-order statistics model for texture perception, that is, the famous Julesz\u2019s conjecture (Julesz, 1962), and (3) MRF models."
                    },
                    "intents": []
                }
            ],
            "corpusId": 29648250,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "4754bfb45726057d98ad47499481e8c172233e20",
            "isKey": false,
            "numCitedBy": 895,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual discrimination experiments were conducted using unfamiliar displays generated by a digital computer. The displays contained two side-by-side fields with different statistical, topological or heuristic properties. Discrimination was defined as that spontaneous visual process which gives the immediate impression of two distinct fields. The condition for such discrimination was found to be based primarily on clusters or lines formed by proximate points of uniform brightness. A similar rule of connectivity with hue replacing brightness was obtained by using varicolored dots of equal subjective brightness. The limitations in discriminating complex line structures were also investigated."
            },
            "slug": "Visual-Pattern-Discrimination-Julesz",
            "title": {
                "fragments": [],
                "text": "Visual Pattern Discrimination"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The condition for discrimination was found to be based primarily on clusters or lines formed by proximate points of uniform brightness, and a similar rule of connectivity with hue replacing brightness was obtained by using varicolored dots of equal subjective brightness."
            },
            "venue": {
                "fragments": [],
                "text": "IRE Trans. Inf. Theory"
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47660376"
                        ],
                        "name": "M. Silverman",
                        "slug": "M.-Silverman",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Silverman",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Silverman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5260610"
                        ],
                        "name": "D. Grosof",
                        "slug": "D.-Grosof",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Grosof",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Grosof"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087168416"
                        ],
                        "name": "R. D. De Valois",
                        "slug": "R.-D.-De-Valois",
                        "structuredName": {
                            "firstName": "R",
                            "lastName": "De Valois",
                            "middleNames": [
                                "L"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. D. De Valois"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4004317"
                        ],
                        "name": "S. Elfar",
                        "slug": "S.-Elfar",
                        "structuredName": {
                            "firstName": "Sylvia",
                            "lastName": "Elfar",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Elfar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 25795992,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "7bfeadeeef1a51ec7ae78c1e2484464d4470b681",
            "isKey": false,
            "numCitedBy": 121,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We measured the spatial-frequency tuning of cells at regular intervals along tangential probes through the monkey striate cortex and correlated the recording sites with the cortical cytochrome oxidase (CytOx) patterns to address three questions with regard to the cortical spatial-frequency organization. (i) Is there a periodic anatomical arrangement of cells tuned to different spatial-frequency ranges? We found there is, because the spatial-frequency tuning of cells along tangential probes changed systematically, varying from a low frequency to a middle range to high frequencies and back again repeatedly over distances of about 0.6-0.7 mm. (ii) Are there just two populations of cells, low-frequency and high-frequency units, at a given eccentricity (perhaps corresponding to the magno- and parvocellular geniculate pathways) or is there a continuum of spatial-frequency peaks? We found a continuum of peak tuning. Most cells are tuned to intermediate spatial frequencies and form a unimodal rather than a bimodal distribution of cell peaks. Furthermore, the cells with different peak frequencies were found to be continuously and smoothly distributed across a module. (iii) What is the relation between the physiological spatial-frequency organization and the regions of high CytOx concentration (\"blobs\")? We found a systematic correlation between the topographical variation in spatial-frequency tuning and the modular CytOx pattern, which also varied continuously in density. Low-frequency cells are at the center of the blobs, and cells tuned to increasingly higher spatial frequencies are at increasing radial distances."
            },
            "slug": "Spatial-frequency-organization-in-primate-striate-Silverman-Grosof",
            "title": {
                "fragments": [],
                "text": "Spatial-frequency organization in primate striate cortex."
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "There is a systematic correlation between the topographical variation in spatial-frequency tuning and the modular CytOx pattern, which varied continuously in density, and low-frequency cells are at the center of the blobs, and cells tuned to increasingly higher spatial frequencies are at increasing radial distances."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17585310"
                        ],
                        "name": "M. I. Jordan",
                        "slug": "M.-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144215175"
                        ],
                        "name": "R. Jacobs",
                        "slug": "R.-Jacobs",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Jacobs",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jacobs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 106
                            }
                        ],
                        "text": "Substituting H( )(I) for ( )(I) in equation (??), we obtain p(I; ) = 1 Z( ) expf K X =1 < ( );H( )(I) >g; (18) which we call the FRAME (Filter, Random eld, And Minimax Entropy) model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 162
                            }
                        ],
                        "text": "This make it inappropriate to use nonparametric inference methods, such\nas kernel methods, radial basis functions (Ripley, 1996), and mixture of gaussian models (Jordan & Jacobs, 1994)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 67000854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6d8a7fc2e2d53923832f9404376512068ca2a57",
            "isKey": false,
            "numCitedBy": 2136,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a tree-structured architecture for supervised learning. The statistical model underlying the architecture is a hierarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models (GLIM's). Learning is treated as a maximum likelihood problem; in particular, we present an Expectation-Maximization (EM) algorithm for adjusting the parameters of the architecture. We also develop an on-line learning algorithm in which the parameters are updated incrementally. Comparative simulation results are presented in the robot dynamics domain."
            },
            "slug": "Hierarchical-Mixtures-of-Experts-and-the-EM-Jordan-Jacobs",
            "title": {
                "fragments": [],
                "text": "Hierarchical Mixtures of Experts and the EM Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "An Expectation-Maximization (EM) algorithm for adjusting the parameters of the tree-structured architecture for supervised learning and an on-line learning algorithm in which the parameters are updated incrementally."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2204972"
                        ],
                        "name": "M. V. Rossum",
                        "slug": "M.-V.-Rossum",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Rossum",
                            "middleNames": [
                                "C.",
                                "W.",
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. V. Rossum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2281536,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "2d5af1ab6368f20a4a9bb2afae23663e5b08b9c6",
            "isKey": false,
            "numCitedBy": 1657,
            "numCiting": 99,
            "paperAbstract": {
                "fragments": [],
                "text": "Lecture Notes for the MSc/DTC module. The brain is a complex computing machine which has evolved to give the ttest output to a given input. Neural computation has as goal to describe the function of the nervous system in mathematical and computational terms. By analysing or simulating the resulting equations, one can better understand its function, research how changes in parameters would eect the function, and try to mimic the nervous system in hardware or software implementations. Neural Computation is a bit like physics, that has been successful in describing numerous physical phenomena. However, approaches developed in those elds not always work for neural computation, because: 1. Physical systems are best studied in reduced, simplied circumstances, but the nervous system is hard to study in isolation. Neurons require a narrow range of operating conditions (temperature, oxygen, presence of other neurons, ion concentrations, ...) under which they work as they should. These conditions are hard to reproduce outside the body. Secondly, the neurons form a highly interconnected network. The function of the nervous systems depends on this connectivity and interaction, by trying to isolate the components, you are likely to alter the function. 2. It is not clear how much detail one needs to describe the computations in the brain. In these lectures we shall see various description levels. 3. Neural signals and neural connectivity are hard to measure, especially, if disturbance and damage to the nervous system is to be kept minimal. Perhaps Neural Computation has more in common with trying to gure out how a complicated machine, such as a computer or car works. Knowledge of the basic physics helps, but is not sucient. Luckily there are factors which perhaps make understanding the brain easier than understanding an arbitrary complicated machine: 1. There is a high degree of conservation across species. This means that animal studies can be used to gain information about the human brain. Furthermore, study of, say, the visual system might help to understand the auditory system. 2. The nervous system is able to develop by combining on one hand a only limited amount of genetic information and, on the other hand, the input it receives. Therefore it might be possible to nd the organising principles and develop a brain from there. This would be easier than guring out the complete 'wiring diagram'. 3. The nervous system is exible and robust, neurons die everyday. This stands \u2026"
            },
            "slug": "Neural-Computation-Rossum",
            "title": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The nervous system is able to develop by combining on one hand a only limited amount of genetic information and, on the other hand, the input it receives, and it might be possible to develop a brain from there."
            },
            "venue": {
                "fragments": [],
                "text": "Artificial Intelligence"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35206065"
                        ],
                        "name": "E. Jaynes",
                        "slug": "E.-Jaynes",
                        "structuredName": {
                            "firstName": "Edwin",
                            "lastName": "Jaynes",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Jaynes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 31
                            }
                        ],
                        "text": "The Maximum Entropy Principle (Jaynes 1957)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 8
                            }
                        ],
                        "text": "2.1 The Maximum Entropy Principle."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 18
                            }
                        ],
                        "text": "The ME principle (Jaynes, 1957) suggests that we should choose p(I) that achieves the maximum entropy to obtain the purest and simplest fusion of the observed features and their statistics."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "(I). The Maximum Entropy Principle ( Jaynes 1957 ).,The ME principle ( Jaynes, 1957 ) suggests that we should choose p.I/ that achieves the maximum entropy to obtain the purest and simplest fusion of the observed features and their statistics."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17870175,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "08b67692bc037eada8d3d7ce76cc70994e7c8116",
            "isKey": true,
            "numCitedBy": 10873,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Treatment of the predictive aspect of statistical mechanics as a form of statistical inference is extended to the density-matrix formalism and applied to a discussion of the relation between irreversibility and information loss. A principle of \"statistical complementarity\" is pointed out, according to which the empirically verifiable probabilities of statistical mechanics necessarily correspond to incomplete predictions. A preliminary discussion is given of the second law of thermodynamics and of a certain class of irreversible processes, in an approximation equivalent to that of the semiclassical theory of radiation."
            },
            "slug": "Information-Theory-and-Statistical-Mechanics-Jaynes",
            "title": {
                "fragments": [],
                "text": "Information Theory and Statistical Mechanics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1957
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2364792"
                        ],
                        "name": "I. Johnstone",
                        "slug": "I.-Johnstone",
                        "structuredName": {
                            "firstName": "Iain",
                            "lastName": "Johnstone",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Johnstone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17008380,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a6ff4e21143fbb73192bfb9a474f1f15a2f5016e",
            "isKey": false,
            "numCitedBy": 373,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Nous decrivons une extension de la methode de la \u00abmeilleure base\u00bb qui permet de selectionner, a partir de donnees bruitees, une base orthonormee dans laquelle le debruitage est d'efficacite presque ideale"
            },
            "slug": "Ideal-denoising-in-an-orthonormal-basis-chosen-from-Donoho-Johnstone",
            "title": {
                "fragments": [],
                "text": "Ideal denoising in an orthonormal basis chosen from a library of bases"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2748598"
                        ],
                        "name": "S. Kullback",
                        "slug": "S.-Kullback",
                        "structuredName": {
                            "firstName": "Solomon",
                            "lastName": "Kullback",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kullback"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Equation A.1 follows a second-order Taylor expansion argument (corollary 4.4 of  Kullback, 1959, p. 48 ), where p 0 is a distribution whose expected"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 86
                            }
                        ],
                        "text": "(A.2)\nEquation A.1 follows a second-order Taylor expansion argument (corollary 4.4 of Kullback, 1959, p. 48), where p\u2032 is a distribution whose expected feature statistics are between those of p and p+, and\nVarp\u2032 [8+(I)] = (\nVarp\u2032 [8(I)] Covp\u2032 [8(I), \u03c6 (\u03b2)(I)]\nCovp\u2032 [\u03c6 (\u03b2)(I),8(I)] Varp\u2032 [\u03c6\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 86412308,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "51739712c9b795f9533131122698cd5d01699f9d",
            "isKey": true,
            "numCitedBy": 82,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "information theory and statistics. Book lovers, when you need a new book to read, find the book here. Never worry not to find what you need. Is the information theory and statistics your needed book now? That's true; you are really a good reader. This is a perfect book that comes from great author to share with you. The book offers the best experience and lesson to take, not only take, but also learn."
            },
            "slug": "Information-Theory-and-Statistics-Kullback",
            "title": {
                "fragments": [],
                "text": "Information Theory and Statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1959
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143723574"
                        ],
                        "name": "D. B. Preston",
                        "slug": "D.-B.-Preston",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Preston",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. B. Preston"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122810709,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "661903691f83b293b7175be8f62a947006964131",
            "isKey": false,
            "numCitedBy": 5190,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface. Preface to Volume 2. Contents of Volume 2. List of Main Notation. Basic Concepts. Elements of Probability Theory. Stationary Random Processes. Spectral Analysis. Estimation in the Time Domain. Estimation in the Frequency Domain. Spectral Analysis in Practice. Analysis of Processes with Mixed Spectra."
            },
            "slug": "Spectral-Analysis-and-Time-Series-Preston",
            "title": {
                "fragments": [],
                "text": "Spectral Analysis and Time Series"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3326400"
                        ],
                        "name": "B. Julesz",
                        "slug": "B.-Julesz",
                        "structuredName": {
                            "firstName": "B\u00e9la",
                            "lastName": "Julesz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Julesz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 141712670,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "a8f214c5b373302b660e2cb037c8f920aae7efb8",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Part 1 Dialogues on general topics: the enterprise of vision research the creative process - conjugacy versus scientific bilingualism the role of theories in psychobiology mathematics and human psychology linking psychology with neurophysiology - the mind-body problem without metaphysics metascientific problems maturational windows and cortical plasticity epilogue. Part 2 Dialogues on specific topics: some strategic questions about visual perception perceptual atoms - the Texton theory revisited the role of early vision in psychobiology and visual cognition a condensed history of my findings recent findings with my co-workers auditory and visual perception compared \"Foundations of Cyclopean Perception\" revisited. Appendices: interactions publications, with comments."
            },
            "slug": "Dialogues-on-perception-Julesz",
            "title": {
                "fragments": [],
                "text": "Dialogues on perception"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 34
                            }
                        ],
                        "text": "Filter Size d0( (1)) d0( (2)) d0( (3)) d0( (4)) d0( (5)) d0( (8)) 1x1 1018."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "Equation A.2 results from the fact that Ep+ [8(I)] = Ep[8(I)], and by the Schur formula, it is well known that Vp\u2032 = V22 \u2212 V21V\u2212111 V12."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 367,
                                "start": 364
                            }
                        ],
                        "text": "Thus we have the following constrained optimization problem, p(I) = argmaxf Z p(I) log p(I)dIg; (2) subject to Ep[ ( )(I)] = Z ( )(I)p(I)dI = ( ) obs; = 1; :::;K; and Z p(I)dI = 1: By an application of the Lagrange multipliers, it is well-known that the solution for p(I) has the following Gibbs distribution form: p(I; ; S) = 1 Z( ) expf K X =1 < ( ); ( )(I) >g; (3) where = ( (1); (2); :::; (K)) is the parameter, ( ) is a vector of the same dimension as ( )(I), < ; > denotes inner product, and Z( ) = Z expf K X =1 < ( ); ( )(I) >gdI is the partition function which normalizes p(I; ) into a probability distribution."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "Substituting H( )(I) for ( )(I) in equation (3), we obtain p(I; ; S) = 1 Z( ) expf K X =1 < ( ); H( )(I) >g; (26) which we call the FRAME (Filter, Random eld, And Minimax Entropy) model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 38
                            }
                        ],
                        "text": "2 Estimation and computation Equation (3) speci es an exponential family of distributions (Brown 1986): S = fp(I; ; S) : 2 Rdg; (4) where d is the total number of parameters, and is solved at \u0003\u0302 which satis es the constraints p(I; \u0003\u0302; S) 2 S , i."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 6
                            }
                        ],
                        "text": "(A.2)\nEquation A.1 follows a second-order Taylor expansion argument (corollary 4.4 of Kullback, 1959, p. 48), where p\u2032 is a distribution whose expected feature statistics are between those of p and p+, and\nVarp\u2032 [8+(I)] = (\nVarp\u2032 [8(I)] Covp\u2032 [8(I), \u03c6 (\u03b2)(I)]\nCovp\u2032 [\u03c6 (\u03b2)(I),8(I)] Varp\u2032 [\u03c6 (\u03b2)(I)]\n)\n= (\nV11 V12 V21 V22\n) ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 9
                            }
                        ],
                        "text": "Proof of Equation 3.6."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 54
                            }
                        ],
                        "text": "As an illustration of the diversity of textures, gure (3) displays some typical texture images."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 263,
                                "start": 241
                            }
                        ],
                        "text": "Second, recent psychophysical research on human texture perception suggests that two homogeneous textures are often difficult to discriminate when they produce similar marginal distributions (histograms) of responses from a bank of filters (Bergen & Adelson, 1991; Chubb & Landy, 1991)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": "In section (3) we study the minimax entropy principle in depth by correcting it in the presence of estimation error and by addressing the issue of variance estimation in homogeneous random elds."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 137
                            }
                        ],
                        "text": "5 Experiments of texture modeling This section describes the modeling of natural textures using the algorithm studied in section (2) and (3)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "Equation 2.3 specifies an exponential family of distributions (Brown, 1986),\n2S = {p(I;3, S) : 3 \u2208 Rd}, (2.4)\nwhere d is the total number of parameters, and 3 is solved at 3\u0302, which satisfies the constraints p(I; 3\u0302, S) \u2208 \u00c4S, that is,\nEp(I;3\u0302,S)[\u03c6 (\u03b1)(I)] = \u00b5(\u03b1)obs. \u03b1 = 1, . . . ,K. (2.5)\nHowever, analytical solution of equation 2.5 is in general unavailable; instead, we solve for p(I; 3\u0302, S) iteratively from 2S by maximum likelihood estimator."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theories of visual texture perception.\" Spatial Vision  D.Regan (eds.)"
            },
            "venue": {
                "fragments": [],
                "text": "CRC press,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": "According to the maximum entropy principle, p(I; ?; S) = arg max p2 S entropy(p): (12) Combining (11) and (12), we have S = arg min jSj=Kfmax p2 S entropy(p)g: (13) We call equation (13) the minimax entropy principle."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 7
                            }
                        ],
                        "text": "(2.12)\nCombining equations 2.11 and 2.12, we have\nS\u2217 = arg min |S|=K {max p\u2208\u00c4S entropy(p)}."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": ", S = arg min jSj=K entropy(p(I; ?; S)): (11) 7"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 0
                            }
                        ],
                        "text": "Combining equations 3.2 and 3.3, we have\nEobs[KL( f, p(I; 3\u0302, S))] = Eobs[entropy(p(I; 3\u0302, S))] \u2212 entropy( f )+ C1 + C2, (3.4)\nwhere the two correction terms are\nC1 = Eobs[KL(p(I;3?, S), p(I; 3\u0302, S))], C2 = Eobs[KL(p(I; 3\u0302, S), p(I;3?, S))]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 100
                            }
                        ],
                        "text": "These histograms are used for pattern classification, recognition, and visual coding (Watson, 1987; Donoho & Johnstone, 1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 276,
                                "start": 252
                            }
                        ],
                        "text": "Examples of filters include the frequency and orientation selective Gabor filters (Daugman, 1985) and some wavelet pyramids based on various coding criteria (Mallat, 1989;\n(a)\nSimoncelli, Freeman, Adelson, & Weeger, 1992; Coifman & Wickerhauser, 1992; Donoho & Johnstone, 1994)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ideal de-noising in an orthonormal basis chosen from a  library of bases. \" Acad.Sci.Paris"
            },
            "venue": {
                "fragments": [],
                "text": "Ser I,"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2748598"
                        ],
                        "name": "S. Kullback",
                        "slug": "S.-Kullback",
                        "structuredName": {
                            "firstName": "Solomon",
                            "lastName": "Kullback",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kullback"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102909471"
                        ],
                        "name": "R. A. Leibler",
                        "slug": "R.-A.-Leibler",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Leibler",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. A. Leibler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 127
                            }
                        ],
                        "text": "The goodness of p(I) constructed in (I) is measured by KL( f, p), that is, the Kullback-Leibler divergence from f (I) to p(I) (Kullback & Leibler, 1951), and it depends on the feature set S that we selected."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 148
                            }
                        ],
                        "text": "Since our goal is to make an inference about the underlying distribution f (I), the goodness of this model can be measured by the Kullback-Leibler (Kullback & Leibler, 1951) divergence from f (I) to p(I;3?, S),\nKL( f, p(I;3?, S)) = \u222b f (I) log f (I)\np(I;3?, S)dI = Ef [log f (I)]\u2212 Ef [log p(I;3?,\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 130
                            }
                        ],
                        "text": "Since our goal is to make an inference about the underlying distribution f (I), the goodness of this model can be measured by the Kullback-Leibler (Kullback & Leibler, 1951) divergence from f (I) to p(I;3?, S),\nKL( f, p(I;3?, S)) = \u222b f (I) log f (I)\np(I;3?, S)dI = Ef [log f (I)]\u2212 Ef [log p(I;3?, S)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 155
                            }
                        ],
                        "text": "Following the notation in section 2.4, at each step K + 1, suppose we choose a new feature \u03c6(\u03b2), and let 8+(I) = (8(I), \u03c6(\u03b2)(I)); the decrease of the expected Kullback-Leibler divergence is:\nEobs[KL( f, p)]\u2212 Eobs[KL( f, p+)] = d(\u03c6(\u03b2))\u2212 1\nM [tr(Varf [8+(I)]Var\u22121p\u2217+ [8+(I)])\n\u2212 tr(Varf [8(I)]Var\u22121p\u2217 [8(I)])]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The goodness of p.I/ constructed in (I) is measured by KL. f; p/, that is, the Kullback-Leibler divergence from f.I/ to p.I/ ( Kullback & Leibler, 1951 ), and it depends on the feature set S that we selected.,Since our goal is to make an inference about the underlying distribution f.I/, the goodness of this model can be measured by the Kullback-Leibler ( Kullback & Leibler, 1951 ) divergence from f.I/ to p.II3 ? ; S/,"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 72
                            }
                        ],
                        "text": "First, let us consider the minimum entropy principle, which relates the Kullback-Leibler divergence KL( f, p(I;3, S)) to the entropy of the model p(I;3, S) for 3 = 3?."
                    },
                    "intents": []
                }
            ],
            "corpusId": 120349231,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c054360ec3ccadae977fdd0d77694c9655478a41",
            "isKey": true,
            "numCitedBy": 10535,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-Information-and-Sufficiency-Kullback-Leibler",
            "title": {
                "fragments": [],
                "text": "On Information and Sufficiency"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1951
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "The intuitive meaning of equation (24) is the following."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 272,
                                "start": 268
                            }
                        ],
                        "text": "let ( ) ?obs and V\u0302obs be the sample mean and variance of f ( ) ? (Iobs i ); i = 1; 2; :::;Mg and let V\u0302syn be the sample variance of f ( ) ? (Isyn i ); i = 1; 2; :::;M 0g, thus we have d0( ( )) 1 2( ( ) syn ( ) obs)0 V\u0302 1 syn( ( ) syn ( ) obs) 1 M tr(V\u0302obsV\u0302 1 syn): (24) We note that in equation (24) tr(V\u0302obsV\u0302 1 syn) = 1 M tr( M Xi=1( ( ) ? (Iobs i ) ( ) ?obs)( ( ) ? (Iobs i ) ( ) ?obs)0V\u0302 1 syn) = 1 M M Xi=1( ( ) ? (Iobs i ) ( ) ?obs)0V\u0302 1 syn( ( ) ? (Iobs i ) ( ) ?obs) 12"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "Now let's consider the feature pursuit criterion in equation (24)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "The algorithm rst computes d0( (1)) according to equations (24) and (25) for each lter, and d0( (1)) for some lters are list in table 1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 158
                            }
                        ],
                        "text": "Examples of filters include the frequency and orientation selective Gabor filters (Daugman, 1985) and some wavelet pyramids based on various coding criteria (Mallat, 1989;\n(a)\nSimoncelli, Freeman, Adelson, & Weeger, 1992; Coifman & Wickerhauser, 1992; Donoho & Johnstone, 1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 373,
                                "start": 369
                            }
                        ],
                        "text": "We choose 2 nonlinear lters {spectrum analyzers SP (17; 0o) and SP (17; 90o) , with their periods T tuned to the periods of the texture, and the window sizes of the lters are 33 33 3The synthetic fur texture in these gures are better than those in (Zhu, Wu, Mumford 1996) since the L1-criterion used here for lter pursuit has been replaced by the criterion of equation (24) 20"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "Similarly V\u0302syn in equation (24) is replaced by V\u0302syn(D1)=N2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 65
                            }
                        ],
                        "text": "Then from the above result, we can approximate V\u0302obs in equation (24) by V\u0302obs(D1)=N2."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "multi-resolution approximations and wavelet orthonormal bases of L2(R)."
            },
            "venue": {
                "fragments": [],
                "text": "Trans.  Amer. Math. Soc. 315,"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 127
                            }
                        ],
                        "text": "The goodness of p(I) constructed in (I) is measured by KL( f, p), that is, the Kullback-Leibler divergence from f (I) to p(I) (Kullback & Leibler, 1951), and it depends on the feature set S that we selected."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 148
                            }
                        ],
                        "text": "Since our goal is to make an inference about the underlying distribution f (I), the goodness of this model can be measured by the Kullback-Leibler (Kullback & Leibler, 1951) divergence from f (I) to p(I;3?, S),\nKL( f, p(I;3?, S)) = \u222b f (I) log f (I)\np(I;3?, S)dI = Ef [log f (I)]\u2212 Ef [log p(I;3?,\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 130
                            }
                        ],
                        "text": "Since our goal is to make an inference about the underlying distribution f (I), the goodness of this model can be measured by the Kullback-Leibler (Kullback & Leibler, 1951) divergence from f (I) to p(I;3?, S),\nKL( f, p(I;3?, S)) = \u222b f (I) log f (I)\np(I;3?, S)dI = Ef [log f (I)]\u2212 Ef [log p(I;3?, S)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 155
                            }
                        ],
                        "text": "Following the notation in section 2.4, at each step K + 1, suppose we choose a new feature \u03c6(\u03b2), and let 8+(I) = (8(I), \u03c6(\u03b2)(I)); the decrease of the expected Kullback-Leibler divergence is:\nEobs[KL( f, p)]\u2212 Eobs[KL( f, p+)] = d(\u03c6(\u03b2))\u2212 1\nM [tr(Varf [8+(I)]Var\u22121p\u2217+ [8+(I)])\n\u2212 tr(Varf [8(I)]Var\u22121p\u2217 [8(I)])]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 72
                            }
                        ],
                        "text": "First, let us consider the minimum entropy principle, which relates the Kullback-Leibler divergence KL( f, p(I;3, S)) to the entropy of the model p(I;3, S) for 3 = 3?."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\On information and suuciency"
            },
            "venue": {
                "fragments": [],
                "text": "Annual Math. Stat"
            },
            "year": 1951
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 68
                            }
                        ],
                        "text": "Thus, if model p(I) reproduces f (\u03b1)(z) for all \u03b1, then p(I) = f (I) (Zhu et al., 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 40
                            }
                        ],
                        "text": "For more discussion on this aspect, see Zhu et al. (1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 61
                            }
                        ],
                        "text": "For detailed analysis of the texture modeling algorithm, see Zhu et al. (1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 153
                            }
                        ],
                        "text": "The computation of the model is complicated by the nature of such isolated peaks, and we proposed an annealing approach for computing 3 (for details see Zhu et al.,\n1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 54
                            }
                        ],
                        "text": "(For a discussion of previous models and methods, see Zhu et al., 1996.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 77
                            }
                        ],
                        "text": "Detailed comparison between the FRAME model and the MRF models is covered in Zhu et al. (1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 93
                            }
                        ],
                        "text": "Notice that the original\n3 The synthetic fur texture in these figures is better than that in Zhu et al. (1996) since the L1 criterion used here for filter pursuit has been replaced by the criterion of equation 3.7.\nobserved texture image is not homogeneous, since the shapes of the blobs vary\u2026"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "FRAME: Filters, random fields and maximum entropy\u2014to a unified theory for texture modeling"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of Int'l Conf on Computer Vision and Pattern Recognition"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 68
                            }
                        ],
                        "text": "Thus, if model p(I) reproduces f (\u03b1)(z) for all \u03b1, then p(I) = f (I) (Zhu et al., 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 40
                            }
                        ],
                        "text": "For more discussion on this aspect, see Zhu et al. (1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 61
                            }
                        ],
                        "text": "For detailed analysis of the texture modeling algorithm, see Zhu et al. (1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 153
                            }
                        ],
                        "text": "The computation of the model is complicated by the nature of such isolated peaks, and we proposed an annealing approach for computing 3 (for details see Zhu et al.,\n1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 54
                            }
                        ],
                        "text": "(For a discussion of previous models and methods, see Zhu et al., 1996.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 77
                            }
                        ],
                        "text": "Detailed comparison between the FRAME model and the MRF models is covered in Zhu et al. (1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 93
                            }
                        ],
                        "text": "Notice that the original\n3 The synthetic fur texture in these figures is better than that in Zhu et al. (1996) since the L1 criterion used here for filter pursuit has been replaced by the criterion of equation 3.7.\nobserved texture image is not homogeneous, since the shapes of the blobs vary\u2026"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "FRAME: Filters, Random elds And Maximum  Entropy|to a uni ed theory for texture modeling"
            },
            "venue": {
                "fragments": [],
                "text": "Harvard Robotics Lab. Technique Report"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 61
                            }
                        ],
                        "text": "Filter Size d0( (1)) d0( (2)) d0( (3)) d0( (4)) d0( (5)) d0( (8)) 1x1 1018."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 337,
                                "start": 334
                            }
                        ],
                        "text": "@L( ; S)) @ ( ) = 1 Z @Z @ ( ) ( ) obs = Ep(I; ;S)[ ( )] ( ) obs; 8 ; (6) @2L( ; S) @ ( ) ( )0 = Ep(I; ;S)[( ( )(I) ( ) obs)( ( )(I) ( ) obs)0 ]; 8 ; : (7) Following equation (6), maximizing the log-likelihood by gradient ascent gives the following equation for solving iteratively, d ( ) dt = Ep(I; ;S)[ ( )(I)] ( ) obs; = 1; :::;K: (8) Obviously equation (8) converges to = \u0003\u0302."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 96
                            }
                        ],
                        "text": "b) plots the histograms of the ltered image F I, with I being the texton image observed in gure (8) (solid curve) and a uniform noise image (dotted curve)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 27
                            }
                        ],
                        "text": "At each step t of equation (8), the computation of Ep(I; ;S)[ ( )(I)] is in general di cult, and we adopt the stochastic gradient method (Younes 1988) for approximation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 276
                            }
                        ],
                        "text": "\u2026adopts some parametric Markov random field (MRF) models in the forms of Gibbs distributions\u2014for example, the general smoothness models in image restoration (Geman & Geman, 1984; Mumford & Shah, 1989) and the conditional autoregression models in texture modeling (Besag, 1973; Cross & Jain, 1983)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Markov random eld texture models."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE, PAMI,"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 127
                            }
                        ],
                        "text": "The goodness of p(I) constructed in (I) is measured by KL( f, p), that is, the Kullback-Leibler divergence from f (I) to p(I) (Kullback & Leibler, 1951), and it depends on the feature set S that we selected."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 148
                            }
                        ],
                        "text": "Since our goal is to make an inference about the underlying distribution f (I), the goodness of this model can be measured by the Kullback-Leibler (Kullback & Leibler, 1951) divergence from f (I) to p(I;3?, S),\nKL( f, p(I;3?, S)) = \u222b f (I) log f (I)\np(I;3?, S)dI = Ef [log f (I)]\u2212 Ef [log p(I;3?,\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "S = arg min jSj=Kf entropy(p(I; \u0003\u0302; S)) + 1 M tr(V arf [ (I)]V ar 1 p [ (I)]) g: (22) In practice, V arf [ (I)] and V arp [ (I)] can be estimated from the observed images and synthesized images respectively."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 130
                            }
                        ],
                        "text": "Since our goal is to make an inference about the underlying distribution f (I), the goodness of this model can be measured by the Kullback-Leibler (Kullback & Leibler, 1951) divergence from f (I) to p(I;3?, S),\nKL( f, p(I;3?, S)) = \u222b f (I) log f (I)\np(I;3?, S)dI = Ef [log f (I)]\u2212 Ef [log p(I;3?, S)]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 155
                            }
                        ],
                        "text": "Following the notation in section 2.4, at each step K + 1, suppose we choose a new feature \u03c6(\u03b2), and let 8+(I) = (8(I), \u03c6(\u03b2)(I)); the decrease of the expected Kullback-Leibler divergence is:\nEobs[KL( f, p)]\u2212 Eobs[KL( f, p+)] = d(\u03c6(\u03b2))\u2212 1\nM [tr(Varf [8+(I)]Var\u22121p\u2217+ [8+(I)])\n\u2212 tr(Varf [8(I)]Var\u22121p\u2217 [8(I)])]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 72
                            }
                        ],
                        "text": "First, let us consider the minimum entropy principle, which relates the Kullback-Leibler divergence KL( f, p(I;3, S)) to the entropy of the model p(I;3, S) for 3 = 3?."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On information and su ciency"
            },
            "venue": {
                "fragments": [],
                "text": "Annual Math. Stat. vol.22,"
            },
            "year": 1951
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 68
                            }
                        ],
                        "text": "Thus, if model p(I) reproduces f (\u03b1)(z) for all \u03b1, then p(I) = f (I) (Zhu et al., 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 40
                            }
                        ],
                        "text": "For more discussion on this aspect, see Zhu et al. (1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 61
                            }
                        ],
                        "text": "For detailed analysis of the texture modeling algorithm, see Zhu et al. (1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 153
                            }
                        ],
                        "text": "The computation of the model is complicated by the nature of such isolated peaks, and we proposed an annealing approach for computing 3 (for details see Zhu et al.,\n1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 66
                            }
                        ],
                        "text": "Thus, if model p(I) reproduces f (z) for all \u03b1, then p(I) = f (I) (Zhu et al., 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 54
                            }
                        ],
                        "text": "(For a discussion of previous models and methods, see Zhu et al., 1996.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 77
                            }
                        ],
                        "text": "Detailed comparison between the FRAME model and the MRF models is covered in Zhu et al. (1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 93
                            }
                        ],
                        "text": "Notice that the original\n3 The synthetic fur texture in these figures is better than that in Zhu et al. (1996) since the L1 criterion used here for filter pursuit has been replaced by the criterion of equation 3.7.\nobserved texture image is not homogeneous, since the shapes of the blobs vary\u2026"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "FRAME: Filters, random fields"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 68
                            }
                        ],
                        "text": "Thus, if model p(I) reproduces f (\u03b1)(z) for all \u03b1, then p(I) = f (I) (Zhu et al., 1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 40
                            }
                        ],
                        "text": "For more discussion on this aspect, see Zhu et al. (1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 61
                            }
                        ],
                        "text": "For detailed analysis of the texture modeling algorithm, see Zhu et al. (1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 153
                            }
                        ],
                        "text": "The computation of the model is complicated by the nature of such isolated peaks, and we proposed an annealing approach for computing 3 (for details see Zhu et al.,\n1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 54
                            }
                        ],
                        "text": "(For a discussion of previous models and methods, see Zhu et al., 1996.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 77
                            }
                        ],
                        "text": "Detailed comparison between the FRAME model and the MRF models is covered in Zhu et al. (1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 93
                            }
                        ],
                        "text": "Notice that the original\n3 The synthetic fur texture in these figures is better than that in Zhu et al. (1996) since the L1 criterion used here for filter pursuit has been replaced by the criterion of equation 3.7.\nobserved texture image is not homogeneous, since the shapes of the blobs vary\u2026"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Filters, Random elds And Maximum Entropy|to a uniied theory for texture modeling"
            },
            "venue": {
                "fragments": [],
                "text": "Harvard Robotics Lab. Technique Report"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70184938"
                        ],
                        "name": "Num\u00e9risation de documents anciens math\u00e9matiques",
                        "slug": "Num\u00e9risation-de-documents-anciens-math\u00e9matiques",
                        "structuredName": {
                            "firstName": "Num\u00e9risation",
                            "lastName": "math\u00e9matiques",
                            "middleNames": [
                                "de",
                                "documents",
                                "anciens"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Num\u00e9risation de documents anciens math\u00e9matiques"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 138
                            }
                        ],
                        "text": "At each step t of equation 2.8, the computation of Ep(I;3,S)[\u03c6(\u03b1)(I)] is in general difficult, and we adopt the stochastic gradient method (Younes 1988) for approximation."
                    },
                    "intents": []
                }
            ],
            "corpusId": 125110486,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "8640a639984deecfe78000b9ebbeb9244e9adc4c",
            "isKey": true,
            "numCitedBy": 12,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Annales-de-l'Institut-Henri-Poincar\u00e9.-Section-B,-et-math\u00e9matiques",
            "title": {
                "fragments": [],
                "text": "Annales de l'Institut Henri Poincar\u00e9. Section B, Calcul des probabilit\u00e9s et statistique"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145524610"
                        ],
                        "name": "L. Brown",
                        "slug": "L.-Brown",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Brown",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Brown"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 63
                            }
                        ],
                        "text": "Equation 2.3 specifies an exponential family of distributions (Brown, 1986),\n2S = {p(I;3, S) : 3 \u2208 Rd}, (2.4)\nwhere d is the total number of parameters, and 3 is solved at 3\u0302, which satisfies the constraints p(I; 3\u0302, S) \u2208 \u00c4S, that is,\nEp(I;3\u0302,S)[\u03c6 (\u03b1)(I)] = \u00b5(\u03b1)obs. \u03b1 = 1, . . . ,K. (2.5)\nHowever,\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Equation 2.3 specifies an exponential family of distributions ( Brown, 1986 ),"
                    },
                    "intents": []
                }
            ],
            "corpusId": 116999240,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "21379ba4269a4d2a7dc59b57318fb515be3647a4",
            "isKey": true,
            "numCitedBy": 533,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Fundamentals-of-statistical-exponential-families:-Brown",
            "title": {
                "fragments": [],
                "text": "Fundamentals of statistical exponential families: with applications in statistical decision theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721284"
                        ],
                        "name": "L. Younes",
                        "slug": "L.-Younes",
                        "structuredName": {
                            "firstName": "Laurent",
                            "lastName": "Younes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Younes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 138
                            }
                        ],
                        "text": "At each step t of equation 2.8, the computation of Ep(I;3,S)[\u03c6(\u03b1)(I)] is in general difficult, and we adopt the stochastic gradient method (Younes 1988) for approximation."
                    },
                    "intents": []
                }
            ],
            "corpusId": 115388639,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4f1fedd8a7238b90d02e36f3319b9dc4aff070f8",
            "isKey": true,
            "numCitedBy": 187,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Estimation-and-annealing-for-Gibbsian-fields-Younes",
            "title": {
                "fragments": [],
                "text": "Estimation and annealing for Gibbsian fields"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Equation A.1 follows a second-order Taylor expansion argument (corollary 4.4 of  Kullback, 1959, p. 48 ), where p 0 is a distribution whose expected"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 86
                            }
                        ],
                        "text": "(A.2)\nEquation A.1 follows a second-order Taylor expansion argument (corollary 4.4 of Kullback, 1959, p. 48), where p\u2032 is a distribution whose expected feature statistics are between those of p and p+, and\nVarp\u2032 [8+(I)] = (\nVarp\u2032 [8(I)] Covp\u2032 [8(I), \u03c6 (\u03b2)(I)]\nCovp\u2032 [\u03c6 (\u03b2)(I),8(I)] Varp\u2032 [\u03c6\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 125523249,
            "fieldsOfStudy": [],
            "id": "11fbf06e4c1c4eddc91a68e434433a4fc5f7cfc4",
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information Theory and Statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1959
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 31
                            }
                        ],
                        "text": "The Maximum Entropy Principle (Jaynes 1957)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 8
                            }
                        ],
                        "text": "2.1 The Maximum Entropy Principle."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 18
                            }
                        ],
                        "text": "The ME principle (Jaynes, 1957) suggests that we should choose p(I) that achieves the maximum entropy to obtain the purest and simplest fusion of the observed features and their statistics."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\ Information theory and statisti al me hani s \""
            },
            "venue": {
                "fragments": [],
                "text": "Physi al Review"
            },
            "year": 1957
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 138
                            }
                        ],
                        "text": "At each step t of equation 2.8, the computation of Ep(I;3,S)[\u03c6(\u03b1)(I)] is in general difficult, and we adopt the stochastic gradient method (Younes 1988) for approximation."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation and annealing for Gibbsian elds (STMA V30 1845)"
            },
            "venue": {
                "fragments": [],
                "text": "Annales  de l'Institut Henri Poincare,"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 138
                            }
                        ],
                        "text": "At each step t of equation 2.8, the computation of Ep(I;3,S)[\u03c6(\u03b1)(I)] is in general difficult, and we adopt the stochastic gradient method (Younes 1988) for approximation."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation and annealing for Gibbsian elds (STMA V30 1845) Annales de l'Institut Henri Poincare, Section B"
            },
            "venue": {
                "fragments": [],
                "text": "Calcul des Probabilities et Statistique"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737945"
                        ],
                        "name": "H. Akaike",
                        "slug": "H.-Akaike",
                        "structuredName": {
                            "firstName": "Hirotugu",
                            "lastName": "Akaike",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Akaike"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Therefore, we arrive at the following form of the Akaike information criterion (Akaike, 1977): Eobs[KL( f, p(I; 3\u0302, S))] \u2248 Eobs[entropy(p(I; 3\u0302, S))]\u2212 entropy( f ) + 1 M tr(Varf [8(I)]Var\u22121 p\u2217 [8(I)]), where we drop the higher-order term O(M\u22123/2)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 80
                            }
                        ],
                        "text": "Therefore, we arrive at the following form of the Akaike information criterion (Akaike, 1977):\nEobs[KL( f, p(I; 3\u0302, S))] \u2248 Eobs[entropy(p(I; 3\u0302, S))]\u2212 entropy( f ) + 1\nM tr(Varf [8(I)]Var\u22121p\u2217 [8(I)]),\nwhere we drop the higher-order term O(M\u22123/2)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 117470511,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb5e8213a462581cb43286f5c5307fa5398576ea",
            "isKey": false,
            "numCitedBy": 693,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-entropy-maximization-principle-Akaike",
            "title": {
                "fragments": [],
                "text": "On entropy maximization principle"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 106
                            }
                        ],
                        "text": "Substituting H( )(I) for ( )(I) in equation (??), we obtain p(I; ) = 1 Z( ) expf K X =1 < ( );H( )(I) >g; (18) which we call the FRAME (Filter, Random eld, And Minimax Entropy) model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 162
                            }
                        ],
                        "text": "This make it inappropriate to use nonparametric inference methods, such\nas kernel methods, radial basis functions (Ripley, 1996), and mixture of gaussian models (Jordan & Jacobs, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 69371331,
            "fieldsOfStudy": [],
            "id": "bc1c64b80c6bb204ffe2467dab0f87afec40896a",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hierarchical Mixtures of Experts and the EM Algorithm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3239730"
                        ],
                        "name": "M. Landy",
                        "slug": "M.-Landy",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Landy",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Landy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145707107"
                        ],
                        "name": "J. Movshon",
                        "slug": "J.-Movshon",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Movshon",
                            "middleNames": [
                                "Anthony"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Movshon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Second, recent psychophysical research on human texture perception suggests that two homogeneous textures are often difficult to discriminate when they produce similar marginal distributions (histograms) of responses from a bank of filters (Bergen & Adelson, 1991;  Chubb & Landy, 1991 )."
                    },
                    "intents": []
                }
            ],
            "corpusId": 59862043,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "923cb6525be620084767d3751273ab220b6f3f6e",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Orthogonal-Distribution-Analysis:-A-New-Approach-to-Landy-Movshon",
            "title": {
                "fragments": [],
                "text": "Orthogonal Distribution Analysis: A New Approach to the Study of Texture Perception"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 86
                            }
                        ],
                        "text": "Another perspective for this issue is the minimum description length (MDL) principle (Rissanen, 1989)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9365056,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "72247e4e34de0dd2d0428522ded24b49fb1632be",
            "isKey": false,
            "numCitedBy": 1772,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Stochastic-Complexity-in-Statistical-Inquiry-Rissanen",
            "title": {
                "fragments": [],
                "text": "Stochastic Complexity in Statistical Inquiry"
            },
            "venue": {
                "fragments": [],
                "text": "World Scientific Series in Computer Science"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3072213"
                        ],
                        "name": "J. Besag",
                        "slug": "J.-Besag",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Besag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Besag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 42087677,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8bf730243ed967afd5349bef053641a6043517a0",
            "isKey": false,
            "numCitedBy": 6163,
            "numCiting": 69,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Spatial-Interaction-and-the-Statistical-Analysis-of-Besag",
            "title": {
                "fragments": [],
                "text": "Spatial Interaction and the Statistical Analysis of Lattice Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Theories of visual texture perception"
            },
            "venue": {
                "fragments": [],
                "text": "\\Theories of visual texture perception"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 86
                            }
                        ],
                        "text": "Another perspective for this issue is the minimum description length (MDL) principle (Rissanen, 1989)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stochastic complexity in statistical inquiry. Singapore: World Scientific"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 156
                            }
                        ],
                        "text": "However, making inferences about f (I) is much more challenging than many of the learning problems in neural modeling (Dayan, Hinton, Neal, & Zernel, 1995; Xu, 1995) for the following reasons."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ying-Yang machine: a Bayesian-Kullback scheme for uni ed learnings and new results  on vector quantization"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Int'l Conf. on Neural Info"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 258
                            }
                        ],
                        "text": "\u2026(1) dynamic equations or replacement rules, which simulate specific physical and chemical processes to generate textures (Witkin & Kass, 1991; Picard, 1996), (2) the kth-order statistics model for texture perception, that is, the famous Julesz\u2019s conjecture (Julesz, 1962), and (3) MRF models."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Visual pattern discrimination. IRE Transactions of Information Theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1962
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical and Computatinal Theories for Image Segmentation, Texture Modeling and Object Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Statistical and Computatinal Theories for Image Segmentation, Texture Modeling and Object Recognition"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 146
                            }
                        ],
                        "text": "\u2026into three categories: (1) dynamic equations or replacement rules, which simulate specific physical and chemical processes to generate textures (Witkin & Kass, 1991; Picard, 1996), (2) the kth-order statistics model for texture perception, that is, the famous Julesz\u2019s conjecture (Julesz,\u2026"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reaction-di usion textures."
            },
            "venue": {
                "fragments": [],
                "text": "Computer Graphics,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 242,
                                "start": 239
                            }
                        ],
                        "text": "Let S = fp(I) : Ep[ ( )(I)] = Ef [ ( )(I)]; 8 ( ) 2 Sg be the set of probability distributions which can reproduce the expected features statistics in S, then according to the maximum entropy principle, pS(I; ?) = arg max p2 S entropy(p): (8) Combining (??) and (??), we have S = arg min jSj=Kfmax p2 S entropy(p)g: (9) We call equation (??) the minimax entropy principle, and we have demonstrated that this principle is consistent with the goal of modeling, i."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 276
                            }
                        ],
                        "text": "\u2026adopts some parametric Markov random field (MRF) models in the forms of Gibbs distributions\u2014for example, the general smoothness models in image restoration (Geman & Geman, 1984; Mumford & Shah, 1989) and the conditional autoregression models in texture modeling (Besag, 1973; Cross & Jain, 1983)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Markov random eld texture"
            },
            "venue": {
                "fragments": [],
                "text": "models.\" IEEE,"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 276,
                                "start": 252
                            }
                        ],
                        "text": "Examples of filters include the frequency and orientation selective Gabor filters (Daugman, 1985) and some wavelet pyramids based on various coding criteria (Mallat, 1989;\n(a)\nSimoncelli, Freeman, Adelson, & Weeger, 1992; Coifman & Wickerhauser, 1992; Donoho & Johnstone, 1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 100
                            }
                        ],
                        "text": "These histograms are used for pattern classification, recognition, and visual coding (Watson, 1987; Donoho & Johnstone, 1994)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ideal de-noising in an orthonormal"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 178
                            }
                        ],
                        "text": "\u2026adopts some parametric Markov random field (MRF) models in the forms of Gibbs distributions\u2014for example, the general smoothness models in image restoration (Geman & Geman, 1984; Mumford & Shah, 1989) and the conditional autoregression models in texture modeling (Besag, 1973; Cross & Jain, 1983)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\ Optimal Approximations by Pie ewise Smooth Fun - tions and Asso iated Variational Problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 98
                            }
                        ],
                        "text": "Our method for texture modeling was inspired by and bears some similarities to the recent work by Heeger and Bergen (1995) on texture synthesis, where many natural-looking texture images are successfully synthesized by matching the histograms of filter responses organized in the form of a pyramid."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 111
                            }
                        ],
                        "text": "Proposition 3 In the above notation, entropy(p(I; ?)) = Eobs[entropy(p(I; \u0003\u0302))] + Eobs[KL(p(I; \u0003\u0302); p(I; ?))]: (15) See appendix for proof."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pyramid-based texture analysis/synthesis.\" Com-  puter Graphics, in press"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 258
                            }
                        ],
                        "text": "\u2026(1) dynamic equations or replacement rules, which simulate specific physical and chemical processes to generate textures (Witkin & Kass, 1991; Picard, 1996), (2) the kth-order statistics model for texture perception, that is, the famous Julesz\u2019s conjecture (Julesz, 1962), and (3) MRF models."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Visual pattern discrimination. /RE Transactions of Information Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Visual pattern discrimination. /RE Transactions of Information Theory"
            },
            "year": 1962
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical and computatinal theories for image segmentation, texture modeling and object recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Annales de l'Institut Henri Poincare,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 150
                            }
                        ],
                        "text": "\u2026images\n{Isyni , i = 1, . . . ,M\u2032} by sampling p(I;3, S)with the Gibbs sampler (Geman & Geman, 1984) or other Markov chain Monte Carlo (MCMC) methods (Winkler, 1995), and approximate Ep(I;3,S)[\u03c6(\u03b1)(I)] by the sample means; that is,\nEp(I;3,S)[\u03c6(\u03b1)(I)] \u2248 \u00b5(\u03b1)obs(3) = 1 M\u2032 M\u2032\u2211 i=1 \u03c6(\u03b1)(Isyni ), \u03b1 = 1,\u2026"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Image Analysis, Random Fields and dynamic Monte Carlo Methods,  Springer-Verlag"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 145
                            }
                        ],
                        "text": "Second, f (I) is often far from being gaussian; therefore some popular dimension-reduction techniques, such as the principal component analysis (Jolliffe, 1986) and spectral analysis (Priestley, 1981), do not appear to be directly applicable."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Principle components analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\ Spatial intera tion and the statisti al analysis of latti e systems ( withdis ussion )"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\ Visual pattern dis rimination"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\ What is the goal of sensory oding ? \""
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 98
                            }
                        ],
                        "text": "Our method for texture modeling was inspired by and bears some similarities to the recent work by Heeger and Bergen (1995) on texture synthesis, where many natural-looking texture images are successfully synthesized by matching the histograms of filter responses organized in the form of a pyramid."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 106
                            }
                        ],
                        "text": "The new ME distribution is p+ = p(I; +; S+) = 1 Z( +) expf K X =1 < ( ) + ; ( )(I) > < ( ) + ; ( )(I) >g: (15) Ep+ [ ( )(I)] = Ef [ ( )(I)] for = 1; 2; :::;K; , and in general, ( ) + 6= ( ) for = 1; :::;K."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pyramid-based texture analysis/synthesis.\"Computer Graph-  ics, in press"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 156
                            }
                        ],
                        "text": "However, making inferences about f (I) is much more challenging than many of the learning problems in neural modeling (Dayan, Hinton, Neal, & Zernel, 1995; Xu, 1995) for the following reasons."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Ying-Yang machine: a Bayesian-Kullback scheme for uniied learnings and new results on vector quantization"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Int'l Conf. on Neural Info. Proc"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "In Applications of Statistics, ed. Krishnaiah, P.R"
            },
            "venue": {
                "fragments": [],
                "text": "In Applications of Statistics, ed. Krishnaiah, P.R"
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Shiftable multi - scale transforms"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans . on Information Theory"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 182
                            }
                        ],
                        "text": "If the intermediate distribution p0 is approximated using the current distribution p, the distance d( ( )) is approximated by d( ( )) 1 2( ( ) obs ( ) syn)0V 1 p ( ( ) obs ( ) syn); (17) where Vp = varp( ( ) ? ) is the variance estimated from the synthesized images."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "The feature pursuit procedure governed by equation (17) has the following intuitive interpretation."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Principle Components Analysis"
            },
            "venue": {
                "fragments": [],
                "text": "New York: Springer,"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ARTICLE Communicated by Pietro Perona"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Principle Components Analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Principle Components Analysis"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical and computatinal theories for image"
            },
            "venue": {
                "fragments": [],
                "text": "Statistique,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 146
                            }
                        ],
                        "text": "\u2026into three categories: (1) dynamic equations or replacement rules, which simulate specific physical and chemical processes to generate textures (Witkin & Kass, 1991; Picard, 1996), (2) the kth-order statistics model for texture perception, that is, the famous Julesz\u2019s conjecture (Julesz,\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The second author is supported by a grant to Donald B. Rubin."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Reaction-diiusion textures"
            },
            "venue": {
                "fragments": [],
                "text": "Computer Graphics"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 146
                            }
                        ],
                        "text": "\u2026into three categories: (1) dynamic equations or replacement rules, which simulate specific physical and chemical processes to generate textures (Witkin & Kass, 1991; Picard, 1996), (2) the kth-order statistics model for texture perception, that is, the famous Julesz\u2019s conjecture (Julesz,\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The second author is supported by a grant to Donald B. Rubin."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\Reaction-diiusion textures"
            },
            "venue": {
                "fragments": [],
                "text": "Computer Graphics"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 158
                            }
                        ],
                        "text": "Examples of filters include the frequency and orientation selective Gabor filters (Daugman, 1985) and some wavelet pyramids based on various coding criteria (Mallat, 1989;\n(a)\nSimoncelli, Freeman, Adelson, & Weeger, 1992; Coifman & Wickerhauser, 1992; Donoho & Johnstone, 1994)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\ multi - resolution approximations and wavelet orthonormal bases ofL 2 ( R ) . \""
            },
            "venue": {
                "fragments": [],
                "text": "Trans . Amer . Math . So ."
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 156
                            }
                        ],
                        "text": "However, making inferences about f (I) is much more challenging than many of the learning problems in neural modeling (Dayan, Hinton, Neal, & Zernel, 1995; Xu, 1995) for the following reasons."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ying-Yang machine: A Bayesian-Kullback scheme for unified learnings and new results on vector quantization"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Int'l Conf. on Neural Info. Proc. Hong Kong"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 120
                            }
                        ],
                        "text": "For a fixed 3, we synthesize some typical images\n{Isyni , i = 1, . . . ,M\u2032} by sampling p(I;3, S)with the Gibbs sampler (Geman & Geman, 1984) or other Markov chain Monte Carlo (MCMC) methods (Winkler, 1995), and approximate Ep(I;3,S)[\u03c6(\u03b1)(I)] by the sample means; that is,\nEp(I;3,S)[\u03c6(\u03b1)(I)] \u2248\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 157
                            }
                        ],
                        "text": "\u2026adopts some parametric Markov random field (MRF) models in the forms of Gibbs distributions\u2014for example, the general smoothness models in image restoration (Geman & Geman, 1984; Mumford & Shah, 1989) and the conditional autoregression models in texture modeling (Besag, 1973; Cross & Jain, 1983)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 180
                            }
                        ],
                        "text": "Second, the Monte Carlo Markov chain for model estimation and texture sampling is guaranteed to converge to a stationary process that follows the estimated distribution p(I;3, S) (Geman & Geman, 1984), and the observed histograms can be matched closely."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\ Sto hasti relaxation , Gibbs distribution , and theBayesian restoration of images"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans . PAMI"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Calcul des Probabilities et Statistique"
            },
            "venue": {
                "fragments": [],
                "text": "Calcul des Probabilities et Statistique"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 83
                            }
                        ],
                        "text": "Examples of filters include the frequency and orientation selective Gabor filters (Daugman, 1985) and some wavelet pyramids based on various coding criteria (Mallat, 1989;\n(a)\nSimoncelli, Freeman, Adelson, & Weeger, 1992; Coifman & Wickerhauser, 1992; Donoho & Johnstone, 1994)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\\ Un ertainty relation for resolution in spa e , spatial frequen y , andorientation optimized by two - dimensional visual orti al lters"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Opti alSo . Amer ."
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "FRAME : Filters , random fields and maximum entropy \u2014 to a unified theory for texture modeling"
            },
            "venue": {
                "fragments": [],
                "text": "Proc . of Int \u2019 l Conf . on Computer Vision and Pattern Recognition"
            },
            "year": 1996
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 2,
            "methodology": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 87,
        "totalPages": 9
    },
    "page_url": "https://www.semanticscholar.org/paper/Minimax-Entropy-Principle-and-Its-Application-to-Zhu-Wu/dd9ed76e8b9fa8b69257d3fc61fbc38bee973016?sort=total-citations"
}