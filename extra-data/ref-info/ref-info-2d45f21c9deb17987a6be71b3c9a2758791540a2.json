{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3118681"
                        ],
                        "name": "J. Hockenmaier",
                        "slug": "J.-Hockenmaier",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Hockenmaier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hockenmaier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 35
                            }
                        ],
                        "text": "The following examples, taken from Hockenmaier (2003a), demonstrate the most common rules."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 152
                            }
                        ],
                        "text": "Sentence categories (S) in CCGbank carry features, such as [dcl] for declarative, [wq] for wh-questions, and [for] for small clauses headed by for; see Hockenmaier (2003a) for the complete list."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 48
                            }
                        ],
                        "text": "Results are also given for Hockenmaier\u2019s parser (Hockenmaier 2003a) which used an earlier, slightly different version of the treebank."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 88
                            }
                        ],
                        "text": "One of the advantages of log-linear models is that it is easy to include such features; Hockenmaier (2003b) describes the difficulties in including such\nfeatures in a generative model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 41
                            }
                        ],
                        "text": "By focusing on those parses which result from the most probable lexical category sequences, we are able to perform effective discriminative training without considering the complete parse space, which for most sentences is prohibitively large."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 30
                            }
                        ],
                        "text": "In fact, the results given in Hockenmaier (2003b) are lower than previous results."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 66
                            }
                        ],
                        "text": "We also showed in Clark and Curran (2004b) that, in contrast with Hockenmaier (2003a), adding distance to the dependency features in the log-linear model does improve parsing accuracy."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 286,
                                "start": 267
                            }
                        ],
                        "text": "The use of conditional log-linear models in this article is designed to overcome some of the weaknesses identified in the approach of Clark, Hockenmaier, and Steedman\n(2002), and to offer a more flexible framework for including features than the generative models of Hockenmaier (2003a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Hockenmaier (2003a) compares a number of generative models, starting with a baseline model based on a PCFG."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 21
                            }
                        ],
                        "text": "The first, following Hockenmaier (2003a), is to define a model in terms of normal-form derivations (Eisner 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 271,
                                "start": 254
                            }
                        ],
                        "text": "The first step can be performed by simply assigning to each word all lexical categories the word is seen with in the training data, together with some strategy for dealing with rare and unknown words (such as assigning the complete lexical category set; Hockenmaier 2003a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We are not aware of any other work which has demonstrated the parsing efficiency benefits of supertagging using an automatically extracted grammar."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Hockenmaier (2003a) gives a detailed description of the procedure used to create CCGbank."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Hockenmaier (2003a) conjectures that the reduced performance is due to the problem of data sparseness, which becomes particularly severe for the generative model when the number of features is increased."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "There has been some other work on defining stochastic categorial grammars, but mainly in the context of grammar learning (Osborne and Briscoe 1997; Watkinson and Manandhar 2001; Zettlemoyer and Collins 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 194
                            }
                        ],
                        "text": "A number of statistical parsing models have recently been developed for CCG and used in parsers applied to newspaper text (Clark, Hockenmaier, and Steedman 2002; Hockenmaier and Steedman 2002b; Hockenmaier 2003b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 243
                            }
                        ],
                        "text": "An advantage of using CCG is that\nthe recovery of long-range dependencies can be integrated into the parsing process in a straightforward manner, rather than be relegated to such a post-processing phase (Clark, Hockenmaier, and Steedman 2002; Hockenmaier 2003a; Clark, Steedman, and Curran 2004)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 9
                            }
                        ],
                        "text": "However, Hockenmaier (2003b) reports that the increased complexity of the model reduces the effectiveness of the dynamic programming used in the parser, and hence a more aggressive beam search is required to produce reasonable parse times."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Hockenmaier (2003a) and Hockenmaier and Steedman (2002b) present a generative model of normal-form derivations, based on various techniques from the statistical parsing literature (Charniak 1997; Goodman 1997; Collins 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 57
                            }
                        ],
                        "text": "The treebank is CCGbank (Hockenmaier and Steedman 2002a; Hockenmaier 2003a), a CCG version of the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Hockenmaier (2003b) presents another generative model of normal-form derivations, which is based on the dependencies in the predicate\u2013argument structure, including long-range dependencies, rather than the dependencies defined by the local trees in the derivation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28807454,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15b6080c3dfdfba4869020d03d089bc443ce022b",
            "isKey": true,
            "numCitedBy": 203,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": "This dissertation is concerned with the creation of training data and the development of probability models for statistical parsing of English with Combinatory Categorial Grammar (CCG). Parsing, or syntactic analysis, is a prerequisite for semantic interpretation, and forms therefore an integral part of any system which requires natural language understanding. Since almost all naturally occurring sentences are ambiguous, it is not sufficient (and often impossible) to generate all possible syntactic analyses. Instead, the parser needs to rank competing analyses and select only the most likely ones. A statistical parser uses a probability model to perform this task. I propose a number of ways in which such probability models can be defined for CCG. The kinds of models developed in this dissertation, generative models over normal-form derivation trees, are particularly simple, and have the further property of restricting the set of syntactic analyses to those corresponding to a canonical derivation structure. This is important to guarantee that parsing can be done efficiently. In order to achieve high parsing accuracy, a large corpus of annotated data is required to estimate the parameters of the probability models. Most existing wide-coverage statistical parsers use models of phrase-structure trees estimated from the Penn Treebank, a 1-million-word corpus of manually annotated sentences from the Wall Street Journal. This dissertation presents an algorithm which translates the phrase-structure analyses of the Penn Treebank to CCG derivations. The resulting corpus, CCGbank, is used to train and test the models proposed in this dissertation. Experimental results indicate that parsing accuracy (when evaluated according to a comparable metric, the recovery of unlabelled word-word dependency relations), is as high as that of standard Penn Treebank parsers which use similar modelling techniques. Most existing wide-coverage statistical parsers use simple phrase-structure grammars whose syntactic analyses fail to capture long-range dependencies, and therefore do not correspond to directly interpretable semantic representations. By contrast, CCG is a grammar formalism in which semantic representations that include long-range dependencies can be built directly during the derivation of syntactic structure. These dependencies define the predicate-argument structure of a sentence, and are used for two purposes in this dissertation: First, the performance of the parser can be evaluated according to how well it recovers these dependencies. In contrast to purely syntactic evaluations, this yields a direct measure of how accurate the semantic interpretations returned by the parser are. Second, I propose a generative model that captures the local and non-local dependencies in the predicate-argument structure, and investigate the impact of modelling non-local in addition to local dependencies."
            },
            "slug": "Data-and-models-for-statistical-parsing-with-Hockenmaier",
            "title": {
                "fragments": [],
                "text": "Data and models for statistical parsing with combinatory categorial grammar"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This dissertation presents an algorithm which translates the phrase-structure analyses of the Penn Treebank to CCG derivations, and proposes a generative model that captures the local and non- local dependencies in the predicate-argument structure, and investigates the impact of modelling non-local in addition to local dependencies."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144523372"
                        ],
                        "name": "S. Clark",
                        "slug": "S.-Clark",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733593"
                        ],
                        "name": "J. Curran",
                        "slug": "J.-Curran",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Curran",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Curran"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7895723,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "19da3a7bc4fd32b697bab29fdbc6fe27bf3b33fb",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes log-linear parsing models for Combinatory Categorial Grammar (CCG). Log-linear models can easily encode the long-range dependencies inherent in coordination and extraction phenomena, which CCG was designed to handle. Log-linear models have previously been applied to statistical parsing, under the assumption that all possible parses for a sentence can be enumerated. Enumerating all parses is infeasible for large grammars; however, dynamic programming over a packed chart can be used to efficiently estimate the model parameters. We describe a parellelised implementation which runs on a Beowulf cluster and allows the complete WSJ Penn Treebank to be used for estimation."
            },
            "slug": "Log-Linear-Models-for-Wide-Coverage-CCG-Parsing-Clark-Curran",
            "title": {
                "fragments": [],
                "text": "Log-Linear Models for Wide-Coverage CCG Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This paper describes log-linear parsing models for Combinatory Categorial Grammar and describes a parellelised implementation which runs on a Beowulf cluster and allows the complete WSJ Penn Treebank to be used for estimation."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152465203"
                        ],
                        "name": "Mark Johnson",
                        "slug": "Mark-Johnson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Eisner (1996) describes a technique for eliminating spurious ambiguity entirely, by defining exactly one normal-form derivation for each semantic equivalence class of derivations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 100
                            }
                        ],
                        "text": "The first, following Hockenmaier (2003a), is to define a model in terms of normal-form derivations (Eisner 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13079001,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2685fc6cc208c67088afa9e8c2743511852fc19c",
            "isKey": true,
            "numCitedBy": 75,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Stochastic unification-based grammars (SUBGs) define exponential distributions over the parses generated by a unification-based grammar (UBG). Existing algorithms for parsing and estimation require the enumeration of all of the parses of a string in order to determine the most likely one, or in order to calculate the statistics needed to estimate a grammar from a training corpus. This paper describes a graph-based dynamic programming algorithm for calculating these statistics from the packed UBG parse representations of Maxwell and Kaplan (1995) which does not require enumerating all parses. Like many graphical algorithms, the dynamic programming algorithm's complexity is worst-case exponential, but is often polynomial. The key observation is that by using Maxwell and Kaplan packed representations, the required statistics can be rewritten as either the max or the sum of a product of functions. This is exactly the kind of problem which can be solved by dynamic programming over graphical models."
            },
            "slug": "Dynamic-programming-for-parsing-and-estimation-of-Geman-Johnson",
            "title": {
                "fragments": [],
                "text": "Dynamic programming for parsing and estimation of stochastic unification-based grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A graph-based dynamic programming algorithm for calculating statistics from the packed UBG parse representations of Maxwell and Kaplan (1995) which does not require enumerating all parses."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 167
                            }
                        ],
                        "text": "A possible response to our policy of adaptive supertagging is that any statistical parser can be made to run faster, for example by changing the beam parameter in the Collins (2003) parser, but that any increase in speed is typically associated with a reduction in accuracy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 138
                            }
                        ],
                        "text": "Long-range dependencies are relatively common in text such as newspaper text, but are typically not recovered by treebank parsers such as Collins (2003) and Charniak (2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 82
                            }
                        ],
                        "text": "In this article we are concerned with the syntactic component; see Steedman (2000) for how a semantic interpretation can be composed during a syntactic derivation, and also Bos et al. (2004) for how semantic interpretations can be built for newspaper text using the wide-coverage parser described in\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 210
                            }
                        ],
                        "text": "Hockenmaier (2003a) and Hockenmaier and Steedman (2002b) present a generative model of normal-form derivations, based on various techniques from the statistical parsing literature (Charniak 1997; Goodman 1997; Collins 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 30
                            }
                        ],
                        "text": "Because there is so much syntactic information in lexical categories, the parser is required to do less work once the lexical categories have been assigned; hence Srinivas and Joshi, in the context of TAG, refer to supertagging as almost parsing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7901127,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3fc44ff7f37ec5585310666c183c65e0a0bb2446",
            "isKey": true,
            "numCitedBy": 2062,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models."
            },
            "slug": "Head-Driven-Statistical-Models-for-Natural-Language-Collins",
            "title": {
                "fragments": [],
                "text": "Head-Driven Statistical Models for Natural Language Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "Three statistical models for natural language parsing are described, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree."
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144523372"
                        ],
                        "name": "S. Clark",
                        "slug": "S.-Clark",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733593"
                        ],
                        "name": "J. Curran",
                        "slug": "J.-Curran",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Curran",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Curran"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 18
                            }
                        ],
                        "text": "We also showed in Clark and Curran (2004b) that, in contrast with Hockenmaier (2003a), adding distance to the dependency features in the log-linear model does improve parsing accuracy."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 21
                            }
                        ],
                        "text": "In our earlier work (Clark and Curran 2004a) the forward\u2013backward algorithm was not used to estimate the probability in Equation (32)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 65
                            }
                        ],
                        "text": "This is a larger grammar than we have used in previous articles (Clark and Curran 2004b, 2004a, 2006), mainly because the improvement in the supertagger since the earlier work means that we can now use a larger grammar but still maintain highly efficient parsing."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Clark and Curran (2004a) describe the supertagger, which uses log-linear models to define a distribution over the lexical category set for each local five-word context containing the target word (Ratnaparkhi 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 48
                            }
                        ],
                        "text": "This set has excellent coverage on unseen data (Clark and Curran 2004a) and is a manageable size for adding the head and dependency information, and also mapping to grammatical relations for evaluation purposes (Section 11)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 21
                            }
                        ],
                        "text": "Thus the scores at the most general level in the GR hierarchy (dependent) correspond to unlabeled accuracy scores."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 65
                            }
                        ],
                        "text": "The lexical category set used by the supertagger is described in Clark and Curran (2004a) and Curran, Clark, and Vadas (2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 170
                            }
                        ],
                        "text": "Neither of these constraints guarantee a normal-form derivation, but they are both effective at reducing the size of the charts, which can greatly increase parser speed (Clark and Curran 2004a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6802974,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9eea85e590f6e522e3681b8e45012684c60b0fd",
            "isKey": false,
            "numCitedBy": 348,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes and evaluates log-linear parsing models for Combinatory Categorial Grammar (CCG). A parallel implementation of the L-BFGS optimisation algorithm is described, which runs on a Beowulf cluster allowing the complete Penn Treebank to be used for estimation. We also develop a new efficient parsing algorithm for CCG which maximises expected recall of dependencies. We compare models which use all CCG derivations, including non-standard derivations, with normal-form models. The performances of the two models are comparable and the results are competitive with existing wide-coverage CCG parsers."
            },
            "slug": "Parsing-the-WSJ-Using-CCG-and-Log-Linear-Models-Clark-Curran",
            "title": {
                "fragments": [],
                "text": "Parsing the WSJ Using CCG and Log-Linear Models"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A parallel implementation of the L-BFGS optimisation algorithm is described, which runs on a Beowulf cluster allowing the complete Penn Treebank to be used for estimation and a new efficient parsing algorithm for CCG which maximises expected recall of dependencies is developed."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3289329"
                        ],
                        "name": "S. Riezler",
                        "slug": "S.-Riezler",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Riezler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Riezler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2142738"
                        ],
                        "name": "Tracy Holloway King",
                        "slug": "Tracy-Holloway-King",
                        "structuredName": {
                            "firstName": "Tracy",
                            "lastName": "King",
                            "middleNames": [
                                "Holloway"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tracy Holloway King"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803660"
                        ],
                        "name": "R. Kaplan",
                        "slug": "R.-Kaplan",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Kaplan",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kaplan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2953252"
                        ],
                        "name": "Dick Crouch",
                        "slug": "Dick-Crouch",
                        "structuredName": {
                            "firstName": "Dick",
                            "lastName": "Crouch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dick Crouch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2265996"
                        ],
                        "name": "John T. Maxwell",
                        "slug": "John-T.-Maxwell",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Maxwell",
                            "middleNames": [
                                "T."
                            ],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John T. Maxwell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145177145"
                        ],
                        "name": "Mark Johnson",
                        "slug": "Mark-Johnson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 10
                            }
                        ],
                        "text": "We follow Riezler et al. (2002) in using a discriminative estimation method by maximizing the conditional log-likelihood of the model given the data, minus a Gaussian prior\n2 We could model predicate\u2013argument dependencies together with the derivation, but we wanted to use features from the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 91
                            }
                        ],
                        "text": "Statistical parsers have been developed for TAG (Chiang 2000; Sarkar and Joshi 2003), LFG (Riezler et al. 2002; Kaplan et al. 2004; Cahill et al. 2004), and HPSG (Toutanova et al. 2002; Toutanova, Markova, and Manning 2004; Miyao and Tsujii 2004; Malouf and van Noord 2004), among others."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 160
                            }
                        ],
                        "text": "Penn Treebank conversions have also been carried out for other linguistic formalisms, including TAG (Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000), LFG (Burke et al. 2004), and HPSG (Miyao, Ninomiya, and Tsujii 2004)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 158
                            }
                        ],
                        "text": "Another way to think of the estimation process is that it attempts to put as much mass as possible on the derivations leading to the goldstandard structures (Riezler et al. 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 87
                            }
                        ],
                        "text": "Long-distance dependency resolution in automatically acquired wide-coverage PCFG-based LFG approximations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 97
                            }
                        ],
                        "text": "Of course some of these advantages could be obtained with other grammar formalisms, such as TAG, LFG, and HPSG, although CCG is especially well-suited to analysing coordination and long-range dependencies."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 70
                            }
                        ],
                        "text": "Log-linear models have previously been applied to statistical parsing (Johnson et al., 1999; Toutanova et al., 2002; Riezler et al., 2002; Malouf and van Noord, 2004) but typically under the assumption that all possible parses for a sentence can be enumerated."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 158
                            }
                        ],
                        "text": "Another way to think of the estimation process is that it attempts to put as much mass as possible on the derivations leading to the gold-standard structures (Riezler et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 32
                            }
                        ],
                        "text": "The main difference is that the LFG grammar is hand-built, resulting in less ambiguity than an automatically extracted grammar and thus requiring fewer resources for model estimation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 416,
                                "start": 159
                            }
                        ],
                        "text": "Another way to think of the estimation process is that it attempts to put as much mass as possible on the derivations leading to the gold-standard structures (Riezler et al., 2002). The Gaussian prior prevents overfitting by penalising any model whose weights get too large in absolute value. The estimation process can also be thought of in terms of the framework of Della Pietra, Della Pietra, and Lafferty (1997), since setting the gradient in (17) to zero yields the usual maximum entropy constraints, namely that the expected value of each feature is equal to its empirical value (again ignoring the Gaussian prior term)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 71
                            }
                        ],
                        "text": "Kaplan et al. (2004) present similar work to ours in the context of an LFG grammar for English."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 115
                            }
                        ],
                        "text": "Log-linear models have previously been applied to statistical parsing (Johnson et al. 1999; Toutanova et al. 2002; Riezler et al. 2002; Malouf and van Noord 2004), but typically under the assumption that all possible parses for a sentence can be enumerated."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 92
                            }
                        ],
                        "text": "Statistical parsers have been developed for TAG (Chiang, 2000; Sarkar and Joshi, 2003), LFG (Riezler et al., 2002; Kaplan et al., 2004; Cahill et al., 2004) and HPSG (Toutanova et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 68
                            }
                        ],
                        "text": "Geman and Johnson (2002) propose a similar method in the context of LFG parsing; an implementation is described in Kaplan et al. (2004)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "Cahill et al. (2004) evaluate an LFG parser, which uses an automatically extracted grammar, against DepBank."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 62
                            }
                        ],
                        "text": "Kaplan et al. (2004) compare the Collins parser with the Parc LFG parser by mapping Penn Treebank parses into the dependencies of DepBank, claiming that the LFG parser is more accurate with only a slight reduction in speed."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 73
                            }
                        ],
                        "text": "Briscoe and Carroll (2006) give a rough comparison of RASP with the Parc LFG parser (Kaplan et al. 2004) on DepBank, obtaining similar results overall, but acknowledging that the results are not strictly comparable because of the different annotation schemes used."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6052790,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7f8f8f33187e20768ae0177780ac5ef78b77feca",
            "isKey": true,
            "numCitedBy": 346,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a stochastic parsing system consisting of a Lexical-Functional Grammar (LFG), a constraint-based parser and a stochastic disambiguation model. We report on the results of applying this system to parsing the UPenn Wall Street Journal (WSJ) treebank. The model combines full and partial parsing techniques to reach full grammar coverage on unseen data. The treebank annotations are used to provide partially labeled data for discriminative statistical estimation using exponential models. Disambiguation performance is evaluated by measuring matches of predicate-argument relations on two distinct test sets. On a gold standard of manually annotated f-structures for a subset of the WSJ treebank, this evaluation reaches 79% F-score. An evaluation on a gold standard of dependency relations for Brown corpus data achieves 76% F-score."
            },
            "slug": "Parsing-the-Wall-Street-Journal-using-a-Grammar-and-Riezler-King",
            "title": {
                "fragments": [],
                "text": "Parsing the Wall Street Journal using a Lexical-Functional Grammar and Discriminative Estimation Techniques"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The model combines full and partial parsing techniques to reach full grammar coverage on unseen data, and on a gold standard of manually annotated f-structures for a subset of the WSJ treebank, reaches 79% F-score."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144523372"
                        ],
                        "name": "S. Clark",
                        "slug": "S.-Clark",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733593"
                        ],
                        "name": "J. Curran",
                        "slug": "J.-Curran",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Curran",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Curran"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 117
                            }
                        ],
                        "text": "For example, we have developed a method for training the dependency model which requires lexical category data only (Clark and Curran 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 65
                            }
                        ],
                        "text": "This is a larger grammar than we have used in previous articles (Clark and Curran 2004b, 2004a, 2006), mainly because the improvement in the supertagger since the earlier work means that we can now use a larger grammar but still maintain highly efficient parsing."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2547341,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa3fc2f48111cb59f776dee467351ea98ef323c6",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a solution to the annotation bottleneck for statistical parsing, by exploiting the lexicalized nature of Combinatory Categorial Grammar (CCG). The parsing model uses predicate-argument dependencies for training, which are derived from sequences of CCG lexical categories rather than full derivations. A simple method is used for extracting dependencies from lexical category sequences, resulting in high precision, yet incomplete and noisy data. The dependency parsing model of Clark and Curran (2004b) is extended to exploit this partial training data. Remarkably, the accuracy of the parser trained on data derived from category sequences alone is only 1.3% worse in terms of F-score than the parser trained on complete dependency structures."
            },
            "slug": "Partial-Training-for-a-Lexicalized-Grammar-Parser-Clark-Curran",
            "title": {
                "fragments": [],
                "text": "Partial Training for a Lexicalized-Grammar Parser"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A solution to the annotation bottleneck for statistical parsing, by exploiting the lexicalized nature of Combinatory Categorial Grammar, which results in high precision, yet incomplete and noisy data."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3118681"
                        ],
                        "name": "J. Hockenmaier",
                        "slug": "J.-Hockenmaier",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Hockenmaier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hockenmaier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145332819"
                        ],
                        "name": "Mark Steedman",
                        "slug": "Mark-Steedman",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Steedman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Steedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 124
                            }
                        ],
                        "text": "Clark (2002) reports that a significant loss in coverage results if the supertagger is used as a front-end to the parser of Hockenmaier and Steedman (2002b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 144
                            }
                        ],
                        "text": "\u2026+ 0 POS + Distance(words) \u3008VBD, S[dcl]\u2192NP S[dcl]\\NP\u3009 + > 2 POS + Distance(punct) \u3008VBD, S[dcl]\u2192NP S[dcl]\\NP\u3009 + 2 POS + Distance(verbs) \u3008VBD, S[dcl]\u2192NP S[dcl]\\NP\u3009 + 0\nFor the normal-form model we follow Hockenmaier and Steedman (2002b) by defining dependency features in terms of the local rule\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 64
                            }
                        ],
                        "text": "We would also like to investigate using the generative model of Hockenmaier and Steedman (2002b) in a similar way."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 190
                            }
                        ],
                        "text": "\u2026model given the data, minus a Gaussian prior\n2 We could model predicate\u2013argument dependencies together with the derivation, but we wanted to use features from the derivation only, following Hockenmaier and Steedman (2002b).\nterm to prevent overfitting (Chen and Rosenfeld 1999; Johnson et al. 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 162
                            }
                        ],
                        "text": "A number of statistical parsing models have recently been developed for CCG and used in parsers applied to newspaper text (Clark, Hockenmaier, and Steedman 2002; Hockenmaier and Steedman 2002b; Hockenmaier 2003b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 24
                            }
                        ],
                        "text": "Hockenmaier (2003a) and Hockenmaier and Steedman (2002b) present a generative model of normal-form derivations, based on various techniques from the statistical parsing literature (Charniak 1997; Goodman 1997; Collins 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 25
                            }
                        ],
                        "text": "The treebank is CCGbank (Hockenmaier and Steedman 2002a; Hockenmaier 2003a), a CCG version of the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 42
                            }
                        ],
                        "text": "Hockenmaier also argues that, compared to Hockenmaier and Steedman (2002b), the predicate\u2013argument model is better suited to languages with freer word order than English."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2876869,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "976c95f69e8ee160868b1d54d477f56212ee794b",
            "isKey": true,
            "numCitedBy": 216,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper compares a number of generative probability models for a wide-coverage Combinatory Categorial Grammar (CCG) parser. These models are trained and tested on a corpus obtained by translating the Penn Treebank trees into CCG normal-form derivations. According to an evaluation of unlabeled word-word dependencies, our best model achieves a performance of 89.9%, comparable to the figures given by Collins (1999) for a linguistically less expressive grammar. In contrast to Gildea (2001), we find a significant improvement from modeling word-word dependencies."
            },
            "slug": "Generative-Models-for-Statistical-Parsing-with-Hockenmaier-Steedman",
            "title": {
                "fragments": [],
                "text": "Generative Models for Statistical Parsing with Combinatory Categorial Grammar"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This paper compares a number of generative probability models for a wide-coverage Combinatory Categorial Grammar (CCG) parser and finds a significant improvement from modeling word-word dependencies."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144523372"
                        ],
                        "name": "S. Clark",
                        "slug": "S.-Clark",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3118681"
                        ],
                        "name": "J. Hockenmaier",
                        "slug": "J.-Hockenmaier",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Hockenmaier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hockenmaier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145332819"
                        ],
                        "name": "Mark Steedman",
                        "slug": "Mark-Steedman",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Steedman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Steedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 74
                            }
                        ],
                        "text": "The philosophy in earlier work which combined the supertagger and parser (Clark, Hockenmaier, and Steedman 2002; Clark and Curran 2003) was to use an unrestrictive setting of the supertagger, but still allow a reasonable compromise between speed and accuracy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 123
                            }
                        ],
                        "text": "Table 8 gives the results for the hybrid dependency model, broken down by relation type, using the same relations given in Clark, Hockenmaier, and Steedman (2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 40
                            }
                        ],
                        "text": "This approach is different from that of Clark, Hockenmaier, and Steedman (2002), who define the probability of a dependency structure simply in terms of the dependencies."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 134
                            }
                        ],
                        "text": "The use of conditional log-linear models in this article is designed to overcome some of the weaknesses identified in the approach of Clark, Hockenmaier, and Steedman\n(2002), and to offer a more flexible framework for including features than the generative models of Hockenmaier (2003a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 89
                            }
                        ],
                        "text": "An early attempt from the Edinburgh project at wide-coverage CCG parsing is presented in Clark, Hockenmaier, and Steedman (2002)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 82
                            }
                        ],
                        "text": "Overall results are given, as well as results broken down by relation type, as in Clark, Hockenmaier, and Steedman (2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 123
                            }
                        ],
                        "text": "A number of statistical parsing models have recently been developed for CCG and used in parsers applied to newspaper text (Clark, Hockenmaier, and Steedman 2002; Hockenmaier and Steedman 2002b; Hockenmaier 2003b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 204
                            }
                        ],
                        "text": "An advantage of using CCG is that\nthe recovery of long-range dependencies can be integrated into the parsing process in a straightforward manner, rather than be relegated to such a post-processing phase (Clark, Hockenmaier, and Steedman 2002; Hockenmaier 2003a; Clark, Steedman, and Curran 2004)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 120
                            }
                        ],
                        "text": "The supertagging approach we adopt for training differs from that used for testing and follows the original approach of Clark, Hockenmaier, and Steedman (2002): If the size of the chart exceeds some threshold, the value of \u03b2 is increased, reducing ambiguity, and the sentence is supertagged and\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 46
                            }
                        ],
                        "text": "The best performing model outperforms that of Clark, Hockenmaier, and Steedman (2002), recovering CCG predicate\u2013argument dependencies with an overall F-score of around 84% using a similar evaluation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6673978,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "760836263a766c71901e8e92417da34654d627df",
            "isKey": false,
            "numCitedBy": 125,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a wide-coverage statistical parser that uses Combinatory Categorial Grammar (CCG) to derive dependency structures. The parser differs from most existing wide-coverage treebank parsers in capturing the long-range dependencies inherent in constructions such as coordination, extraction, raising and control, as well as the standard local predicate-argument dependencies. A set of dependency structures used for training and testing the parser is obtained from a treebank of CCG normal-form derivations, which have been derived (semi-) automatically from the Penn Treebank. The parser correctly recovers over 80% of labelled dependencies, and around 90% of unlabelled dependencies."
            },
            "slug": "Building-Deep-Dependency-Structures-using-a-CCG-Clark-Hockenmaier",
            "title": {
                "fragments": [],
                "text": "Building Deep Dependency Structures using a Wide-Coverage CCG Parser"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A wide-coverage statistical parser that uses Combinatory Categorial Grammar (CCG) to derive dependency structures and correctly recovers over 80% of labelled dependencies, and around 90% of unlabelled dependencies."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2554988"
                        ],
                        "name": "Laura Rimell",
                        "slug": "Laura-Rimell",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Rimell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laura Rimell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144523372"
                        ],
                        "name": "S. Clark",
                        "slug": "S.-Clark",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Clark"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 38886829,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7169099bb9ff0074038f12ac0bc0c94f6d59eaf3",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Porting-a-lexicalized-grammar-parser-to-the-domain-Rimell-Clark",
            "title": {
                "fragments": [],
                "text": "Porting a lexicalized-grammar parser to the biomedical domain"
            },
            "venue": {
                "fragments": [],
                "text": "J. Biomed. Informatics"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060101052"
                        ],
                        "name": "Terry Koo",
                        "slug": "Terry-Koo",
                        "structuredName": {
                            "firstName": "Terry",
                            "lastName": "Koo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Terry Koo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 80
                            }
                        ],
                        "text": "Long-range dependencies are relatively common in text such as newspaper text, but are typically not recovered by treebank parsers such as Collins (2003) and Charniak (2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 121
                            }
                        ],
                        "text": "Using a generative model\u2019s score as a feature in a discriminative framework has been beneficial for reranking approaches (Collins and Koo 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 405878,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "844db702be4bc149b06b822b47247e15f5894cc3",
            "isKey": false,
            "numCitedBy": 776,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 F-measure, a 13 relative decrease in F-measure error over the baseline model's score of 88.2. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative-in terms of both simplicity and efficiency-to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation."
            },
            "slug": "Discriminative-Reranking-for-Natural-Language-Collins-Koo",
            "title": {
                "fragments": [],
                "text": "Discriminative Reranking for Natural Language Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The boosting approach to ranking problems described in Freund et al. (1998) is applied to parsing the Wall Street Journal treebank, and it is argued that the method is an appealing alternative-in terms of both simplicity and efficiency-to work on feature selection methods within log-linear (maximum-entropy) models."
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144523372"
                        ],
                        "name": "S. Clark",
                        "slug": "S.-Clark",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145332819"
                        ],
                        "name": "Mark Steedman",
                        "slug": "Mark-Steedman",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Steedman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Steedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733593"
                        ],
                        "name": "J. Curran",
                        "slug": "J.-Curran",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Curran",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Curran"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 220
                            }
                        ],
                        "text": "Because the purpose of this article is to demonstrate the feasibility of wide-coverage parsing with CCG, we do not give an evaluation targeted specifically at long-range dependencies; such an evaluation was presented in Clark, Steedman, and Curran (2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 262
                            }
                        ],
                        "text": "An advantage of using CCG is that\nthe recovery of long-range dependencies can be integrated into the parsing process in a straightforward manner, rather than be relegated to such a post-processing phase (Clark, Hockenmaier, and Steedman 2002; Hockenmaier 2003a; Clark, Steedman, and Curran 2004)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 0
                            }
                        ],
                        "text": "Clark, Steedman, and Curran (2004) applied the parser described in this article to questions from the TREC Question Answering (QA) track."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 26952,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a8399a28aa322ed6b27b6408d34f44abdbf7b46",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Accurate dependency recovery has recently been reported for a number of wide-coverage statistical parsers using Combinatory Categorial Grammar (CCG). However, overall figures give no indication of a parser\u2019s performance on specific constructions, nor how suitable a parser is for specific applications. In this paper we give a detailed evaluation of a CCG parser on object extraction dependencies found in WSJ text. We also show how the parser can be used to parse questions for Question Answering. The accuracy of the original parser on questions is very poor, and we propose a novel technique for porting the parser to a new domain, by creating new labelled data at the lexical category level only. Using a supertagger to assign categories to words, trained on the new data, leads to a dramatic increase in question parsing accuracy."
            },
            "slug": "Object-Extraction-and-Question-Parsing-using-CCG-Clark-Steedman",
            "title": {
                "fragments": [],
                "text": "Object-Extraction and Question-Parsing using CCG"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A detailed evaluation of a CCG parser on object extraction dependencies found in WSJ text and a novel technique for porting the parser to a new domain, by creating new labelled data at the lexical category level only are proposed."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749837"
                        ],
                        "name": "Eugene Charniak",
                        "slug": "Eugene-Charniak",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Charniak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Charniak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 181
                            }
                        ],
                        "text": "Hockenmaier (2003a) and Hockenmaier and Steedman (2002b) present a generative model of normal-form derivations, based on various techniques from the statistical parsing literature (Charniak 1997; Goodman 1997; Collins 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 1
                            }
                        ],
                        "text": "Because there is so much syntactic information in lexical categories, the parser is required to do less work once the lexical categories have been assigned; hence Srinivas and Joshi, in the context of TAG, refer to supertagging as almost parsing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9880507,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a5e619f2c5f4220438b1357e596db5b1578398d",
            "isKey": false,
            "numCitedBy": 643,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a parsing system based upon a language model for English that is, in turn, based upon assigning probabilities to possible parses for a sentence. This model is used in a parsing system by finding the parse for the sentence with the highest probability. This system outperforms previous schemes. As this is the third in a series of parsers by different authors that are similar enough to invite detailed comparisons but different enough to give rise to different levels of performance, we also report on some experiments designed to identify what aspects of these systems best explain their relative performance."
            },
            "slug": "Statistical-Parsing-with-a-Context-Free-Grammar-and-Charniak",
            "title": {
                "fragments": [],
                "text": "Statistical Parsing with a Context-Free Grammar and Word Statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A parsing system based upon a language model for English that is, in turn, based upon assigning probabilities to possible parses for a sentence that outperforms previous schemes is described."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108265283"
                        ],
                        "name": "Johnathan M. Chen",
                        "slug": "Johnathan-M.-Chen",
                        "structuredName": {
                            "firstName": "Johnathan",
                            "lastName": "Chen",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Johnathan M. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400820110"
                        ],
                        "name": "K. Vijay-Shanker",
                        "slug": "K.-Vijay-Shanker",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Vijay-Shanker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Vijay-Shanker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 60
                            }
                        ],
                        "text": "Nasr and Rambow (2004) investigate the potential impact of LTAG supertagging on parsing speed and accuracy by performing a number of oracle experiments."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 97
                            }
                        ],
                        "text": "Also, the supertagging idea which is central to the efficiency of the CCG parser originated with TAG (Bangalore and Joshi 1999)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 46
                            }
                        ],
                        "text": "One possible way to increase the accuracy of LTAG supertagging is to use a Maximum Entropy, rather than HMM, tagger (as discussed previously), but this is likely to result in an improvement of only a few percentage points."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "Statistical parsers have been developed for TAG (Chiang 2000; Sarkar and Joshi 2003), LFG (Riezler et al. 2002; Kaplan et al. 2004; Cahill et al. 2004), and HPSG (Toutanova et al. 2002; Toutanova, Markova, and Manning 2004; Miyao and Tsujii 2004; Malouf and van Noord 2004), among others."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 101
                            }
                        ],
                        "text": "Penn Treebank conversions have also been carried out for other linguistic formalisms, including TAG (Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000), LFG (Burke et al. 2004), and HPSG (Miyao, Ninomiya, and Tsujii 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 92
                            }
                        ],
                        "text": "Of course some of these advantages could be obtained with other grammar formalisms, such as TAG, LFG, and HPSG, although CCG is especially well-suited to analysing coordination and long-range dependencies."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 120
                            }
                        ],
                        "text": "Chen et al. (2002) describe the results of reranking the output of an HMM supertagger using an automatically extracted LTAG."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 42
                            }
                        ],
                        "text": "The idea of supertagging originated with LTAG; however, in contrast to the CCG grammars used in this article, the automatically extracted LTAG grammars have, as yet, been too large to enable effective supertagging (as discussed in the previous section)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 70
                            }
                        ],
                        "text": "In order to deal with the problem of the additional, nonstandard CCG derivations, a conditional model of dependency structures is presented, based on Collins (1996), in which the dependencies are modeled directly and derivations are not modeled at all."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "In Proceedings of the TAG+ Workshop, pages 259\u2013268, Venice, Italy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "\u00a9 2007 Association for Computational Linguistics"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 145
                            }
                        ],
                        "text": "In Miyao and Tsujii (2003b, 2003a) log-linear models are developed for automatically extracted grammars for Lexicalized Tree Adjoining Grammar (LTAG) and Head Driven Phrase Structure Grammar (HPSG)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 271,
                                "start": 268
                            }
                        ],
                        "text": "The Maximum Entropy supertagger we use could also be applied to Miyao and Tsujii\u2019s grammars, although whether similar performance would be obtained depends on the characteristics of the grammar; see subsequent sections for more discussion of this issue in relation to LTAG."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 60
                            }
                        ],
                        "text": "The formalism most closely related to CCG from this list is TAG."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "In Proceedings of the TAG+ Workshop, pages 19\u201324, Venice, Italy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 24
                            }
                        ],
                        "text": "Automated extraction of TAGS from the Penn Treebank."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 32
                            }
                        ],
                        "text": "Interestingly, the accuracy of LTAG supertaggers using automatically extracted grammars is significantly below the accuracy of the CCG supertagger."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 201
                            }
                        ],
                        "text": "Because there is so much syntactic information in lexical categories, the parser is required to do less work once the lexical categories have been assigned; hence Srinivas and Joshi, in the context of TAG, refer to supertagging as almost parsing."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 90
                            }
                        ],
                        "text": "Miyao and Tsujii (2003a) present another log-linear model for an automatically extracted LTAG which uses a simple unigram model of the elementary trees together with a loglinear model of the attachments."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 126
                            }
                        ],
                        "text": "TAG grammars have been automatically extracted from the Penn Treebank, using techniques similar to those used by Hockenmaier (Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "In Proceedings of the TAG+7 Workshop, Vancouver, Canada."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60785379,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e1b4d6ba9970fb69545d20d6e7882e4d478a812f",
            "isKey": true,
            "numCitedBy": 109,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "The accuracy of statistical parsing models can be improved with the use of lexical information. Statistical parsing using Lexicalized tree adjoining grammar (LTAG), a kind of lexicalized grammar, has remained relatively unexplored. We believe that is largely in part due to the absence of large corpora accurately bracketed in terms of a perspicuous yet broad coverage LTAG. Our work attempts to alleviate this difficulty. We extract different LTAGs from the Penn Treebank. We show that certain strategies yield an improved extracted LTAG in terms of compactness, broad coverage, and supertagging accuracy. Furthermore, we perform a preliminary investigation in smoothing these grammars by means of an external linguistic resource, namely, the tree families of an XTAG grammar, a hand built grammar of English."
            },
            "slug": "Automated-Extraction-of-TAGs-from-the-Penn-Treebank-Chen-Vijay-Shanker",
            "title": {
                "fragments": [],
                "text": "Automated Extraction of TAGs from the Penn Treebank"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work extracts different LTAGs from the Penn Treebank and shows that certain strategies yield an improved extracted LTAG in terms of compactness, broad coverage, and supertagging accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "IWPT"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2953252"
                        ],
                        "name": "Dick Crouch",
                        "slug": "Dick-Crouch",
                        "structuredName": {
                            "firstName": "Dick",
                            "lastName": "Crouch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dick Crouch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803660"
                        ],
                        "name": "R. Kaplan",
                        "slug": "R.-Kaplan",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Kaplan",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kaplan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2142738"
                        ],
                        "name": "Tracy Holloway King",
                        "slug": "Tracy-Holloway-King",
                        "structuredName": {
                            "firstName": "Tracy",
                            "lastName": "King",
                            "middleNames": [
                                "Holloway"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tracy Holloway King"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3289329"
                        ],
                        "name": "S. Riezler",
                        "slug": "S.-Riezler",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Riezler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Riezler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 188
                            }
                        ],
                        "text": "The second difficulty is that some constructions may be analyzed differently across formalisms, and even apparently trivial differences such as tokenization can complicate the comparison (Crouch et al. 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10108214,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b361b5941d79d0568efef14145417274240017ad",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper reports on the use of two distinct evaluation metrics for assessing a stochastic parsing model consisting of a broad-coverage Lexical-Functional Grammar (LFG), an efficient constraint-based parser and a stochastic disambiguation model. The first evaluation metric measures matches of predicate-argument relations in LFG f-structures (henceforth the LFG annotation scheme) to a gold standard of manually annotated f-structures for a subset of the UPenn Wall Street Journal treebank. The other metric maps predicate-argument relations in LFG f-structures to dependency relations (henceforth DR annotations) as proposed by Carroll et al. (Carroll et al., 1999). For evaluation, these relations are matched against Carroll et al.\u2019s gold standard which was manually annnotated on a subset of the Brown corpus. The parser plus stochastic disambiguator gives an F-measure of 79% (LFG) or 73% (DR) on the WSJ test set. This shows that the two evaluation schemes are similar in spirit, although accuracy is impaired systematically by mapping one annotation scheme to the other. A systematic loss of accuracy is incurred also by corpus variation: Training the stochastic disambiguation model on WSJ data and testing on Carroll et al.\u2019s Brown corpus data yields an F-score of 74% (DR) for dependency-relation match. A variant of this measure comparable to the measure reported by Carroll et al. yields an F-measure of 76%. We examine divergences between annotation schemes aiming at a future improvement of methods for assessing parser quality."
            },
            "slug": "A-Comparison-of-Evaluation-Metrics-for-a-Stochastic-Crouch-Kaplan",
            "title": {
                "fragments": [],
                "text": "A Comparison of Evaluation Metrics for a Broad-Coverage Stochastic Parser"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Divergences between annotation schemes aiming at a future improvement of methods for assessing parser quality are examined, showing that the two evaluation schemes are similar in spirit, although accuracy is impaired systematically by mapping one annotation scheme to the other."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144523372"
                        ],
                        "name": "S. Clark",
                        "slug": "S.-Clark",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733593"
                        ],
                        "name": "J. Curran",
                        "slug": "J.-Curran",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Curran",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Curran"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 18
                            }
                        ],
                        "text": "We also showed in Clark and Curran (2004b) that, in contrast with Hockenmaier (2003a), adding distance to the dependency features in the log-linear model does improve parsing accuracy."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 21
                            }
                        ],
                        "text": "In our earlier work (Clark and Curran 2004a) the forward\u2013backward algorithm was not used to estimate the probability in Equation (32)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 65
                            }
                        ],
                        "text": "This is a larger grammar than we have used in previous articles (Clark and Curran 2004b, 2004a, 2006), mainly because the improvement in the supertagger since the earlier work means that we can now use a larger grammar but still maintain highly efficient parsing."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Clark and Curran (2004a) describe the supertagger, which uses log-linear models to define a distribution over the lexical category set for each local five-word context containing the target word (Ratnaparkhi 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 48
                            }
                        ],
                        "text": "This set has excellent coverage on unseen data (Clark and Curran 2004a) and is a manageable size for adding the head and dependency information, and also mapping to grammatical relations for evaluation purposes (Section 11)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 70
                            }
                        ],
                        "text": "Cahill et al. (2004) evaluate an LFG parser, which uses an automatically extracted grammar, against DepBank."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 65
                            }
                        ],
                        "text": "The lexical category set used by the supertagger is described in Clark and Curran (2004a) and Curran, Clark, and Vadas (2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 233
                            }
                        ],
                        "text": "\u2026because some of the tokenization is different between DepBank and CCGbank (even though both resources are based on the Penn Treebank), and there are some sentences in DepBank which have been significantly shortened (for no apparent reason) compared to the original Penn Treebank sentences."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 170
                            }
                        ],
                        "text": "Neither of these constraints guarantee a normal-form derivation, but they are both effective at reducing the size of the charts, which can greatly increase parser speed (Clark and Curran 2004a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 1
                            }
                        ],
                        "text": "The percentage of sentences without an analysis is now over 6% (with automatically assigned POS tags), but the parser is extremely fast, processing over 30 sentences per second."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9096784,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "662cc508b94f3a99585053caff7d0d5a63857700",
            "isKey": false,
            "numCitedBy": 166,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the role of supertagging in a wide-coverage CCG parser which uses a log-linear model to select an analysis. The supertagger reduces the derivation space over which model estimation is performed, reducing the space required for discriminative training. It also dramatically increases the speed of the parser. We show that large increases in speed can be obtained by tightly integrating the supertagger with the CCG grammar and parser. This is the first work we are aware of to successfully integrate a supertagger with a full parser which uses an automatically extracted grammar. We also further reduce the derivation space using constraints on category combination. The result is an accurate wide-coverage CCG parser which is an order of magnitude faster than comparable systems for other linguistically motivated formalisms."
            },
            "slug": "The-Importance-of-Supertagging-for-Wide-Coverage-Clark-Curran",
            "title": {
                "fragments": [],
                "text": "The Importance of Supertagging for Wide-Coverage CCG Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This paper describes the role of supertagging in a wide-coverage CCG parser which uses a log-linear model to select an analysis and shows that large increases in speed can be obtained by tightly integrating the supertagger with the CCG grammar and parser."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144523372"
                        ],
                        "name": "S. Clark",
                        "slug": "S.-Clark",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733593"
                        ],
                        "name": "J. Curran",
                        "slug": "J.-Curran",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Curran",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Curran"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 27
                            }
                        ],
                        "text": "11 The results reported in Clark and Curran (2007) differ from those here because Clark and Curran used the normal-form model and Viterbi decoder."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 1801055,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6cc701c93616b8308877a569db4a2e880de74cbc",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A key question facing the parsing community is how to compare parsers which use different grammar formalisms and produce different output. Evaluating a parser on the same resource used to create it can lead to non-comparable accuracy scores and an over-optimistic view of parser performance. In this paper we evaluate a CCG parser on DepBank, and demonstrate the difficulties in converting the parser output into DepBank grammatical relations. In addition we present a method for measuring the effectiveness of the conversion, which provides an upper bound on parsing accuracy. The CCG parser obtains an F-score of 81.9% on labelled dependencies, against an upper bound of 84.8%. We compare the CCG parser against the RASP parser, outperforming RASP by over 5% overall and on the majority of dependency types."
            },
            "slug": "Formalism-Independent-Parser-Evaluation-with-CCG-Clark-Curran",
            "title": {
                "fragments": [],
                "text": "Formalism-Independent Parser Evaluation with CCG and DepBank"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper evaluates a CCG parser on DepBank, and demonstrates the difficulties in converting the parser output into DepBank grammatical relations, and presents a method for measuring the effectiveness of the conversion, which provides an upper bound on parsing accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145177220"
                        ],
                        "name": "Mark Johnson",
                        "slug": "Mark-Johnson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 256
                            }
                        ],
                        "text": "Two possible extensions, which we have not investigated, include defining dependency features which account for all three elements of the triple in a PP-attachment (Collins and Brooks 1995), and defining a rule feature which includes the grandparent node (Johnson 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 177
                            }
                        ],
                        "text": "\u2026increasing the amount of lexicalization; generating a lexical category at its maximal projection; conditioning the probability of a rule instantiation on the grandparent node (Johnson 1998); adding features designed to deal with coordination; and adding distance to the dependency features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 132
                            }
                        ],
                        "text": "The idea of supertagging originated with LTAG; however, in contrast to the CCG grammars used in this article, the automatically extracted LTAG grammars have, as yet, been too large to enable effective supertagging (as discussed in the previous section)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 7978249,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c9f553e723a40a6713453b734b552c1928bf52b",
            "isKey": false,
            "numCitedBy": 441,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The kinds of tree representations used in a treebank corpus can have a dramatic effect on performance of a parser based on the PCFG estimated from that corpus, causing the estimated likelihood of a tree to differ substantially from its frequency in the training corpus. This paper points out that the Penn II treebank representations are of the kind predicted to have such an effect, and describes a simple node relabeling transformation that improves a treebank PCFG-based parser's average precision and recall by around 8%, or approximately half of the performance difference between a simple PCFG model and the best broad-coverage parsers available today. This performance variation comes about because any PCFG, and hence the corpus of trees from which the PCFG is induced, embodies independence assumptions about the distribution of words and phrases. The particular independence assumptions implicit in a tree representation can be studied theoretically and investigated empirically by means of a tree transformation / detransformation process."
            },
            "slug": "PCFG-Models-of-Linguistic-Tree-Representations-Johnson",
            "title": {
                "fragments": [],
                "text": "PCFG Models of Linguistic Tree Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A simple node relabeling transformation is described that improves a treebank PCFG-based parser's average precision and recall by around 8%, or approximately half of the performance difference between a simple PCFG model and the best broad-coverage parsers available today."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757166"
                        ],
                        "name": "Kenji Sagae",
                        "slug": "Kenji-Sagae",
                        "structuredName": {
                            "firstName": "Kenji",
                            "lastName": "Sagae",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenji Sagae"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784914"
                        ],
                        "name": "A. Lavie",
                        "slug": "A.-Lavie",
                        "structuredName": {
                            "firstName": "Alon",
                            "lastName": "Lavie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lavie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Sagae and Lavie (2005) is a classifier-based linear time parser."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 173611,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a350dc635ea3d0fd6dc8c0224fd8d6b55cc03271",
            "isKey": false,
            "numCitedBy": 148,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a classifier-based parser that produces constituent trees in linear time. The parser uses a basic bottom-up shift-reduce algorithm, but employs a classifier to determine parser actions instead of a grammar. This can be seen as an extension of the deterministic dependency parser of Nivre and Scholz (2004) to full constituent parsing. We show that, with an appropriate feature set used in classification, a very simple one-path greedy parser can perform at the same level of accuracy as more complex parsers. We evaluate our parser on section 23 of the WSJ section of the Penn Treebank, and obtain precision and recall of 87.54% and 87.61%, respectively."
            },
            "slug": "A-Classifier-Based-Parser-with-Linear-Run-Time-Sagae-Lavie",
            "title": {
                "fragments": [],
                "text": "A Classifier-Based Parser with Linear Run-Time Complexity"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that, with an appropriate feature set used in classification, a very simple one-path greedy parser can perform at the same level of accuracy as more complex parsers."
            },
            "venue": {
                "fragments": [],
                "text": "IWPT"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 63
                            }
                        ],
                        "text": "First, they are used for parser evaluation: The accuracy of a parsing model is measured using precision and recall over CCG predicate\u2013argument dependencies."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 90
                            }
                        ],
                        "text": "The probabilities of the dependencies are estimated using relative frequencies, following Collins (1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 150
                            }
                        ],
                        "text": "In order to deal with the problem of the additional, nonstandard CCG derivations, a conditional model of dependency structures is presented, based on Collins (1996), in which the dependencies are modeled directly and derivations are not modeled at all."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12615602,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3764baa7465201f054083d02b58fa75f883c4461",
            "isKey": false,
            "numCitedBy": 736,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words. Tests using Wall Street Journal data show that the method performs at least as well as SPATTER (Magerman 95; Jelinek et al. 94), which has the best published results for a statistical parser on this task. The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes. With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy."
            },
            "slug": "A-New-Statistical-Parser-Based-on-Bigram-Lexical-Collins",
            "title": {
                "fragments": [],
                "text": "A New Statistical Parser Based on Bigram Lexical Dependencies"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A new statistical parser which is based on probabilities of dependencies between head-words in the parse tree, which trains on 40,000 sentences in under 15 minutes and can be improved to over 200 sentences a minute with negligible loss in accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793475"
                        ],
                        "name": "A. Ratnaparkhi",
                        "slug": "A.-Ratnaparkhi",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ratnaparkhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ratnaparkhi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3231298,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e6140a793a4554806eb39d15c018d8f782d2ac1e",
            "isKey": false,
            "numCitedBy": 287,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a machine learning system for parsing natural language that learns from manually parsed example sentences, and parses unseen data at state-of-the-art accuracies. Its machine learning technology, based on the maximum entropy framework, is highly reusable and not specific to the parsing problem, while the linguistic hints that it uses to learn can be specified concisely. It therefore requires a minimal amount of human effort and linguistic knowledge for its construction. In practice, the running time of the parser on a test sentence is linear with respect to the sentence length. We also demonstrate that the parser can train from other domains without modification to the modeling framework or the linguistic hints it uses to learn. Furthermore, this paper shows that research into rescoring the top 20 parses returned by the parser might yield accuracies dramatically higher than the state-of-the-art."
            },
            "slug": "Learning-to-Parse-Natural-Language-with-Maximum-Ratnaparkhi",
            "title": {
                "fragments": [],
                "text": "Learning to Parse Natural Language with Maximum Entropy Models"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A machine learning system for parsing natural language that learns from manually parsed example sentences, and parses unseen data at state-of-the-art accuracies, and it is demonstrated that the parser can train from other domains without modification to the modeling framework or the linguistic hints it uses to learn."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145557710"
                        ],
                        "name": "A. Cahill",
                        "slug": "A.-Cahill",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Cahill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cahill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152422954"
                        ],
                        "name": "Michael Burke",
                        "slug": "Michael-Burke",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Burke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Burke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1402022856"
                        ],
                        "name": "Ruth O'Donovan",
                        "slug": "Ruth-O'Donovan",
                        "structuredName": {
                            "firstName": "Ruth",
                            "lastName": "O'Donovan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ruth O'Donovan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7519068"
                        ],
                        "name": "Josef van Genabith",
                        "slug": "Josef-van-Genabith",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "van Genabith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josef van Genabith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144315616"
                        ],
                        "name": "Andy Way",
                        "slug": "Andy-Way",
                        "structuredName": {
                            "firstName": "Andy",
                            "lastName": "Way",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andy Way"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 132
                            }
                        ],
                        "text": "Statistical parsers have been developed for TAG (Chiang 2000; Sarkar and Joshi 2003), LFG (Riezler et al. 2002; Kaplan et al. 2004; Cahill et al. 2004), and HPSG (Toutanova et al. 2002; Toutanova, Markova, and Manning 2004; Miyao and Tsujii 2004; Malouf and van Noord 2004), among others."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 160
                            }
                        ],
                        "text": "Penn Treebank conversions have also been carried out for other linguistic formalisms, including TAG (Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000), LFG (Burke et al. 2004), and HPSG (Miyao, Ninomiya, and Tsujii 2004)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 87
                            }
                        ],
                        "text": "Long-distance dependency resolution in automatically acquired wide-coverage PCFG-based LFG approximations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 97
                            }
                        ],
                        "text": "Of course some of these advantages could be obtained with other grammar formalisms, such as TAG, LFG, and HPSG, although CCG is especially well-suited to analysing coordination and long-range dependencies."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1116,
                                "start": 6
                            }
                        ],
                        "text": "2004; Cahill et al. 2004), and HPSG (Toutanova et al. 2002; Toutanova, Markova, and Manning 2004; Miyao and Tsujii 2004; Malouf and van Noord 2004), among others. The motivation for using these formalisms is that many NLP tasks, such as Machine Translation, Information Extraction, and Question Answering, could benefit from the more sophisticated linguistic analyses they provide. The formalismmost closely related to CCG from this list is TAG. TAG grammars have been automatically extracted from the Penn Treebank, using techniques similar to those used by Hockenmaier (Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000). Also, the supertagging ideawhich is central to the efficiency of the CCG parser originatedwith TAG (Bangalore and Joshi 1999). Chen et al. (2002) describe the results of reranking the output of an HMM supertagger using an automatically extracted LTAG. The accuracy for a single supertag per word is slightly over 80%. This figure is increased to over 91% when the tagger is run in n-best mode, but at a considerable cost in ambiguity, with 8 supertags per word. Nasr and Rambow (2004) investigate the potential impact of LTAG supertagging on parsing speed and accuracy by performing a number of oracle experiments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 0
                            }
                        ],
                        "text": "Cahill et al. (2004) evaluate an LFG parser, which uses an automatically extracted grammar, against DepBank. Miyao and Tsujii (2004) evaluate their HPSG parser against PropBank (Palmer, Gildea, and Kingsbury 2005). Kaplan et al. (2004) compare the Collins parser with the Parc LFG parser by mapping Penn Treebank parses into the dependencies of DepBank, claiming that the LFG parser is more accurate with only a slight reduction in speed."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 32
                            }
                        ],
                        "text": "The main difference is that the LFG grammar is hand-built, resulting in less ambiguity than an automatically extracted grammar and thus requiring fewer resources for model estimation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 0
                            }
                        ],
                        "text": "Cahill et al. (2004) evaluate an LFG parser, which uses an automatically extracted grammar, against DepBank. Miyao and Tsujii (2004) evaluate their HPSG parser against PropBank (Palmer, Gildea, and Kingsbury 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 71
                            }
                        ],
                        "text": "Kaplan et al. (2004) present similar work to ours in the context of an LFG grammar for English."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 68
                            }
                        ],
                        "text": "Geman and Johnson (2002) propose a similar method in the context of LFG parsing; an implementation is described in Kaplan et al. (2004)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 453,
                                "start": 0
                            }
                        ],
                        "text": "Cahill et al. (2004) evaluate an LFG parser, which uses an automatically extracted grammar, against DepBank. Miyao and Tsujii (2004) evaluate their HPSG parser against PropBank (Palmer, Gildea, and Kingsbury 2005). Kaplan et al. (2004) compare the Collins parser with the Parc LFG parser by mapping Penn Treebank parses into the dependencies of DepBank, claiming that the LFG parser is more accurate with only a slight reduction in speed. Preiss (2003) compares the parsers of Collins and Charniak, the grammatical relations finder of Buchholz, Veenstra, and Daelemans (1999), and the Briscoe and Carroll (2002) parser, using the gold-standard grammatical relations (GRs) from Carroll, Briscoe, and Sanfilippo (1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 777,
                                "start": 6
                            }
                        ],
                        "text": "2004; Cahill et al. 2004), and HPSG (Toutanova et al. 2002; Toutanova, Markova, and Manning 2004; Miyao and Tsujii 2004; Malouf and van Noord 2004), among others. The motivation for using these formalisms is that many NLP tasks, such as Machine Translation, Information Extraction, and Question Answering, could benefit from the more sophisticated linguistic analyses they provide. The formalismmost closely related to CCG from this list is TAG. TAG grammars have been automatically extracted from the Penn Treebank, using techniques similar to those used by Hockenmaier (Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000). Also, the supertagging ideawhich is central to the efficiency of the CCG parser originatedwith TAG (Bangalore and Joshi 1999). Chen et al. (2002) describe the results of reranking the output of an HMM supertagger using an automatically extracted LTAG."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Cahill et al. (2004) evaluate an LFG parser, which uses an automatically extracted grammar, against DepBank."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 90
                            }
                        ],
                        "text": "Statistical parsers have been developed for TAG (Chiang 2000; Sarkar and Joshi 2003), LFG (Riezler et al. 2002; Kaplan et al. 2004; Cahill et al. 2004), and HPSG (Toutanova et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 62
                            }
                        ],
                        "text": "Kaplan et al. (2004) compare the Collins parser with the Parc LFG parser by mapping Penn Treebank parses into the dependencies of DepBank, claiming that the LFG parser is more accurate with only a slight reduction in speed."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 717,
                                "start": 0
                            }
                        ],
                        "text": "Cahill et al. (2004) evaluate an LFG parser, which uses an automatically extracted grammar, against DepBank. Miyao and Tsujii (2004) evaluate their HPSG parser against PropBank (Palmer, Gildea, and Kingsbury 2005). Kaplan et al. (2004) compare the Collins parser with the Parc LFG parser by mapping Penn Treebank parses into the dependencies of DepBank, claiming that the LFG parser is more accurate with only a slight reduction in speed. Preiss (2003) compares the parsers of Collins and Charniak, the grammatical relations finder of Buchholz, Veenstra, and Daelemans (1999), and the Briscoe and Carroll (2002) parser, using the gold-standard grammatical relations (GRs) from Carroll, Briscoe, and Sanfilippo (1998). The Penn Treebank trees of the Collins and Charniak parsers, and the GRs of the Buchholz parser, are mapped into the required grammatical relations, with the result that the GR finder of Buchholz is the most accurate."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 73
                            }
                        ],
                        "text": "Briscoe and Carroll (2006) give a rough comparison of RASP with the Parc LFG parser (Kaplan et al. 2004) on DepBank, obtaining similar results overall, but acknowledging that the results are not strictly comparable because of the different annotation schemes used."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5825251,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e05bbfd8b36ec5d5122c78d7e415bbfaa4d0e3e",
            "isKey": true,
            "numCitedBy": 145,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper shows how finite approximations of long distance dependency (LDD) resolution can be obtained automatically for wide-coverage, robust, probabilistic Lexical-Functional Grammar (LFG) resources acquired from treebanks. We extract LFG subcategorisation frames and paths linking LDD reentrancies from f-structures generated automatically for the Penn-II treebank trees and use them in an LDD resolution algorithm to parse new text. Unlike (Collins, 1999; Johnson, 2000), in our approach resolution of LDDs is done at f-structure (attribute-value structure representations of basic predicate-argument or dependency structure) without empty productions, traces and coindexation in CFG parse trees. Currently our best automatically induced grammars achieve 80.97% f-score for f-structures parsing section 23 of the WSJ part of the Penn-II treebank and evaluating against the DCU 1051 and 80.24% against the PARC 700 Dependency Bank (King et al., 2003), performing at the same or a slightly better level than state-of-the-art hand-crafted grammars (Kaplan et al., 2004)."
            },
            "slug": "Long-Distance-Dependency-Resolution-in-Acquired-LFG-Cahill-Burke",
            "title": {
                "fragments": [],
                "text": "Long-Distance Dependency Resolution in Automatically Acquired Wide-Coverage PCFG-Based LFG Approximations"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper shows how finite approximations of long distance dependency (LDD) resolution can be obtained automatically for wide-coverage, robust, probabilistic Lexical-Functional Grammar (LFG) resources acquired from treebanks."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145043214"
                        ],
                        "name": "Jason Eisner",
                        "slug": "Jason-Eisner",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Eisner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason Eisner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Eisner (1996) describes a technique for eliminating spurious ambiguity entirely, by defining exactly one normal-form derivation for each semantic equivalence class of derivations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 137
                            }
                        ],
                        "text": "They are popular because of the ease with which complex discriminating features can be included in the model, and have been shown to give good performance across a range of NLP tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 100
                            }
                        ],
                        "text": "The first, following Hockenmaier (2003a), is to define a model in terms of normal-form derivations (Eisner 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3205175,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "26b0fc4bd7ec9f3aa25b205b9f52ee6c380748b2",
            "isKey": true,
            "numCitedBy": 111,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Under categorial grammars that have powerful rules like composition, a simple n-word sentence can have exponentially many parses. Generating all parses is inefficient and obscures whatever true semantic ambiguities are in the input. This paper addresses the problem for a fairly general form of Combinatory Categorial Grammar, by means of an efficient, correct, and easy to implement normal-form parsing technique. The parser is proved to find exactly one parse in each semantic equivalence class of allowable parses; that is, spurious ambiguity (as carefully defined) is shown to be both safely and completely eliminated."
            },
            "slug": "Efficient-Normal-Form-Parsing-for-Combinatory-Eisner",
            "title": {
                "fragments": [],
                "text": "Efficient Normal-Form Parsing for Combinatory Categorial Grammar"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The parser is proved to find exactly one parse in each semantic equivalence class of allowable parses; that is, spurious ambiguity is shown to be both safely and completely eliminated."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145287425"
                        ],
                        "name": "David Chiang",
                        "slug": "David-Chiang",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Chiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Chiang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 60
                            }
                        ],
                        "text": "Nasr and Rambow (2004) investigate the potential impact of LTAG supertagging on parsing speed and accuracy by performing a number of oracle experiments."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 97
                            }
                        ],
                        "text": "Also, the supertagging idea which is central to the efficiency of the CCG parser originated with TAG (Bangalore and Joshi 1999)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 46
                            }
                        ],
                        "text": "One possible way to increase the accuracy of LTAG supertagging is to use a Maximum Entropy, rather than HMM, tagger (as discussed previously), but this is likely to result in an improvement of only a few percentage points."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 49
                            }
                        ],
                        "text": "Statistical parsers have been developed for TAG (Chiang 2000; Sarkar and Joshi 2003), LFG (Riezler et al. 2002; Kaplan et al. 2004; Cahill et al. 2004), and HPSG (Toutanova et al. 2002; Toutanova, Markova, and Manning 2004; Miyao and Tsujii 2004; Malouf and van Noord 2004), among others."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 96
                            }
                        ],
                        "text": "Penn Treebank conversions have also been carried out for other linguistic formalisms, including TAG (Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000), LFG (Burke et al. 2004), and HPSG (Miyao, Ninomiya, and Tsujii 2004)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 92
                            }
                        ],
                        "text": "Of course some of these advantages could be obtained with other grammar formalisms, such as TAG, LFG, and HPSG, although CCG is especially well-suited to analysing coordination and long-range dependencies."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 120
                            }
                        ],
                        "text": "Chen et al. (2002) describe the results of reranking the output of an HMM supertagger using an automatically extracted LTAG."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 42
                            }
                        ],
                        "text": "The idea of supertagging originated with LTAG; however, in contrast to the CCG grammars used in this article, the automatically extracted LTAG grammars have, as yet, been too large to enable effective supertagging (as discussed in the previous section)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "In Proceedings of the TAG+ Workshop, pages 259\u2013268, Venice, Italy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "\u00a9 2007 Association for Computational Linguistics"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 145
                            }
                        ],
                        "text": "In Miyao and Tsujii (2003b, 2003a) log-linear models are developed for automatically extracted grammars for Lexicalized Tree Adjoining Grammar (LTAG) and Head Driven Phrase Structure Grammar (HPSG)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 271,
                                "start": 268
                            }
                        ],
                        "text": "The Maximum Entropy supertagger we use could also be applied to Miyao and Tsujii\u2019s grammars, although whether similar performance would be obtained depends on the characteristics of the grammar; see subsequent sections for more discussion of this issue in relation to LTAG."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 60
                            }
                        ],
                        "text": "The formalism most closely related to CCG from this list is TAG."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "In Proceedings of the TAG+ Workshop, pages 19\u201324, Venice, Italy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 24
                            }
                        ],
                        "text": "Automated extraction of TAGS from the Penn Treebank."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 32
                            }
                        ],
                        "text": "Interestingly, the accuracy of LTAG supertaggers using automatically extracted grammars is significantly below the accuracy of the CCG supertagger."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 201
                            }
                        ],
                        "text": "Because there is so much syntactic information in lexical categories, the parser is required to do less work once the lexical categories have been assigned; hence Srinivas and Joshi, in the context of TAG, refer to supertagging as almost parsing."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 90
                            }
                        ],
                        "text": "Miyao and Tsujii (2003a) present another log-linear model for an automatically extracted LTAG which uses a simple unigram model of the elementary trees together with a loglinear model of the attachments."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "TAG grammars have been automatically extracted from the Penn Treebank, using techniques similar to those used by Hockenmaier (Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "In Proceedings of the TAG+7 Workshop, Vancouver, Canada."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1471139,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "351752e0080d7635aef227b5d0bd6461cf0b14bd",
            "isKey": true,
            "numCitedBy": 205,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss the advantages of lexicalized tree-adjoining grammar as an alternative to lexicalized PCFG for statistical parsing, describing the induction of a probabilistic LTAG model from the Penn Treebank and evaluating its parsing performance. We find that this induction method is an improvement over the EM-based method of (Hwa, 1998), and that the induced model yields results comparable to lexicalized PCFG."
            },
            "slug": "Statistical-Parsing-with-an-Automatically-Extracted-Chiang",
            "title": {
                "fragments": [],
                "text": "Statistical Parsing with an Automatically-Extracted Tree Adjoining Grammar"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work describes the induction of a probabilistic LTAG model from the Penn Treebank and finds that this induction method is an improvement over the EM-based method of (Hwa, 1998), and that the induced model yields results comparable to lexicalized PCFG."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768065"
                        ],
                        "name": "Yusuke Miyao",
                        "slug": "Yusuke-Miyao",
                        "structuredName": {
                            "firstName": "Yusuke",
                            "lastName": "Miyao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yusuke Miyao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737901"
                        ],
                        "name": "Junichi Tsujii",
                        "slug": "Junichi-Tsujii",
                        "structuredName": {
                            "firstName": "Junichi",
                            "lastName": "Tsujii",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junichi Tsujii"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Miyao and Tsujii (2005) address the issue of practical estimation using an automatically extracted HPSG grammar."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7828046,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6e9f4338f7b50ca9bb6d7dd23cefb6bfbfabc37f",
            "isKey": false,
            "numCitedBy": 136,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper reports the development of log-linear models for the disambiguation in wide-coverage HPSG parsing. The estimation of log-linear models requires high computational cost, especially with wide-coverage grammars. Using techniques to reduce the estimation cost, we trained the models using 20 sections of Penn Tree-bank. A series of experiments empirically evaluated the estimation techniques, and also examined the performance of the disambiguation models on the parsing of real-world sentences."
            },
            "slug": "Probabilistic-Disambiguation-Models-for-HPSG-Miyao-Tsujii",
            "title": {
                "fragments": [],
                "text": "Probabilistic Disambiguation Models for Wide-Coverage HPSG Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "The development of log-linear models for the disambiguation in wide-coverage HPSG parsing using techniques to reduce the estimation cost and a series of experiments empirically evaluated the estimation techniques, and also examined the performance of the dis Ambiguation models on the parsing of real-world sentences."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143643017"
                        ],
                        "name": "R. Levy",
                        "slug": "R.-Levy",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Levy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Levy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2029816,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "502a0987e09450129a4ab22492e69448a08bedc9",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a linguistically-motivated algorithm for reconstructing nonlocal dependency in broad-coverage context-free parse trees derived from treebanks. We use an algorithm based on loglinear classifiers to augment and reshape context-free trees so as to reintroduce underlying nonlocal dependencies lost in the context-free approximation. We find that our algorithm compares favorably with prior work on English using an existing evaluation metric, and also introduce and argue for a new dependency-based evaluation metric. By this new evaluation metric our algorithm achieves 60% error reduction on gold-standard input trees and 5% error reduction on state-of-the-art machine-parsed input trees, when compared with the best previous work. We also present the first results on non-local dependency reconstruction for a language other than English, comparing performance on English and German. Our new evaluation metric quantitatively corroborates the intuition that in a language with freer word order, the surface dependencies in context-free parse trees are a poorer approximation to underlying dependency structure."
            },
            "slug": "Deep-Dependencies-from-Context-Free-Statistical-the-Levy-Manning",
            "title": {
                "fragments": [],
                "text": "Deep Dependencies from Context-Free Statistical Parsers: Correcting the Surface Dependency Approximation"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A linguistically-motivated algorithm for reconstructing nonlocal dependency in broad-coverage context-free parse trees derived from treebanks is presented and a new dependency-based evaluation metric is introduced, which quantitatively corroborates the intuition that in a language with freer word order, the surface dependencies in context- free parse trees are a poorer approximation to underlying dependency structure."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803660"
                        ],
                        "name": "R. Kaplan",
                        "slug": "R.-Kaplan",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Kaplan",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kaplan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3289329"
                        ],
                        "name": "S. Riezler",
                        "slug": "S.-Riezler",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Riezler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Riezler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2142738"
                        ],
                        "name": "Tracy Holloway King",
                        "slug": "Tracy-Holloway-King",
                        "structuredName": {
                            "firstName": "Tracy",
                            "lastName": "King",
                            "middleNames": [
                                "Holloway"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tracy Holloway King"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2265996"
                        ],
                        "name": "John T. Maxwell",
                        "slug": "John-T.-Maxwell",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Maxwell",
                            "middleNames": [
                                "T."
                            ],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John T. Maxwell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12755257"
                        ],
                        "name": "A. Vasserman",
                        "slug": "A.-Vasserman",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Vasserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vasserman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2953252"
                        ],
                        "name": "Dick Crouch",
                        "slug": "Dick-Crouch",
                        "structuredName": {
                            "firstName": "Dick",
                            "lastName": "Crouch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dick Crouch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6967888,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "29899547c993f30a9afcc3514ef55358d45d6b97",
            "isKey": false,
            "numCitedBy": 138,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : This paper reports some experiments that Compare the accuracy and performance of two stochastic parsing systems. The currently popular Collins parser is a shallow parser whose output contains more detailed semantically relevant information than other such parsers. The XLE parser is a deep-parsing system that couples a Lexical Functional Grammar to a log- linear disambiguation component and provides much richer representations theory. We measured the accuracy of both systems against a gold standard of the PARC 700 dependency bank, and also measured their processing times. We found the deep-parsing system to be more accurate than the Collins parser with only a slight reduction in parsing speed."
            },
            "slug": "Speed-and-Accuracy-in-Shallow-and-Deep-Stochastic-Kaplan-Riezler",
            "title": {
                "fragments": [],
                "text": "Speed and Accuracy in Shallow and Deep Stochastic Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This paper reports some experiments that Compare the accuracy and performance of two stochastic parsing systems and found the deep-parsing system to be more accurate than the Collins parser with only a slight reduction in parsing speed."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749837"
                        ],
                        "name": "Eugene Charniak",
                        "slug": "Eugene-Charniak",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Charniak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Charniak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 101
                            }
                        ],
                        "text": "In this article we are concerned with the syntactic component; see Steedman (2000) for how a semantic interpretation can be composed during a syntactic derivation, and also Bos et al. (2004) for how semantic interpretations can be built for newspaper text using the wide-coverage parser described in\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 157
                            }
                        ],
                        "text": "Long-range dependencies are relatively common in text such as newspaper text, but are typically not recovered by treebank parsers such as Collins (2003) and Charniak (2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 538122,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "76d5e3fa888bee872b7adb7fa810089aa8ab1d58",
            "isKey": false,
            "numCitedBy": 1855,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trained and tested on the previously established [5, 9, 10, 15, 17] \"standard\" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. The major technical innovation is the use of a \"maximum-entropy-inspired\" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head."
            },
            "slug": "A-Maximum-Entropy-Inspired-Parser-Charniak",
            "title": {
                "fragments": [],
                "text": "A Maximum-Entropy-Inspired Parser"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less and 89.5% when trained and tested on the previously established sections of the Wall Street Journal treebank is presented."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733593"
                        ],
                        "name": "J. Curran",
                        "slug": "J.-Curran",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Curran",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Curran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144523372"
                        ],
                        "name": "S. Clark",
                        "slug": "S.-Clark",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699885"
                        ],
                        "name": "David Vadas",
                        "slug": "David-Vadas",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Vadas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Vadas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 0
                            }
                        ],
                        "text": "Curran, Clark, and Vadas (2006) investigate the improvement obtained from using the forward\u2013backward algorithm, and also address the drop in supertagger accuracy when using automatically assigned POS tags."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 3
                            }
                        ],
                        "text": "In Curran, Clark, and Vadas (2006) we investigate using the multi-tagging techniques developed for the supertagger at the POS tag level."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 94
                            }
                        ],
                        "text": "The lexical category set used by the supertagger is described in Clark and Curran (2004a) and Curran, Clark, and Vadas (2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12418427,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8af70241b409aa12af225b9e17b346fbd101d2bc",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "With performance above 97% accuracy for newspaper text, part of speech (POS) tagging might be considered a solved problem. Previous studies have shown that allowing the parser to resolve POS tag ambiguity does not improve performance. However, for grammar formalisms which use more fine-grained grammatical categories, for example TAG and CCG, tagging accuracy is much lower. In fact, for these formalisms, premature ambiguity resolution makes parsing infeasible.We describe a multi-tagging approach which maintains a suitable level of lexical category ambiguity for accurate and efficient CCG parsing. We extend this multi-tagging approach to the POS level to overcome errors introduced by automatically assigned POS tags. Although POS tagging accuracy seems high, maintaining some POS tag ambiguity in the language processing pipeline results in more accurate CCG supertagging."
            },
            "slug": "Multi-Tagging-for-Lexicalized-Grammar-Parsing-Curran-Clark",
            "title": {
                "fragments": [],
                "text": "Multi-Tagging for Lexicalized-Grammar Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "This work describes a multi-tagging approach which maintains a suitable level of lexical category ambiguity for accurate and efficient CCG parsing and extends this approach to the POS level to overcome errors introduced by automatically assigned POS tags."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35313721"
                        ],
                        "name": "S. Bangalore",
                        "slug": "S.-Bangalore",
                        "structuredName": {
                            "firstName": "Srinivas",
                            "lastName": "Bangalore",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bangalore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714374"
                        ],
                        "name": "A. Joshi",
                        "slug": "A.-Joshi",
                        "structuredName": {
                            "firstName": "Aravind",
                            "lastName": "Joshi",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Joshi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 60
                            }
                        ],
                        "text": "Nasr and Rambow (2004) investigate the potential impact of LTAG supertagging on parsing speed and accuracy by performing a number of oracle experiments."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 102
                            }
                        ],
                        "text": "Also, the supertagging idea which is central to the efficiency of the CCG parser originated with TAG (Bangalore and Joshi 1999)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 18
                            }
                        ],
                        "text": "Estimating a log-linear model involves computing expectations of feature values."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 46
                            }
                        ],
                        "text": "One possible way to increase the accuracy of LTAG supertagging is to use a Maximum Entropy, rather than HMM, tagger (as discussed previously), but this is likely to result in an improvement of only a few percentage points."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "Statistical parsers have been developed for TAG (Chiang 2000; Sarkar and Joshi 2003), LFG (Riezler et al. 2002; Kaplan et al. 2004; Cahill et al. 2004), and HPSG (Toutanova et al. 2002; Toutanova, Markova, and Manning 2004; Miyao and Tsujii 2004; Malouf and van Noord 2004), among others."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 96
                            }
                        ],
                        "text": "Penn Treebank conversions have also been carried out for other linguistic formalisms, including TAG (Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000), LFG (Burke et al. 2004), and HPSG (Miyao, Ninomiya, and Tsujii 2004)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 92
                            }
                        ],
                        "text": "Of course some of these advantages could be obtained with other grammar formalisms, such as TAG, LFG, and HPSG, although CCG is especially well-suited to analysing coordination and long-range dependencies."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 120
                            }
                        ],
                        "text": "Chen et al. (2002) describe the results of reranking the output of an HMM supertagger using an automatically extracted LTAG."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 42
                            }
                        ],
                        "text": "The idea of supertagging originated with LTAG; however, in contrast to the CCG grammars used in this article, the automatically extracted LTAG grammars have, as yet, been too large to enable effective supertagging (as discussed in the previous section)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 41
                            }
                        ],
                        "text": "Later we show that use of a supertagger (Bangalore and Joshi 1999) prior to parsing can produce an extremely efficient parser."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "In Proceedings of the TAG+ Workshop, pages 259\u2013268, Venice, Italy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 145
                            }
                        ],
                        "text": "In Miyao and Tsujii (2003b, 2003a) log-linear models are developed for automatically extracted grammars for Lexicalized Tree Adjoining Grammar (LTAG) and Head Driven Phrase Structure Grammar (HPSG)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 135
                            }
                        ],
                        "text": "One of Miyao and Tsujii\u2019s motivations is to model predicate\u2013argument dependencies, including long-range dependencies, which was one of the original motivations of the wide-coverage CCG parsing project."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 271,
                                "start": 268
                            }
                        ],
                        "text": "The Maximum Entropy supertagger we use could also be applied to Miyao and Tsujii\u2019s grammars, although whether similar performance would be obtained depends on the characteristics of the grammar; see subsequent sections for more discussion of this issue in relation to LTAG."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 153
                            }
                        ],
                        "text": "For all of the figures reported on Section 23, unless stated otherwise, we chose settings for the various parameters which resulted in a coverage of 99.6%."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 60
                            }
                        ],
                        "text": "The formalism most closely related to CCG from this list is TAG."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 111
                            }
                        ],
                        "text": "Second, it greatly increases the efficiency of the parser, which was the original motivation for supertagging (Bangalore and Joshi 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "In Proceedings of the TAG+ Workshop, pages 19\u201324, Venice, Italy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 24
                            }
                        ],
                        "text": "Automated extraction of TAGS from the Penn Treebank."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 32
                            }
                        ],
                        "text": "Interestingly, the accuracy of LTAG supertaggers using automatically extracted grammars is significantly below the accuracy of the CCG supertagger."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 201
                            }
                        ],
                        "text": "Because there is so much syntactic information in lexical categories, the parser is required to do less work once the lexical categories have been assigned; hence Srinivas and Joshi, in the context of TAG, refer to supertagging as almost parsing."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 90
                            }
                        ],
                        "text": "Miyao and Tsujii (2003a) present another log-linear model for an automatically extracted LTAG which uses a simple unigram model of the elementary trees together with a loglinear model of the attachments."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "TAG grammars have been automatically extracted from the Penn Treebank, using techniques similar to those used by Hockenmaier (Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "In Proceedings of the TAG+7 Workshop, Vancouver, Canada."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 70
                            }
                        ],
                        "text": "In this article we take a different approach, by using a supertagger (Bangalore and Joshi 1999) to perform step one."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3143783,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "bcd7cc92f6162f3fcc29c1c109acca8bef27e1e9",
            "isKey": true,
            "numCitedBy": 435,
            "numCiting": 97,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we have proposed novel methods for robust parsing that integrate the flexibility of linguistically motivated lexical descriptions with the robustness of statistical techniques. Our thesis is that the computation of linguistic structure can be localized if lexical items are associated with rich descriptions (supertags) that impose complex constraints in a local context. The supertags are designed such that only those elements on which the lexical item imposes constraints appear within a given supertag. Further, each lexical item is associated with as many supertags as the number of different syntactic contexts in which the lexical item can appear. This makes the number of different descriptions for each lexical item much larger than when the descriptions are less complex, thus increasing the local ambiguity for a parser. But this local ambiguity can be resolved by using statistical distributions of supertag co-occurrences collected from a corpus of parses. We have explored these ideas in the context of the Lexicalized Tree-Adjoining Grammar (LTAG) framework. The supertags in LTAG combine both phrase structure information and dependency information in a single representation. Supertag disambiguation results in a representation that is effectively a parse (an almost parse), and the parser need \"only\" combine the individual supertags. This method of parsing can also be used to parse sentence fragments such as in spoken utterances where the disambiguated supertag sequence may not combine into a single structure."
            },
            "slug": "Supertagging:-An-Approach-to-Almost-Parsing-Bangalore-Joshi",
            "title": {
                "fragments": [],
                "text": "Supertagging: An Approach to Almost Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "Novel methods for robust parsing that integrate the flexibility of linguistically motivated lexical descriptions with the robustness of statistical techniques are proposed."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793218"
                        ],
                        "name": "D. Gildea",
                        "slug": "D.-Gildea",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Gildea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gildea"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 196105,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee7e21dd09949a5a53b39c13fca9cd3d55e2bc50",
            "isKey": false,
            "numCitedBy": 326,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Most work in statistical parsing has focused on a single corpus: the Wall Street Journal portion of the Penn Treebank. While this has allowed for quantitative comparison of parsing techniques, it has left open the question of how other types of text might a ect parser performance, and how portable parsing models are across corpora. We examine these questions by comparing results for the Brown and WSJ corpora, and also consider which parts of the parser's probability model are particularly tuned to the corpus on which it was trained. This leads us to a technique for pruning parameters to reduce the size of the parsing model."
            },
            "slug": "Corpus-Variation-and-Parser-Performance-Gildea",
            "title": {
                "fragments": [],
                "text": "Corpus Variation and Parser Performance"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work examines how other types of text might a ect parser performance, and how portable parsing models are across corpora by comparing results for the Brown and WSJ corpora, and considers which parts of the parser's probability model are particularly tuned to the corpus on which it was trained."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49497622"
                        ],
                        "name": "K. Lari",
                        "slug": "K.-Lari",
                        "structuredName": {
                            "firstName": "Kaveh",
                            "lastName": "Lari",
                            "middleNames": [
                                "Sookhak"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Lari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145259603"
                        ],
                        "name": "S. Young",
                        "slug": "S.-Young",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Young",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Young"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 189
                            }
                        ],
                        "text": "The dynamic programming method uses inside and outside scores to calculate expectations, similar to the inside\u2013outside algorithm for estimating the parameters of a PCFG from unlabeled data (Lari and Young 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The dynamic programming method uses inside and outside scores to calculate expectations, similar to the inside-outside algorithm for estimating the parameters of a PCFG from unlabelled data ( Lari and Young 1990 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 53736294,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9ba08e0a53bfdcbe0b70a4761c3e2b62f150fc74",
            "isKey": false,
            "numCitedBy": 713,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Applications-of-stochastic-context-free-grammars-Lari-Young",
            "title": {
                "fragments": [],
                "text": "Applications of stochastic context-free grammars using the Inside-Outside algorithm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145002066"
                        ],
                        "name": "D. Younger",
                        "slug": "D.-Younger",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Younger",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Younger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 95
                            }
                        ],
                        "text": "The algorithm used to build the packed charts is the CKY chart parsing algorithm (Kasami 1965; Younger 1967) described in Steedman (2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 40504606,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "30da8ecce8b3ebc3e9344a79e5c2f8dc4c423bd2",
            "isKey": false,
            "numCitedBy": 1035,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Recognition-and-Parsing-of-Context-Free-Languages-Younger",
            "title": {
                "fragments": [],
                "text": "Recognition and Parsing of Context-Free Languages in Time n^3"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Control."
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768065"
                        ],
                        "name": "Yusuke Miyao",
                        "slug": "Yusuke-Miyao",
                        "structuredName": {
                            "firstName": "Yusuke",
                            "lastName": "Miyao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yusuke Miyao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737901"
                        ],
                        "name": "Junichi Tsujii",
                        "slug": "Junichi-Tsujii",
                        "structuredName": {
                            "firstName": "Junichi",
                            "lastName": "Tsujii",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junichi Tsujii"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 3
                            }
                        ],
                        "text": "In Miyao and Tsujii (2003b, 2003a) log-linear models are developed for automatically extracted grammars for Lexicalized Tree Adjoining Grammar (LTAG) and Head Driven Phrase Structure Grammar (HPSG)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Miyao and Tsujii (2003a) present another log-linear model for an automatically extracted LTAG which uses a simple unigram model of the elementary trees together with a loglinear model of the attachments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18046005,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "c1634dfe5ca4b31ded29c6f6e59c54d9e3bfad72",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new approach to syntactic disambiguation based on lexicalized grammars. While existing disambiguation models decompose the probability of parsing results into that of primitive dependencies of two words, our model selects the most probable parsing result from a set of candidates allowed by a lexicalized grammar. Since parsing results given by the lexicalized grammar cannot be decomposed into independent sub-events, we apply a maximum entropy model for feature forests, which allows probabilistic modeling without the independence assumption. Our approach provides a general method of producing a consistent probabilistic model of parsing results given by lexicalized grammars."
            },
            "slug": "A-model-of-syntactic-disambiguation-based-on-Miyao-Tsujii",
            "title": {
                "fragments": [],
                "text": "A model of syntactic disambiguation based on lexicalized grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The approach provides a general method of producing a consistent probabilistic model of parsing results given by lexicalized grammars, and applies a maximum entropy model for feature forests, which allows Probabilistic modeling without the independence assumption."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768065"
                        ],
                        "name": "Yusuke Miyao",
                        "slug": "Yusuke-Miyao",
                        "structuredName": {
                            "firstName": "Yusuke",
                            "lastName": "Miyao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yusuke Miyao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067242699"
                        ],
                        "name": "Takashi Ninomiya",
                        "slug": "Takashi-Ninomiya",
                        "structuredName": {
                            "firstName": "Takashi",
                            "lastName": "Ninomiya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Takashi Ninomiya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737901"
                        ],
                        "name": "Junichi Tsujii",
                        "slug": "Junichi-Tsujii",
                        "structuredName": {
                            "firstName": "Junichi",
                            "lastName": "Tsujii",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junichi Tsujii"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14142528,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb66bff453b6f405cbefbfb6a2804346b19da29b",
            "isKey": false,
            "numCitedBy": 161,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a method of semi-automatically acquiring an English HPSG grammar from the Penn Treebank. First, heuristic rules are employed to annotate the treebank with partially-specified derivation trees of HPSG. Lexical entries are automatically extracted from the annotated corpus by inversely applying HPSG schemata to partially-specified derivation trees. Predefined HPSG schemata assure the acquired lexicon to conform to the theoretical formulation of HPSG. Experimental results revealed that this approach enabled us to develop an HPSG grammar with significant robustness at small cost."
            },
            "slug": "Corpus-Oriented-Grammar-Development-for-Acquiring-a-Miyao-Ninomiya",
            "title": {
                "fragments": [],
                "text": "Corpus-Oriented Grammar Development for Acquiring a Head-Driven Phrase Structure Grammar from the Penn Treebank"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This paper describes a method of semi-automatically acquiring an English HPSG grammar from the Penn Treebank and shows how heuristic rules are employed to annotate the treebank with partially-specified derivation trees of H PSG."
            },
            "venue": {
                "fragments": [],
                "text": "IJCNLP"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38666915"
                        ],
                        "name": "D. Klein",
                        "slug": "D.-Klein",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Klein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 95
                            }
                        ],
                        "text": "It may also not be possible to train or run the system on anything other than short sentences (Taskar et al. 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8313435,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e19a94d547ee023837c14c361139185e2353fc0",
            "isKey": false,
            "numCitedBy": 246,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel discriminative approach to parsing inspired by the large-margin criterion underlying support vector machines. Our formulation uses a factorization analogous to the standard dynamic programs for parsing. In particular, it allows one to efficiently learn a model which discriminates among the entire space of parse trees, as opposed to reranking the top few candidates. Our models can condition on arbitrary features of input sentences, thus incorporating an important kind of lexical information without the added algorithmic complexity of modeling headedness. We provide an efficient algorithm for learning such models and show experimental evidence of the model\u2019s improved performance over a natural baseline model and a lexicalized probabilistic context-free grammar."
            },
            "slug": "Max-Margin-Parsing-Taskar-Klein",
            "title": {
                "fragments": [],
                "text": "Max-Margin Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A novel discriminative approach to parsing inspired by the large-margin criterion underlying support vector machines is presented, which allows one to efficiently learn a model which discriminates among the entire space of parse trees, as opposed to reranking the top few candidates."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3118681"
                        ],
                        "name": "J. Hockenmaier",
                        "slug": "J.-Hockenmaier",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Hockenmaier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hockenmaier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145332819"
                        ],
                        "name": "Mark Steedman",
                        "slug": "Mark-Steedman",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Steedman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Steedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 124
                            }
                        ],
                        "text": "Clark (2002) reports that a significant loss in coverage results if the supertagger is used as a front-end to the parser of Hockenmaier and Steedman (2002b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 144
                            }
                        ],
                        "text": "\u2026+ 0 POS + Distance(words) \u3008VBD, S[dcl]\u2192NP S[dcl]\\NP\u3009 + > 2 POS + Distance(punct) \u3008VBD, S[dcl]\u2192NP S[dcl]\\NP\u3009 + 2 POS + Distance(verbs) \u3008VBD, S[dcl]\u2192NP S[dcl]\\NP\u3009 + 0\nFor the normal-form model we follow Hockenmaier and Steedman (2002b) by defining dependency features in terms of the local rule\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 64
                            }
                        ],
                        "text": "We would also like to investigate using the generative model of Hockenmaier and Steedman (2002b) in a similar way."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 190
                            }
                        ],
                        "text": "\u2026model given the data, minus a Gaussian prior\n2 We could model predicate\u2013argument dependencies together with the derivation, but we wanted to use features from the derivation only, following Hockenmaier and Steedman (2002b).\nterm to prevent overfitting (Chen and Rosenfeld 1999; Johnson et al. 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 162
                            }
                        ],
                        "text": "A number of statistical parsing models have recently been developed for CCG and used in parsers applied to newspaper text (Clark, Hockenmaier, and Steedman 2002; Hockenmaier and Steedman 2002b; Hockenmaier 2003b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 24
                            }
                        ],
                        "text": "Hockenmaier (2003a) and Hockenmaier and Steedman (2002b) present a generative model of normal-form derivations, based on various techniques from the statistical parsing literature (Charniak 1997; Goodman 1997; Collins 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 25
                            }
                        ],
                        "text": "The treebank is CCGbank (Hockenmaier and Steedman 2002a; Hockenmaier 2003a), a CCG version of the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 42
                            }
                        ],
                        "text": "Hockenmaier also argues that, compared to Hockenmaier and Steedman (2002b), the predicate\u2013argument model is better suited to languages with freer word order than English."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8667731,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5332134b56af552515548e7e2ea1923cd63a5805",
            "isKey": true,
            "numCitedBy": 121,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract We present an algorithm which translates the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations. To do this we have needed to make several systematic changes to the Treebank which have to effect of cleaning up a number of errors and inconsistencies. This process has yielded a cleaner treebank that can potentially be used in any framework. We also show how unary type-changing rules for certain types of modifiers can be introduced in a CCG grammar to ensure a compact lexicon without augmenting the generative power of the system. We demonstrate how the combination of preprocessing and type-changing rules minimizes the lexical coverage problem."
            },
            "slug": "Acquiring-Compact-Lexicalized-Grammars-from-a-Hockenmaier-Steedman",
            "title": {
                "fragments": [],
                "text": "Acquiring Compact Lexicalized Grammars from a Cleaner Treebank"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "An algorithm which translates the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations is presented and it is demonstrated how the combination of preprocessing and type-changing rules minimizes the lexical coverage problem."
            },
            "venue": {
                "fragments": [],
                "text": "LREC"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145757665"
                        ],
                        "name": "Fei Sha",
                        "slug": "Fei-Sha",
                        "structuredName": {
                            "firstName": "Fei",
                            "lastName": "Sha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fei Sha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 140
                            }
                        ],
                        "text": "Initially we used generalized iterative scaling (GIS) for the parsing models described here, but found that convergence was extremely slow; Sha and Pereira (2003) present a similar finding for globally optimized log-linear models for sequences."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13936575,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "897249c93f55ef1c0d2aa1e799eb67b414c6d4a6",
            "isKey": false,
            "numCitedBy": 1544,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Conditional random fields for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position. Among sequence labeling tasks in language processing, shallow parsing has received much attention, with the development of standard evaluation datasets and extensive comparison among methods. We show here how to train a conditional random field to achieve performance as good as any reported base noun-phrase chunking method on the CoNLL task, and better than any reported single model. Improved training methods based on modern optimization algorithms were critical in achieving these results. We present extensive comparisons between models and training methods that confirm and strengthen previous results on shallow parsing and training methods for maximum-entropy models."
            },
            "slug": "Shallow-Parsing-with-Conditional-Random-Fields-Sha-Pereira",
            "title": {
                "fragments": [],
                "text": "Shallow Parsing with Conditional Random Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work shows how to train a conditional random field to achieve performance as good as any reported base noun-phrase chunking method on the CoNLL task, and better than any reported single model."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1982950"
                        ],
                        "name": "Luke Zettlemoyer",
                        "slug": "Luke-Zettlemoyer",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Zettlemoyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luke Zettlemoyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 41
                            }
                        ],
                        "text": "The overall F-score for the CCG parser, 81.14%, is only 3.6 points below that for CCGbank, which provides an upper bound for the CCG parser. Briscoe and Carroll (2006) give a rough comparison of RASP with the Parc LFG parser (Kaplan et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 178
                            }
                        ],
                        "text": "There has been some other work on defining stochastic categorial grammars, but mainly in the context of grammar learning (Osborne and Briscoe 1997; Watkinson and Manandhar 2001; Zettlemoyer and Collins 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 814,
                                "start": 127
                            }
                        ],
                        "text": "These methods are either too slow or sacrifice parsing performance, and so we use a parallelized version of BFGS running on an 18-node Beowulf cluster to perform the estimation. Even given the large number of derivations and the large feature sets in our models, the estimation time for the best-performing model is less than three hours. This gives us a practical framework for developing a statistical parser. A corollary of CCG\u2019s base-generative treatment of long-range dependencies in relative clauses and coordinate constructions is that the standard predicate\u2013argument relations can be derived via nonstandard surface derivations. The addition of \u201cspurious\u201d derivations in CCG complicates the modeling and parsing problems. In this article we consider two solutions. The first, following Hockenmaier (2003a), is to define a model in terms of normal-form derivations (Eisner 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1287,
                                "start": 127
                            }
                        ],
                        "text": "These methods are either too slow or sacrifice parsing performance, and so we use a parallelized version of BFGS running on an 18-node Beowulf cluster to perform the estimation. Even given the large number of derivations and the large feature sets in our models, the estimation time for the best-performing model is less than three hours. This gives us a practical framework for developing a statistical parser. A corollary of CCG\u2019s base-generative treatment of long-range dependencies in relative clauses and coordinate constructions is that the standard predicate\u2013argument relations can be derived via nonstandard surface derivations. The addition of \u201cspurious\u201d derivations in CCG complicates the modeling and parsing problems. In this article we consider two solutions. The first, following Hockenmaier (2003a), is to define a model in terms of normal-form derivations (Eisner 1996). In this approach we recover only one derivation leading to a given set of predicate\u2013argument dependencies and ignore the rest. The second approach is to define a model over the predicate\u2013argument dependencies themselves, by summing the probabilities of all derivations leading to a given set of dependencies. We also define a new efficient parsing algorithm for such a model, based on Goodman (1996), which maximizes the expected recall of dependencies."
                    },
                    "intents": []
                }
            ],
            "corpusId": 449252,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74fe7ec751cd50295b15cfd46389a8fefb37c414",
            "isKey": true,
            "numCitedBy": 874,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of mapping natural language sentences to lambda\u2013calculus encodings of their meaning. We describe a learning algorithm that takes as input a training set of sentences labeled with expressions in the lambda calculus. The algorithm induces a grammar for the problem, along with a log-linear model that represents a distribution over syntactic and semantic analyses conditioned on the input sentence. We apply the method to the task of learning natural language interfaces to databases and show that the learned parsers outperform previous methods in two benchmark database domains."
            },
            "slug": "Learning-to-Map-Sentences-to-Logical-Form:-with-Zettlemoyer-Collins",
            "title": {
                "fragments": [],
                "text": "Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "A learning algorithm is described that takes as input a training set of sentences labeled with expressions in the lambda calculus and induces a grammar for the problem, along with a log-linear model that represents a distribution over syntactic and semantic analyses conditioned on the input sentence."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144551937"
                        ],
                        "name": "Alexis Nasr",
                        "slug": "Alexis-Nasr",
                        "structuredName": {
                            "firstName": "Alexis",
                            "lastName": "Nasr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexis Nasr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702447"
                        ],
                        "name": "Owen Rambow",
                        "slug": "Owen-Rambow",
                        "structuredName": {
                            "firstName": "Owen",
                            "lastName": "Rambow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Owen Rambow"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u00a9 2007 Association for Computational Linguistics"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Nasr and Rambow (2004) investigate the potential impact of LTAG supertagging on parsing speed and accuracy by performing a number of oracle experiments."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 198622,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6181ebb4bd44b9719af0fc2057773adfac03a083",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate an approach to parsing in which lexical information is used only in a first phase, supertagging, in which lexical syntactic properties are determined without building structure. In the second phase, the best parse tree is determined without using lexical information. We investigate different probabilistic models for adjunction, and we show that, assuming hypothetically perfect performance in the first phase, the error rate on dependency arc attachment can be reduced to 2.3% using a full chart parser. This is an improvement of about 50% over previously reported results using a simple heuristic parser."
            },
            "slug": "SuperTagging-and-Full-Parsing-Nasr-Rambow",
            "title": {
                "fragments": [],
                "text": "SuperTagging and Full Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that, assuming hypothetically perfect performance in the first phase, the error rate on dependency arc attachment can be reduced to 2.3% using a full chart parser, an improvement of about 50% over previously reported results using a simple heuristic parser."
            },
            "venue": {
                "fragments": [],
                "text": "TAG+"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793475"
                        ],
                        "name": "A. Ratnaparkhi",
                        "slug": "A.-Ratnaparkhi",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ratnaparkhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ratnaparkhi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2600845,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b49db3ac26d96b6c5c081dc6c2cc24da93e633f1",
            "isKey": false,
            "numCitedBy": 554,
            "numCiting": 89,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis demonstrates that several important kinds of natural language ambiguities can be resolved to state-of-the-art accuracies using a single statistical modeling technique based on the principle of maximum entropy. \nWe discuss the problems of sentence boundary detection, part-of-speech tagging, prepositional phrase attachment, natural language parsing, and text categorization under the maximum entropy framework. In practice, we have found that maximum entropy models offer the following advantages: \nState-of-the-art accuracy. The probability models for all of the tasks discussed perform at or near state-of-the-art accuracies, or outperform competing learning algorithms when trained and tested under similar conditions. Methods which outperform those presented here require much more supervision in the form of additional human involvement or additional supporting resources. \nKnowledge-poor features. The facts used to model the data, or features, are linguistically very simple, or \"knowledge-poor\", but yet succeed in approximating complex linguistic relationships. \nReusable software technology. The mathematics of the maximum entropy framework are essentially independent of any particular task, and a single software implementation can be used for all of the probability models in this thesis. \nThe experiments in this thesis suggest that experimenters can obtain state-of-the-art accuracies on a wide range of natural language tasks, with little task-specific effort, by using maximum entropy probability models."
            },
            "slug": "Maximum-entropy-models-for-natural-language-Ratnaparkhi-Marcus",
            "title": {
                "fragments": [],
                "text": "Maximum entropy models for natural language ambiguity resolution"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This thesis demonstrates that several important kinds of natural language ambiguities can be resolved to state-of-the-art accuracies using a single statistical modeling technique based on the principle of maximum entropy."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145047601"
                        ],
                        "name": "Rebecca Watson",
                        "slug": "Rebecca-Watson",
                        "structuredName": {
                            "firstName": "Rebecca",
                            "lastName": "Watson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rebecca Watson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144708726"
                        ],
                        "name": "John A. Carroll",
                        "slug": "John-A.-Carroll",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Carroll",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John A. Carroll"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145693410"
                        ],
                        "name": "Ted Briscoe",
                        "slug": "Ted-Briscoe",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Briscoe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Briscoe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 0
                            }
                        ],
                        "text": "Watson, Carroll, and Briscoe (2005) have also applied our algorithm to the grammatical relations output by the RASP parser."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2454260,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "63115b85305724e6f3d5097aea14f29e9bc58dcf",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel approach for applying the Inside-Outside Algorithm to a packed parse forest produced by a unification-based parser. The approach allows a node in the forest to be assigned multiple inside and outside probabilities, enabling a set of 'weighted GRs' to be computed directly from the forest. The approach improves on previous work which either loses efficiency by unpacking the parse forest before extracting weighted GRs, or places extra constraints on which nodes can be packed, leading to less compact forests. Our experiments demonstrate substantial increases in parser accuracy and throughput for weighted GR output."
            },
            "slug": "Efficient-Extraction-of-Grammatical-Relations-Watson-Carroll",
            "title": {
                "fragments": [],
                "text": "Efficient Extraction of Grammatical Relations"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A novel approach for applying the Inside-Outside Algorithm to a packed parse forest produced by a unification-based parser that allows a node in the forest to be assigned multiple inside and outside probabilities, enabling a set of 'weighted GRs' to be computed directly from the forest."
            },
            "venue": {
                "fragments": [],
                "text": "IWPT"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37582951"
                        ],
                        "name": "Judita Preiss",
                        "slug": "Judita-Preiss",
                        "structuredName": {
                            "firstName": "Judita",
                            "lastName": "Preiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Judita Preiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Preiss (2003) compares the parsers of Collins and Charniak, the grammatical relations finder of Buchholz, Veenstra, and Daelemans (1999), and the Briscoe and Carroll (2002) parser, using the gold-standard grammatical relations (GRs) from Carroll, Briscoe, and Sanfilippo (1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18007880,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "716edcdc92b0c33242395f1e08ea87d30b8bd4f7",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We use the grammatical relations (GRs) described in Carroll et al. (1998) to compare a number of parsing algorithms. A first ranking of the parsers is provided by comparing the extracted GRs to a gold standard GR annotation of 500 Susanne sentences: this required an implementation of GR extraction software for Penn Treebank style parsers. In addition, we perform an experiment using the extracted GRs as input to the Lappin and Leass (1994) anaphora resolution algorithm. This produces a second ranking of the parsers, and we investigate the number of errors that are caused by the incorrect 'GRs."
            },
            "slug": "Using-Grammatical-Relations-to-Compare-Parsers-Preiss",
            "title": {
                "fragments": [],
                "text": "Using Grammatical Relations to Compare Parsers"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "The grammatical relations (GRs) described in Carroll et al. (1998) are used to compare a number of parsing algorithms and the number of errors that are caused by the incorrect 'GRs is investigated."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152465203"
                        ],
                        "name": "Mark Johnson",
                        "slug": "Mark-Johnson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 10
                            }
                        ],
                        "text": "Geman and Johnson (2002) propose a similar method in the context of LFG parsing; an implementation is described in Kaplan et al. (2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 298,
                                "start": 286
                            }
                        ],
                        "text": "\u2026we are concerned with the syntactic component; see Steedman (2000) for how a semantic interpretation can be composed during a syntactic derivation, and also Bos et al. (2004) for how semantic interpretations can be built for newspaper text using the wide-coverage parser described in this article."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 183
                            }
                        ],
                        "text": "This has led to a number of proposals for post-processing the output of the Collins and Charniak parsers, in which trace sites are located and the antecedent of the trace determined (Johnson 2002; Dienes and Dubey 2003; Levy and Manning 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12807398,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c37a1e4e9d453fe6960e20bb3e0c89f16e1f0766",
            "isKey": false,
            "numCitedBy": 135,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a simple pattern-matching algorithm for recovering empty nodes and identifying their co-indexed antecedents in phrase structure trees that do not contain this information. The patterns are minimal connected tree fragments containing an empty node and all other nodes co-indexed with it. This paper also proposes an evaluation procedure for empty node recovery procedures which is independent of most of the details of phrase structure, which makes it possible to compare the performance of empty node recovery on parser output with the empty node annotations in a gold-standard corpus. Evaluating the algorithm on the output of Charniak's parser (Charniak, 2000) and the Penn treebank (Marcus et al., 1993) shows that the pattern-matching algorithm does surprisingly well on the most frequently occuring types of empty nodes given its simplicity."
            },
            "slug": "A-Simple-Pattern-matching-Algorithm-for-Recovering-Johnson",
            "title": {
                "fragments": [],
                "text": "A Simple Pattern-matching Algorithm for Recovering Empty Nodes and their Antecedents"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An evaluation procedure for empty node recovery procedures is proposed which is independent of most of the details of phrase structure, which makes it possible to compare the performance of empty nodes recovery on parser output with the empty node annotations in a gold-standard corpus."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794100"
                        ],
                        "name": "Brian Roark",
                        "slug": "Brian-Roark",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Roark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Roark"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 127
                            }
                        ],
                        "text": "Another alternative for future work is to compare the dynamic programming approach taken here with the beam-search approach of Collins and Roark (2004), which allows more \u201cglobal\u201d features."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10366378,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41828fc3dab24784f95e6976e8aaa73f68e1840e",
            "isKey": false,
            "numCitedBy": 448,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an incremental parsing approach where parameters are estimated using a variant of the perceptron algorithm. A beam-search algorithm is used during both training and decoding phases of the method. The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank. We demonstrate that training a perceptron model to combine with the generative model during search provides a 2.1 percent F-measure improvement over the generative model alone, to 88.8 percent."
            },
            "slug": "Incremental-Parsing-with-the-Perceptron-Algorithm-Collins-Roark",
            "title": {
                "fragments": [],
                "text": "Incremental Parsing with the Perceptron Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is demonstrated that training a perceptron model to combine with the generative model during search provides a 2.1 percent F-measure improvement over the Generative model alone, to 88.8 percent."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1387994164"
                        ],
                        "name": "Jason Baldridge",
                        "slug": "Jason-Baldridge",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Baldridge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason Baldridge"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 80
                            }
                        ],
                        "text": "A recent development in the theory of CCG is the multi-modal treatment given by Baldridge (2002) and Baldridge and Kruijff (2003), following the type-logical approaches to Categorial Grammar (Moortgat 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 58380055,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "a987c99f64264a7c6c6c6a26b52ae7bde388b92f",
            "isKey": false,
            "numCitedBy": 160,
            "numCiting": 148,
            "paperAbstract": {
                "fragments": [],
                "text": "This dissertation elaborates several refinements to the Combinatory Categorial Grammar (ccg) framework which are motivated by phenomena in parametrically diverse languages such as English, Dutch, Tagalog, Toba Batak, and Turkish. I present Multi-Modal Combinatory Categorial Grammar, a formulation of ccg which incorporates devices and category constructors from related categorial frameworks and demonstrate the effectiveness of these modifications both for providing parsimonious linguistic analyses and for improving the representation of the lexicon and computational processing. I begin by introducing the various grammar frameworks which set the background for this dissertation and then discuss aspects of providing substantive universals for the theory of ccg. Most importantly, I lay out some of the components necessary for providing a theory of the lexicon, outlining previous approaches and suggesting directions forward. I then turn to a description of the syntactic extraction asymmetries found in English, Tagalog, and Toba Batak and the word order flexibility of Tagalog and Turkish, and discuss previous approaches to handling the data. Having explicated the foundations and the linguistic motivations for the dissertation, I show how the multi-modal perspective on grammatical composition provided by the logical tradition of categorial grammar can be incorporated into ccg\u2019s rulebased approach. The enhanced resource-sensitivity of this perspective allows me to utilize an invariant rule component, controlling the applicability of the combinatory rules via lexical specification rather than with constraints on the rules themselves. This control is shown to be necessary for many aspects of English syntax, and I furthermore demonstrate that the multi-modal approach can improve upon existing ccg analyses for English and Dutch. The second major development is a redefinition of categories and combinatory rules which relaxes the strict ordering inherent in categories that is normally assumed in categorial grammars. The manner in which this is done permits an intuitive account of local scrambling behavior without increasing the generative power of the system. Bounded long-distance scrambling is handled with the same mechanisms as ccg \u2013 type-raising and crossed composition rules. I furthermore show how the resource-sensitivity of the system effectively limits the permutative possibilities for some constructions in the otherwise quite free grammar of Turkish. Having thus motivated and developed the multi-modal ccg system, I present"
            },
            "slug": "Lexically-specified-derivational-control-in-grammar-Baldridge",
            "title": {
                "fragments": [],
                "text": "Lexically specified derivational control in combinatory categorial grammar"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "This dissertation elaborates several refinements to the Combinatory Categorial Grammar (ccg) framework, and shows how the multi-modal perspective on grammatical composition provided by the logical tradition of categorial grammar can be incorporated into ccg\u2019s rulebased approach."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145693410"
                        ],
                        "name": "Ted Briscoe",
                        "slug": "Ted-Briscoe",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Briscoe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Briscoe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144708726"
                        ],
                        "name": "John A. Carroll",
                        "slug": "John-A.-Carroll",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Carroll",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John A. Carroll"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 146
                            }
                        ],
                        "text": "Preiss (2003) compares the parsers of Collins and Charniak, the grammatical relations finder of Buchholz, Veenstra, and Daelemans (1999), and the Briscoe and Carroll (2002) parser, using the gold-standard grammatical relations (GRs) from Carroll, Briscoe, and Sanfilippo (1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5823614,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8fe99b6dc76dc342fe9fb47740fee40381fa13d",
            "isKey": false,
            "numCitedBy": 326,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a robust accurate domain-independent approach to statistical parsing incorporated into the new release of the ANLT toolkit, and publicly available as a research tool. The system has been used to parse many well known corpora in order to produce data for lexical acquisition efforts; it has also been used as a component in an open-domain question answering project. The performance of the system is competitive with that of statistical parsers using highly lexicalised parse selection models. However, we plan to extend the system to improve parse coverage, depth and accuracy."
            },
            "slug": "Robust-Accurate-Statistical-Annotation-of-General-Briscoe-Carroll",
            "title": {
                "fragments": [],
                "text": "Robust Accurate Statistical Annotation of General Text"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A robust accurate domain-independent approach to statistical parsing incorporated into the new release of the ANLT toolkit, and publicly available as a research tool."
            },
            "venue": {
                "fragments": [],
                "text": "LREC"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145804005"
                        ],
                        "name": "Robert Malouf",
                        "slug": "Robert-Malouf",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Malouf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Malouf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66583218"
                        ],
                        "name": "Van Noord",
                        "slug": "Van-Noord",
                        "structuredName": {
                            "firstName": "Van",
                            "lastName": "Noord",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Van Noord"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u00a9 2007 Association for Computational Linguistics"
                    },
                    "intents": []
                }
            ],
            "corpusId": 11125139,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a9c9605885e7cb52fe8ea89fd1a7e93d7f36899b",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Stochastic Attribute Value Grammars (SAVG) provide an attractive framework for syntactic analysis, because they allow the combination of linguistic sophistication with a principled treatment of ambiguity. The paper introduces a widecoverage SAVG for Dutch, known as Alpino, and we show how this SAVG can be efficiently applied, using a beam search algorithm to recover parses from a shared parse forest. Experimental results for a number of different corpora suggest that the SAVG framework is applicable for realistically sized grammars and corpora."
            },
            "slug": "Wide-Coverage-Parsing-with-Stochastic-Attribute-Malouf-Noord",
            "title": {
                "fragments": [],
                "text": "Wide Coverage Parsing with Stochastic Attribute Value Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Experimental results for a number of different corpora suggest that the SAVG framework is applicable for realistically sized grammars and corpora."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152465203"
                        ],
                        "name": "Mark Johnson",
                        "slug": "Mark-Johnson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3289329"
                        ],
                        "name": "S. Riezler",
                        "slug": "S.-Riezler",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Riezler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Riezler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 93
                            }
                        ],
                        "text": "These scores could be incorporated as real-valued features, or as auxiliary functions, as in Johnson and Riezler (2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2965282,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "91dc5e5d86d2ba93b490547565e122269880ded9",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a method for estimating conditional probability distributions over the parses of \"unification-based\" grammars which can utilize auxiliary distributions that are estimated by other means. We show how this can be used to incorporate information about lexical selectional preferences gathered from other sources into Stochastic \"Unification-based\" Grammars (SUBGs). While we apply this estimator to a Stochastic Lexical-Functional Grammar, the method is general, and should be applicable to stochastic versions of HPSGs, categorial grammars and transformational grammars."
            },
            "slug": "Exploiting-auxiliary-distributions-in-stochastic-Johnson-Riezler",
            "title": {
                "fragments": [],
                "text": "Exploiting auxiliary distributions in stochastic unification-based grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "This paper describes a method for estimating conditional probability distributions over the parses of \"unification-based\" grammars which can utilize auxiliary distributions that are estimated by other means, and applies this estimator to a Stochastic Lexical-Functional Grammar."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057788"
                        ],
                        "name": "M. Osborne",
                        "slug": "M.-Osborne",
                        "structuredName": {
                            "firstName": "Miles",
                            "lastName": "Osborne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Osborne"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 82
                            }
                        ],
                        "text": "Another difference is that Malouf and van Noord use the random sampling method of Osborne (2000) to allow practical estimation, whereas we construct the complete parse forest but use the supertagger to limit the size of the charts."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Osborne (2000) investigates training on a sample of the analyses for each sentence, for example the top-n most probable according to some other probability model, or simply a random sample."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 488,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "827598a57a6f8ee03a7b3d993c50705b751e380e",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We argue that some of the computational complexity associated with estimation of stochastic attribute value grammars can be reduced by training upon an informative subset of the full training set. Results using the parsed Wall Street Journal corpus show that in some circumstances, it is possible to obtain better estimation results using an informative sample than when training upon all the available material. Further experimentation demonstrates that with unlexicalised models, a Gaussian prior can reduce overfitting. However, when models are lexicalised and contain overlapping features, overfitting does not seem to be a problem, and a Gaussian prior makes minimal difference to performance. Our approach is applicable for situations when there are an infeasibly large number of parses in the training set, or else for when recovery of these parses from a packed representation is itself computationally expensive."
            },
            "slug": "Estimation-of-Stochastic-Attribute-Value-Grammars-Osborne",
            "title": {
                "fragments": [],
                "text": "Estimation of Stochastic Attribute-Value Grammars using an Informative Sample"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "It is argued that some of the computational complexity associated with estimation of stochastic attribute value grammars can be reduced by training upon an informative subset of the full training set, or when recovery of these parses from a packed representation is itself computationally expensive."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144774224"
                        ],
                        "name": "O. Kiselyov",
                        "slug": "O.-Kiselyov",
                        "structuredName": {
                            "firstName": "Oleg",
                            "lastName": "Kiselyov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Kiselyov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 135
                            }
                        ],
                        "text": "Log-linear models have been applied to a number of problems in NLP, for example, POS tagging (Ratnaparkhi 1996; Lafferty, McCallum, and Pereira 2001), named entity recognition (Borthwick 1999), chunking (Koeling 2000), and parsing (Johnson et al. 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8112731,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1cb4a2af98b6d9cd55348e7a183fa3ff6339eda7",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We present the grammar/semantic formalism of Applicative Abstract Categorial Grammars (AACG), based on the recent techniques from functional programming: applicative functors, staged languages and typed final language embeddings. AACG is a generalization of Abstract Categorial Grammars (ACG), retaining the benefits of ACG as a grammar formalism and making it possible and convenient to express a variety of semantic theories. We use the AACG formalism to uniformly formulate Potts\u2019 analyses of expressives, the dynamic-logic account of anaphora, and the continuation tower treatment of quantifier strength, quantifier ambiguity and scope islands. Carrying out these analyses in ACG required compromises with the accompanying ballooning of parsing complexity, or was not possible at all. The AACG formalism brings modularity, which comes from the compositionality of applicative functors, in contrast to monads, and the extensibility of the typed final embedding. The separately developed analyses of expressives and QNP are used as they are to compute truth conditions of sentences with both these features. AACG is implemented as a \u2018semantic calculator\u2019, which is the ordinary Haskell interpreter. The calculator lets us interactively write grammar derivations in a linguist-readable form and see their yields, inferred types and computed truth conditions. We easily extend fragments with more lexical items and operators, and experiment with different semantic-mapping assemblies. The mechanization lets a semanticist test more and more complex examples, making empirical tests of a semantic theory more extensive, organized and systematic."
            },
            "slug": "Categorial-Grammars-Kiselyov",
            "title": {
                "fragments": [],
                "text": "Categorial Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The AACG formalism is used to uniformly formulate Potts\u2019 analyses of expressives, the dynamic-logic account of anaphora, and the continuation tower treatment of quantifier strength, quantifier ambiguity and scope islands, and brings modularity."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1856296"
                        ],
                        "name": "R. Prins",
                        "slug": "R.-Prins",
                        "structuredName": {
                            "firstName": "Robbert",
                            "lastName": "Prins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Prins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66583218"
                        ],
                        "name": "Van Noord",
                        "slug": "Van-Noord",
                        "structuredName": {
                            "firstName": "Van",
                            "lastName": "Noord",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Van Noord"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17419641,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "19857e09101e6e91c68abdd265a94650fc28c87a",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Lexical ambiguity is an important source of inefciency for wide-coverage HPSG parsing. In this paper, we propose a lexical analysis lter which removes unlikely lexical cat- egories. The lter is implemented as a straightforward HMM n-gram POS-tagger, which com- putes the 'a posteriori' probability of each lexical category. A lexical category is removed if a competing lexical category is sufciently more likely. The novel aspect of our approach is the fact that the tagger is trained on the output of the parser itself; therefore there is no need for hand-annotated material. Use of this lter increases the speed of the parser considerably, and in addition gives rise to an improvement in parsing accuracy. R \u00b7 ESUM \u00b7 E. L'ambigu\u00a4 \u00b7 e lexicale est une source importante de l'inefcacit \u00b7 e de l'analyse syn- ecision."
            },
            "slug": "Reinforcing-Parser-Preferences-through-Tagging-Prins-Noord",
            "title": {
                "fragments": [],
                "text": "Reinforcing Parser Preferences through Tagging"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A lexical analysis which removes unlikely lexical cat- egories, implemented as a straightforward HMM n-gram POS-tagger, which com- putes the 'a posteriori' probability of each lexical category."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5896745"
                        ],
                        "name": "John Chen",
                        "slug": "John-Chen",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35313721"
                        ],
                        "name": "S. Bangalore",
                        "slug": "S.-Bangalore",
                        "structuredName": {
                            "firstName": "Srinivas",
                            "lastName": "Bangalore",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bangalore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702447"
                        ],
                        "name": "Owen Rambow",
                        "slug": "Owen-Rambow",
                        "structuredName": {
                            "firstName": "Owen",
                            "lastName": "Rambow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Owen Rambow"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Chen et al. (2002) describe the results of reranking the output of an HMM supertagger using an automatically extracted LTAG."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "\u00a9 2007 Association for Computational Linguistics"
                    },
                    "intents": []
                }
            ],
            "corpusId": 14955723,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d74033f0e2a79ee7b02c1638e2db8e4bcf03af7",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "As shown by Srinivas (1997), standard n-gram modeling may be used to perform supertag disambiguation with accuracy that is adequate for partial parsing, but in general not sufficient for full parsing. A serious problem is that n-gram modeling usually considers a very small, fixed context and does not perform well with large tag sets, such as those generated by automatic grammar extraction (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2000). As an alternative, Chen, Bangalore and Vijay-Shanker (1999) introduce class-based supertagging. An example of class tagging is n-best trigram-based supertagging, which assigns to each word the top n most likely supertags as determined by an n-gram supertagging model. Class-based supertagging can be performed much more accurately than supertagging with only a small increase in ambiguity. In a second phase, the most likely candidate from the class is chosen. In this paper, we investigate an approach to such a choice based on reranking a set of candidate supertags and their confidence scores. RankBoost (Freund et al., 1998) is the boosting algorithm that we use in order to learn to rerank outputs. It also has been used with good effect in reranking outputs of a statistical parser (Collins, 2000) and ranking sentence plans (Walker, Rambow and Rogati, 2001). RankBoost may learn to correct biases that are inherent in n-gram modeling which lead to systematic errors in supertagging (cf. (van Halteren, 1996)). RankBoost can also use a variety of local and long distance features more easily than n-gram-based approaches (cf. (Chen, Bangalore and Vijay-Shanker, 1999)) because it makes sparse data less of an issue. The outline of this paper is as follows. First, we develop the background and motivations behind the task of reranking the output of an n-best trigram supertagger. Second, we introduce RankBoost as the approach that we adopt in order to train the reranker. Third, we perform an initial set of experiments where the reranker is trained with different feature subsets. Fourth, we perform an in-depth analysis of several reranking models. Fifth, after pointing out causes that at times render the reranker ineffective, we develop and test some new models that attempt to sidestep these limitations. Lastly, after some significance testing results, we state our conclusions and remark on potential future directions."
            },
            "slug": "Reranking-an-n-gram-supertagger-Chen-Bangalore",
            "title": {
                "fragments": [],
                "text": "Reranking an n-gram supertagger"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "RankBoost is introduced as the approach that is adopted in order to train the reranker, and RankBoost may learn to correct biases that are inherent in n-gram modeling which lead to systematic errors in supertagging."
            },
            "venue": {
                "fragments": [],
                "text": "TAG+"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2612754"
                        ],
                        "name": "T. Kasami",
                        "slug": "T.-Kasami",
                        "structuredName": {
                            "firstName": "Tadao",
                            "lastName": "Kasami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kasami"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 82
                            }
                        ],
                        "text": "The algorithm used to build the packed charts is the CKY chart parsing algorithm (Kasami 1965; Younger 1967) described in Steedman (2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 82
                            }
                        ],
                        "text": "The algorithm used to build the packed charts is the CKY chart parsing algorithm (Kasami 1965; Younger 1967) described in Steedman (2000). The CKY algorithm applies naturally to CCG because the grammar is binary."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61491815,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "af66165c454a0e94afbab36271fe3deaae0b421a",
            "isKey": false,
            "numCitedBy": 608,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : An efficient algorithm of recognition and syntaxanalysis for the full class of context-free languages without the difficulty of exponential growth of computing time with the length n of input sequence is presented. This algorithm makes use of the essential property of a context-free language as a multi-parenthesis system. It is shown in this paper that a context-free language is n cubed-recognizable in the sense of Hartmanis and Stearns ('Computational complexity of recursive sequences'. Proc. Fifth Annual Symposium of Switching Circuit Theory and Logical Design (Oct. 1964) p.82-90) by a double-tape or double-head single-tape Turing machine and it is n to the 4th power-recognizable by a single-head single-tape Turing machine. If we use a random-access memory whose size is proportional to n cubed, the computing time required for syntaxanalysis is upperbounded by C(1)n cubed + C(2)n squared N, where N denotes the number of nonequivalent valid derivation sequences for a given input sequence and C(i)'s are constants independent of input sequences. If we use a tape of length C(3)n cubed and one of length C(4)n squared as working memories, the computing time for syntax-analysis is upperbounded by C(5)n cubed (1 + N). The size of required memory can be reduced to the order of n squared, but the computing time rises to the order of n to the 4th power. (Author)"
            },
            "slug": "An-Efficient-Recognition-and-Syntax-Analysis-for-Kasami",
            "title": {
                "fragments": [],
                "text": "An Efficient Recognition and Syntax-Analysis Algorithm for Context-Free Languages"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown in this paper that a context-free language is n cubed-recognizable in the sense of Hartmanis and Stearns and it is n to the 4th power- Recognizable by a single-head single-tape Turing machine."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3118681"
                        ],
                        "name": "J. Hockenmaier",
                        "slug": "J.-Hockenmaier",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Hockenmaier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hockenmaier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 35
                            }
                        ],
                        "text": "The following examples, taken from Hockenmaier (2003a), demonstrate the most common rules."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 152
                            }
                        ],
                        "text": "Sentence categories (S) in CCGbank carry features, such as [dcl] for declarative, [wq] for wh-questions, and [for] for small clauses headed by for; see Hockenmaier (2003a) for the complete list."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 48
                            }
                        ],
                        "text": "Results are also given for Hockenmaier\u2019s parser (Hockenmaier 2003a) which used an earlier, slightly different version of the treebank."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 88
                            }
                        ],
                        "text": "One of the advantages of log-linear models is that it is easy to include such features; Hockenmaier (2003b) describes the difficulties in including such\nfeatures in a generative model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 30
                            }
                        ],
                        "text": "In fact, the results given in Hockenmaier (2003b) are lower than previous results."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 66
                            }
                        ],
                        "text": "We also showed in Clark and Curran (2004b) that, in contrast with Hockenmaier (2003a), adding distance to the dependency features in the log-linear model does improve parsing accuracy."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 286,
                                "start": 267
                            }
                        ],
                        "text": "The use of conditional log-linear models in this article is designed to overcome some of the weaknesses identified in the approach of Clark, Hockenmaier, and Steedman\n(2002), and to offer a more flexible framework for including features than the generative models of Hockenmaier (2003a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Hockenmaier (2003a) compares a number of generative models, starting with a baseline model based on a PCFG."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 21
                            }
                        ],
                        "text": "The first, following Hockenmaier (2003a), is to define a model in terms of normal-form derivations (Eisner 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 3
                            }
                        ],
                        "text": "We also extend the existing parsing techniques for CCG by developing a new model and efficient parsing algorithm which exploits all derivations, including CCG\u2019s nonstandard derivations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 271,
                                "start": 254
                            }
                        ],
                        "text": "The first step can be performed by simply assigning to each word all lexical categories the word is seen with in the training data, together with some strategy for dealing with rare and unknown words (such as assigning the complete lexical category set; Hockenmaier 2003a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Hockenmaier (2003a) gives a detailed description of the procedure used to create CCGbank."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Hockenmaier (2003a) conjectures that the reduced performance is due to the problem of data sparseness, which becomes particularly severe for the generative model when the number of features is increased."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 194
                            }
                        ],
                        "text": "A number of statistical parsing models have recently been developed for CCG and used in parsers applied to newspaper text (Clark, Hockenmaier, and Steedman 2002; Hockenmaier and Steedman 2002b; Hockenmaier 2003b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 243
                            }
                        ],
                        "text": "An advantage of using CCG is that\nthe recovery of long-range dependencies can be integrated into the parsing process in a straightforward manner, rather than be relegated to such a post-processing phase (Clark, Hockenmaier, and Steedman 2002; Hockenmaier 2003a; Clark, Steedman, and Curran 2004)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 9
                            }
                        ],
                        "text": "However, Hockenmaier (2003b) reports that the increased complexity of the model reduces the effectiveness of the dynamic programming used in the parser, and hence a more aggressive beam search is required to produce reasonable parse times."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Hockenmaier (2003a) and Hockenmaier and Steedman (2002b) present a generative model of normal-form derivations, based on various techniques from the statistical parsing literature (Charniak 1997; Goodman 1997; Collins 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 57
                            }
                        ],
                        "text": "The treebank is CCGbank (Hockenmaier and Steedman 2002a; Hockenmaier 2003a), a CCG version of the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Hockenmaier (2003b) presents another generative model of normal-form derivations, which is based on the dependencies in the predicate\u2013argument structure, including long-range dependencies, rather than the dependencies defined by the local trees in the derivation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2604241,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "397afb869d1df69ea5f1a41ea2e575db1f29aa65",
            "isKey": true,
            "numCitedBy": 83,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The model used by the CCG parser of Hockenmaier and Steedman (2002b) would fail to capture the correct bilexical dependencies in a language with freer word order, such as Dutch. This paper argues that probabilistic parsers should therefore model the dependencies in the predicate-argument structure, as in the model of Clark et al. (2002), and defines a generative model for CCG derivations that captures these dependencies, including bounded and unbounded long-range dependencies."
            },
            "slug": "Parsing-with-Generative-Models-of-Structure-Hockenmaier",
            "title": {
                "fragments": [],
                "text": "Parsing with Generative Models of Predicate-Argument Structure"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper argues that probabilistic parsers should model the dependencies in the predicate-argument structure, as in the model of Clark et al. (2002), and defines a generative model for CCG derivations that captures these dependencies, including bounded and unbounded long-range dependencies."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145332819"
                        ],
                        "name": "Mark Steedman",
                        "slug": "Mark-Steedman",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Steedman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Steedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 38
                            }
                        ],
                        "text": "Log-linear models have been applied to a number of problems in NLP, for example, POS tagging (Ratnaparkhi 1996; Lafferty, McCallum, and Pereira 2001), named entity recognition (Borthwick 1999), chunking (Koeling 2000), and parsing (Johnson et al. 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 38
                            }
                        ],
                        "text": "Combinatory Categorial Grammar (CCG) (Steedman 1996, 2000) is a type-driven lexicalized theory of grammar based on Categorial Grammar (Wood 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12352908,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "ded3f3aaaef970030dffd2326be77024b48bac77",
            "isKey": false,
            "numCitedBy": 497,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Combinatory Categorial Grammar (CCG) offers a new approach to the theory of natural language grammar. Coordination, relativization, and related prosodic phenomena have been analyzed in CCG in terms of a radically revised notion of surface structure. CCG surface structures do not exhibit traditional notions of syntactic dominance and command, and do not constitute an autonomous level of representation. Instead, they reflect the computations by which a sentence may be realized or analyzed, to synchronously define a predicate-argument structure, or logical form. Surface Structure and Interpretation shows that binding and control can be captured at this level, preserving the advantages of CCG as an account of coordination and unbounded dependency.The core of the book is a detailed treatment of extraction, a focus of syntactic research since the early work of Chomsky and Ross. The topics addressed include the sources of subject-object asymmetries and phenomena attributed to the Empty Category Principle (ECP), asymmetric islands, parasitic gaps, and the relation of coordination and extraction, including their interactions with binding theory. In his conclusion, the author relates CCG to other categorial and type-driven approaches and to proposals for minimalism in linguistic theory.Linguistic Inquiry Monograph No. 30"
            },
            "slug": "Surface-structure-and-interpretation-Steedman",
            "title": {
                "fragments": [],
                "text": "Surface structure and interpretation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The core of the book is a detailed treatment of extraction, a focus of syntactic research since the early work of Chomsky and Ross, and relates CCG to other categorial and type-driven approaches and to proposals for minimalism in linguistic theory."
            },
            "venue": {
                "fragments": [],
                "text": "Linguistic inquiry"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1402022856"
                        ],
                        "name": "Ruth O'Donovan",
                        "slug": "Ruth-O'Donovan",
                        "structuredName": {
                            "firstName": "Ruth",
                            "lastName": "O'Donovan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ruth O'Donovan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152422954"
                        ],
                        "name": "Michael Burke",
                        "slug": "Michael-Burke",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Burke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Burke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145557710"
                        ],
                        "name": "A. Cahill",
                        "slug": "A.-Cahill",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Cahill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cahill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7519068"
                        ],
                        "name": "Josef van Genabith",
                        "slug": "Josef-van-Genabith",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "van Genabith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josef van Genabith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144315616"
                        ],
                        "name": "Andy Way",
                        "slug": "Andy-Way",
                        "structuredName": {
                            "firstName": "Andy",
                            "lastName": "Way",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andy Way"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 46195,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1bcab843f26e3a6a5d9cb78f7d8eca34bab3b3fd",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a methodology for extracting subcategorisation frames based on an automatic LFG f-structure annotation algorithm for the Penn-II Treebank. We extract abstract syntactic function-based subcategorisation frames (LFG semantic forms), traditional CFG category-based subcategorisation frames as well as mixed function/category-based frames, with or without preposition information for obliques and particle information for particle verbs. Our approach does not predefine frames, associates probabilities with frames conditional on the lemma, distinguishes between active and passive frames, and fully reflects the effects of long-distance dependencies in the source data structures. We extract 3586 verb lemmas, 14348 semantic form types (an average of 4 per lemma) with 577 frame types. We present a large-scale evaluation of the complete set of forms extracted against the full COMLEX resource."
            },
            "slug": "Large-Scale-Induction-and-Evaluation-of-Lexical-the-O'Donovan-Burke",
            "title": {
                "fragments": [],
                "text": "Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II Treebank"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A methodology for extracting subcategorisation frames based on an automatic LFG f-structure annotation algorithm for the Penn-II Treebank and a large-scale evaluation of the complete set of forms extracted against the full COMLEX resource is presented."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2097547014"
                        ],
                        "name": "Vladimir Solmon",
                        "slug": "Vladimir-Solmon",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Solmon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vladimir Solmon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 189
                            }
                        ],
                        "text": "The dynamic programming method uses inside and outside scores to calculate expectations, similar to the inside\u2013outside algorithm for estimating the parameters of a PCFG from unlabeled data (Lari and Young 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 190
                            }
                        ],
                        "text": "The dynamic programming method uses inside and outside scores to calculate expectations, similar to the inside-outside algorithm for estimating the parameters of a PCFG from unlabelled data (Lari and Young, 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 160022951,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c655420146f87152f86d3ddaccd155cea256ad5f",
            "isKey": false,
            "numCitedBy": 447,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-estimation-of-stochastic-context-free-grammars-Solmon",
            "title": {
                "fragments": [],
                "text": "The estimation of stochastic context-free grammars using the Inside-Outside algorithm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145755155"
                        ],
                        "name": "Martha Palmer",
                        "slug": "Martha-Palmer",
                        "structuredName": {
                            "firstName": "Martha",
                            "lastName": "Palmer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martha Palmer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2489901"
                        ],
                        "name": "Paul R. Kingsbury",
                        "slug": "Paul-R.-Kingsbury",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Kingsbury",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul R. Kingsbury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793218"
                        ],
                        "name": "D. Gildea",
                        "slug": "D.-Gildea",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Gildea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gildea"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 69
                            }
                        ],
                        "text": "Miyao and Tsujii (2004) evaluate their HPSG parser against PropBank (Palmer, Gildea, and Kingsbury 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2486369,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "99d2dcdcf4cf05facaa101a48c7e31d140b4736d",
            "isKey": false,
            "numCitedBy": 2395,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "The Proposition Bank project takes a practical approach to semantic representation, adding a layer of predicate-argument information, or semantic role labels, to the syntactic structures of the Penn Treebank. The resulting resource can be thought of as shallow, in that it does not represent coreference, quantification, and many other higher-order phenomena, but also broad, in that it covers every instance of every verb in the corpus and allows representative statistics to be calculated. We discuss the criteria used to define the sets of semantic roles used in the annotation process and to analyze the frequency of syntactic/semantic alternations in the corpus. We describe an automatic system for semantic role tagging trained on the corpus and discuss the effect on its performance of various types of information, including a comparison of full syntactic parsing with a flat representation and the contribution of the empty trace categories of the treebank."
            },
            "slug": "The-Proposition-Bank:-An-Annotated-Corpus-of-Roles-Palmer-Kingsbury",
            "title": {
                "fragments": [],
                "text": "The Proposition Bank: An Annotated Corpus of Semantic Roles"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An automatic system for semantic role tagging trained on the corpus is described and the effect on its performance of various types of information is discussed, including a comparison of full syntactic parsing with a flat representation and the contribution of the empty trace categories of the treebank."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17446277"
                        ],
                        "name": "J. Goodman",
                        "slug": "J.-Goodman",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Goodman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Goodman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 196
                            }
                        ],
                        "text": "Hockenmaier (2003a) and Hockenmaier and Steedman (2002b) present a generative model of normal-form derivations, based on various techniques from the statistical parsing literature (Charniak 1997; Goodman 1997; Collins 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 16
                            }
                        ],
                        "text": "Because there is so much syntactic information in lexical categories, the parser is required to do less work once the lexical categories have been assigned; hence Srinivas and Joshi, in the context of TAG, refer to supertagging as almost parsing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18774233,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2588593c42126e059fb8aad7673fa1736755f1e1",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new formalism, probabilistic feature grammar (PFG). PFGs combine most of the best properties of several other formalisms, including those of Collins, Magerman, and Charniak, and in experiments have comparable or better performance. PFGs generate features one at a time, probabilistically, conditioning the probabilities of each feature on other features in a local context. Because the conditioning is local, efficient polynomial time parsing algorithms exist for computing inside, outside, and Viterbi parses. PFGs can produce probabilities of strings, making them potentially useful for language modeling. Precision and recall results are comparable to the state of the art with words, and the best reported without words."
            },
            "slug": "Probabilistic-Feature-Grammars-Goodman",
            "title": {
                "fragments": [],
                "text": "Probabilistic Feature Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "Probabilistic feature grammar combines most of the best properties of several other formalisms, including those of Collins, Magerman, and Charniak, and in experiments have comparable or better performance."
            },
            "venue": {
                "fragments": [],
                "text": "IWPT"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145693410"
                        ],
                        "name": "Ted Briscoe",
                        "slug": "Ted-Briscoe",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Briscoe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Briscoe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144708726"
                        ],
                        "name": "John A. Carroll",
                        "slug": "John-A.-Carroll",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Carroll",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John A. Carroll"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 184
                            }
                        ],
                        "text": "The coverage of the RASP parser is also 100%: 84% of the analyses are complete parses rooted in S and the rest are obtained using a robustness technique based on fragmentary analyses (Briscoe and Carroll 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 41
                            }
                        ],
                        "text": "The GRs are described in Briscoe (2006), Briscoe and Carroll (2006), and Briscoe, Carroll, and Watson (2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Briscoe and Carroll (2006) describe the differences between the two schemes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Briscoe and Carroll (2006) evaluate the RASP parser on the Parc Dependency Bank (DepBank; King et al. 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 69
                            }
                        ],
                        "text": "For the gold standard we chose the version of DepBank reannotated by Briscoe and Carroll (2006) (hereafter B&C), consisting of 700 sentences from Section 23 of the Penn Treebank."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Briscoe and Carroll (2006) give a rough comparison of RASP with the Parc LFG parser (Kaplan et al. 2004) on DepBank, obtaining similar results overall, but acknowledging that the results are not strictly comparable because of the different annotation schemes used."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 16332736,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ddb75603049c27ca5141f06380976ab992a0af64",
            "isKey": true,
            "numCitedBy": 74,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We evaluate the accuracy of an unlexicalized statistical parser, trained on 4K treebanked sentences from balanced data and tested on the PARC DepBank. We demonstrate that a parser which is competitive in accuracy (without sacrificing processing speed) can be quickly tuned without reliance on large in-domain manually-constructed treebanks. This makes it more practical to use statistical parsers in applications that need access to aspects of predicate-argument structure. The comparison of systems using DepBank is not straightforward, so we extend and validate DepBank and highlight a number of representation and scoring issues for relational evaluation schemes."
            },
            "slug": "Evaluating-the-Accuracy-of-an-Unlexicalized-Parser-Briscoe-Carroll",
            "title": {
                "fragments": [],
                "text": "Evaluating the Accuracy of an Unlexicalized Statistical Parser on the PARC DepBank"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "It is demonstrated that a parser which is competitive in accuracy (without sacrificing processing speed) can be quickly tuned without reliance on large in-domain manually-constructed treebanks, making it more practical to use statistical parsers in applications that need access to aspects of predicate-argument structure."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35551590"
                        ],
                        "name": "Steven P. Abney",
                        "slug": "Steven-P.-Abney",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Abney",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven P. Abney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 36
                            }
                        ],
                        "text": "A different approach is proposed by Abney (1997), who develops log-linear models for attribute-value grammars, such as Head-driven Phrase Structure Grammar (HPSG)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5361885,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "61dffff2116f3543e71d536a18308fa4fc5e53c3",
            "isKey": false,
            "numCitedBy": 236,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Probabilistic analogues of regular and context-free grammars are well known in computational linguistics, and currently the subject of intensive research. To date, however, no satisfactory probabilistic analogue of attribute-value grammars has been proposed: previous attempts have failed to define an adequate parameter-estimation algorithm.In the present paper, I define stochastic attribute-value grammars and give an algorithm for computing the maximum-likelihood estimate of their parameters. The estimation algorithm is adapted from Della Pietra, Della Pietra, and Lafferty (1995). To estimate model parameters, it is necessary to compute the expectations of certain functions under random fields. In the application discussed by Della Pietra, Della Pietra, and Lafferty (representing English orthographic constraints), Gibbs sampling can be used to estimate the needed expectations. The fact that attribute-value grammars generate constrained languages makes Gibbs sampling inapplicable, but I show that sampling can be done using the more general Metropolis-Hastings algorithm."
            },
            "slug": "Stochastic-Attribute-Value-Grammars-Abney",
            "title": {
                "fragments": [],
                "text": "Stochastic Attribute-Value Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Stochastic attribute-value grammars are defined and an algorithm for computing the maximum-likelihood estimate of their parameters is given and it is shown that sampling can be done using the more general Metropolis-Hastings algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145693410"
                        ],
                        "name": "Ted Briscoe",
                        "slug": "Ted-Briscoe",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Briscoe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Briscoe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 25
                            }
                        ],
                        "text": "The GRs are described in Briscoe (2006), Briscoe and Carroll (2006), and Briscoe, Carroll, and Watson (2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 114
                            }
                        ],
                        "text": "The dynamic programming method uses inside and outside scores to calculate expectations, similar to the inside\u2013outside algorithm for estimating the parameters of a PCFG from unlabeled data (Lari and Young 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 74
                            }
                        ],
                        "text": "The ta relation, which identifies text adjuncts delimited by punctuation (Briscoe 2006), is difficult to assign correctly to the parser output."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16136632,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2fa44c586aac209a0022b54c8d385e77e49f13ea",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 69,
            "paperAbstract": {
                "fragments": [],
                "text": "This report describes the tag sequence grammars released as part of the Robust Accurate Statistical Parsing (RASP) system. It is intended to help users of RASP understand the linguistic and engineering rationale behind the grammars and prepare them to customize the system for their application. It also contains a fairly exhaustive list of references to extant work utilizing the RASP parser."
            },
            "slug": "An-introduction-to-tag-sequence-grammars-and-the-Briscoe",
            "title": {
                "fragments": [],
                "text": "An introduction to tag sequence grammars and the RASP system parser"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This report describes the tag sequence grammars released as part of the Robust Accurate Statistical Parsing (RASP) system to help users of RASP understand the linguistic and engineering rationale behind the Grammars and prepare them to customize the system for their application."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3259253"
                        ],
                        "name": "Kristina Toutanova",
                        "slug": "Kristina-Toutanova",
                        "structuredName": {
                            "firstName": "Kristina",
                            "lastName": "Toutanova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristina Toutanova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692491"
                        ],
                        "name": "S. Shieber",
                        "slug": "S.-Shieber",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Shieber",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shieber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3209288"
                        ],
                        "name": "D. Flickinger",
                        "slug": "D.-Flickinger",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Flickinger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Flickinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2949607"
                        ],
                        "name": "S. Oepen",
                        "slug": "S.-Oepen",
                        "structuredName": {
                            "firstName": "Stephan",
                            "lastName": "Oepen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Oepen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "Probabilistic disambiguation models for wide-coverage HPSG parsing."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 163
                            }
                        ],
                        "text": "Statistical parsers have been developed for TAG (Chiang 2000; Sarkar and Joshi 2003), LFG (Riezler et al. 2002; Kaplan et al. 2004; Cahill et al. 2004), and HPSG (Toutanova et al. 2002; Toutanova, Markova, and Manning 2004; Miyao and Tsujii 2004; Malouf and van Noord 2004), among others."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 189
                            }
                        ],
                        "text": "Penn Treebank conversions have also been carried out for other linguistic formalisms, including TAG (Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000), LFG (Burke et al. 2004), and HPSG (Miyao, Ninomiya, and Tsujii 2004)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 106
                            }
                        ],
                        "text": "Of course some of these advantages could be obtained with other grammar formalisms, such as TAG, LFG, and HPSG, although CCG is especially well-suited to analysing coordination and long-range dependencies."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "Miyao and Tsujii (2005) address the issue of practical estimation using an automatically extracted HPSG grammar."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "Parse disambiguation for a rich HPSG grammar."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 92
                            }
                        ],
                        "text": "Log-linear models have previously been applied to statistical parsing (Johnson et al. 1999; Toutanova et al. 2002; Riezler et al. 2002; Malouf and van Noord 2004), but typically under the assumption that all possible parses for a sentence can be enumerated."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 192
                            }
                        ],
                        "text": "In Miyao and Tsujii (2003b, 2003a) log-linear models are developed for automatically extracted grammars for Lexicalized Tree Adjoining Grammar (LTAG) and Head Driven Phrase Structure Grammar (HPSG)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "Miyao and Tsujii (2004) evaluate their HPSG parser against PropBank (Palmer, Gildea, and Kingsbury 2005)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 79
                            }
                        ],
                        "text": "Malouf and van Noord (2004) present similar work to ours, in the context of an HPSG grammar for Dutch."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "The leaf projection path view of parse trees: Exploring string kernels for HPSG parse selection."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 157
                            }
                        ],
                        "text": "A different approach is proposed by Abney (1997), who develops log-linear models for attribute-value grammars, such as Head-driven Phrase Structure Grammar (HPSG)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 218465887,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dd8a36ddf3ab42cf5310c2ef0e1a2fb7742a6876",
            "isKey": true,
            "numCitedBy": 69,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe experiments on HPSG parse disambiguation using the Redwoods HPSG treebank. We have explored building probabilistic models for parse disambiguation using this rich HPSG treebank, assessing the effectiveness of different kinds of information. We describe generative and discriminative models using analogous features and compare their performance on the disambiguation task."
            },
            "slug": "Parse-Disambiguation-for-a-Rich-HPSG-Grammar-Toutanova-Manning",
            "title": {
                "fragments": [],
                "text": "Parse Disambiguation for a Rich HPSG Grammar"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This paper explores building probabilistic models for parse disambiguation using the Redwoods HPSG treebank, and describes generative and discriminative models using analogous features and compares their performance on the disambigsuation task."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747771"
                        ],
                        "name": "Matthew Lease",
                        "slug": "Matthew-Lease",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Lease",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew Lease"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749837"
                        ],
                        "name": "Eugene Charniak",
                        "slug": "Eugene-Charniak",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Charniak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Charniak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 257,
                                "start": 234
                            }
                        ],
                        "text": "There is some experimental evidence showing that, perhaps not surprisingly, the performance of parsers trained on the WSJ Penn Treebank drops significantly when the parser is applied to domains outside of newspaper text (Gildea 2001; Lease and Charniak 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 892890,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7c3c210952f8707f2f4f5dfcb4e476d7d76c2e41",
            "isKey": false,
            "numCitedBy": 137,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a preliminary study of several parser adaptation techniques evaluated on the GENIA corpus of MEDLINE abstracts [1,2]. We begin by observing that the Penn Treebank (PTB) is lexically impoverished when measured on various genres of scientific and technical writing, and that this significantly impacts parse accuracy. To resolve this without requiring in-domain treebank data, we show how existing domain-specific lexical resources may be leveraged to augment PTB-training: part-of-speech tags, dictionary collocations, and named-entities. Using a state-of-the-art statistical parser [3] as our baseline, our lexically-adapted parser achieves a 14.2% reduction in error. With oracle-knowledge of named-entities, this error reduction improves to 21.2%."
            },
            "slug": "Parsing-Biomedical-Literature-Lease-Charniak",
            "title": {
                "fragments": [],
                "text": "Parsing Biomedical Literature"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown how existing domain-specific lexical resources may be leveraged to augment PTB-training: part-of-speech tags, dictionary collocations, and named-entities, without requiring in-domain treebank data."
            },
            "venue": {
                "fragments": [],
                "text": "IJCNLP"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17446277"
                        ],
                        "name": "J. Goodman",
                        "slug": "J.-Goodman",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Goodman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Goodman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 76
                            }
                        ],
                        "text": "We also define a new efficient parsing algorithm for such a model, based on Goodman (1996), which maximizes the expected recall of dependencies."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 665441,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dd85cca2c133835ea29069a6a4438c70185bd427",
            "isKey": false,
            "numCitedBy": 172,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Many different metrics exist for evaluating parsing results, including Viterbi, Crossing Brackets Rate, Zero Crossing Brackets Rate, and several others. However, most parsing algorithms, including the Viterbi algorithm, attempt to optimize the same metric, namely the probability of getting the correct labelled tree. By choosing a parsing algorithm appropriate for the evaluation metric, better performance can be achieved. We present two new algorithms: the \"Labelled Recall Algorithm,\" which maximizes the expected Labelled Recall Rate, and the \"Bracketed Recall Algorithm,\" which maximizes the Bracketed Recall Rate. Experimental results are given, showing that the two new algorithms have improved performance over the Viterbi algorithm on many criteria, especially the ones that they optimize."
            },
            "slug": "Parsing-Algorithms-and-Metrics-Goodman",
            "title": {
                "fragments": [],
                "text": "Parsing Algorithms and Metrics"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "Two new algorithms are presented: the \"Labelled Recall Algorithm,\" which maximizes the expected Labelled Recall Rate, and the \"Bracketed Recall Al algorithm\", which maximized the Bracketed recall Rate."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152465203"
                        ],
                        "name": "Mark Johnson",
                        "slug": "Mark-Johnson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47428006"
                        ],
                        "name": "S. Canon",
                        "slug": "S.-Canon",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Canon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Canon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3140471"
                        ],
                        "name": "Zhiyi Chi",
                        "slug": "Zhiyi-Chi",
                        "structuredName": {
                            "firstName": "Zhiyi",
                            "lastName": "Chi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhiyi Chi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3289329"
                        ],
                        "name": "S. Riezler",
                        "slug": "S.-Riezler",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Riezler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Riezler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 232
                            }
                        ],
                        "text": "Log-linear models have been applied to a number of problems in NLP, for example, POS tagging (Ratnaparkhi 1996; Lafferty, McCallum, and Pereira 2001), named entity recognition (Borthwick 1999), chunking (Koeling 2000), and parsing (Johnson et al. 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 91
                            }
                        ],
                        "text": "The conditional-likelihood estimator is also consistent for the conditional distributions (Johnson et al. 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 71
                            }
                        ],
                        "text": "Log-linear models have previously been applied to statistical parsing (Johnson et al. 1999; Toutanova et al. 2002; Riezler et al. 2002; Malouf and van Noord 2004), but typically under the assumption that all possible parses for a sentence can be enumerated."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 278
                            }
                        ],
                        "text": "\u2026model given the data, minus a Gaussian prior\n2 We could model predicate\u2013argument dependencies together with the derivation, but we wanted to use features from the derivation only, following Hockenmaier and Steedman (2002b).\nterm to prevent overfitting (Chen and Rosenfeld 1999; Johnson et al. 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 26
                            }
                        ],
                        "text": "This is not a problem for Johnson et al. (1999) because their grammars are hand-written and constraining enough to allow the analyses for each sentence to be enumerated."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 14
                            }
                        ],
                        "text": "E-mail: james@it.usyd.edu.au."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Johnson et al. (1999) propose an alternative solution, which is to maximize the conditional likelihood function."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17435621,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "463dbd690d912b23d29b7581fb6b253b36f50394",
            "isKey": true,
            "numCitedBy": 233,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Log-linear models provide a statistically sound framework for Stochastic \"Unification-Based\" Grammars (SUBGs) and stochastic versions of other kinds of grammars. We describe two computationally-tractable ways of estimating the parameters of such grammars from a training corpus of syntactic analyses, and apply these to estimate a stochastic version of Lexical-Functional Grammar."
            },
            "slug": "Estimators-for-Stochastic-\"Unification-Based\"-Johnson-Geman",
            "title": {
                "fragments": [],
                "text": "Estimators for Stochastic \"Unification-Based\" Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "Two computationally-tractable ways of estimating the parameters of Stochastic \"Unification-Based\" Grammars from a training corpus of syntactic analyses are described and applied to estimate a stochastic version of Lexical-Functional Grammar."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145693410"
                        ],
                        "name": "Ted Briscoe",
                        "slug": "Ted-Briscoe",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Briscoe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Briscoe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144708726"
                        ],
                        "name": "John A. Carroll",
                        "slug": "John-A.-Carroll",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Carroll",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John A. Carroll"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145047601"
                        ],
                        "name": "Rebecca Watson",
                        "slug": "Rebecca-Watson",
                        "structuredName": {
                            "firstName": "Rebecca",
                            "lastName": "Watson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rebecca Watson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 136
                            }
                        ],
                        "text": "The results in Table 16 were obtained by parsing the sentences from CCGbank corresponding to those in the 560-sentence test set used by Briscoe, Carroll, and Watson (2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 221
                            }
                        ],
                        "text": "In order to facilitate comparisons with parsers using different formalisms, we also evaluate on the publicly available DepBank (King et al. 2003), using the Briscoe and Carroll annotation consistent with the RASP parser (Briscoe, Carroll, and Watson 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 73
                            }
                        ],
                        "text": "The GRs are described in Briscoe (2006), Briscoe and Carroll (2006), and Briscoe, Carroll, and Watson (2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 113
                            }
                        ],
                        "text": "All the results were obtained using the RASP evaluation scripts, with the results for the RASP parser taken from Briscoe, Carroll, and Watson (2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 0
                            }
                        ],
                        "text": "Briscoe, Carroll,\nand Watson (2006) split the 700 sentences in DepBank into a test and development set, but the latter only consists of 140 sentences which we found was not enough to reliably create the transformation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 198
                            }
                        ],
                        "text": "The micro-averaged scores are calculated by aggregating the counts for all the relations in the hierarchy, whereas the macro-averaged scores are the mean of the individual scores for each relation (Briscoe, Carroll, and Watson 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7294316,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7410c010a38e7e23f38c3c6e898d5695a4874c61",
            "isKey": true,
            "numCitedBy": 437,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the new release of the RASP (robust accurate statistical parsing) system, designed for syntactic annotation of free text. The new version includes a revised and more semantically-motivated output representation, an enhanced grammar and part-of-speech tagger lexicon, and a more flexible and semi-supervised training method for the structural parse ranking model. We evaluate the released version on the WSJ using a relational evaluation scheme, and describe how the new release allows users to enhance performance using (in-domain) lexical information."
            },
            "slug": "The-Second-Release-of-the-RASP-System-Briscoe-Carroll",
            "title": {
                "fragments": [],
                "text": "The Second Release of the RASP System"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "The new release of the RASP (robust accurate statistical parsing) system, designed for syntactic annotation of free text, is described, including a revised and more semantically-motivated output representation, an enhanced grammar and part-of-speech tagger lexicon, and a more flexible and semi-supervised training method for the structural parse ranking model."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3461596"
                        ],
                        "name": "Johan Bos",
                        "slug": "Johan-Bos",
                        "structuredName": {
                            "firstName": "Johan",
                            "lastName": "Bos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Johan Bos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144523372"
                        ],
                        "name": "S. Clark",
                        "slug": "S.-Clark",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145332819"
                        ],
                        "name": "Mark Steedman",
                        "slug": "Mark-Steedman",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Steedman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Steedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733593"
                        ],
                        "name": "J. Curran",
                        "slug": "J.-Curran",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Curran",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Curran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3118681"
                        ],
                        "name": "J. Hockenmaier",
                        "slug": "J.-Hockenmaier",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Hockenmaier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hockenmaier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9317139,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c7a1d0b183d0db47a438299d023684491461eac",
            "isKey": false,
            "numCitedBy": 260,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper shows how to construct semantic representations from the derivations produced by a wide-coverage CCG parser. Unlike the dependency structures returned by the parser itself, these can be used directly for semantic interpretation. We demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen WSJ text. We believe this is a major step towards widecoverage semantic interpretation, one of the key objectives of the field of NLP."
            },
            "slug": "Wide-Coverage-Semantic-Representations-from-a-CCG-Bos-Clark",
            "title": {
                "fragments": [],
                "text": "Wide-Coverage Semantic Representations from a CCG Parser"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is demonstrated that well-formed semantic representations can be produced for over 97% of the sentences in unseen WSJ text, a major step towards widecoverage semantic interpretation, one of the key objectives of the field of NLP."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144956443"
                        ],
                        "name": "F. Xia",
                        "slug": "F.-Xia",
                        "structuredName": {
                            "firstName": "F.",
                            "lastName": "Xia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Xia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145755155"
                        ],
                        "name": "Martha Palmer",
                        "slug": "Martha-Palmer",
                        "structuredName": {
                            "firstName": "Martha",
                            "lastName": "Palmer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martha Palmer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714374"
                        ],
                        "name": "A. Joshi",
                        "slug": "A.-Joshi",
                        "structuredName": {
                            "firstName": "Aravind",
                            "lastName": "Joshi",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Joshi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u00a9 2007 Association for Computational Linguistics"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 99
                            }
                        ],
                        "text": "In order to deal with the problem of the additional, nonstandard CCG derivations, a conditional model of dependency structures is presented, based on Collins (1996), in which the dependencies are modeled directly and derivations are not modeled at all."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1880978,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d013dc1ade36711fc468cc9a52fc70142a0f116b",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Grammars are core elements of many NLP applications. In this paper, we present a system that automatically extracts lexicalized grammars from annotated corpora. The data produced by this system have been used in several tasks, such as training NLP tools (such as Supertaggers) and estimating the coverage of hand-crafted grammars. We report experimental results on two of those tasks and compare our approaches with related work."
            },
            "slug": "A-Uniform-Method-of-Grammar-Extraction-and-Its-Xia-Palmer",
            "title": {
                "fragments": [],
                "text": "A Uniform Method of Grammar Extraction and Its Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "This paper presents a system that automatically extracts lexicalized grammars from annotated corpora, and reports experimental results on two of those tasks and compares the approaches with related work."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2424234"
                        ],
                        "name": "Beatrice Santorini",
                        "slug": "Beatrice-Santorini",
                        "structuredName": {
                            "firstName": "Beatrice",
                            "lastName": "Santorini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beatrice Santorini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2063206"
                        ],
                        "name": "Mary Ann Marcinkiewicz",
                        "slug": "Mary-Ann-Marcinkiewicz",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Marcinkiewicz",
                            "middleNames": [
                                "Ann"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mary Ann Marcinkiewicz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 98
                            }
                        ],
                        "text": "Corpus-oriented grammar development for acquiring a head-driven phrase structure grammar from the Penn Treebank."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 55
                            }
                        ],
                        "text": "An early attempt from the Edinburgh project at wide-coverage CCG parsing is presented in Clark, Hockenmaier, and Steedman (2002)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 183
                            }
                        ],
                        "text": "Another difference between DepBank and CCGbank is that DepBank has been manually corrected, whereas CCGbank, including the test sections, has been produced semiautomatically from the Penn Treebank."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Penn Treebank conversions have also been carried out for other linguistic formalisms, including TAG (Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000), LFG (Burke et al. 2004), and HPSG (Miyao, Ninomiya, and Tsujii 2004)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 60
                            }
                        ],
                        "text": "Developing the techniques to extract a CCG grammar from the Penn Treebank, together with the preprocessing of the Penn Treebank which was required, took a number of years; and developing the Penn Treebank itself also took a number of years."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 196
                            }
                        ],
                        "text": "This earlier GR scheme is less like the dependencies output by the CCG parser, and the comparison would be complicated further by fact that, unlike CCGbank, the SUSANNE corpus is not based on the Penn Treebank."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 169
                            }
                        ],
                        "text": "One difference is that we use a Maximum Entropy tagger which allows more flexibility in terms of the features that can\nbe encoded; for example, we have found that using Penn Treebank POS tags as features significantly improves supertagging accuracy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 68
                            }
                        ],
                        "text": "CCGbank was created by converting the phrase-structure trees in the Penn Treebank into CCG normal-form derivations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2374,
                                "start": 2361
                            }
                        ],
                        "text": "S[ng]\\NP \u21d2 (S\\NP)\\(S\\NP) S[adj]\\NP \u21d2 (S\\NP)\\(S\\NP) S[to]\\NP \u21d2 (S\\NP)\\(S\\NP)\nS[ng]\\NP \u21d2 (S\\NP)/(S\\NP) S[pss]\\NP \u21d2 S/S S[ng]\\NP \u21d2 S/S S[adj]\\NP \u21d2 S/S S[to]\\NP \u21d2 S/S\nS[ng]\\NP \u21d2 S\\S S[dcl] \u21d2 S\\S\nS[ng]\\NP \u21d2 NP S[to]\\NP \u21d2 N\\N\nPunctuation Rules\nA number of categories absorb a comma to the left, implementing the following schema:\n, X \u21d2 X\nThe categories are as follows, where S[\u2217] matches an S category with any or no feature:\nN, NP, S[\u2217], N/N, NP\\NP, PP\\PP, S/S, S\\S, S[\u2217]\\NP, (S\\NP)\\(S\\NP), (S\\NP)/(S\\NP), ((S\\NP)\\(S\\NP))\\((S\\NP)\\(S\\NP))\nSimilarly, a number of categories absorb a comma to the right, implementing the following schema:\nX , \u21d2 X\nThe categories are as follows:\nN, NP, PP, S[dcl], N/N, NP\\NP, S/S, S\\S, S[\u2217]\\NP, (S[dcl]\\NP)/S, (S[dcl]\\S[dcl])\\NP, (S[dcl]\\NP)/NP, (S[dcl]\\NP)/PP, (NP\\NP)/(S[dcl]\\NP), (S\\NP)\\(S\\NP), (S\\NP)/(S\\NP)\nThese are the categories which absorb a colon or semicolon to the left, in the same way as for the comma:\nN, NP, S[dcl], NP\\NP, S[\u2217]\\NP, (S\\NP)\\(S\\NP)\nThese are the categories which absorb a colon or semicolon to the right:\nN, NP, PP, S[dcl], NP\\NP, S/S, S[\u2217]\\NP, (S[dcl]\\NP)/S[dcl], (S\\NP)\\(S\\NP), (S\\NP)/(S\\NP)\nThese are the categories which absorb a period to the right:\nN, NP, S[\u2217], PP, NP\\NP, S\\S, S[\u2217]\\NP, S[\u2217]\\PP, (S[dcl]\\S[\u2217])\\NP, (S\\NP)\\(S\\NP)\nThese are the categories which absorb a round bracket to the left:\nN, NP, S[dcl], NP\\NP, (S\\NP)\\(S\\NP)\nThese are the categories which absorb a round bracket to the right:\nN, NP, S[dcl], N\\N, N/N, NP\\NP, S[dcl]\\NP, S/S, S\\S, (N/N)\\(N/N), (S\\NP)\\(S\\NP), (S\\NP)/(S\\NP)\nThere are some binary type-changing rules involving commas, where the two categories on the left are rewritten bottom-up as the category on the right:\n, NP \u21d2 (S\\NP)\\(S\\NP) NP , \u21d2 S/S\nS[dcl]/S[dcl] , \u21d2 S/S S[dcl]/S[dcl] , \u21d2 (S\\NP)\\(S\\NP) S[dcl]/S[dcl] , \u21d2 (S\\NP)/(S\\NP) S[dcl]/S[dcl] , \u21d2 S\\S S[dcl]\\S[dcl] , \u21d2 S/S\nFinally, there is a comma coordination rule, and a semicolon coordination rule, represented by the following two schema:\n, X \u21d2 X\\X\n; X \u21d2 X\\X\nThe categories which instantiate the comma schema are as follows:\nN, NP, S[\u2217], N/N, NP\\NP, S[\u2217]\\NP, (S\\NP)\\(S\\NP)\nThe categories which instantiate the semicolon schema are as follows:\nNP, S[\u2217], S[\u2217]\\NP\nOther Rules\nThere are two rules for combining sequences of noun phrases and sequences of declarative sentences:\nNP NP \u21d2 NP S[dcl] S[dcl] \u21d2 S[dcl]\nFinally, there are some coordination constructions in the original Penn Treebank which were difficult to convert into CCGbank analyses, for which the following rule is used:\nconj N \u21d2 N"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 158
                            }
                        ],
                        "text": "The lexicalized grammar formalism used is Combinatory Categorial Grammar (CCG), and the grammar is automatically extracted from CCGbank, a CCG version of the Penn Treebank."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 164
                            }
                        ],
                        "text": "For the gold standard we chose the version of DepBank reannotated by Briscoe and Carroll (2006) (hereafter B&C), consisting of 700 sentences from Section 23 of the Penn Treebank."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 172
                            }
                        ],
                        "text": "There are some constructions in CCGbank\u2014 noun compounds being a prominent example\u2014which are often incorrectly analyzed, simply because the required information is not in the Penn Treebank."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 82
                            }
                        ],
                        "text": "We used the CCGbank sentences because these differ in some ways from the original Penn Treebank sentences (there are no quotation marks in CCGbank, for example) and the parser has been trained on CCGbank."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 183
                            }
                        ],
                        "text": "This method would be useful when converting the output of the Collins parser into an alternative representation (Kaplan et al. 2004): Applying the transformation to the gold-standard Penn Treebank trees and comparing with DepBank would provide an upper bound on the performance of the Collins parser and give some indication of the effectiveness of the transformation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 4
                            }
                        ],
                        "text": "The Penn Treebank trees of the Collins and Charniak parsers, and the GRs of the Buchholz parser, are mapped into the required grammatical relations, with the result that the GR finder of Buchholz is the most accurate."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 174
                            }
                        ],
                        "text": "Even here we experienced some unexpected difficulties, because some of the tokenization is different between DepBank and CCGbank (even though both resources are based on the Penn Treebank), and there are some sentences in DepBank which have been significantly shortened (for no apparent reason) compared to the original Penn Treebank sentences."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 122
                            }
                        ],
                        "text": "There is some experimental evidence showing that, perhaps not surprisingly, the performance of parsers trained on the WSJ Penn Treebank drops significantly when the parser is applied to domains outside of newspaper text (Gildea 2001; Lease and Charniak 2005)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 106
                            }
                        ],
                        "text": "One question we are often asked is whether the CCG derivations\noutput by the parser could be converted to Penn Treebank\u2013style trees to enable a comparison with, for example, the Collins and Charniak parsers."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 75
                            }
                        ],
                        "text": "The difficulty is that CCG derivations often have a different shape to the Penn Treebank analyses (coordination being a prime example) and reversing the mapping used by Hockenmaier to create CCGbank is a far from trivial task."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 215
                            }
                        ],
                        "text": "One downside of hand-built grammars is that they are typically less robust, which Kaplan et al. address by developing a \u201cfragment\u201d grammar, together with a \u201cskimming mode,\u201d which increases coverage on Section 23 of the Penn Treebank from 80% to 100%."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 48
                            }
                        ],
                        "text": "Because of the small number of questions in the Penn Treebank, the performance of the parser was extremely poor\u2014 well below that required for a working QA system."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 84
                            }
                        ],
                        "text": "Kaplan et al. (2004) compare the Collins parser with the Parc LFG parser by mapping Penn Treebank parses into the dependencies of DepBank, claiming that the LFG parser is more accurate with only a slight reduction in speed."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 38
                            }
                        ],
                        "text": "Automated extraction of TAGS from the Penn Treebank."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 113
                            }
                        ],
                        "text": "The treebank is CCGbank (Hockenmaier and Steedman 2002a; Hockenmaier 2003a), a CCG version of the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 100
                            }
                        ],
                        "text": "A similar comment would apply to other parsers evaluated on, and using grammars extracted from, the Penn Treebank."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 50
                            }
                        ],
                        "text": "Building a large annotated corpus of English: The Penn Treebank."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 56
                            }
                        ],
                        "text": "TAG grammars have been automatically extracted from the Penn Treebank, using techniques similar to those used by Hockenmaier (Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 86
                            }
                        ],
                        "text": "This is especially true if the alternative output is significantly different from the Penn Treebank trees and if the information required to produce the alternative output is hard to extract."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 134
                            }
                        ],
                        "text": "We were able to annotate approximately 1, 000 questions in around a week, which led to an accurate supertagger and, combined with the Penn Treebank parsing model, an accurate parser of questions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 61
                            }
                        ],
                        "text": "This is because some of the phrase structure subtrees in the\nPenn Treebank are difficult to convert to CCG combinatory rules, and because of noise introduced by the Treebank conversion process."
                    },
                    "intents": []
                }
            ],
            "corpusId": 252796,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b44fcbeea9415d400c5f5789d6b892b6f98daff",
            "isKey": true,
            "numCitedBy": 8177,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant."
            },
            "slug": "Building-a-Large-Annotated-Corpus-of-English:-The-Marcus-Santorini",
            "title": {
                "fragments": [],
                "text": "Building a Large Annotated Corpus of English: The Penn Treebank"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "As a result of this grant, the researchers have now published on CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, which includes a fully hand-parsed version of the classic Brown corpus."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144324746"
                        ],
                        "name": "Mark Stevenson",
                        "slug": "Mark-Stevenson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Stevenson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Stevenson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48960199"
                        ],
                        "name": "M. Greenwood",
                        "slug": "M.-Greenwood",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Greenwood",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Greenwood"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 55989318,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23200cb65981f58e6c0d634ad31d42235b33b5fd",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Several techniques for the automatic acquisition of Information Extraction (IE) systems have used dependency trees to form the basis of an extraction pattern representation. These approaches have used a variety of pattern models (schemes for representing IE patterns based on particular parts of the dependency analysis). An appropriate pattern model should be expressive enough to represent the information which is to be extracted from text without being overly complex. Previous investigations into the appropriateness of the currently proposed models have been limited. This paper compares a variety of pattern models, including ones which have been previously reported and variations of them. Each model is evaluated using existing data consisting of IE scenarios from two very different domains (newswire stories and biomedical journal articles). The models are analysed in terms of their ability to represent relevant information, number of patterns generated and performance on an IE scenario. It was found that the best performance was observed from two models which use the majority of relevant portions of the dependency tree without including irrelevant sections."
            },
            "slug": "Dependency-Pattern-Models-for-Information-Stevenson-Greenwood",
            "title": {
                "fragments": [],
                "text": "Dependency Pattern Models for Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper compares a variety of pattern models, including ones which have been previously reported and variations of them, and finds that the best performance was observed from two models which use the majority of relevant portions of the dependency tree without including irrelevant sections."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714577"
                        ],
                        "name": "S. D. Pietra",
                        "slug": "S.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pietra",
                            "middleNames": [
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 982,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b951b9f78b98a186ba259027996a48e4189d37e5",
            "isKey": false,
            "numCitedBy": 1305,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classification in natural language processing."
            },
            "slug": "Inducing-Features-of-Random-Fields-Pietra-Pietra",
            "title": {
                "fragments": [],
                "text": "Inducing Features of Random Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733593"
                        ],
                        "name": "J. Curran",
                        "slug": "J.-Curran",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Curran",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Curran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144523372"
                        ],
                        "name": "S. Clark",
                        "slug": "S.-Clark",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Clark"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 158
                            }
                        ],
                        "text": "Generalized Iterative Scaling (Darroch and Ratcliff 1972) is a common choice in the NLP literature for estimating a log-linear model (e.g., Ratnaparkhi 1998; Curran and Clark 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 79
                            }
                        ],
                        "text": "The CCG parser results are based on automatically assigned POS tags, using the Curran and Clark (2003) tagger."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 78
                            }
                        ],
                        "text": "In our notation, %l indicates the word associated with the lexical category and %f is the head of the constituent filling the\nargument slot."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 62
                            }
                        ],
                        "text": "With automatically assigned POS tags, using the POS tagger of Curran and Clark (2003), the accuracies drop to 91.5% and 32.5%."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 163
                            }
                        ],
                        "text": "The table gives results when using gold standard POS tags and, in the final two columns, when using POS tags automatically assigned by the POS tagger described in Curran and Clark (2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 261,
                                "start": 240
                            }
                        ],
                        "text": "\u2026iterative scaling methods for many NLP tasks.3 Malouf uses standard numerical computation libraries\n3 One NLP task for which we have found GIS to be especially suitable is sequence tagging, and we still use GIS to estimate tagging models (Curran and Clark 2003).\nas the basis of his implementation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 94
                            }
                        ],
                        "text": "A key component of the parsing system is a Maximum Entropy CCG supertagger (Ratnaparkhi 1996; Curran and Clark 2003) which assigns lexical categories to words in a sentence."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1400617,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1358bc877f106ffd066796f66ed6f8242b99d2d1",
            "isKey": false,
            "numCitedBy": 125,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper investigates two elements of Maximum Entropy tagging: the use of a correction feature in the Generalised Iterative Scaling (GIS) estimation algorithm, and techniques for model smoothing. We show analytically and empirically that the correction feature, assumed to be required for the correctness of GIS, is unnecessary. We also explore the use of a Gaussian prior and a simple cutoff for smoothing. The experiments are performed with two tagsets: the standard Penn Treebank POS tagset and the larger set of lexical types from Combinatory Categorial Grammar."
            },
            "slug": "Investigating-GIS-and-Smoothing-for-Maximum-Entropy-Curran-Clark",
            "title": {
                "fragments": [],
                "text": "Investigating GIS and Smoothing for Maximum Entropy Taggers"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is shown analytically and empirically that the correction feature, assumed to be required for the correctness of GIS, is unnecessary, and the use of a Gaussian prior and a simple cutoff for smoothing is explored."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756339"
                        ],
                        "name": "M. Moortgat",
                        "slug": "M.-Moortgat",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Moortgat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Moortgat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 30
                            }
                        ],
                        "text": "A supertagger for Combinatory Categorial Grammar."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 192
                            }
                        ],
                        "text": "A recent development in the theory of CCG is the multi-modal treatment given by Baldridge (2002) and Baldridge and Kruijff (2003), following the type-logical approaches to Categorial Grammar (Moortgat 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 24
                            }
                        ],
                        "text": "Multi-modal Combinatory Categorial Grammar."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 54
                            }
                        ],
                        "text": "The lexicalized grammar formalism used is Combinatory Categorial Grammar (CCG), and the grammar is automatically extracted from CCGbank, a CCG version of the Penn Treebank."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Categorial Grammars."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 12
                            }
                        ],
                        "text": "Combinatory Categorial Grammar (CCG) (Steedman 1996, 2000) is a type-driven lexicalized theory of grammar based on Categorial Grammar (Wood 1993)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 56
                            }
                        ],
                        "text": "Lexically Specified Derivational Control in Combinatory Categorial Grammar."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 46
                            }
                        ],
                        "text": "Efficient normal-form parsing for Combinatory Categorial Grammar."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 56
                            }
                        ],
                        "text": "The lexicalized grammar formalism we use is Combinatory Categorial Grammar (CCG; Steedman 2000)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 16
                            }
                        ],
                        "text": "In the original Categorial Grammar (Bar-Hillel 1953), which is context-free, there are two rules of functional application:\nX/Y Y \u21d2 X (>) (3)\nY X\\Y \u21d2 X ( ) (4)\nwhere X and Y denote categories (either basic or complex)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 25
                            }
                        ],
                        "text": "CCG extends the original Categorial Grammar by introducing a number of additional combinatory rules."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 68
                            }
                        ],
                        "text": "Malouf and van Noord (2004) present similar work to ours, in the context of an HPSG grammar for Dutch."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 64
                            }
                        ],
                        "text": "2003a. Data and Models for Statistical Parsing with Combinatory Categorial Grammar."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 66
                            }
                        ],
                        "text": "2002b. Generative models for statistical parsing with Combinatory Categorial Grammar."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5272107,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "b92b7893190bf647f06b89d8024ece85c4d6b02f",
            "isKey": true,
            "numCitedBy": 571,
            "numCiting": 326,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Categorial-Type-Logics-Moortgat",
            "title": {
                "fragments": [],
                "text": "Categorial Type Logics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708708"
                        ],
                        "name": "G. Kruijff",
                        "slug": "G.-Kruijff",
                        "structuredName": {
                            "firstName": "Geert-Jan",
                            "lastName": "Kruijff",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kruijff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1387994164"
                        ],
                        "name": "Jason Baldridge",
                        "slug": "Jason-Baldridge",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Baldridge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason Baldridge"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 101
                            }
                        ],
                        "text": "A recent development in the theory of CCG is the multi-modal treatment given by Baldridge (2002) and Baldridge and Kruijff (2003), following the type-logical approaches to Categorial Grammar (Moortgat 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14212327,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c678ff91c66442ecadb146d5e1ab9e76aa3bbe5",
            "isKey": false,
            "numCitedBy": 157,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper shows how Combinatory Categorial Grammar (CCG) can be adapted to take advantage of the extra resource-sensitivity provided by the Categorial Type Logic framework. The resulting reformulation, Multi-Modal CCG, supports lexically specified control over the applicability of combinatory rules, permitting a universal rule component and shedding the need for language-specific restrictions on rules. We discuss some of the linguistic motivation for these changes, define the Multi-Modal CCG system and demonstrate how it works on some basic examples. We furthermore outline some possible extensions and address computational aspects of Multi-Modal CCG."
            },
            "slug": "Multi-Modal-Combinatory-Categorial-Grammar-Kruijff-Baldridge",
            "title": {
                "fragments": [],
                "text": "Multi-Modal Combinatory Categorial Grammar"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "The paper shows how Combinatory Categorial Grammar (CCG) can be adapted to take advantage of the extra resource-sensitivity provided by theCategorial Type Logic framework, and supports lexically specified control over the applicability of combinatory rules."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "67337974"
                        ],
                        "name": "Miyao Yusuke",
                        "slug": "Miyao-Yusuke",
                        "structuredName": {
                            "firstName": "Miyao",
                            "lastName": "Yusuke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Miyao Yusuke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737901"
                        ],
                        "name": "Junichi Tsujii",
                        "slug": "Junichi-Tsujii",
                        "structuredName": {
                            "firstName": "Junichi",
                            "lastName": "Tsujii",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junichi Tsujii"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15084210,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "51836a978517a4fdd6b68f69d3821c0d1a339e09",
            "isKey": false,
            "numCitedBy": 124,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm is proposed for maximum entropy modeling. It enables probabilistic modeling of complete structures, such as transition sequences in Markov models and parse trees, without dividing them into independent sub-events. A probabilistic event is represented by a feature forest, which is a packed representation of features with ambiguities. The parameters are efficiently estimated by traversing each node in a feature forest by dynamic programming. Experiments showed the algorithm worked efficiently even when ambiguities in a feature forest cause an exponential explosion of unpacked structures."
            },
            "slug": "Maximum-entropy-estimation-for-feature-forests-Yusuke-Tsujii",
            "title": {
                "fragments": [],
                "text": "Maximum entropy estimation for feature forests"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "An algorithm is proposed for maximum entropy modeling that enables probabilistic modeling of complete structures, such as transition sequences in Markov models and parse trees, without dividing them into independent sub-events."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145493610"
                        ],
                        "name": "M. Kay",
                        "slug": "M.-Kay",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Kay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 122
                            }
                        ],
                        "text": "The algorithm used to build the packed charts is the CKY chart parsing algorithm (Kasami 1965; Younger 1967) described in Steedman (2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 34
                            }
                        ],
                        "text": "In this coordination example from Steedman (2000), backward crossed composition is used to combine the categories for buy, (S\\NP)/NP, and today, (S\\NP)\\(S\\NP), and similarly for cook and tomorrow, producing categories of the same type which can be coordinated."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 132
                            }
                        ],
                        "text": "Osborne (2000) investigates training on a sample of the analyses for each sentence, for example the top-n most probable according to some other probability model, or simply a random sample."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 38
                            }
                        ],
                        "text": "Combinatory Categorial Grammar (CCG) (Steedman 1996, 2000) is a type-driven lexicalized theory of grammar based on Categorial Grammar (Wood 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 10
                            }
                        ],
                        "text": "Following Steedman (2000), we place the following constraint on backward crossed composition (for all models): The Y category in (7) cannot be an N or NP category."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 67
                            }
                        ],
                        "text": "In this article we are concerned with the syntactic component; see Steedman (2000) for how a semantic interpretation can be composed during a syntactic derivation, and also Bos et al. (2004) for how semantic interpretations can be built for newspaper text using the wide-coverage parser described in\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 397,
                                "start": 380
                            }
                        ],
                        "text": "We follow Riezler et al. (2002) in using a discriminative estimation method by maximizing the conditional log-likelihood of the model given the data, minus a Gaussian prior\n2 We could model predicate\u2013argument dependencies together with the derivation, but we wanted to use features from the derivation only, following Hockenmaier and Steedman (2002b).\nterm to prevent overfitting (Chen and Rosenfeld 1999; Johnson et al. 1999)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 81
                            }
                        ],
                        "text": "The lexicalized grammar formalism we use is Combinatory Categorial Grammar (CCG; Steedman 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 25
                            }
                        ],
                        "text": "Chen, Stanley and Ronald Rosenfeld."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 271
                            }
                        ],
                        "text": "\u2026backward composition (  B) and backward crossed composition (  BX):\nY\\Z X\\Y \u21d2B X\\Z (  B) (6)\nY/Z X\\Y \u21d2B X/Z (  BX) (7)\nBackward composition provides an analysis for sentences involving \u201cargument cluster coordination,\u201d such as I gave a teacher an apple and a policeman a flower (Steedman 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Steedman (2000) gives a more precise definition of generalized forward composition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 126
                            }
                        ],
                        "text": "For example, the following combination allows analysis of sentences such as I offered, and may give, a flower to a policeman (Steedman 2000):\nmay give\n(S\\NP)/(S\\NP) ((S\\NP)/PP)/NP >B\n((S\\NP)/PP)/NP\nThis example shows how the categories for may and give combine, resulting in a category of the same\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 81
                            }
                        ],
                        "text": "This smoothing method for log-linear models is also proposed by Chen and Rosenfeld (1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 58106824,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "ce8cca19455e8d3055c57a9bafe882984c95a201",
            "isKey": true,
            "numCitedBy": 911,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "In computational linguistics, which began in the 1950's with machine translation, systems that are based mainly on the lexicon have a longer t r a d i t i o n than anything e l se f o r these purposes, twenty f i ve years must be allowed to count as a tradition. The bulk of many of the early translation systems was made up by a d ic t ionary whose ent r ies consisted of a rb i t ra ry ins t ruc t ions In machine language. In the early 60's, computational llnsulsts---at least those with theoretical pretentlons---abandoned this way of doing business for at least three related reasons:"
            },
            "slug": "Syntactic-Process-Kay",
            "title": {
                "fragments": [],
                "text": "Syntactic Process"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "In computational linguistics, which began in the 1950's with machine translation, systems that are based mainly on the lexicon have a longer lifespan than anything else, so twenty years must be allowed to count as a tradition."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2142738"
                        ],
                        "name": "Tracy Holloway King",
                        "slug": "Tracy-Holloway-King",
                        "structuredName": {
                            "firstName": "Tracy",
                            "lastName": "King",
                            "middleNames": [
                                "Holloway"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tracy Holloway King"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2953252"
                        ],
                        "name": "Dick Crouch",
                        "slug": "Dick-Crouch",
                        "structuredName": {
                            "firstName": "Dick",
                            "lastName": "Crouch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dick Crouch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3289329"
                        ],
                        "name": "S. Riezler",
                        "slug": "S.-Riezler",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Riezler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Riezler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281709"
                        ],
                        "name": "M. Dalrymple",
                        "slug": "M.-Dalrymple",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Dalrymple",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Dalrymple"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803660"
                        ],
                        "name": "R. Kaplan",
                        "slug": "R.-Kaplan",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Kaplan",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kaplan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 128
                            }
                        ],
                        "text": "In order to facilitate comparisons with parsers using different formalisms, we also evaluate on the publicly available DepBank (King et al. 2003), using the Briscoe and Carroll annotation consistent with the RASP parser (Briscoe, Carroll, and Watson 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 90
                            }
                        ],
                        "text": "Briscoe and Carroll (2006) evaluate the RASP parser on the Parc Dependency Bank (DepBank; King et al. 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 21
                            }
                        ],
                        "text": "The dynamic programming method uses inside and outside scores to calculate expectations, similar to the inside\u2013outside algorithm for estimating the parameters of a PCFG from unlabeled data (Lari and Young 1990)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 126783,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47a4a47c3fba8b2e1d649d1353355011f5de62c3",
            "isKey": false,
            "numCitedBy": 163,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we discuss the construction, features, and current uses of the PARC 700 DEPBANK. The PARC 700 DEPBANK is a dependency bank containing predicate-argument relations and a wide variety of other grammatical features. It was semi-automatically produced and boot-strapped from the output of a deep parser: this allowed for greater consistency of analysis and for more rapid construction."
            },
            "slug": "The-PARC-700-Dependency-Bank-King-Crouch",
            "title": {
                "fragments": [],
                "text": "The PARC 700 Dependency Bank"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "The PARC 700 DEPBANK is a dependency bank containing predicate-argument relations and a wide variety of other grammatical features that was semi-automatically produced and boot-strapped from the output of a deep parser."
            },
            "venue": {
                "fragments": [],
                "text": "LINC@EACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768065"
                        ],
                        "name": "Yusuke Miyao",
                        "slug": "Yusuke-Miyao",
                        "structuredName": {
                            "firstName": "Yusuke",
                            "lastName": "Miyao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yusuke Miyao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737901"
                        ],
                        "name": "Junichi Tsujii",
                        "slug": "Junichi-Tsujii",
                        "structuredName": {
                            "firstName": "Junichi",
                            "lastName": "Tsujii",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junichi Tsujii"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Miyao and Tsujii (2004) evaluate their HPSG parser against PropBank (Palmer, Gildea, and Kingsbury 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 224
                            }
                        ],
                        "text": "Statistical parsers have been developed for TAG (Chiang 2000; Sarkar and Joshi 2003), LFG (Riezler et al. 2002; Kaplan et al. 2004; Cahill et al. 2004), and HPSG (Toutanova et al. 2002; Toutanova, Markova, and Manning 2004; Miyao and Tsujii 2004; Malouf and van Noord 2004), among others."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1788449,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20164aaff56d0edca267fd9471fee3631fc960e3",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper evaluates the accuracy of HPSG parsing in terms of the identification of predicate-argument relations. We could directly compare the output of HPSG parsing with PropBank annotations, by assuming a unique mapping from HPSG semantic representation into PropBank annotation. Even though PropBank was not used for the training of a disambiguation model, an HPSG parser achieved the accuracy competitive with existing studies on the task of identifying PropBank annotations."
            },
            "slug": "Deep-Linguistic-Analysis-for-the-Accurate-of-Miyao-Tsujii",
            "title": {
                "fragments": [],
                "text": "Deep Linguistic Analysis for the Accurate Identification of Predicate-Argument Relations"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Even though PropBank was not used for the training of a disambiguation model, an HPSG parser achieved the accuracy competitive with existing studies on the task of identifying PropBank annotations."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113414328"
                        ],
                        "name": "Fernando Pereira",
                        "slug": "Fernando-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 112
                            }
                        ],
                        "text": "Log-linear models have been applied to a number of problems in NLP, for example, POS tagging (Ratnaparkhi 1996; Lafferty, McCallum, and Pereira 2001), named entity recognition (Borthwick 1999), chunking (Koeling 2000), and parsing (Johnson et al. 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 102
                            }
                        ],
                        "text": "A further motivation for using global models is that these do not suffer from the label bias problem (Lafferty, McCallum, and Pereira 2001), which is a potential problem for Ratnaparkhi\u2019s approach."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 104
                            }
                        ],
                        "text": "Wide-Coverage Efficient Statistical Parsing with CCG and Log-Linear Models\nStephen Clark\u2217 University of Oxford\nJames R. Curran\u2217\u2217 University of Sydney\nThis article describes a number of log-linear parsing models for an automatically extracted lexicalized grammar."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 219683473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4ba954b0412773d047dc41231c733de0c1f4926",
            "isKey": false,
            "numCitedBy": 13413,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."
            },
            "slug": "Conditional-Random-Fields:-Probabilistic-Models-for-Lafferty-McCallum",
            "title": {
                "fragments": [],
                "text": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work presents iterative parameter estimation algorithms for conditional random fields and compares the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2531738"
                        ],
                        "name": "P. Dienes",
                        "slug": "P.-Dienes",
                        "structuredName": {
                            "firstName": "P\u00e9ter",
                            "lastName": "Dienes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dienes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39305096"
                        ],
                        "name": "Amit Dubey",
                        "slug": "Amit-Dubey",
                        "structuredName": {
                            "firstName": "Amit",
                            "lastName": "Dubey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amit Dubey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 1
                            }
                        ],
                        "text": "Categories can be either basic or complex."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 197
                            }
                        ],
                        "text": "This has led to a number of proposals for post-processing the output of the Collins and Charniak parsers, in which trace sites are located and the antecedent of the trace determined (Johnson 2002; Dienes and Dubey 2003; Levy and Manning 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3246073,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9bba115683812358c7023b7f638179891d99a64d",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel approach for finding discontinuities that outperforms previously published results on this task. Rather than using a deeper grammar formalism, our system combines a simple unlexicalized PCFG parser with a shallow pre-processor. This pre-processor, which we call a trace tagger, does surprisingly well on detecting where discontinuities can occur without using phase structure information."
            },
            "slug": "Deep-Syntactic-Processing-by-Combining-Shallow-Dienes-Dubey",
            "title": {
                "fragments": [],
                "text": "Deep Syntactic Processing by Combining Shallow Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A novel approach for finding discontinuities that outperforms previously published results on this task is presented, which combines a simple unlexicalized PCFG parser with a shallow pre-processor that does surprisingly well on detecting where discontinUities can occur without using phase structure information."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057788"
                        ],
                        "name": "M. Osborne",
                        "slug": "M.-Osborne",
                        "structuredName": {
                            "firstName": "Miles",
                            "lastName": "Osborne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Osborne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145693410"
                        ],
                        "name": "Ted Briscoe",
                        "slug": "Ted-Briscoe",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Briscoe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Briscoe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 8
                            }
                        ],
                        "text": "Long-range dependencies are relatively common in text such as newspaper text, but are typically not recovered by treebank parsers such as Collins (2003) and Charniak (2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 122
                            }
                        ],
                        "text": "There has been some other work on defining stochastic categorial grammars, but mainly in the context of grammar learning (Osborne and Briscoe 1997; Watkinson and Manandhar 2001; Zettlemoyer and Collins 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 747480,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6744bfa03ae3195d3db6c5558221aaaa0761ed4b",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Stochastic categorial grammars (SCGs) are introduced as a more appropriate formalism for statistical language learners to est imate than stochastic context free grammars. As a vehicle for demonstrating SCG estimation, we show, in terms of crossing rates and in coverage, that when training material is limited, SCG estimation using the Minimum Description Length Principle is preferable to SCG estimation using an indifferent prior."
            },
            "slug": "Learning-Stochastic-Categorial-Grammars-Osborne-Briscoe",
            "title": {
                "fragments": [],
                "text": "Learning Stochastic Categorial Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown, in terms of crossing rates and in coverage, that when training material is limited, SCG estimation using the Minimum Description Length Principle is preferable to SCGs estimation using an indifferent prior."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067634566"
                        ],
                        "name": "James Brooks",
                        "slug": "James-Brooks",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Brooks",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Brooks"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 165
                            }
                        ],
                        "text": "Two possible extensions, which we have not investigated, include defining dependency features which account for all three elements of the triple in a PP-attachment (Collins and Brooks 1995), and defining a rule feature which includes the grandparent node (Johnson 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 106
                            }
                        ],
                        "text": "The correct and complete forests can be used to estimate the required log-likelihood value and feature expectations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 543,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc9e5bf851dc95369e26f1869c2637b1d8919e6c",
            "isKey": false,
            "numCitedBy": 224,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work has considered corpus-based or statistical approaches to the problem of prepositional phrase attachment ambiguity. Typically, ambiguous verb phrases of the form v np1 p np2 are resolved through a model which considers values of the four head words (v, n1, p and n2). This paper shows that the problem is analogous to n-gram language models in speech recognition, and that one of the most common methods for language modeling, the backed-off estimate, is applicable. Results on Wall Street Journal data of 84.5% accuracy are obtained using this method. A surprising result is the importance of low-count events \u2014 ignoring events which occur less than 5 times in training data reduces performance to 81.6%."
            },
            "slug": "Prepositional-Phrase-Attachment-through-a-Model-Collins-Brooks",
            "title": {
                "fragments": [],
                "text": "Prepositional Phrase Attachment through a Backed-off Model"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This paper shows that the problem of prepositional phrase attachment ambiguity is analogous to n-gram language models in speech recognition, and that one of the most common methods for language modeling, the backed-off estimate, is applicable."
            },
            "venue": {
                "fragments": [],
                "text": "VLC@ACL"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145804005"
                        ],
                        "name": "Robert Malouf",
                        "slug": "Robert-Malouf",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Malouf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Malouf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 3
                            }
                        ],
                        "text": "As Malouf (2002) demonstrates, general purpose numerical optimization algorithms such as BFGS can converge much faster than iterative scaling algorithms (including Improved Iterative Scaling; Della Pietra, Della Pietra, and Lafferty 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Malouf (2002) gives a more thorough description of numerical optimization methods applied to log-linear models."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 212
                            }
                        ],
                        "text": "And finally, packed charts are an instance of a feature forest, which Miyao and Tsujii (2002) show can be used to efficiently estimate expected values of features, even though the expectation may involve a sum over an exponential number of trees in the forest."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6249194,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "878783964ab23c97052ea82685368099d85c500d",
            "isKey": false,
            "numCitedBy": 741,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Conditional maximum entropy (ME) models provide a general purpose machine learning technique which has been successfully applied to fields as diverse as computer vision and econometrics, and which is used for a wide variety of classification problems in natural language processing. However, the flexibility of ME models is not without cost. While parameter estimation for ME models is conceptually straightforward, in practice ME models for typical natural language tasks are very large, and may well contain many thousands of free parameters. In this paper, we consider a number of algorithms for estimating the parameters of ME models, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods. Sur-prisingly, the standardly used iterative scaling algorithms perform quite poorly in comparison to the others, and for all of the test problems, a limited-memory variable metric algorithm outperformed the other choices."
            },
            "slug": "A-Comparison-of-Algorithms-for-Maximum-Entropy-Malouf",
            "title": {
                "fragments": [],
                "text": "A Comparison of Algorithms for Maximum Entropy Parameter Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A number of algorithms for estimating the parameters of ME models are considered, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35371521"
                        ],
                        "name": "Kristina Toutanvoa",
                        "slug": "Kristina-Toutanvoa",
                        "structuredName": {
                            "firstName": "Kristina",
                            "lastName": "Toutanvoa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristina Toutanvoa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 94
                            }
                        ],
                        "text": "Log-linear models have been applied to a number of problems in NLP, for example, POS tagging (Ratnaparkhi 1996; Lafferty, McCallum, and Pereira 2001), named entity recognition (Borthwick 1999), chunking (Koeling 2000), and parsing (Johnson et al. 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 18
                            }
                        ],
                        "text": "A supertagger for Combinatory Categorial Grammar."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 133
                            }
                        ],
                        "text": "The first application of log-linear models to parsing is the work of Ratnaparkhi and colleagues (Ratnaparkhi, Roukos, and Ward 1994; Ratnaparkhi 1996, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 86
                            }
                        ],
                        "text": "Wide-Coverage Efficient Statistical Parsing with CCG and Log-Linear Models\nStephen Clark\u2217 University of Oxford\nJames R. Curran\u2217\u2217 University of Sydney\nThis article describes a number of log-linear parsing models for an automatically extracted lexicalized grammar."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 12
                            }
                        ],
                        "text": "Multi-modal Combinatory Categorial Grammar."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 42
                            }
                        ],
                        "text": "The lexicalized grammar formalism used is Combinatory Categorial Grammar (CCG), and the grammar is automatically extracted from CCGbank, a CCG version of the Penn Treebank."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 196
                            }
                        ],
                        "text": "Clark and Curran (2004a) describe the supertagger, which uses log-linear models to define a distribution over the lexical category set for each local five-word context containing the target word (Ratnaparkhi 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 141
                            }
                        ],
                        "text": "The parse times and speeds include the failed sentences, and include the time taken by the supertagger, but not the POS tagger; however, the POS tagger is extremely efficient, taking less than 4 seconds to supertag Section 23, most of which consists of load time for the Maximum Entropy model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Combinatory Categorial Grammar (CCG) (Steedman 1996, 2000) is a type-driven lexicalized theory of grammar based on Categorial Grammar (Wood 1993)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 44
                            }
                        ],
                        "text": "Lexically Specified Derivational Control in Combinatory Categorial Grammar."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 34
                            }
                        ],
                        "text": "Efficient normal-form parsing for Combinatory Categorial Grammar."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 63
                            }
                        ],
                        "text": "The lexicalized grammar formalism we use is Combinatory Categorial Grammar (CCG; Steedman 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 52
                            }
                        ],
                        "text": "2003a. Data and Models for Statistical Parsing with Combinatory Categorial Grammar."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 54
                            }
                        ],
                        "text": "2002b. Generative models for statistical parsing with Combinatory Categorial Grammar."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 76
                            }
                        ],
                        "text": "A key component of the parsing system is a Maximum Entropy CCG supertagger (Ratnaparkhi 1996; Curran and Clark 2003) which assigns lexical categories to words in a sentence."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10807721,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1504a9d5829033a8cb4cf37b8bb13dfd4baddc7b",
            "isKey": true,
            "numCitedBy": 1574,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents results for a maximum-entropy-based part of speech tagger, which achieves superior performance principally by enriching the information sources used for tagging. In particular, we get improved results by incorporating these features: (i) more extensive treatment of capitalization for unknown words; (ii) features for the disambiguation of the tense forms of verbs; (iii) features for disambiguating particles from prepositions and adverbs. The best resulting accuracy for the tagger on the Penn Treebank is 96.86% overall, and 86.91% on previously unseen words."
            },
            "slug": "Enriching-the-Knowledge-Sources-Used-in-a-Maximum-Toutanvoa-Manning",
            "title": {
                "fragments": [],
                "text": "Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "This paper presents results for a maximum-entropy-based part of speech tagger, which achieves superior performance principally by enriching the information sources used for tagging by incorporating these features: more extensive treatment of capitalization for unknown words, and features for the disambiguation of the tense forms of verbs."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34730654"
                        ],
                        "name": "J. Caroll",
                        "slug": "J.-Caroll",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Caroll",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Caroll"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145693410"
                        ],
                        "name": "Ted Briscoe",
                        "slug": "Ted-Briscoe",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Briscoe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Briscoe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143661868"
                        ],
                        "name": "A. Sanfilippo",
                        "slug": "A.-Sanfilippo",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Sanfilippo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sanfilippo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7042755,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1bfc525e48e3dab1087b7308da3ba462d6fc382c",
            "isKey": false,
            "numCitedBy": 290,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a critical overview of the state-of-the-art in parser evaluation methodologies and metrics. A discussion of their relative strengths and weaknesses motivates a new\u2014and we claim more informative and generally applicable\u2014technique of measuring parser accuracy, based on the use of grammatical relations. We conclude with some preliminary results of experiments in which we use this new scheme to evaluate a robust parser of English."
            },
            "slug": "Parser-evaluation:-a-survey-and-a-new-proposal-Caroll-Briscoe",
            "title": {
                "fragments": [],
                "text": "Parser evaluation: a survey and a new proposal"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new\u2014and reportedly more informative and generally applicable\u2014technique of measuring parser accuracy, based on the use of grammatical relations, is claimed to be applicable to parser accuracy evaluation."
            },
            "venue": {
                "fragments": [],
                "text": "LREC"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110909951"
                        ],
                        "name": "Stanley F. Chen",
                        "slug": "Stanley-F.-Chen",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Chen",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stanley F. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145903504"
                        ],
                        "name": "R. Rosenfeld",
                        "slug": "R.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rosenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 64
                            }
                        ],
                        "text": "It does this by considering the topology of the feature space and moving in a direction which is guaranteed to increase the value of the objective function."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 64
                            }
                        ],
                        "text": "This smoothing method for log-linear models is also proposed by Chen and Rosenfeld (1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 276,
                                "start": 253
                            }
                        ],
                        "text": "\u2026model given the data, minus a Gaussian prior\n2 We could model predicate\u2013argument dependencies together with the derivation, but we wanted to use features from the derivation only, following Hockenmaier and Steedman (2002b).\nterm to prevent overfitting (Chen and Rosenfeld 1999; Johnson et al. 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17052790,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0043ccb045bc7d07ea6c9b719a72a6a01df1ab0a",
            "isKey": false,
            "numCitedBy": 388,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : In certain contexts, maximum entropy (ME) modeling can be viewed as maximum likelihood training for exponential models, and like other maximum likelihood methods is prone to overfitting of training data. Several smoothing methods for maximum entropy models have been proposed to address this problem, but previous results do not make it clear how these smoothing methods compare with smoothing methods for other types of related models. In this work, we survey previous work in maximum entropy smoothing and compare the performance of several of these algorithms with conventional techniques for smoothing n-gram language models. Because of the mature body of research in n-gram model smoothing and the close connection between maximum entropy and conventional n-gram models, this domain is well-suited to gauge the performance of maximum entropy smoothing methods. Over a large number of data sets, we find that an ME smoothing method proposed to us by Lafferty performs as well as or better than all other algorithms under consideration. This general and efficient method involves using a Gaussian prior on the parameters of the model and selecting maximum a posteriori instead of maximum likelihood parameter values. We contrast this method with previous n-gram smoothing methods to explain its superior performance."
            },
            "slug": "A-Gaussian-Prior-for-Smoothing-Maximum-Entropy-Chen-Rosenfeld",
            "title": {
                "fragments": [],
                "text": "A Gaussian Prior for Smoothing Maximum Entropy Models"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Over a large number of data sets, it is found that an ME smoothing method proposed to us by Lafferty performs as well as or better than all other algorithms under consideration."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788050"
                        ],
                        "name": "R. Grishman",
                        "slug": "R.-Grishman",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Grishman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Grishman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32183233"
                        ],
                        "name": "Andrew Borthwick",
                        "slug": "Andrew-Borthwick",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Borthwick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Borthwick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60779558,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "149f5137d1c7e34764e1f4d4d7b97b8e6bdeda2a",
            "isKey": false,
            "numCitedBy": 516,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis describes a novel statistical named-entity (i.e. \u201cproper name\u201d) recognition system known as \u201cMENE\u201d (Maximum Entropy Named Entity). Named entity (N.E.) recognition is a form of information extraction in which we seek to classify every word in a document as being a person-name, organization, location, date, time, monetary value, percentage, or \u201cnone of the above\u201d. The task has particular significance for Internet search engines, machine translation, the automatic indexing of documents, and as a foundation for work on more complex information extraction tasks. \nTwo of the most significant problems facing the constructor of a named entity system are the questions of portability and system performance. A practical N.E. system will need to be ported frequently to new bodies of text and even to new languages. The challenge is to build a system which can be ported with minimal expense (in particular minimal programming by a computational linguist) while maintaining a high degree of accuracy in the new domains or languages. \nMENE attempts to address these issues through the use of maximum entropy probabilistic modeling. It utilizes a very flexible object-based architecture which allows it to make use of a broad range of knowledge sources in making its tagging decisions. In the DARPA-sponsored MUC-7 named entity evaluation, the system displayed an accuracy rate which was well-above the median, demonstrating that it can achieve the performance goal. In addition, we demonstrate that the system can be used as a post-processing tool to enhance the output of a hand-coded named entity recognizer through experiments in which MENE improved on the performance of N.E. systems from three different sites. Furthermore, when all three external recognizers are combined under MENE, we are able to achieve very strong results which, in some cases, appear to be competitive with human performance. \nFinally, we demonstrate the trans-lingual portability of the system. We ported the system to two Japanese-language named entity tasks, one of which involved a new named entity category, \u201cartifact\u201d. Our results on these tasks were competitive with the best systems built by native Japanese speakers despite the fact that the author speaks no Japanese."
            },
            "slug": "A-Maximum-Entropy-Approach-to-Named-Entity-Grishman-Borthwick",
            "title": {
                "fragments": [],
                "text": "A Maximum Entropy Approach to Named Entity Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This thesis describes a novel statistical named-entity recognition system known as MENE (Maximum Entropy Named Entity), and demonstrates the trans-lingual portability of the system, which was competitive with the best systems built by native Japanese speakers despite the fact that the author speaks no Japanese."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782178"
                        ],
                        "name": "S. Buchholz",
                        "slug": "S.-Buchholz",
                        "structuredName": {
                            "firstName": "Sabine",
                            "lastName": "Buchholz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Buchholz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143667674"
                        ],
                        "name": "J. Veenstra",
                        "slug": "J.-Veenstra",
                        "structuredName": {
                            "firstName": "Jorn",
                            "lastName": "Veenstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Veenstra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735272"
                        ],
                        "name": "Walter Daelemans",
                        "slug": "Walter-Daelemans",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Daelemans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Walter Daelemans"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 96
                            }
                        ],
                        "text": "Preiss (2003) compares the parsers of Collins and Charniak, the grammatical relations finder of Buchholz, Veenstra, and Daelemans (1999), and the Briscoe and Carroll (2002) parser, using the gold-standard grammatical relations (GRs) from Carroll, Briscoe, and Sanfilippo (1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2117334,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "5db4b1405a1e77311a4a78414fe7eedc7712c4ed",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we discuss cascaded Memory-Based grammatical relations assignment. In the first stages of the cascade, we find chunks of several types (NP,VP,ADJP,ADVP,PP) and label them with their adverbial function (e.g. local, temporal). In the last stage, we assign grammatical relations to pairs of chunks. We studied the effect of adding several levels to this cascaded classifier and we found that even the less performing chunkers enhanced the performance of the relation finder."
            },
            "slug": "Cascaded-Grammatical-Relation-Assignment-Buchholz-Veenstra",
            "title": {
                "fragments": [],
                "text": "Cascaded Grammatical Relation Assignment"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The effect of adding several levels to this cascaded classifier was studied and it was found that even the less performing chunkers enhanced the performance of the relation finder."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768065"
                        ],
                        "name": "Yusuke Miyao",
                        "slug": "Yusuke-Miyao",
                        "structuredName": {
                            "firstName": "Yusuke",
                            "lastName": "Miyao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yusuke Miyao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067242699"
                        ],
                        "name": "Takashi Ninomiya",
                        "slug": "Takashi-Ninomiya",
                        "structuredName": {
                            "firstName": "Takashi",
                            "lastName": "Ninomiya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Takashi Ninomiya"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 3
                            }
                        ],
                        "text": "In Miyao and Tsujii (2003b, 2003a) log-linear models are developed for automatically extracted grammars for Lexicalized Tree Adjoining Grammar (LTAG) and Head Driven Phrase Structure Grammar (HPSG)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Miyao and Tsujii (2003a) present another log-linear model for an automatically extracted LTAG which uses a simple unigram model of the elementary trees together with a loglinear model of the attachments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59746457,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c34f08b676dd911f14337d18d3a6eaa90bcbb7c7",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a method of estimating a consistent probabilistic model of argument structures, which represent non-local dependencies such as unbounded dependencies and control relations. Non-local dependencies violate the independence assumption, which is assumed in traditional probabilistic models. Our solution is to apply feature forest models, which enable the probabilistic modeling of sentence structures without the independence assumption."
            },
            "slug": "Probabilistic-modeling-of-argument-structures-Miyao-Ninomiya",
            "title": {
                "fragments": [],
                "text": "Probabilistic modeling of argument structures including non-local dependencies"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "This work proposes a method of estimating a consistent probabilistic model of argument structures, which represent non-local dependencies such as unbounded dependencies and control relations, to apply feature forest models, which enable the Probabilistic modeling of sentence structures without the independence assumption."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1856296"
                        ],
                        "name": "R. Prins",
                        "slug": "R.-Prins",
                        "structuredName": {
                            "firstName": "Robbert",
                            "lastName": "Prins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Prins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143715131"
                        ],
                        "name": "Gertjan van Noord",
                        "slug": "Gertjan-van-Noord",
                        "structuredName": {
                            "firstName": "Gertjan",
                            "lastName": "Noord",
                            "middleNames": [
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gertjan van Noord"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 192585753,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "75517629ac61c431d072e556f0ab032ecb1bdc58",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "L'ambiguite lexicale est une source importante de l'inefficacite de l'analyse syntaxique HPSG a large couverture. Dans cette contribution, nous proposons un filtre analyseur lexical qui elimine des categories lexicales improbables. Le filtre est implemente comme un etiqueteur markovien (HMM) n-gramme standard, qui calcule la probabilite a posteriori' de chaque categorie lexicale. Une categorie lexicale est rejetee, quand celui-ci est en concurrence avec des categories lexicales qui sont suffisamment plus probables. La nouveaute de l'approche exposee ici consiste a entrainer l'etiqueteur sur la sortie de l'analyseur lui-meme ; par consequence, on n'a pas besoin de corpus etiquetes manuellement. L'emploi de ce filtre augmente considerablement la rapidite de l'analyseur et en plus, en ameliore la precision."
            },
            "slug": "Reinforcing-parser-preferences-through-tagging-:-en-Prins-Noord",
            "title": {
                "fragments": [],
                "text": "Reinforcing parser preferences through tagging : \u00c9volutions en analyse syntaxique"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2299876"
                        ],
                        "name": "R. Koeling",
                        "slug": "R.-Koeling",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Koeling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Koeling"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 204
                            }
                        ],
                        "text": "Log-linear models have been applied to a number of problems in NLP, for example, POS tagging (Ratnaparkhi 1996; Lafferty, McCallum, and Pereira 2001), named entity recognition (Borthwick 1999), chunking (Koeling 2000), and parsing (Johnson et al. 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 199
                            }
                        ],
                        "text": "Wide-Coverage Efficient Statistical Parsing with CCG and Log-Linear Models\nStephen Clark\u2217 University of Oxford\nJames R. Curran\u2217\u2217 University of Sydney\nThis article describes a number of log-linear parsing models for an automatically extracted lexicalized grammar."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6553227,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2be4a6cfe8228b6f4f648ff10dbf1e62fc4562f7",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper I discuss a first attempt to create a text chunker using a Maximum Entropy model. The first experiments, implementing classifiers that tag every word in a sentence with a phrase-tag using very local lexical information, part-of-speech tags and phrase tags of surrounding words, give encouraging results."
            },
            "slug": "Chunking-with-Maximum-Entropy-Models-Koeling",
            "title": {
                "fragments": [],
                "text": "Chunking with Maximum Entropy Models"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A first attempt to create a text chunker using a Maximum Entropy model is discussed, implementing classifiers that tag every word in a sentence with a phrase-tag using very local lexical information, part-of-speech tags and phrase tags of surrounding words."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL/LLL"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732301"
                        ],
                        "name": "G. Zarri",
                        "slug": "G.-Zarri",
                        "structuredName": {
                            "firstName": "Gian",
                            "lastName": "Zarri",
                            "middleNames": [
                                "Piero"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Zarri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60666097,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa42da2ad4a42a69bcdf25367d0ff0addb3e59ee",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A big amount of important, economically relevant information, is buried within the huge mass of multimedia documents that correspond to some form of narrative description. Due to the ubiquity of these narrative resources, representing in a general, accurate, and effective way their semantic content i.e., their key meaning is then both conceptually relevant and economically important. This book presents the main properties of NKRL (Narrative Knowledge Representation Language), a language expressly designed for representing and managing, in a standardised way, the meaning of complex multimedia narrative documents. NKRL is a fully implemented language/environment that exists in two versions: a relational database-supported version and a file-oriented one. It constitutes probably the most complete and realistic effort realised so far to deal with the huge industrial potentialities of the narrative domain. Written from a multidisciplinary perspective, this book not only supplies an exhaustive description of NKRL and of the associated knowledge representation principles, it also constitutes an invaluable source of reference for practitioners, researchers and graduates in domains that range over narrative theories, linguistics and computational linguistics, artificial intelligence, knowledge bases, information retrieval, ontologies and the semantic Web."
            },
            "slug": "Representation-and-Management-of-Narrative-and-Zarri",
            "title": {
                "fragments": [],
                "text": "Representation and Management of Narrative Information - Theoretical Principles and Implementation"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This book presents the main properties of NKRL (Narrative Knowledge Representation Language), a language expressly designed for representing and managing, in a standardised way, the meaning of complex multimedia narrative documents."
            },
            "venue": {
                "fragments": [],
                "text": "Advanced Information and Knowledge Processing"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703559"
                        ],
                        "name": "W. Gropp",
                        "slug": "W.-Gropp",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Gropp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gropp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695428"
                        ],
                        "name": "E. Lusk",
                        "slug": "E.-Lusk",
                        "structuredName": {
                            "firstName": "Ewing",
                            "lastName": "Lusk",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Lusk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32834137"
                        ],
                        "name": "N. Doss",
                        "slug": "N.-Doss",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Doss",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Doss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734586"
                        ],
                        "name": "A. Skjellum",
                        "slug": "A.-Skjellum",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Skjellum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Skjellum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 80
                            }
                        ],
                        "text": "We use the the Message Passing Interface (MPI) standard for the implementation (Gropp et al. 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 304801,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "61608bde8d502dbac561b9de1f36e676491b0b30",
            "isKey": false,
            "numCitedBy": 2401,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-High-Performance,-Portable-Implementation-of-the-Gropp-Lusk",
            "title": {
                "fragments": [],
                "text": "A High-Performance, Portable Implementation of the MPI Message Passing Interface Standard"
            },
            "venue": {
                "fragments": [],
                "text": "Parallel Comput."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157892296"
                        ],
                        "name": "D. K. Smith",
                        "slug": "D.-K.-Smith",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Smith",
                            "middleNames": [
                                "K.",
                                "Skip"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. K. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 148
                            }
                        ],
                        "text": "One of our aims was to provide a self contained estimation code base, and so we implemented our own version of the L-BFGS algorithm as described in Nocedal and Wright (1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 68
                            }
                        ],
                        "text": "As an alternative to GIS, we use the limited-memory BFGS algorithm (Nocedal and Wright 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 98
                            }
                        ],
                        "text": "The limited memory BFGS (L-BFGS) algorithm is a general purpose numerical optimization algorithm (Nocedal and Wright 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 31
                            }
                        ],
                        "text": "First, they compactly represent every \u3008derivation, dependency-structure\u3009 pair, by grouping together equivalent chart entries."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 189864167,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "bf86896c23300a46b7fc76298e365984c0b05105",
            "isKey": true,
            "numCitedBy": 10989,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "no exception. MRP II and JIT=TQC in purchasing and supplier education are covered in Chapter 15. Without proper education MRP II and JIT=TQC will not be successful and will not generate their true bene\u00aets. Suppliers are key to the success of MRP II and JIT=TQC. They therefore need to understand these disciplines. Purchasing in the 21st century is going to be marked by continuous changes, by who can gain the competitive edge \u00aerst, who will be the most \u0304exible and who will build the best supplier relationships. This will only be achieved by following the process as described in Schorr in a step by step fashion. An organization must however be willing to, as Schorr states in Chapter 16, `create the spark, ignite change'! Only then can it happen! If you really want to know something about purchasing then this is the book to read. It is most de\u00aenitely relevant and more importantly up to date. It will certainly be a handy reference book for a course on purchasing."
            },
            "slug": "Numerical-Optimization-Smith",
            "title": {
                "fragments": [],
                "text": "Numerical Optimization"
            },
            "venue": {
                "fragments": [],
                "text": "J. Oper. Res. Soc."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49223598"
                        ],
                        "name": "J. Darroch",
                        "slug": "J.-Darroch",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Darroch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Darroch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12360582"
                        ],
                        "name": "D. Ratcliff",
                        "slug": "D.-Ratcliff",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Ratcliff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ratcliff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 538,
                                "start": 31
                            }
                        ],
                        "text": "Generalized Iterative Scaling (Darroch and Ratcliff 1972) is a common choice in the NLP literature for estimating a log-linear model (e.g., Ratnaparkhi 1998; Curran and Clark 2003). Initially we used generalized iterative scaling (GIS) for the parsing models described here, but found that convergence was extremely slow; Sha and Pereira (2003) present a similar finding for globally optimized log-linear models for sequences. As an alternative to GIS, we use the limited-memory BFGS algorithm (Nocedal and Wright 1999). As Malouf (2002) demonstrates, general purpose numerical optimization algorithms such as BFGS can converge much faster than iterative scaling algorithms (including Improved Iterative Scaling; Della Pietra, Della Pietra, and Lafferty 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 345,
                                "start": 31
                            }
                        ],
                        "text": "Generalized Iterative Scaling (Darroch and Ratcliff 1972) is a common choice in the NLP literature for estimating a log-linear model (e.g., Ratnaparkhi 1998; Curran and Clark 2003). Initially we used generalized iterative scaling (GIS) for the parsing models described here, but found that convergence was extremely slow; Sha and Pereira (2003) present a similar finding for globally optimized log-linear models for sequences."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 173
                            }
                        ],
                        "text": "As Malouf (2002) demonstrates, general purpose numerical optimization algorithms such as BFGS can converge much faster than iterative scaling algorithms (including Improved Iterative Scaling; Della Pietra, Della Pietra, and Lafferty 1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 31
                            }
                        ],
                        "text": "Generalized Iterative Scaling (Darroch and Ratcliff 1972) is a common choice in the NLP literature for estimating a log-linear model (e.g., Ratnaparkhi 1998; Curran and Clark 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120862597,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "37c931cbaa9217b829596dd196520a838562a109",
            "isKey": true,
            "numCitedBy": 1329,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generalized-Iterative-Scaling-for-Log-Linear-Models-Darroch-Ratcliff",
            "title": {
                "fragments": [],
                "text": "Generalized Iterative Scaling for Log-Linear Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "65971371"
                        ],
                        "name": "Y. Bar-Hillel",
                        "slug": "Y.-Bar-Hillel",
                        "structuredName": {
                            "firstName": "Yehoshua",
                            "lastName": "Bar-Hillel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Bar-Hillel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 60
                            }
                        ],
                        "text": "Nasr and Rambow (2004) investigate the potential impact of LTAG supertagging on parsing speed and accuracy by performing a number of oracle experiments."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 102
                            }
                        ],
                        "text": "Also, the supertagging idea which is central to the efficiency of the CCG parser originated with TAG (Bangalore and Joshi 1999)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 46
                            }
                        ],
                        "text": "One possible way to increase the accuracy of LTAG supertagging is to use a Maximum Entropy, rather than HMM, tagger (as discussed previously), but this is likely to result in an improvement of only a few percentage points."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "Statistical parsers have been developed for TAG (Chiang 2000; Sarkar and Joshi 2003), LFG (Riezler et al. 2002; Kaplan et al. 2004; Cahill et al. 2004), and HPSG (Toutanova et al. 2002; Toutanova, Markova, and Manning 2004; Miyao and Tsujii 2004; Malouf and van Noord 2004), among others."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 96
                            }
                        ],
                        "text": "Penn Treebank conversions have also been carried out for other linguistic formalisms, including TAG (Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000), LFG (Burke et al. 2004), and HPSG (Miyao, Ninomiya, and Tsujii 2004)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 92
                            }
                        ],
                        "text": "Of course some of these advantages could be obtained with other grammar formalisms, such as TAG, LFG, and HPSG, although CCG is especially well-suited to analysing coordination and long-range dependencies."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 120
                            }
                        ],
                        "text": "Chen et al. (2002) describe the results of reranking the output of an HMM supertagger using an automatically extracted LTAG."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 42
                            }
                        ],
                        "text": "The idea of supertagging originated with LTAG; however, in contrast to the CCG grammars used in this article, the automatically extracted LTAG grammars have, as yet, been too large to enable effective supertagging (as discussed in the previous section)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 41
                            }
                        ],
                        "text": "Later we show that use of a supertagger (Bangalore and Joshi 1999) prior to parsing can produce an extremely efficient parser."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "In Proceedings of the TAG+ Workshop, pages 259\u2013268, Venice, Italy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 145
                            }
                        ],
                        "text": "In Miyao and Tsujii (2003b, 2003a) log-linear models are developed for automatically extracted grammars for Lexicalized Tree Adjoining Grammar (LTAG) and Head Driven Phrase Structure Grammar (HPSG)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 271,
                                "start": 268
                            }
                        ],
                        "text": "The Maximum Entropy supertagger we use could also be applied to Miyao and Tsujii\u2019s grammars, although whether similar performance would be obtained depends on the characteristics of the grammar; see subsequent sections for more discussion of this issue in relation to LTAG."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 60
                            }
                        ],
                        "text": "The formalism most closely related to CCG from this list is TAG."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 111
                            }
                        ],
                        "text": "Second, it greatly increases the efficiency of the parser, which was the original motivation for supertagging (Bangalore and Joshi 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "In Proceedings of the TAG+ Workshop, pages 19\u201324, Venice, Italy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 24
                            }
                        ],
                        "text": "Automated extraction of TAGS from the Penn Treebank."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 32
                            }
                        ],
                        "text": "Interestingly, the accuracy of LTAG supertaggers using automatically extracted grammars is significantly below the accuracy of the CCG supertagger."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 201
                            }
                        ],
                        "text": "Because there is so much syntactic information in lexical categories, the parser is required to do less work once the lexical categories have been assigned; hence Srinivas and Joshi, in the context of TAG, refer to supertagging as almost parsing."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 90
                            }
                        ],
                        "text": "Miyao and Tsujii (2003a) present another log-linear model for an automatically extracted LTAG which uses a simple unigram model of the elementary trees together with a loglinear model of the attachments."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "TAG grammars have been automatically extracted from the Penn Treebank, using techniques similar to those used by Hockenmaier (Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "In Proceedings of the TAG+7 Workshop, Vancouver, Canada."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 70
                            }
                        ],
                        "text": "In this article we take a different approach, by using a supertagger (Bangalore and Joshi 1999) to perform step one."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 147237352,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "cb75609c5e2a558e63880d12082067653f8c7dac",
            "isKey": true,
            "numCitedBy": 497,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Quasi-Arithmetical-Notation-for-Syntactic-Bar-Hillel",
            "title": {
                "fragments": [],
                "text": "A Quasi-Arithmetical Notation for Syntactic Description"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1953
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "Probabilistic disambiguation models for wide-coverage HPSG parsing."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 157
                            }
                        ],
                        "text": "Statistical parsers have been developed for TAG (Chiang 2000; Sarkar and Joshi 2003), LFG (Riezler et al. 2002; Kaplan et al. 2004; Cahill et al. 2004), and HPSG (Toutanova et al. 2002; Toutanova, Markova, and Manning 2004; Miyao and Tsujii 2004; Malouf and van Noord 2004), among others."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 195
                            }
                        ],
                        "text": "Penn Treebank conversions have also been carried out for other linguistic formalisms, including TAG (Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000), LFG (Burke et al. 2004), and HPSG (Miyao, Ninomiya, and Tsujii 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 106
                            }
                        ],
                        "text": "Of course some of these advantages could be obtained with other grammar formalisms, such as TAG, LFG, and HPSG, although CCG is especially well-suited to analysing coordination and long-range dependencies."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "Miyao and Tsujii (2005) address the issue of practical estimation using an automatically extracted HPSG grammar."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "Parse disambiguation for a rich HPSG grammar."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 192
                            }
                        ],
                        "text": "In Miyao and Tsujii (2003b, 2003a) log-linear models are developed for automatically extracted grammars for Lexicalized Tree Adjoining Grammar (LTAG) and Head Driven Phrase Structure Grammar (HPSG)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "Miyao and Tsujii (2004) evaluate their HPSG parser against PropBank (Palmer, Gildea, and Kingsbury 2005)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 79
                            }
                        ],
                        "text": "Malouf and van Noord (2004) present similar work to ours, in the context of an HPSG grammar for Dutch."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "The leaf projection path view of parse trees: Exploring string kernels for HPSG parse selection."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 157
                            }
                        ],
                        "text": "A different approach is proposed by Abney (1997), who develops log-linear models for attribute-value grammars, such as Head-driven Phrase Structure Grammar (HPSG)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 214
                            }
                        ],
                        "text": "\u2026sentence Sj and \u03b8(Sj) is the set of possible derivations for Sj. Note that \u03b8(Sj) could contain some non-normal-form derivations; however, because any non-normal-form derivations will be considered incorrect, the resulting model will typically assign low probabilities to non-normal-form derivations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Corpus-oriented grammar development for acquiring a head-driven phrase structure grammar Computational Linguistics"
            },
            "venue": {
                "fragments": [],
                "text": "Corpus-oriented grammar development for acquiring a head-driven phrase structure grammar Computational Linguistics"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 419,
                                "start": 158
                            }
                        ],
                        "text": "Another way to think of the estimation process is that it attempts to put as much mass as possible on the derivations leading to the goldstandard structures (Riezler et al. 2002). The Gaussian prior term prevents overfitting by penalizing any model whose weights get too large in absolute value. The estimation process can also be thought of in terms of the framework of Della Pietra, Della Pietra, and Lafferty (1997), because setting the gradient in Equation (17) to zero yields the usual maximum entropy constraints, namely that the expected value of each feature is equal to its empirical value (again ignoring the Gaussian prior term)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 10
                            }
                        ],
                        "text": "We follow Riezler et al. (2002) in using a discriminative estimation method by maximizing the conditional log-likelihood of the model given the data, minus a Gaussian prior\n2 We could model predicate\u2013argument dependencies together with the derivation, but we wanted to use features from the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 91
                            }
                        ],
                        "text": "Statistical parsers have been developed for TAG (Chiang 2000; Sarkar and Joshi 2003), LFG (Riezler et al. 2002; Kaplan et al. 2004; Cahill et al. 2004), and HPSG (Toutanova et al. 2002; Toutanova, Markova, and Manning 2004; Miyao and Tsujii 2004; Malouf and van Noord 2004), among others."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 160
                            }
                        ],
                        "text": "Penn Treebank conversions have also been carried out for other linguistic formalisms, including TAG (Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000), LFG (Burke et al. 2004), and HPSG (Miyao, Ninomiya, and Tsujii 2004)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 158
                            }
                        ],
                        "text": "Another way to think of the estimation process is that it attempts to put as much mass as possible on the derivations leading to the goldstandard structures (Riezler et al. 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 87
                            }
                        ],
                        "text": "Long-distance dependency resolution in automatically acquired wide-coverage PCFG-based LFG approximations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 97
                            }
                        ],
                        "text": "Of course some of these advantages could be obtained with other grammar formalisms, such as TAG, LFG, and HPSG, although CCG is especially well-suited to analysing coordination and long-range dependencies."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 32
                            }
                        ],
                        "text": "The main difference is that the LFG grammar is hand-built, resulting in less ambiguity than an automatically extracted grammar and thus requiring fewer resources for model estimation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 71
                            }
                        ],
                        "text": "Kaplan et al. (2004) present similar work to ours in the context of an LFG grammar for English."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 115
                            }
                        ],
                        "text": "Log-linear models have previously been applied to statistical parsing (Johnson et al. 1999; Toutanova et al. 2002; Riezler et al. 2002; Malouf and van Noord 2004), but typically under the assumption that all possible parses for a sentence can be enumerated."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 10
                            }
                        ],
                        "text": "We follow Riezler et al. (2002) in using a discriminative estimation method by maximizing the conditional log-likelihood of the model given the data, minus a Gaussian prior"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 68
                            }
                        ],
                        "text": "Geman and Johnson (2002) propose a similar method in the context of LFG parsing; an implementation is described in Kaplan et al. (2004)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "Cahill et al. (2004) evaluate an LFG parser, which uses an automatically extracted grammar, against DepBank."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 90
                            }
                        ],
                        "text": "Statistical parsers have been developed for TAG (Chiang 2000; Sarkar and Joshi 2003), LFG (Riezler et al. 2002; Kaplan et al. 2004; Cahill et al. 2004), and HPSG (Toutanova et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 62
                            }
                        ],
                        "text": "Kaplan et al. (2004) compare the Collins parser with the Parc LFG parser by mapping Penn Treebank parses into the dependencies of DepBank, claiming that the LFG parser is more accurate with only a slight reduction in speed."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 73
                            }
                        ],
                        "text": "Briscoe and Carroll (2006) give a rough comparison of RASP with the Parc LFG parser (Kaplan et al. 2004) on DepBank, obtaining similar results overall, but acknowledging that the results are not strictly comparable because of the different annotation schemes used."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Parsing the Wall Street Journal using a LexicalFunctional Grammar and discriminative"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "93274910"
                        ],
                        "name": "W. Frederick",
                        "slug": "W.-Frederick",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Frederick",
                            "middleNames": [
                                "Crittenden"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Frederick"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 115
                            }
                        ],
                        "text": "The parser is able to parse 20 Wall Street Journal (WSJ) sentences per second on standard hardware, using our best-performing model, which compares very favorably with other parsers using linguistically motivated grammars."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 156
                            }
                        ],
                        "text": "The first is forward composition, which Steedman denotes\nby > B (because B is the symbol used by Curry to denote function composition in combinatory logic; Curry and Feys 1958):\nX/Y Y/Z \u21d2B X/Z (> B) (5)\nForward composition is often used in conjunction with type-raising (T), as in Figure 2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61350652,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "bb96a91e8c3748668bdf31a06d9bf3ee3e9f6ba6",
            "isKey": false,
            "numCitedBy": 568,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Combinatory-Logic-Frederick",
            "title": {
                "fragments": [],
                "text": "A Combinatory Logic"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3259253"
                        ],
                        "name": "Kristina Toutanova",
                        "slug": "Kristina-Toutanova",
                        "structuredName": {
                            "firstName": "Kristina",
                            "lastName": "Toutanova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristina Toutanova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34818326"
                        ],
                        "name": "P. Markova",
                        "slug": "P.-Markova",
                        "structuredName": {
                            "firstName": "Penka",
                            "lastName": "Markova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Markova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59818670,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3f048227038c8f36513c3eadc0fa519f415372bb",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Leaf-Projection-Path-View-of-Parse-Trees:-for-Toutanova-Markova",
            "title": {
                "fragments": [],
                "text": "The Leaf Projection Path View of Parse Trees: Exploring String Kernels for HPSG Parse Selection"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793475"
                        ],
                        "name": "A. Ratnaparkhi",
                        "slug": "A.-Ratnaparkhi",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ratnaparkhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ratnaparkhi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781292"
                        ],
                        "name": "S. Roukos",
                        "slug": "S.-Roukos",
                        "structuredName": {
                            "firstName": "Salim",
                            "lastName": "Roukos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roukos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144582029"
                        ],
                        "name": "T. Ward",
                        "slug": "T.-Ward",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Ward",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ward"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 97
                            }
                        ],
                        "text": "The first application of log-linear models to parsing is the work of Ratnaparkhi and colleagues (Ratnaparkhi, Roukos, and Ward 1994; Ratnaparkhi 1996, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15061443,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f374db77b414bfdd142616434162a0aa88d5a551",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-maximum-entropy-model-for-parsing-Ratnaparkhi-Roukos",
            "title": {
                "fragments": [],
                "text": "A maximum entropy model for parsing"
            },
            "venue": {
                "fragments": [],
                "text": "ICSLP"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 236492239,
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tree Adjoining Grammars and its Application to Statistical Parsing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 156
                            }
                        ],
                        "text": "The first is forward composition, which Steedman denotes\nby > B (because B is the symbol used by Curry to denote function composition in combinatory logic; Curry and Feys 1958):\nX/Y Y/Z \u21d2B X/Z (> B) (5)\nForward composition is often used in conjunction with type-raising (T), as in Figure 2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 287
                            }
                        ],
                        "text": "\u2026in this article, the feature functions are integer valued and count the number of times some feature appears in a syntactic analysis.1 Abney calls the feature functions frequency functions and, like Abney, we will not always distinguish between a feature and its corresponding frequency function."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Combinatory Logic: Vol. I. Amsterdam, The Netherlands"
            },
            "venue": {
                "fragments": [],
                "text": "Combinatory Logic: Vol. I. Amsterdam, The Netherlands"
            },
            "year": 1958
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 69
                            }
                        ],
                        "text": "Miyao and Tsujii (2004) evaluate their HPSG parser against PropBank (Palmer, Gildea, and Kingsbury 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Proposition Bank: A corpus annotated with semantic roles"
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 3
                            }
                        ],
                        "text": "As Chiang (2003) points out, Miyao and Tsujii do not provide a way of constructing a feature forest given a sentence, but provide the mathematical tools for estimation once the feature forest has been constructed."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mildly context sensitive grammars for estimating maximum entropy models"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 8th Conference on Formal Grammar"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 34
                            }
                        ],
                        "text": "Long-range dependencies are relatively common in text such as newspaper text, but are typically not recovered by treebank parsers such as Collins (2003) and Charniak (2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 148
                            }
                        ],
                        "text": "There has been some other work on defining stochastic categorial grammars, but mainly in the context of grammar learning (Osborne and Briscoe 1997; Watkinson and Manandhar 2001; Zettlemoyer and Collins 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Acquisition of large categorial grammar lexicons"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Conference of the Pacific Association for Computational Linguistics (PACLING-01)"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 128
                            }
                        ],
                        "text": "In order to facilitate comparisons with parsers using different formalisms, we also evaluate on the publicly available DepBank (King et al. 2003), using the Briscoe and Carroll annotation consistent with the RASP parser (Briscoe, Carroll, and Watson 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 90
                            }
                        ],
                        "text": "Briscoe and Carroll (2006) evaluate the RASP parser on the Parc Dependency Bank (DepBank; King et al. 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The PARC 700 Dependency"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 124
                            }
                        ],
                        "text": "The probability of the category sequence is estimated using a maximum entropy model, following the supertagger described in Clark (2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 0
                            }
                        ],
                        "text": "Clark (2002) reports that a significant loss in coverage results if the supertagger is used as a front-end to the parser of Hockenmaier and Steedman (2002b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We are not aware of any other work which has demonstrated the parsing efficiency benefits of supertagging using an automatically extracted grammar."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A supertagger for Combinatory Categorial Grammar"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the TAG+ Workshop"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reinforcing parser preferences through"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Clark and Curran Wide-Coverage Statistical Parsing"
            },
            "venue": {
                "fragments": [],
                "text": "Clark and Curran Wide-Coverage Statistical Parsing"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 192
                            }
                        ],
                        "text": "There has been some other work on defining stochastic categorial grammars, but mainly in the context of grammar learning (Osborne and Briscoe 1997; Watkinson and Manandhar 2001; Zettlemoyer and Collins 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We are not aware of any other work which has demonstrated the parsing efficiency benefits of supertagging using an automatically extracted grammar."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 94
                            }
                        ],
                        "text": "If enough unlabeled data is parsed, then the large volume can overcome the noise in the data (Steedman et al. 2002; Prins and van Noord 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semi-supervised training for statistical parsing: Final report"
            },
            "venue": {
                "fragments": [],
                "text": "Semi-supervised training for statistical parsing: Final report"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 36
                            }
                        ],
                        "text": "A different approach is proposed by Abney (1997), who develops log-linear models for attribute-value grammars, such as Head-driven Phrase Structure Grammar (HPSG)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stochastic attributevalue grammars"
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proceedings of the First International Joint Conference on Natural Language Processing (IJCNLP-04)"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the First International Joint Conference on Natural Language Processing (IJCNLP-04)"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "}\\NP {Y }}1 ){ }/NP {Z }}2 ){ }, then the head on the resulting S [dcl ] is buys"
            },
            "venue": {
                "fragments": [],
                "text": "}\\NP {Y }}1 ){ }/NP {Z }}2 ){ }, then the head on the resulting S [dcl ] is buys"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Combinatory Logic: Vol"
            },
            "venue": {
                "fragments": [],
                "text": "I. Amsterdam, North Holland."
            },
            "year": 1958
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 59,
            "methodology": 73,
            "result": 8
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 117,
        "totalPages": 12
    },
    "page_url": "https://www.semanticscholar.org/paper/Wide-Coverage-Efficient-Statistical-Parsing-with-Clark-Curran/2d45f21c9deb17987a6be71b3c9a2758791540a2?sort=total-citations"
}