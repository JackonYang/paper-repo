{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802785"
                        ],
                        "name": "S. Nowlan",
                        "slug": "S.-Nowlan",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Nowlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nowlan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60866167,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "59fa47fc237a0781b4bf1c84fedb728d20db26a1",
            "isKey": false,
            "numCitedBy": 167,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In this thesis, we consider learning algorithms for neural networks which are based on fitting a mixture probability density to a set of data. \nWe begin with an unsupervised algorithm which is an alternative to the classical winner-take-all competitive algorithms. Rather than updating only the parameters of the \"winner\" on each case, the parameters of all competitors are updated in proportion to their relative responsibility for the case. Use of such a \"soft\" competitive algorithm is shown to give better performance than the more traditional algorithms, with little additional cost. \nWe then consider a supervised modular architecture in which a number of simple \"expert\" networks compete to solve distinct pieces of a large task. A soft competitive mechanism is used to determine how much an expert learns on a case, based on how well the expert performs relative to the other expert networks. At the same time, a separate gating network learns to weight the output of each expert according to a prediction of its relative performance based on the input to the system. Experiments on a number of tasks illustrate that this architecture is capable of uncovering interesting task decompositions and of generalizing better than a single network with small training sets. \nFinally, we consider learning algorithms in which we assume that the actual output of the network should fall into one of a small number of classes or clusters. The objective of learning is to make the variance of these classes as small as possible. In the classical decision-directed algorithm, we decide that an output belongs to the class it is closest to and minimize the squared distance between the output and the center (mean) of this closest class. In the \"soft\" version of this algorithm, we minimize the squared distance between the actual output and a weighted average of the means of all of the classes. The weighting factors are the relative probability that the output belongs to each class. This idea may also be used to model the weights of a network, to produce networks which generalize better from small training sets."
            },
            "slug": "Soft-competitive-adaptation:-neural-network-based-Nowlan",
            "title": {
                "fragments": [],
                "text": "Soft competitive adaptation: neural network learning algorithms based on fitting statistical mixtures"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An unsupervised algorithm which is an alternative to the classical winner-take-all competitive algorithms and a supervised modular architecture in which a number of simple \"expert\" networks compete to solve distinct pieces of a large task are considered."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3462565"
                        ],
                        "name": "G. Kendall",
                        "slug": "G.-Kendall",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Kendall",
                            "middleNames": [
                                "David"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kendall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145406935"
                        ],
                        "name": "T. Hall",
                        "slug": "T.-Hall",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Hall",
                            "middleNames": [
                                "James"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hall"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 20365283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0402e4cb303677a7cc01bd214748a2c30c3b410a",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "It has been established that the generalization ability of an artificial neural network is strongly dependent on the number of hidden processing elements and weights (Baum and Haussler 1989). There have been several attempts to determine the optimal size of a neural network as part of the learning process. These typically alter the number of hidden nodes and/or connection weightings in a multilayer perceptron by either heuristic methods (Le Cun et al. 1990; Fahlman and Lebiere 1990) or inherently via some network size penalty (Chauvin 1989; Weigend et al. 1991; Nowlan and Hinton 1992). In this note an objective method for network optimization is proposed that eliminates the need for a network size penalty parameter."
            },
            "slug": "Optimal-Network-Construction-by-Minimum-Description-Kendall-Hall",
            "title": {
                "fragments": [],
                "text": "Optimal Network Construction by Minimum Description Length"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An objective method for network optimization is proposed that eliminates the need for a network size penalty parameter."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403454"
                        ],
                        "name": "E. Baum",
                        "slug": "E.-Baum",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Baum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Baum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15659829,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "25406e6733a698bfc4ac836f8e74f458e75dad4f",
            "isKey": false,
            "numCitedBy": 1696,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the question of when a network can be expected to generalize from m random training examples chosen from some arbitrary probability distribution, assuming that future test examples are drawn from the same distribution. Among our results are the following bounds on appropriate sample vs. network size. Assume 0 < \u220a 1/8. We show that if m O(W/\u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 1 \u220a of future test examples drawn from the same distribution. Conversely, for fully-connected feedforward nets with one hidden layer, any learning algorithm using fewer than (W/\u220a) random training examples will, for some distributions of examples consistent with an appropriate weight choice, fail at least some fixed fraction of the time to find a weight choice that will correctly classify more than a 1 \u220a fraction of the future test examples."
            },
            "slug": "What-Size-Net-Gives-Valid-Generalization-Baum-Haussler",
            "title": {
                "fragments": [],
                "text": "What Size Net Gives Valid Generalization?"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that if m O(W/ \u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 2 \u220a of future test examples drawn from the same distribution."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2024710"
                        ],
                        "name": "A. Weigend",
                        "slug": "A.-Weigend",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Weigend",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Weigend"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794321"
                        ],
                        "name": "B. Huberman",
                        "slug": "B.-Huberman",
                        "structuredName": {
                            "firstName": "Bernardo",
                            "lastName": "Huberman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Huberman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 217236,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f707a81a278d1598cd0a4493ba73f22dcdf90639",
            "isKey": false,
            "numCitedBy": 655,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Inspired by the information theoretic idea of minimum description length, we add a term to the back propagation cost function that penalizes network complexity. We give the details of the procedure, called weight-elimination, describe its dynamics, and clarify the meaning of the parameters involved. From a Bayesian perspective, the complexity term can be usefully interpreted as an assumption about prior distribution of the weights. We use this procedure to predict the sunspot time series and the notoriously noisy series of currency exchange rates."
            },
            "slug": "Generalization-by-Weight-Elimination-with-to-Weigend-Rumelhart",
            "title": {
                "fragments": [],
                "text": "Generalization by Weight-Elimination with Application to Forecasting"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This work adds a term to the back propagation cost function that penalizes network complexity, called weight-elimination, and uses this procedure to predict the sunspot time series and the notoriously noisy series of currency exchange rates."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2024710"
                        ],
                        "name": "A. Weigend",
                        "slug": "A.-Weigend",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Weigend",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Weigend"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794321"
                        ],
                        "name": "B. Huberman",
                        "slug": "B.-Huberman",
                        "structuredName": {
                            "firstName": "Bernardo",
                            "lastName": "Huberman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Huberman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 36452235,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2cee043045b529fceda7964a70e626d45657245a",
            "isKey": false,
            "numCitedBy": 842,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the effectiveness of connectionist architectures for predicting the future behavior of nonlinear dynamical systems. We focus on real-world time series of limited record length. Two examples are analyzed: the benchmark sunspot series and chaotic data from a computational ecosystem. The problem of overfitting, particularly serious for short records of noisy data, is addressed both by using the statistical method of validation and by adding a complexity term to the cost function (\"back-propagation with weight-elimination\"). The dimension of the dynamics underlying the time series, its Liapunov coefficient, and its nonlinearity can be determined via the network. We also show why sigmoid units are superior in performance to radial basis functions for high-dimensional input spaces. Furthermore, since the ultimate goal is accuracy in the prediction, we find that sigmoid networks trained with the weight-elimination algorithm outperform traditional nonlinear statistical approaches."
            },
            "slug": "Predicting-the-Future:-a-Connectionist-Approach-Weigend-Huberman",
            "title": {
                "fragments": [],
                "text": "Predicting the Future: a Connectionist Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Since the ultimate goal is accuracy in the prediction, it is found that sigmoid networks trained with the weight-elimination algorithm outperform traditional nonlinear statistical approaches."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Neural Syst."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052130521"
                        ],
                        "name": "Russell Reed",
                        "slug": "Russell-Reed",
                        "structuredName": {
                            "firstName": "Russell",
                            "lastName": "Reed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Russell Reed"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35912477,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d82c68980943718a306df67c3ed95f782e9f93a",
            "isKey": false,
            "numCitedBy": 1660,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "A rule of thumb for obtaining good generalization in systems trained by examples is that one should use the smallest system that will fit the data. Unfortunately, it usually is not obvious what size is best; a system that is too small will not be able to learn the data while one that is just big enough may learn very slowly and be very sensitive to initial conditions and learning parameters. This paper is a survey of neural network pruning algorithms. The approach taken by the methods described here is to train a network that is larger than necessary and then remove the parts that are not needed."
            },
            "slug": "Pruning-algorithms-a-survey-Reed",
            "title": {
                "fragments": [],
                "text": "Pruning algorithms-a survey"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The approach taken by the methods described here is to train a network that is larger than necessary and then remove the parts that are not needed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144798098"
                        ],
                        "name": "N. Morgan",
                        "slug": "N.-Morgan",
                        "structuredName": {
                            "firstName": "Nelson",
                            "lastName": "Morgan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Morgan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733733"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18821787,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f3175b3930d0c71495a52a7bccb3889e5f33520",
            "isKey": false,
            "numCitedBy": 270,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We have done an empirical study of the relation of the number of parameters (weights) in a feedforward net to generalization performance. Two experiments are reported. In one, we use simulated data sets with well-controlled parameters, such as the signal-to-noise ratio of continuous-valued data. In the second, we train the network on vector-quantized mel cepstra from real speech samples. In each case, we use back-propagation to train the feedforward net to discriminate in a multiple class pattern classification problem. We report the results of these studies, and show the application of cross-validation techniques to prevent overfitting."
            },
            "slug": "Generalization-and-Parameter-Estimation-in-Netws:-Morgan-Bourlard",
            "title": {
                "fragments": [],
                "text": "Generalization and Parameter Estimation in Feedforward Netws: Some Experiments"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "An empirical study of the relation of the number of parameters (weights) in a feedforward net to generalization performance and the application of cross-validation techniques to prevent overfitting is done."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759839"
                        ],
                        "name": "S. Solla",
                        "slug": "S.-Solla",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Solla",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Solla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7785881,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e7297db245c3feb1897720b173a59fe7e36babb7",
            "isKey": false,
            "numCitedBy": 3494,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, several improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative information to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application."
            },
            "slug": "Optimal-Brain-Damage-LeCun-Denker",
            "title": {
                "fragments": [],
                "text": "Optimal Brain Damage"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A class of practical and nearly optimal schemes for adapting the size of a neural network by using second-derivative information to make a tradeoff between network complexity and training set error is derived."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754328"
                        ],
                        "name": "D. Hush",
                        "slug": "D.-Hush",
                        "structuredName": {
                            "firstName": "Don",
                            "lastName": "Hush",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hush"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35216199"
                        ],
                        "name": "B. Horne",
                        "slug": "B.-Horne",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Horne",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Horne"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 3191120,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56e02786bad4cf8781950b5df615729a417b31d7",
            "isKey": false,
            "numCitedBy": 1261,
            "numCiting": 148,
            "paperAbstract": {
                "fragments": [],
                "text": "Theoretical results concerning the capabilities and limitations of various neural network models are summarized, and some of their extensions are discussed. The network models considered are divided into two basic categories: static networks and dynamic networks. Unlike static networks, dynamic networks have memory. They fall into three groups: networks with feedforward dynamics, networks with output feedback, and networks with state feedback, which are emphasized in this work. Most of the networks discussed are trained using supervised learning.<<ETX>>"
            },
            "slug": "Progress-in-supervised-neural-networks-Hush-Horne",
            "title": {
                "fragments": [],
                "text": "Progress in supervised neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Theoretical results concerning the capabilities and limitations of various neural network models are summarized, and some of their extensions are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Processing Magazine"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1861262"
                        ],
                        "name": "T. Caelli",
                        "slug": "T.-Caelli",
                        "structuredName": {
                            "firstName": "Terry",
                            "lastName": "Caelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Caelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760329"
                        ],
                        "name": "D. Squire",
                        "slug": "D.-Squire",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Squire",
                            "middleNames": [
                                "McG."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Squire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073638894"
                        ],
                        "name": "Tom P. J. Wild",
                        "slug": "Tom-P.-J.-Wild",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Wild",
                            "middleNames": [
                                "P.",
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom P. J. Wild"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 30430399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2bb25fd4d911b307a26df66538c7c2f6b7d11212",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Model-based-neural-networks-Caelli-Squire",
            "title": {
                "fragments": [],
                "text": "Model-based neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70219052"
                        ],
                        "name": "Wray L. Buntine",
                        "slug": "Wray-L.-Buntine",
                        "structuredName": {
                            "firstName": "Wray",
                            "lastName": "Buntine",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wray L. Buntine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2024710"
                        ],
                        "name": "A. Weigend",
                        "slug": "A.-Weigend",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Weigend",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Weigend"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14814125,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c83684f6207697c12850db423fd9747572cf1784",
            "isKey": false,
            "numCitedBy": 376,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Connectionist feed-forward networks, t rained with backpropagat ion, can be used both for nonlinear regression and for (discrete one-of-C ) classification. This paper presents approximate Bayesian meth ods to statistical components of back-propagat ion: choosing a cost funct ion and penalty term (interpreted as a form of prior probability), pruning insignifican t weights, est imat ing the uncertainty of weights, predict ing for new pat terns (\"out -of-sample\") , est imating the uncertainty in the choice of this predict ion (\"erro r bars\" ), estimating the generalizat ion erro r, comparing different network st ructures, and handling missing values in the t raining patterns. These methods extend some heurist ic techniques suggested in the literature, and in most cases require a small addit ional facto r in comput at ion during back-propagat ion, or computation once back-pro pagat ion has finished."
            },
            "slug": "Bayesian-Back-Propagation-Buntine-Weigend",
            "title": {
                "fragments": [],
                "text": "Bayesian Back-Propagation"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143791812"
                        ],
                        "name": "S. Gull",
                        "slug": "S.-Gull",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Gull",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gull"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u201d We have used this second approach, which is advocated by Gull (1988), who has shown that the use of such flexible hyperpriors can lead to considerable improvement in the quality of image reconstructions (Gull 1989; Skilling 1989) compared to the use of more classical priors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 118754484,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "82fa37d5be8e747131a5857992cc33bb95469ce3",
            "isKey": false,
            "numCitedBy": 316,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The Bayesian derivation of \u201cClassic\u201d MaxEnt image processing (Skilling 1989a) shows that exp(\u03b1S(f,m)), where S(f,m) is the entropy of image f relative to model m, is the only consistent prior probability distribution for positive, additive images. In this paper the derivation of \u201cClassic\u201d MaxEnt is completed, showing that it leads to a natural choice for the regularising parameter \u03b1, that supersedes the traditional practice of setting x2=N. The new condition is that the dimensionless measure of structure -2\u03b1S should be equal to the number of good singular values contained in the data. The performance of this new condition is discussed with reference to image deconvolution, but leads to a reconstruction that is visually disappointing. A deeper hypothesis space is proposed that overcomes these difficulties, by allowing for spatial correlations across the image."
            },
            "slug": "Developments-in-Maximum-Entropy-Data-Analysis-Gull",
            "title": {
                "fragments": [],
                "text": "Developments in Maximum Entropy Data Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055557361"
                        ],
                        "name": "C. Peterson",
                        "slug": "C.-Peterson",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Peterson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Peterson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2588243"
                        ],
                        "name": "Thorsteinn S. R\u00f6gnvaldsson",
                        "slug": "Thorsteinn-S.-R\u00f6gnvaldsson",
                        "structuredName": {
                            "firstName": "Thorsteinn",
                            "lastName": "R\u00f6gnvaldsson",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thorsteinn S. R\u00f6gnvaldsson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2309947"
                        ],
                        "name": "L. L\u00f6nnblad",
                        "slug": "L.-L\u00f6nnblad",
                        "structuredName": {
                            "firstName": "Leif",
                            "lastName": "L\u00f6nnblad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. L\u00f6nnblad"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61099988,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cf3406ade38bdf5187866f125a3c2bd6ecf24eab",
            "isKey": false,
            "numCitedBy": 202,
            "numCiting": 99,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "JETNET-3.0\u2014A-versatile-artificial-neural-network-Peterson-R\u00f6gnvaldsson",
            "title": {
                "fragments": [],
                "text": "JETNET 3.0\u2014A versatile artificial neural network package"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2471881"
                        ],
                        "name": "J. McDonnell",
                        "slug": "J.-McDonnell",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "McDonnell",
                            "middleNames": [
                                "Robert"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. McDonnell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3123199"
                        ],
                        "name": "D. Waagen",
                        "slug": "D.-Waagen",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Waagen",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Waagen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 24176621,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "65a46593f9d36dd3a53bc65b00c6a80a321cd8c4",
            "isKey": false,
            "numCitedBy": 146,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Evolutionary programming, a systematic multi-agent stochastic search technique, is used to generate recurrent perceptrons (nonlinear IIR filters). A hybrid optimization scheme is proposed that embeds a single-agent stochastic search technique, the method of Solis and Wets, into the evolutionary programming paradigm. The proposed hybrid optimization approach is further augmented by \"blending\" randomly selected parent vectors to create additional offspring. The first part of this work investigates the performance of the suggested hybrid stochastic search method. After demonstration on the Bohachevsky and Rosenbrock response surfaces, the hybrid stochastic optimization approach is applied in determining both the model order and the coefficients of recurrent perceptron time-series models. An information criterion is used to evaluate each recurrent perceptron structure as a candidate solution. It is speculated that the stochastic training method implemented in this study for training recurrent perceptrons can be used to train perceptron networks that have radically recurrent architectures."
            },
            "slug": "Evolving-recurrent-perceptrons-for-time-series-McDonnell-Waagen",
            "title": {
                "fragments": [],
                "text": "Evolving recurrent perceptrons for time-series modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is speculated that the stochastic training method implemented in this study for training recurrent perceptrons can be used to train perceptron networks that have radically recurrent architectures."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2010050"
                        ],
                        "name": "J. Skilling",
                        "slug": "J.-Skilling",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Skilling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Skilling"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118543247,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b35e86ce2b9947c23a13fed84cb66ae84bc2f6d",
            "isKey": false,
            "numCitedBy": 246,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a fully Bayesian derivation of maximum entropy image reconstruction. The argument repeatedly goes from the particular to the general, in that if there are general theories then they must apply to special cases. Two such special cases, formalised as the \u201cCox axioms \u201c, lead to the well-known fact that Bayesian probability theory is the only consistent language of inference. Further cases, formalised as the axioms of maximum entropy, show that the prior probability distribution for any positive, additive distribution must be monotonic in the entropy. Finally, a quantified special case shows that this monotonic function must be the exponential, leaving only a single dimensional scaling factor to be determined a posteriori. Many types of distribution, including probability distributions themselves, are positive and additive, so the entropy exponential is very general."
            },
            "slug": "Classic-Maximum-Entropy-Skilling",
            "title": {
                "fragments": [],
                "text": "Classic Maximum Entropy"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This paper presents a fully Bayesian derivation of maximum entropy image reconstruction, formalised as the axioms ofmaximum entropy, which shows that the prior probability distribution for any positive, additive distribution must be monotonic in the entropy."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2540974"
                        ],
                        "name": "H. Pi",
                        "slug": "H.-Pi",
                        "structuredName": {
                            "firstName": "Hong",
                            "lastName": "Pi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Pi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055557361"
                        ],
                        "name": "C. Peterson",
                        "slug": "C.-Peterson",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Peterson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Peterson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16632678,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e77dcb73788a27620f4a58f087982fbb903e7daf",
            "isKey": false,
            "numCitedBy": 155,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a general method, the -test, which establishes functional dependencies given a sequence of measurements. The approach is based on calculating conditional probabilities from vector component distances. Imposing the requirement of continuity of the underlying function, the obtained values of the conditional probabilities carry information on the embedding dimension and variable dependencies. The power of the method is illustrated on synthetic time-series with different time-lag dependencies and noise levels and on the sunspot data. The virtue of the method for preprocessing data in the context of feedforward neural networks is demonstrated. Also, its applicability for tracking residual errors in output units is stressed."
            },
            "slug": "Finding-the-Embedding-Dimension-and-Variable-in-Pi-Peterson",
            "title": {
                "fragments": [],
                "text": "Finding the Embedding Dimension and Variable Dependencies in Time Series"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "The -test, a general method which establishes functional dependencies given a sequence of measurements, is presented, based on calculating conditional probabilities from vector component distances, which is illustrated on synthetic time-series with different time-lag dependencies and noise levels."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8909922"
                        ],
                        "name": "N. Intrator",
                        "slug": "N.-Intrator",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Intrator",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Intrator"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16533383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a23675052ea71447037baf1716444dbe689ce728",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-combination-of-supervised-and-unsupervised-Intrator",
            "title": {
                "fragments": [],
                "text": "On the combination of supervised and unsupervised learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For multilayer networks, it is hard to find a good theoretical justification for this prior, but  Hinton (1987)  justifies it empirically by showing that it greatly improves generalization on a very difficult task."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 227369,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3e6bea2649298c68d17b9421fc7dd19eeacc935e",
            "isKey": false,
            "numCitedBy": 206,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "One major goal of research on massively parallel networks of neuron-like processing elements is to discover efficient methods for recognizing patterns. Another goal is to discover general learning procedures that allow networks to construct the internal representations that are required for complex tasks. This paper describes a recently developed procedure that can learn to perform a recognition task. The network is trained on examples in which the input vector represents an instance of a pattern in a particular position and the required output vector represents its name. After prolonged training, the network develops canonical internal representations of the patterns and it uses these canonical representations to identify familiar patterns in novel positions."
            },
            "slug": "Learning-Translation-Invariant-Recognition-in-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning Translation Invariant Recognition in Massively Parallel Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper describes a recently developed procedure that can learn to perform a recognition task and uses canonical internal representations of the patterns to identify familiar patterns in novel positions."
            },
            "venue": {
                "fragments": [],
                "text": "PARLE"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143791812"
                        ],
                        "name": "S. Gull",
                        "slug": "S.-Gull",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Gull",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gull"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The complexity cost we are using corresponds more closely to a Bayesian hyperprior (Jaynes 1986;  Gull 1988 ).,... have the common feature of favoring sets of weights in which the weights in a set are clustered about a small number of values.20 When using a hyperprior, we can deal with the hyperparameters either by marginalizing over them (in effect, integrating them out) (Buntine and Weigend 19911, or by allowing the data (i.e., the weights) to determine their values a posteriori.\u201d We have used this second approach, which is advocated by  Gull (1988) , ...,(See Jeffreys 1939 for further discussion.) An uninformative prior for a location parameter is uniform in the parameter, but an uninformative prior for a scale parameter is uniform in the log of the parameter ke., uniform in 7, rather than a,,  Gull 1988 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 117915279,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6272baf82e2e442edab4fb613ef2b7186bf5f1fb",
            "isKey": false,
            "numCitedBy": 288,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The principles of Bayesian reasoning are reviewed and applied to problems of inference from data sampled from Poisson, Gaussian and Cauchy distributions. Probability distributions (priors and likelihoods) are assigned in appropriate hypothesis spaces using the Maximum Entropy Principle, and then manipulated via Bayes\u2019 Theorem. Bayesian hypothesis testing requires careful consideration of the prior ranges of any parameters involved, and this leads to a quantitive statement of Occam\u2019s Razor. As an example of this general principle we offer a solution to an important problem in regression analysis; determining the optimal number of parameters to use when fitting graphical data with a set of basis functions."
            },
            "slug": "Bayesian-Inductive-Inference-and-Maximum-Entropy-Gull",
            "title": {
                "fragments": [],
                "text": "Bayesian Inductive Inference and Maximum Entropy"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8909922"
                        ],
                        "name": "N. Intrator",
                        "slug": "N.-Intrator",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Intrator",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Intrator"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8926225,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1046704930644e99b1305228c71216dfcbead6c",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 104,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel classification and regression method that combines exploratory projection pursuit (unsupervised training) with projection pursuit regression (supervised training), to yield a new family of cost/complexity penalty terms. Some improved generalization properties are demonstrated on real-world problems."
            },
            "slug": "Combining-Exploratory-Projection-Pursuit-and-with-Intrator",
            "title": {
                "fragments": [],
                "text": "Combining Exploratory Projection Pursuit and Projection Pursuit Regression with Application to Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A novel classification and regression method that combines exploratory projection pursuit (unsupervised training) with projection pursuit regression ( supervised training), to yield a new family of cost/complexity penalty terms."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49464494"
                        ],
                        "name": "Kevin J. Lang",
                        "slug": "Kevin-J.-Lang",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Lang",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin J. Lang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1234937,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e08d090d1e586610d636a46004876e9f3ded8209",
            "isKey": false,
            "numCitedBy": 640,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-time-delay-neural-network-architecture-for-word-Lang-Waibel",
            "title": {
                "fragments": [],
                "text": "A time-delay neural network architecture for isolated word recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A number of authors have suggested that when attempting to approximate an unknown function with some parametric approximation scheme (such as a network), the proper measure to optimize combines an estimate of the cost of the misfit with an estimate of the cost of describing the parametric approximation (Akaike 1973;  Rissanen 1978;  Barron and Barron 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30140639,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d382b9c11e5c6a8e173fbeb442545e3be8d3e3a5",
            "isKey": false,
            "numCitedBy": 6261,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Modeling-By-Shortest-Data-Description*-Rissanen",
            "title": {
                "fragments": [],
                "text": "Modeling By Shortest Data Description*"
            },
            "venue": {
                "fragments": [],
                "text": "Autom."
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37274089"
                        ],
                        "name": "D. Henderson",
                        "slug": "D.-Henderson",
                        "structuredName": {
                            "firstName": "Donnie",
                            "lastName": "Henderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Henderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2799635"
                        ],
                        "name": "R. Howard",
                        "slug": "R.-Howard",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Howard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34859193"
                        ],
                        "name": "W. Hubbard",
                        "slug": "W.-Hubbard",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Hubbard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Hubbard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041866"
                        ],
                        "name": "L. Jackel",
                        "slug": "L.-Jackel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Jackel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jackel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2542741,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "86ab4cae682fbd49c5a5bedb630e5a40fa7529f6",
            "isKey": false,
            "numCitedBy": 2931,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an application of back-propagation networks to handwritten digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1% error rate and about a 9% reject rate on zipcode digits provided by the U.S. Postal Service."
            },
            "slug": "Handwritten-Digit-Recognition-with-a-Network-LeCun-Boser",
            "title": {
                "fragments": [],
                "text": "Handwritten Digit Recognition with a Back-Propagation Network"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task, and has 1% error rate and about a 9% reject rate on zipcode digits provided by the U.S. Postal Service."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35206065"
                        ],
                        "name": "E. Jaynes",
                        "slug": "E.-Jaynes",
                        "structuredName": {
                            "firstName": "Edwin",
                            "lastName": "Jaynes",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Jaynes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8154444,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6ba8a74338ccb89d8b7242884289753653b86e7",
            "isKey": false,
            "numCitedBy": 184,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We note the main points of history, as a framework on which to hang many background remarks concerning the nature and motivation of Bayesian/Maximum Entropy methods. Experience has shown that these are needed in order to understand recent work and problems. A more complete account of the history, with many more details and references, is given in Jaynes (1978). The following discussion is essentially nontechnical; the aim is only to convey a little introductory \\feel\" for our outlook, purpose, and terminology, and to alert newcomers to common pitfalls of misunderstanding."
            },
            "slug": "Maximum-Entropy-and-Bayesian-Methods-in-Applied-Jaynes",
            "title": {
                "fragments": [],
                "text": "Maximum Entropy and Bayesian Methods in Applied Statistics: Bayesian Methods: General Background"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "The main points of history are noted, as a framework on which to hang many background remarks concerning the nature and motivation of Bayesian/Maximum Entropy methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688681"
                        ],
                        "name": "T. Kohonen",
                        "slug": "T.-Kohonen",
                        "structuredName": {
                            "firstName": "Teuvo",
                            "lastName": "Kohonen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kohonen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60862320,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "f3a217c11175f2cf904b2f7f6378b7ade176f2d0",
            "isKey": false,
            "numCitedBy": 593,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Why should wait for some days to get or receive the associative memory a system theoretical approach book that you order? Why should you take it if you can get the faster one? You can find the same book that you order right here. This is it the book that you can receive directly after purchasing. This associative memory a system theoretical approach is well known book in the world, of course many people will try to own it. Why don't you become the first? Still confused with the way?"
            },
            "slug": "Associative-memory.-A-system-theoretical-approach-Kohonen",
            "title": {
                "fragments": [],
                "text": "Associative memory. A system-theoretical approach"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This associative memory a system theoretical approach is well known book in the world, of course many people will try to own it and this is it the book that you can receive directly after purchasing."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": false,
            "numCitedBy": 48406,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737945"
                        ],
                        "name": "H. Akaike",
                        "slug": "H.-Akaike",
                        "structuredName": {
                            "firstName": "Hirotugu",
                            "lastName": "Akaike",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Akaike"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 64903870,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "400b45a803d642b752a84147ef547af7811e8f3f",
            "isKey": false,
            "numCitedBy": 19577,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper it is shown that the classical maximum likelihood principle can be considered to be a method of asymptotic realization of an optimum estimate with respect to a very general information theoretic criterion. This observation shows an extension of the principle to provide answers to many practical problems of statistical model fitting."
            },
            "slug": "Information-Theory-and-an-Extension-of-the-Maximum-Akaike",
            "title": {
                "fragments": [],
                "text": "Information Theory and an Extension of the Maximum Likelihood Principle"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "The classical maximum likelihood principle can be considered to be a method of asymptotic realization of an optimum estimate with respect to a very general information theoretic criterion to provide answers to many practical problems of statistical model fitting."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144473519"
                        ],
                        "name": "M. Mozer",
                        "slug": "M.-Mozer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Mozer",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mozer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748557"
                        ],
                        "name": "P. Smolensky",
                        "slug": "P.-Smolensky",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Smolensky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Smolensky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17934228,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "f8830ea439ca695e7dd848275e534f1024c2fe8a",
            "isKey": false,
            "numCitedBy": 330,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a means of using the knowledge in a network to determine the functionality or relevance of individual units, both for the purpose of understanding the network's behavior and imp..."
            },
            "slug": "Using-Relevance-to-Reduce-Network-Size-Mozer-Smolensky",
            "title": {
                "fragments": [],
                "text": "Using Relevance to Reduce Network Size Automatically"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "This paper proposes a means of using the knowledge in a network to determine the functionality or relevance of individual units, both for the purpose of understanding the network's behavior and impinging on the structure of the network."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143723574"
                        ],
                        "name": "D. B. Preston",
                        "slug": "D.-B.-Preston",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Preston",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. B. Preston"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122810709,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "661903691f83b293b7175be8f62a947006964131",
            "isKey": false,
            "numCitedBy": 5190,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface. Preface to Volume 2. Contents of Volume 2. List of Main Notation. Basic Concepts. Elements of Probability Theory. Stationary Random Processes. Spectral Analysis. Estimation in the Time Domain. Estimation in the Frequency Domain. Spectral Analysis in Practice. Analysis of Processes with Mixed Spectra."
            },
            "slug": "Spectral-Analysis-and-Time-Series-Preston",
            "title": {
                "fragments": [],
                "text": "Spectral Analysis and Time Series"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143776736"
                        ],
                        "name": "H. Tong",
                        "slug": "H.-Tong",
                        "structuredName": {
                            "firstName": "Howell",
                            "lastName": "Tong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Tong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111916193"
                        ],
                        "name": "K. S. Lim",
                        "slug": "K.-S.-Lim",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Lim",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. S. Lim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 38634480,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "800109701e147c1832b159f91633c3b346450013",
            "isKey": false,
            "numCitedBy": 1092,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Threshold-Autoregression,-Limit-Cycles-and-Cyclical-Tong-Lim",
            "title": {
                "fragments": [],
                "text": "Threshold Autoregression, Limit Cycles and Cyclical Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067137798"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48873590"
                        ],
                        "name": "F. Souli\u00e9",
                        "slug": "F.-Souli\u00e9",
                        "structuredName": {
                            "firstName": "Fr\u00e9d\u00e9ric",
                            "lastName": "Souli\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Souli\u00e9"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 149015233,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd6b44fde142686ed22836c4fdb4f6684a007028",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Modeles-connexionnistes-de-l'apprentissage-LeCun-Souli\u00e9",
            "title": {
                "fragments": [],
                "text": "Modeles connexionnistes de l'apprentissage"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 222411842,
            "fieldsOfStudy": [],
            "id": "acb94e335c64a4bbda6a493d6386e1138bcd109c",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory of probability"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1896
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143701969"
                        ],
                        "name": "M. Priestley",
                        "slug": "M.-Priestley",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Priestley",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Priestley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118665644,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8ff28e76cce06e40fdfbc44c3039a491226ebe22",
            "isKey": false,
            "numCitedBy": 1247,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Non-linear-and-non-stationary-time-series-analysis-Priestley",
            "title": {
                "fragments": [],
                "text": "Non-linear and non-stationary time series analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145409498"
                        ],
                        "name": "J. Justice",
                        "slug": "J.-Justice",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Justice",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Justice"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 115305104,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8150272d9b5aa033273161611664f6a611c355b7",
            "isKey": false,
            "numCitedBy": 166,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Maximum-entropy-and-bayesian-methods-in-applied-Justice",
            "title": {
                "fragments": [],
                "text": "Maximum entropy and bayesian methods in applied statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "67059748"
                        ],
                        "name": "\ud64d\uc7ac\uadfc",
                        "slug": "\ud64d\uc7ac\uadfc",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "\ud64d\uc7ac\uadfc",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\ud64d\uc7ac\uadfc"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 110439338,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "70adf349060dda058256414d945f18f146f91cc0",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Time-Delay-Neural-Network\ub97c-\uc774\uc6a9\ud55c-\uc74c\uc131-\uc778\uc2dd-\ud64d\uc7ac\uadfc",
            "title": {
                "fragments": [],
                "text": "Time Delay Neural Network\ub97c \uc774\uc6a9\ud55c \uc74c\uc131 \uc778\uc2dd"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1516909860"
                        ],
                        "name": "D. Touretzky",
                        "slug": "D.-Touretzky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Touretzky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Touretzky"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60256361,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "b073e08b683640aebeed4e589efaeb9c90cc8482",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Connectionist-models-:-proceedings-of-the-1990-Touretzky",
            "title": {
                "fragments": [],
                "text": "Connectionist models : proceedings of the 1990 summer school"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59861896,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01b6affe3ea4eae1978aec54e87087feb76d9215",
            "isKey": false,
            "numCitedBy": 863,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generalization-and-network-design-strategies-LeCun",
            "title": {
                "fragments": [],
                "text": "Generalization and network design strategies"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "At the end of learning, the magnitude of a weight is exactly proportional to its error derivative, which makes it particularly easy to interpret the weights (see, for example,  Hinton 1986 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 53796860,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ade4934db522fe6d634ff6f48887da46eedb4d1",
            "isKey": false,
            "numCitedBy": 902,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-distributed-representations-of-concepts.-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning distributed representations of concepts."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123729051"
                        ],
                        "name": "L. M. M.-T.",
                        "slug": "L.-M.-M.-T.",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "M.-T.",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. M. M.-T."
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4036480,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f1f4386524be3ed96caaf05f661aacb94db1e566",
            "isKey": false,
            "numCitedBy": 5289,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theory-of-Probability-M.-T.",
            "title": {
                "fragments": [],
                "text": "Theory of Probability"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1929
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109973472"
                        ],
                        "name": "Subutai Ahmad",
                        "slug": "Subutai-Ahmad",
                        "structuredName": {
                            "firstName": "Subutai",
                            "lastName": "Ahmad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Subutai Ahmad"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 36257694,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "6658a593bcf994abff1c9e86e425375dfe83f734",
            "isKey": false,
            "numCitedBy": 1,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "David-Touretzky,-Jeffrey-Elman,-Terrence-Sejnowski-Ahmad",
            "title": {
                "fragments": [],
                "text": "David Touretzky, Jeffrey Elman, Terrence Sejnowski and Geoffrey Hinton, eds., Connectionist Models: Proceedings of the 1990 Summer School"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Connectionist models: Proceedings"
            },
            "venue": {
                "fragments": [],
                "text": "eds. Artificial Intelligence"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Threshold autoregression, limit cycles, and"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nonlinear Modelling and Prediction by Successive Approximation Using Radial Basis Functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical learning networks: A unifying view"
            },
            "venue": {
                "fragments": [],
                "text": "1988 Symposium on the Interface: Statistics and Computing Science, Reston, Virginia, April 21-23. Baum,"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Handwritten digit recognition with a backpropagation network"
            },
            "venue": {
                "fragments": [],
                "text": "In"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 43
                            }
                        ],
                        "text": "(TAR),I5 and the multilayer RBF network of He and Lapedes (1991) (RBF)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nonlinear Modelling and Prediction by Successive Approximation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Experiments on Learning by Backpropagation"
            },
            "venue": {
                "fragments": [],
                "text": "Tech. Rep. CMU-CS-86-126, Carnegie Mellon University, Pittsburgh, PA."
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vols. I and 11"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian methods: General background"
            },
            "venue": {
                "fragments": [],
                "text": "Maximum Entropy and Bayesian Methods in Applied Statistics, J. H. Justice, ed., pp. 1-25. Cambridge University Press, Cambridge."
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 356,
                                "start": 304
                            }
                        ],
                        "text": "A number of authors have suggested that when attempting to approximate an unknown function with some parametric approximation scheme (such as a network), the proper measure to optimize combines an estimate of the cost of the misfit with an estimate of the cost of describing the parametric approximation (Akaike 1973; Rissanen 1978; Barron and Barron 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Information theory and an extension of the maximum like"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian modeling and neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Ph . D . thesis , Computation and Neural Systems"
            },
            "year": 1991
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 5,
            "methodology": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 51,
        "totalPages": 6
    },
    "page_url": "https://www.semanticscholar.org/paper/Simplifying-Neural-Networks-by-Soft-Weight-Sharing-Nowlan-Hinton/de75e4e15e22d4376300e5c968e2db44be29ac9e?sort=total-citations"
}