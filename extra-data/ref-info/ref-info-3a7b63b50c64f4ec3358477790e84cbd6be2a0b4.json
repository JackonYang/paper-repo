{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34994191"
                        ],
                        "name": "Bhuwan Dhingra",
                        "slug": "Bhuwan-Dhingra",
                        "structuredName": {
                            "firstName": "Bhuwan",
                            "lastName": "Dhingra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bhuwan Dhingra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2391802"
                        ],
                        "name": "Hanxiao Liu",
                        "slug": "Hanxiao-Liu",
                        "structuredName": {
                            "firstName": "Hanxiao",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hanxiao Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109512754"
                        ],
                        "name": "Zhilin Yang",
                        "slug": "Zhilin-Yang",
                        "structuredName": {
                            "firstName": "Zhilin",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhilin Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50056360"
                        ],
                        "name": "William W. Cohen",
                        "slug": "William-W.-Cohen",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Cohen",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William W. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 74
                            }
                        ],
                        "text": "The next best approaches for the single model and ensemble runs, GAReader (Dhingra et al., 2016) and ReasoNet (Shen et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 236
                            }
                        ],
                        "text": "The third group (considered as variants of Memory Network (Weston et al., 2015)) repeats computing an attention vector between the query and the context through multiple layers, typically referred to as multi-hop (Sordoni et al., 2016; Dhingra et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6529193,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f2ea810c16275dc74e880296e20dbd83b1bae1c",
            "isKey": false,
            "numCitedBy": 359,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we study the problem of answering cloze-style questions over documents. Our model, the Gated-Attention (GA) Reader, integrates a multi-hop architecture with a novel attention mechanism, which is based on multiplicative interactions between the query embedding and the intermediate states of a recurrent neural network document reader. This enables the reader to build query-specific representations of tokens in the document for accurate answer selection. The GA Reader obtains state-of-the-art results on three benchmarks for this task\u2013the CNN & Daily Mail news stories and the Who Did What dataset. The effectiveness of multiplicative interaction is demonstrated by an ablation study, and by comparing to alternative compositional operators for implementing the gated-attention."
            },
            "slug": "Gated-Attention-Readers-for-Text-Comprehension-Dhingra-Liu",
            "title": {
                "fragments": [],
                "text": "Gated-Attention Readers for Text Comprehension"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "The Gated-Attention (GA) Reader, a model that integrates a multi-hop architecture with a novel attention mechanism, which is based on multiplicative interactions between the query embedding and the intermediate states of a recurrent neural network document reader, enables the reader to build query-specific representations of tokens in the document for accurate answer selection."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3043830"
                        ],
                        "name": "Yiming Cui",
                        "slug": "Yiming-Cui",
                        "structuredName": {
                            "firstName": "Yiming",
                            "lastName": "Cui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiming Cui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46842323"
                        ],
                        "name": "Z. Chen",
                        "slug": "Z.-Chen",
                        "structuredName": {
                            "firstName": "Z.",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144572674"
                        ],
                        "name": "Si Wei",
                        "slug": "Si-Wei",
                        "structuredName": {
                            "firstName": "Si",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Si Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108620507"
                        ],
                        "name": "Shijin Wang",
                        "slug": "Shijin-Wang",
                        "structuredName": {
                            "firstName": "Shijin",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shijin Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40282288"
                        ],
                        "name": "Ting Liu",
                        "slug": "Ting-Liu",
                        "structuredName": {
                            "firstName": "Ting",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ting Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40936264"
                        ],
                        "name": "Guoping Hu",
                        "slug": "Guoping-Hu",
                        "structuredName": {
                            "firstName": "Guoping",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guoping Hu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 32
                            }
                        ],
                        "text": "Attention-over-attention model (Cui et al., 2016) uses a 2D similarity matrix between the query and context words (similar to Equation 1) to compute the weighted average of query-to-context attention."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9205021,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6e5df6322659276da6133f9b734a389d7a255e8",
            "isKey": false,
            "numCitedBy": 368,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Cloze-style reading comprehension is a representative problem in mining relationship between document and query. In this paper, we present a simple but novel model called attention-over-attention reader for better solving cloze-style reading comprehension task. The proposed model aims to place another attention mechanism over the document-level attention and induces \u201cattended attention\u201d for final answer predictions. One advantage of our model is that it is simpler than related works while giving excellent performance. In addition to the primary model, we also propose an N-best re-ranking strategy to double check the validity of the candidates and further improve the performance. Experimental results show that the proposed methods significantly outperform various state-of-the-art systems by a large margin in public datasets, such as CNN and Children\u2019s Book Test."
            },
            "slug": "Attention-over-Attention-Neural-Networks-for-Cui-Chen",
            "title": {
                "fragments": [],
                "text": "Attention-over-Attention Neural Networks for Reading Comprehension"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A simple but novel model called attention-over-attention reader for better solving cloze-style reading comprehension task, which significantly outperform various state-of-the-art systems by a large margin in public datasets, such as CNN and Children\u2019s Book Test."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2544107"
                        ],
                        "name": "Kenton Lee",
                        "slug": "Kenton-Lee",
                        "structuredName": {
                            "firstName": "Kenton",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenton Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15652489"
                        ],
                        "name": "T. Kwiatkowski",
                        "slug": "T.-Kwiatkowski",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Kwiatkowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kwiatkowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144729897"
                        ],
                        "name": "Ankur P. Parikh",
                        "slug": "Ankur-P.-Parikh",
                        "structuredName": {
                            "firstName": "Ankur",
                            "lastName": "Parikh",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ankur P. Parikh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143790066"
                        ],
                        "name": "Dipanjan Das",
                        "slug": "Dipanjan-Das",
                        "structuredName": {
                            "firstName": "Dipanjan",
                            "lastName": "Das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dipanjan Das"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6537520,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97e6ed1f7e5de0034f71c370c01f59c87aaf9a72",
            "isKey": false,
            "numCitedBy": 139,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The reading comprehension task, that asks questions about a given evidence document, is a central problem in natural language understanding. Recent formulations of this task have typically focused on answer selection from a set of candidates pre-defined manually or through the use of an external NLP pipeline. However, Rajpurkar et al. (2016) recently released the SQUAD dataset in which the answers can be arbitrary strings from the supplied text. In this paper, we focus on this answer extraction task, presenting a novel model architecture that efficiently builds fixed length representations of all spans in the evidence document with a recurrent network. We show that scoring explicit span representations significantly improves performance over other approaches that factor the prediction into separate predictions about words or start and end markers. Our approach improves upon the best published results of Wang & Jiang (2016) by 5% and decreases the error of Rajpurkar et al.\u2019s baseline by > 50%."
            },
            "slug": "Learning-Recurrent-Span-Representations-for-Lee-Kwiatkowski",
            "title": {
                "fragments": [],
                "text": "Learning Recurrent Span Representations for Extractive Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper presents a novel model architecture that efficiently builds fixed length representations of all spans in the evidence document with a recurrent network, and shows that scoring explicit span representations significantly improves performance over other approaches that factor the prediction into separate predictions about words or start and end markers."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2100019"
                        ],
                        "name": "Rudolf Kadlec",
                        "slug": "Rudolf-Kadlec",
                        "structuredName": {
                            "firstName": "Rudolf",
                            "lastName": "Kadlec",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rudolf Kadlec"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8649018"
                        ],
                        "name": "Martin Schmid",
                        "slug": "Martin-Schmid",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3363139"
                        ],
                        "name": "Ondrej Bajgar",
                        "slug": "Ondrej-Bajgar",
                        "structuredName": {
                            "firstName": "Ondrej",
                            "lastName": "Bajgar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ondrej Bajgar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1773749"
                        ],
                        "name": "Jan Kleindienst",
                        "slug": "Jan-Kleindienst",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Kleindienst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan Kleindienst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11022639,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f2e50e2ee4021f199877c8920f1f984481c723aa",
            "isKey": false,
            "numCitedBy": 288,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Several large cloze-style context-question-answer datasets have been introduced recently: the CNN and Daily Mail news data and the Children's Book Test. Thanks to the size of these datasets, the associated text comprehension task is well suited for deep-learning techniques that currently seem to outperform all alternative approaches. We present a new, simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models. This makes the model particularly suitable for question-answering problems where the answer is a single word from the document. Ensemble of our models sets new state of the art on all evaluated datasets."
            },
            "slug": "Text-Understanding-with-the-Attention-Sum-Reader-Kadlec-Schmid",
            "title": {
                "fragments": [],
                "text": "Text Understanding with the Attention Sum Reader Network"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new, simple model is presented that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models, making the model particularly suitable for question-answering problems where the answer is a single word from the document."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041695"
                        ],
                        "name": "Alessandro Sordoni",
                        "slug": "Alessandro-Sordoni",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sordoni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alessandro Sordoni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143902541"
                        ],
                        "name": "Philip Bachman",
                        "slug": "Philip-Bachman",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Bachman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip Bachman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 209
                            }
                        ],
                        "text": "In this paper, we introduce BIDAF, a multi-stage hierarchical process that represents the context at different levels of granularity and uses a bi-directional attention flow mechanism to achieve a queryaware context representation without early summarization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 214
                            }
                        ],
                        "text": "The third group (considered as variants of Memory Network (Weston et al., 2015)) repeats computing an attention vector between the query and the context through multiple layers, typically referred to as multi-hop (Sordoni et al., 2016; Dhingra et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 88
                            }
                        ],
                        "text": "Unlike previously popular attention mechanisms (Weston et al., 2015; Hill et al., 2016; Sordoni et al., 2016; Shen et al., 2016), the attention flow layer is not used to summarize the query and context into single feature vectors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14500125,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f314339651cb25e4234e0b96fe8bd87206847993",
            "isKey": false,
            "numCitedBy": 106,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel neural attention architecture to tackle machine comprehension tasks, such as answering Cloze-style queries with respect to a document. Unlike previous models, we do not collapse the query into a single vector, instead we deploy an iterative alternating attention mechanism that allows a fine-grained exploration of both the query and the document. Our model outperforms state-of-the-art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children\u2019s Book Test (CBT) dataset."
            },
            "slug": "Iterative-Alternating-Neural-Attention-for-Machine-Sordoni-Bachman",
            "title": {
                "fragments": [],
                "text": "Iterative Alternating Neural Attention for Machine Reading"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "This work proposes a novel neural attention architecture to tackle machine comprehension tasks, such as answering Cloze-style queries with respect to a document that outperforms state-of-the-art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children\u2019s Book Test dataset."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2992833"
                        ],
                        "name": "Shuohang Wang",
                        "slug": "Shuohang-Wang",
                        "structuredName": {
                            "firstName": "Shuohang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuohang Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144924150"
                        ],
                        "name": "Jing Jiang",
                        "slug": "Jing-Jiang",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Jiang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5592690,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff1861b71eaedba46cb679bbe2c585dbe18f9b19",
            "isKey": false,
            "numCitedBy": 541,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Machine comprehension of text is an important problem in natural language processing. A recently released dataset, the Stanford Question Answering Dataset (SQuAD), offers a large number of real questions and their answers created by humans through crowdsourcing. SQuAD provides a challenging testbed for evaluating machine comprehension algorithms, partly because compared with previous datasets, in SQuAD the answers do not come from a small set of candidate answers and they have variable lengths. We propose an end-to-end neural architecture for the task. The architecture is based on match-LSTM, a model we proposed previously for textual entailment, and Pointer Net, a sequence-to-sequence model proposed by Vinyals et al.(2015) to constrain the output tokens to be from the input sequences. We propose two ways of using Pointer Net for our task. Our experiments show that both of our two models substantially outperform the best results obtained by Rajpurkar et al.(2016) using logistic regression and manually crafted features."
            },
            "slug": "Machine-Comprehension-Using-Match-LSTM-and-Answer-Wang-Jiang",
            "title": {
                "fragments": [],
                "text": "Machine Comprehension Using Match-LSTM and Answer Pointer"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This work proposes an end-to-end neural architecture for the Stanford Question Answering Dataset (SQuAD), based on match-LSTM, a model previously proposed previously for textual entailment, and Pointer Net, a sequence- to-sequence model proposed by Vinyals et al.(2015) to constrain the output tokens to be from the input sequences."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152847920"
                        ],
                        "name": "Yang Yu",
                        "slug": "Yang-Yu",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47527881"
                        ],
                        "name": "Wei Zhang",
                        "slug": "Wei-Zhang",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064634"
                        ],
                        "name": "K. Hasan",
                        "slug": "K.-Hasan",
                        "structuredName": {
                            "firstName": "Kazi",
                            "lastName": "Hasan",
                            "middleNames": [
                                "Saidul"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hasan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2482533"
                        ],
                        "name": "Mo Yu",
                        "slug": "Mo-Yu",
                        "structuredName": {
                            "firstName": "Mo",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mo Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144028698"
                        ],
                        "name": "Bing Xiang",
                        "slug": "Bing-Xiang",
                        "structuredName": {
                            "firstName": "Bing",
                            "lastName": "Xiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bing Xiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145218984"
                        ],
                        "name": "Bowen Zhou",
                        "slug": "Bowen-Zhou",
                        "structuredName": {
                            "firstName": "Bowen",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bowen Zhou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17654651,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0680f04750b1e257ffdd161e85382031dc73ea7f",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes dynamic chunk reader (DCR), an end-to-end neural reading comprehension (RC) model that is able to extract and rank a set of answer candidates from a given document to answer questions. DCR is able to predict answers of variable lengths, whereas previous neural RC models primarily focused on predicting single tokens or entities. DCR encodes a document and an input question with recurrent neural networks, and then applies a word-by-word attention mechanism to acquire question-aware representations for the document, followed by the generation of chunk representations and a ranking module to propose the top-ranked chunk as the answer. Experimental results show that DCR could achieve a 66.3% Exact match and 74.7% F1 score on the Stanford Question Answering Dataset."
            },
            "slug": "End-to-End-Answer-Chunk-Extraction-and-Ranking-for-Yu-Zhang",
            "title": {
                "fragments": [],
                "text": "End-to-End Answer Chunk Extraction and Ranking for Reading Comprehension"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "This paper proposes dynamic chunk reader (DCR), an end-to-end neural reading comprehension model that is able to extract and rank a set of answer candidates from a given document to answer questions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2228109"
                        ],
                        "name": "Caiming Xiong",
                        "slug": "Caiming-Xiong",
                        "structuredName": {
                            "firstName": "Caiming",
                            "lastName": "Xiong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Caiming Xiong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3375440"
                        ],
                        "name": "Stephen Merity",
                        "slug": "Stephen-Merity",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Merity",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen Merity"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 110
                            }
                        ],
                        "text": "At the coarse level of granularity, the question attends to different patches in the image (Zhu et al., 2016; Xiong et al., 2016a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 277
                            }
                        ],
                        "text": "\u2026the advancement has been the use of neural attention mechanism, which enables the system to focus on a targeted area within a context paragraph (for MC) or within an image (for Visual QA), that is most relevant to answer the question (Weston et al., 2015; Antol et al., 2015; Xiong et al., 2016a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 76
                            }
                        ],
                        "text": "Second, in the text domain, they are often temporally dynamic, whereby the attention weights at the current time step are a function of the attended vector at the previous time step."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14294589,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f96898d15a1bf1fa8925b1280d0e07a7a8e72194",
            "isKey": false,
            "numCitedBy": 659,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural network architectures with memory and attention mechanisms exhibit certain reasoning capabilities required for question answering. One such architecture, the dynamic memory network (DMN), obtained high accuracy on a variety of language tasks. However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images. Based on an analysis of the DMN, we propose several improvements to its memory and input modules. Together with these changes we introduce a novel input module for images in order to be able to answer visual questions. Our new DMN+ model improves the state of the art on both the Visual Question Answering dataset and the \\babi-10k text question-answering dataset without supporting fact supervision."
            },
            "slug": "Dynamic-Memory-Networks-for-Visual-and-Textual-Xiong-Merity",
            "title": {
                "fragments": [],
                "text": "Dynamic Memory Networks for Visual and Textual Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The new DMN+ model improves the state of the art on both the Visual Question Answering dataset and the \\babi-10k text question-answering dataset without supporting fact supervision."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8387085"
                        ],
                        "name": "Zichao Yang",
                        "slug": "Zichao-Yang",
                        "structuredName": {
                            "firstName": "Zichao",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zichao Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144137069"
                        ],
                        "name": "Xiaodong He",
                        "slug": "Xiaodong-He",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800422"
                        ],
                        "name": "Jianfeng Gao",
                        "slug": "Jianfeng-Gao",
                        "structuredName": {
                            "firstName": "Jianfeng",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianfeng Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 123
                            }
                        ],
                        "text": "A hybrid approach is to combine questions representations at multiple levels of granularity (unigrams, bigrams, trigrams) (Yang et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8849206,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c1890864c1c2b750f48316dc8b650ba4772adc5",
            "isKey": false,
            "numCitedBy": 1474,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents stacked attention networks (SANs) that learn to answer natural language questions from images. SANs use semantic representation of a question as query to search for the regions in an image that are related to the answer. We argue that image question answering (QA) often requires multiple steps of reasoning. Thus, we develop a multiple-layer SAN in which we query an image multiple times to infer the answer progressively. Experiments conducted on four image QA data sets demonstrate that the proposed SANs significantly outperform previous state-of-the-art approaches. The visualization of the attention layers illustrates the progress that the SAN locates the relevant visual clues that lead to the answer of the question layer-by-layer."
            },
            "slug": "Stacked-Attention-Networks-for-Image-Question-Yang-He",
            "title": {
                "fragments": [],
                "text": "Stacked Attention Networks for Image Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A multiple-layer SAN is developed in which an image is queried multiple times to infer the answer progressively, and the progress that the SAN locates the relevant visual clues that lead to the answer of the question layer-by-layer."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2228109"
                        ],
                        "name": "Caiming Xiong",
                        "slug": "Caiming-Xiong",
                        "structuredName": {
                            "firstName": "Caiming",
                            "lastName": "Xiong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Caiming Xiong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3428769"
                        ],
                        "name": "Victor Zhong",
                        "slug": "Victor-Zhong",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Zhong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Zhong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 110
                            }
                        ],
                        "text": "At the coarse level of granularity, the question attends to different patches in the image (Zhu et al., 2016; Xiong et al., 2016a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 277
                            }
                        ],
                        "text": "\u2026the advancement has been the use of neural attention mechanism, which enables the system to focus on a targeted area within a context paragraph (for MC) or within an image (for Visual QA), that is most relevant to answer the question (Weston et al., 2015; Antol et al., 2015; Xiong et al., 2016a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3714278,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e978d832a4d86571e1b52aa1685dc32ccb250f50",
            "isKey": false,
            "numCitedBy": 614,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Several deep learning models have been proposed for question answering. However, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointing decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1."
            },
            "slug": "Dynamic-Coattention-Networks-For-Question-Answering-Xiong-Zhong",
            "title": {
                "fragments": [],
                "text": "Dynamic Coattention Networks For Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The Dynamic Coattention Network (DCN) for question answering first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both, then a dynamic pointing decoder iterates over potential answer spans to recover from initial local maxima corresponding to incorrect answers."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2706258"
                        ],
                        "name": "Pranav Rajpurkar",
                        "slug": "Pranav-Rajpurkar",
                        "structuredName": {
                            "firstName": "Pranav",
                            "lastName": "Rajpurkar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pranav Rajpurkar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2151810148"
                        ],
                        "name": "Jian Zhang",
                        "slug": "Jian-Zhang",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2787620"
                        ],
                        "name": "Konstantin Lopyrev",
                        "slug": "Konstantin-Lopyrev",
                        "structuredName": {
                            "firstName": "Konstantin",
                            "lastName": "Lopyrev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Konstantin Lopyrev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145419642"
                        ],
                        "name": "Percy Liang",
                        "slug": "Percy-Liang",
                        "structuredName": {
                            "firstName": "Percy",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Percy Liang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 64
                            }
                        ],
                        "text": "We evaluate the performance of our comprehension system on both SQuAD and CNN/DailyMail datasets."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "SQuAD is a machine comprehension dataset on a large set of Wikipedia articles, with more than 100,000 questions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 51
                            }
                        ],
                        "text": "The results of these variations on the dev data of SQuAD are shown in Table 5."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 108
                            }
                        ],
                        "text": "In this section, we evaluate our model on the task of question answering using the recently released SQuAD (Rajpurkar et al., 2016), which has gained a huge attention over a few months."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 97
                            }
                        ],
                        "text": "Table 4 summarizes the modes of errors by BIDAF and shows examples for each category of error in SQuAD."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 15
                            }
                        ],
                        "text": "More recently, Rajpurkar et al. (2016) released the Stanford Question Answering (SQuAD) dataset with over 100,000 questions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 120
                            }
                        ],
                        "text": "Wang & Jiang (2016) reverse the direction of the attention (attending on query words as the context RNN progresses) for SQuAD."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 69
                            }
                        ],
                        "text": "Table 1b shows the performance of our model and its ablations on the SQuAD dev set."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 58
                            }
                        ],
                        "text": "We now provide a qualitative analysis of our model on the SQuAD dev set."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 116
                            }
                        ],
                        "text": "Our BIDAF model1 outperforms all previous approaches on the highly-competitive Stanford Question Answering Dataset (SQuAD) test set leaderboard at the time of submission."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 70
                            }
                        ],
                        "text": "The model architecture used for this task is very similar to that for SQuAD (Section 4) with only a few small changes to adapt it to the cloze test."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 246
                            }
                        ],
                        "text": "We also provide an in-depth ablation study of our model on the SQuAD development set, visualize the intermediate feature spaces in our model, and analyse its performance as compared to a more traditional language model for machine comprehension (Rajpurkar et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 127
                            }
                        ],
                        "text": "The experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 34
                            }
                        ],
                        "text": "Another important difference from SQuAD is that the answer entity might appear more than once in the context paragraph."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 95
                            }
                        ],
                        "text": "We analyse the performance of our our model with a traditional language-featurebased baseline (Rajpurkar et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11816014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "05dd7254b632376973f3a1b4d39485da17814df5",
            "isKey": true,
            "numCitedBy": 4263,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. \nThe dataset is freely available at this https URL"
            },
            "slug": "SQuAD:-100,000+-Questions-for-Machine-Comprehension-Rajpurkar-Zhang",
            "title": {
                "fragments": [],
                "text": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
            },
            "tldr": {
                "abstractSimilarityScore": 35,
                "text": "A strong logistic regression model is built, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%)."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752875"
                        ],
                        "name": "Yelong Shen",
                        "slug": "Yelong-Shen",
                        "structuredName": {
                            "firstName": "Yelong",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yelong Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2421691"
                        ],
                        "name": "Po-Sen Huang",
                        "slug": "Po-Sen-Huang",
                        "structuredName": {
                            "firstName": "Po-Sen",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Po-Sen Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800422"
                        ],
                        "name": "Jianfeng Gao",
                        "slug": "Jianfeng-Gao",
                        "structuredName": {
                            "firstName": "Jianfeng",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianfeng Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109136147"
                        ],
                        "name": "Weizhu Chen",
                        "slug": "Weizhu-Chen",
                        "structuredName": {
                            "firstName": "Weizhu",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weizhu Chen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 110
                            }
                        ],
                        "text": "Unlike previously popular attention mechanisms (Weston et al., 2015; Hill et al., 2016; Sordoni et al., 2016; Shen et al., 2016), the attention flow layer is not used to summarize the query and context into single feature vectors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 21
                            }
                        ],
                        "text": ", 2016) and ReasoNet (Shen et al., 2016), both compute attention dynamically over several hops of a recurrent layer."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Shen et al. (2016) combine Memory Networks with Reinforcement Learning in order to dynamically control the number of hops."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6300274,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c636a2dd242908fe2e598a1077c0c57bfdea8633",
            "isKey": false,
            "numCitedBy": 290,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Teaching a computer to read and answer general questions pertaining to a document is a challenging yet unsolved problem. In this paper, we describe a novel neural network architecture called the Reasoning Network (ReasoNet) for machine comprehension tasks. ReasoNets make use of multiple turns to effectively exploit and then reason over the relation among queries, documents, and answers. Different from previous approaches using a fixed number of turns during inference, ReasoNets introduce a termination state to relax this constraint on the reasoning depth. With the use of reinforcement learning, ReasoNets can dynamically determine whether to continue the comprehension process after digesting intermediate results, or to terminate reading when it concludes that existing information is adequate to produce an answer. ReasoNets achieve superior performance in machine comprehension datasets, including unstructured CNN and Daily Mail datasets, the Stanford SQuAD dataset, and a structured Graph Reachability dataset."
            },
            "slug": "ReasoNet:-Learning-to-Stop-Reading-in-Machine-Shen-Huang",
            "title": {
                "fragments": [],
                "text": "ReasoNet: Learning to Stop Reading in Machine Comprehension"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel neural network architecture called the Reasoning Network (ReasoNet) for machine comprehension tasks, which makes use of multiple turns to effectively exploit and then reason over the relation among queries, documents, and answers."
            },
            "venue": {
                "fragments": [],
                "text": "CoCo@NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8553015"
                        ],
                        "name": "Jiasen Lu",
                        "slug": "Jiasen-Lu",
                        "structuredName": {
                            "firstName": "Jiasen",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiasen Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145743311"
                        ],
                        "name": "Jianwei Yang",
                        "slug": "Jianwei-Yang",
                        "structuredName": {
                            "firstName": "Jianwei",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianwei Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746610"
                        ],
                        "name": "Dhruv Batra",
                        "slug": "Dhruv-Batra",
                        "structuredName": {
                            "firstName": "Dhruv",
                            "lastName": "Batra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dhruv Batra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153432684"
                        ],
                        "name": "Devi Parikh",
                        "slug": "Devi-Parikh",
                        "structuredName": {
                            "firstName": "Devi",
                            "lastName": "Parikh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Devi Parikh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We thank the anonymous reviewers for their helpful comments."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 0
                            }
                        ],
                        "text": "Lu et al. (2016) have recently shown that in addition to attending from the question to image patches, attending from the image back to the question words provides an improvement on the VQA task."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 868693,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb9d253258d6b3beceb9d6cd7bba6e0a29ab875b",
            "isKey": false,
            "numCitedBy": 1121,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "A number of recent works have proposed attention models for Visual Question Answering (VQA) that generate spatial maps highlighting image regions relevant to answering the question. In this paper, we argue that in addition to modeling \"where to look\" or visual attention, it is equally important to model \"what words to listen to\" or question attention. We present a novel co-attention model for VQA that jointly reasons about image and question attention. In addition, our model reasons about the question (and consequently the image via the co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional convolution neural networks (CNN). Our model improves the state-of-the-art on the VQA dataset from 60.3% to 60.5%, and from 61.6% to 63.3% on the COCO-QA dataset. By using ResNet, the performance is further improved to 62.1% for VQA and 65.4% for COCO-QA."
            },
            "slug": "Hierarchical-Question-Image-Co-Attention-for-Visual-Lu-Yang",
            "title": {
                "fragments": [],
                "text": "Hierarchical Question-Image Co-Attention for Visual Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper presents a novel co-attention model for VQA that jointly reasons about image and question attention in a hierarchical fashion via a novel 1-dimensional convolution neural networks (CNN)."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144422314"
                        ],
                        "name": "Matthew Richardson",
                        "slug": "Matthew-Richardson",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Richardson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew Richardson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1859813"
                        ],
                        "name": "Erin Renshaw",
                        "slug": "Erin-Renshaw",
                        "structuredName": {
                            "firstName": "Erin",
                            "lastName": "Renshaw",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Erin Renshaw"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 30
                            }
                        ],
                        "text": "Early datasets such as MCTest (Richardson et al., 2013) were too small to train end to end neural models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 31
                            }
                        ],
                        "text": "Early datasets such as MCTest (Richardson et al., 2013) were too\nsmall to train end-to-end neural models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2100831,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "564257469fa44cdb57e4272f85253efb9acfd69d",
            "isKey": false,
            "numCitedBy": 594,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We present MCTest, a freely available set of stories and associated questions intended for research on the machine comprehension of text. Previous work on machine comprehension (e.g., semantic modeling) has made great strides, but primarily focuses either on limited-domain datasets, or on solving a more restricted goal (e.g., open-domain relation extraction). In contrast, MCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension. Reading comprehension can test advanced abilities such as causal reasoning and understanding the world, yet, by being multiple-choice, still provide a clear metric. By being fictional, the answer typically can be found only in the story itself. The stories and questions are also carefully limited to those a young child would understand, reducing the world knowledge that is required for the task. We present the scalable crowd-sourcing methods that allow us to cheaply construct a dataset of 500 stories and 2000 questions. By screening workers (with grammar tests) and stories (with grading), we have ensured that the data is the same quality as another set that we manually edited, but at one tenth the editing cost. By being open-domain, yet carefully restricted, we hope MCTest will serve to encourage research and provide a clear metric for advancement on the machine comprehension of text. 1 Reading Comprehension A major goal for NLP is for machines to be able to understand text as well as people. Several research disciplines are focused on this problem: for example, information extraction, relation extraction, semantic role labeling, and recognizing textual entailment. Yet these techniques are necessarily evaluated individually, rather than by how much they advance us towards the end goal. On the other hand, the goal of semantic parsing is the machine comprehension of text (MCT), yet its evaluation requires adherence to a specific knowledge representation, and it is currently unclear what the best representation is, for open-domain text. We believe that it is useful to directly tackle the top-level task of MCT. For this, we need a way to measure progress. One common method for evaluating someone\u2019s understanding of text is by giving them a multiple-choice reading comprehension test. This has the advantage that it is objectively gradable (vs. essays) yet may test a range of abilities such as causal or counterfactual reasoning, inference among relations, or just basic understanding of the world in which the passage is set. Therefore, we propose a multiple-choice reading comprehension task as a way to evaluate progress on MCT. We have built a reading comprehension dataset containing 500 fictional stories, with 4 multiple choice questions per story. It was built using methods which can easily scale to at least 5000 stories, since the stories were created, and the curation was done, using crowd sourcing almost entirely, at a total of $4.00 per story. We plan to periodically update the dataset to ensure that methods are not overfitting to the existing data. The dataset is open-domain, yet restricted to concepts and words that a 7 year old is expected to understand. This task is still beyond the capability of today\u2019s computers and algorithms."
            },
            "slug": "MCTest:-A-Challenge-Dataset-for-the-Open-Domain-of-Richardson-Burges",
            "title": {
                "fragments": [],
                "text": "MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "MCTest is presented, a freely available set of stories and associated questions intended for research on the machine comprehension of text that requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50536468"
                        ],
                        "name": "Danqi Chen",
                        "slug": "Danqi-Chen",
                        "structuredName": {
                            "firstName": "Danqi",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danqi Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40756403"
                        ],
                        "name": "Jason Bolton",
                        "slug": "Jason-Bolton",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Bolton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason Bolton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Chen et al. (2016) show that simply using bilinear term for computing the attention weights in the same model drastically improves the accuracy."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 31
                            }
                        ],
                        "text": "Bilinear term has been used by Chen et al. (2016)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6360322,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b1e20420982a4f923c08652941666b189b11b7fe",
            "isKey": false,
            "numCitedBy": 493,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Enabling a computer to understand a document so that it can answer comprehension questions is a central, yet unsolved goal of NLP. A key factor impeding its solution by machine learned systems is the limited availability of human-annotated data. Hermann et al. (2015) seek to solve this problem by creating over a million training examples by pairing CNN and Daily Mail news articles with their summarized bullet points, and show that a neural network can then be trained to give good performance on this task. In this paper, we conduct a thorough examination of this new reading comprehension task. Our primary aim is to understand what depth of language understanding is required to do well on this task. We approach this from one side by doing a careful hand-analysis of a small subset of the problems and from the other by showing that simple, carefully designed systems can obtain accuracies of 73.6% and 76.6% on these two datasets, exceeding current state-of-the-art results by 7-10% and approaching what we believe is the ceiling for performance on this task."
            },
            "slug": "A-Thorough-Examination-of-the-CNN/Daily-Mail-Task-Chen-Bolton",
            "title": {
                "fragments": [],
                "text": "A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A thorough examination of this new reading comprehension task by creating over a million training examples by pairing CNN and Daily Mail news articles with their summarized bullet points, and showing that a neural network can be trained to give good performance on this task."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3456592"
                        ],
                        "name": "Sosuke Kobayashi",
                        "slug": "Sosuke-Kobayashi",
                        "structuredName": {
                            "firstName": "Sosuke",
                            "lastName": "Kobayashi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sosuke Kobayashi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066087250"
                        ],
                        "name": "Ran Tian",
                        "slug": "Ran-Tian",
                        "structuredName": {
                            "firstName": "Ran",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ran Tian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764004"
                        ],
                        "name": "Naoaki Okazaki",
                        "slug": "Naoaki-Okazaki",
                        "structuredName": {
                            "firstName": "Naoaki",
                            "lastName": "Okazaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naoaki Okazaki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3040648"
                        ],
                        "name": "Kentaro Inui",
                        "slug": "Kentaro-Inui",
                        "structuredName": {
                            "firstName": "Kentaro",
                            "lastName": "Inui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kentaro Inui"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10239453,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4916a5fb50bcc73213b6f054c42ad10c68c52cd",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel neural network model for machine reading, DER Network, which explicitly implements a reader building dynamic meaning representations for entities by gathering and accumulating information around the entities as it reads a document. Evaluated on a recent large scale dataset (Hermann et al., 2015), our model exhibits better results than previous research, and we find that max-pooling is suited for modeling the accumulation of information on entities. Further analysis suggests that our model can put together multiple pieces of information encoded in different sentences to answer complicated questions. Our code for the model is available at https://github. com/soskek/der-network"
            },
            "slug": "Dynamic-Entity-Representation-with-Max-pooling-Kobayashi-Tian",
            "title": {
                "fragments": [],
                "text": "Dynamic Entity Representation with Max-pooling Improves Machine Reading"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A novel neural network model for machine reading, DER Network, which explicitly implements a reader building dynamic meaning representations for entities by gathering and accumulating information around the entities as it reads a document, finds that max-pooling is suited for modeling the accumulation of information on entities."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46485395"
                        ],
                        "name": "Huijuan Xu",
                        "slug": "Huijuan-Xu",
                        "structuredName": {
                            "firstName": "Huijuan",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huijuan Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2903226"
                        ],
                        "name": "Kate Saenko",
                        "slug": "Kate-Saenko",
                        "structuredName": {
                            "firstName": "Kate",
                            "lastName": "Saenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kate Saenko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We thank the anonymous reviewers for their helpful comments."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 124
                            }
                        ],
                        "text": "At a finer level, each question word attends to each image patch and the highest attention value for each spatial location (Xu & Saenko, 2016) is adopted."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10363459,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1cf6bc0866226c1f8e282463adc8b75d92fba9bb",
            "isKey": false,
            "numCitedBy": 652,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of Visual Question Answering (VQA), which requires joint image and language understanding to answer a question about a given photograph. Recent approaches have applied deep image captioning methods based on convolutional-recurrent networks to this problem, but have failed to model spatial inference. To remedy this, we propose a model we call the Spatial Memory Network and apply it to the VQA task. Memory networks are recurrent neural networks with an explicit attention mechanism that selects certain parts of the information stored in memory. Our Spatial Memory Network stores neuron activations from different spatial regions of the image in its memory, and uses the question to choose relevant regions for computing the answer, a process of which constitutes a single \"hop\" in the network. We propose a novel spatial attention architecture that aligns words with image patches in the first hop, and obtain improved results by adding a second attention hop which considers the whole question to choose visual evidence based on the results of the first hop. To better understand the inference process learned by the network, we design synthetic questions that specifically require spatial inference and visualize the attention weights. We evaluate our model on two published visual question answering datasets, DAQUAR [1] and VQA [2], and obtain improved results compared to a strong deep baseline model (iBOWIMG) which concatenates image and question features to predict the answer [3]."
            },
            "slug": "Ask,-Attend-and-Answer:-Exploring-Question-Guided-Xu-Saenko",
            "title": {
                "fragments": [],
                "text": "Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The Spatial Memory Network, a novel spatial attention architecture that aligns words with image patches in the first hop, is proposed and improved results are obtained compared to a strong deep baseline model which concatenates image and question features to predict the answer."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3382568"
                        ],
                        "name": "Adam Trischler",
                        "slug": "Adam-Trischler",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Trischler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Trischler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "83368021"
                        ],
                        "name": "Zheng Ye",
                        "slug": "Zheng-Ye",
                        "structuredName": {
                            "firstName": "Zheng",
                            "lastName": "Ye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zheng Ye"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2854297"
                        ],
                        "name": "Xingdi Yuan",
                        "slug": "Xingdi-Yuan",
                        "structuredName": {
                            "firstName": "Xingdi",
                            "lastName": "Yuan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xingdi Yuan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143902541"
                        ],
                        "name": "Philip Bachman",
                        "slug": "Philip-Bachman",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Bachman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip Bachman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041695"
                        ],
                        "name": "Alessandro Sordoni",
                        "slug": "Alessandro-Sordoni",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Sordoni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alessandro Sordoni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2987426"
                        ],
                        "name": "Kaheer Suleman",
                        "slug": "Kaheer-Suleman",
                        "structuredName": {
                            "firstName": "Kaheer",
                            "lastName": "Suleman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaheer Suleman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 711424,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "75fa915984f1903cd8d0e1ea54b9d008d5a87fe5",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We present the EpiReader, a novel model for machine comprehension of text. Machine comprehension of unstructured, real-world text is a major research goal for natural language processing. Current tests of machine comprehension pose questions whose answers can be inferred from some supporting text, and evaluate a model's response to the questions. The EpiReader is an end-to-end neural model comprising two components: the first component proposes a small set of candidate answers after comparing a question to its supporting text, and the second component formulates hypotheses using the proposed candidates and the question, then reranks the hypotheses based on their estimated concordance with the supporting text. We present experiments demonstrating that the EpiReader sets a new state-of-the-art on the CNN and Children's Book Test machine comprehension benchmarks, outperforming previous neural models by a significant margin."
            },
            "slug": "Natural-Language-Comprehension-with-the-EpiReader-Trischler-Ye",
            "title": {
                "fragments": [],
                "text": "Natural Language Comprehension with the EpiReader"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The EpiReader is an end-to-end neural model comprising two components: the first component proposes a small set of candidate answers after comparing a question to its supporting text, and the second component formulates hypotheses using the proposed candidates and the question, then reranks the hypotheses based on their estimated concordance with the supporting text."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117748"
                        ],
                        "name": "Yuke Zhu",
                        "slug": "Yuke-Zhu",
                        "structuredName": {
                            "firstName": "Yuke",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuke Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50499889"
                        ],
                        "name": "O. Groth",
                        "slug": "O.-Groth",
                        "structuredName": {
                            "firstName": "Oliver",
                            "lastName": "Groth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Groth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145879842"
                        ],
                        "name": "Michael S. Bernstein",
                        "slug": "Michael-S.-Bernstein",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Bernstein",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael S. Bernstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 92
                            }
                        ],
                        "text": "At the coarse level of granularity, the question attends to different patches in the image (Zhu et al., 2016; Xiong et al., 2016a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 91
                            }
                        ],
                        "text": "At the coarse level of granularity, the question attends to different patches in the image (Zhu et al., 2016; Xiong et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5714907,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "def584565d05d6a8ba94de6621adab9e301d375d",
            "isKey": false,
            "numCitedBy": 591,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "We have seen great progress in basic perceptual tasks such as object recognition and detection. However, AI models still fail to match humans in high-level vision tasks due to the lack of capacities for deeper reasoning. Recently the new task of visual question answering (QA) has been proposed to evaluate a model's capacity for deep image understanding. Previous works have established a loose, global association between QA sentences and images. However, many questions and answers, in practice, relate to local regions in the images. We establish a semantic link between textual descriptions and image regions by object-level grounding. It enables a new type of QA with visual answers, in addition to textual answers used in previous work. We study the visual QA tasks in a grounded setting with a large collection of 7W multiple-choice QA pairs. Furthermore, we evaluate human performance and several baseline models on the QA tasks. Finally, we propose a novel LSTM model with spatial attention to tackle the 7W QA tasks."
            },
            "slug": "Visual7W:-Grounded-Question-Answering-in-Images-Zhu-Groth",
            "title": {
                "fragments": [],
                "text": "Visual7W: Grounded Question Answering in Images"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A semantic link between textual descriptions and image regions by object-level grounding enables a new type of QA with visual answers, in addition to textual answers used in previous work, and proposes a novel LSTM model with spatial attention to tackle the 7W QA tasks."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109512754"
                        ],
                        "name": "Zhilin Yang",
                        "slug": "Zhilin-Yang",
                        "structuredName": {
                            "firstName": "Zhilin",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhilin Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34994191"
                        ],
                        "name": "Bhuwan Dhingra",
                        "slug": "Bhuwan-Dhingra",
                        "structuredName": {
                            "firstName": "Bhuwan",
                            "lastName": "Dhingra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bhuwan Dhingra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112460718"
                        ],
                        "name": "Ye Yuan",
                        "slug": "Ye-Yuan",
                        "structuredName": {
                            "firstName": "Ye",
                            "lastName": "Yuan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ye Yuan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145919378"
                        ],
                        "name": "Junjie Hu",
                        "slug": "Junjie-Hu",
                        "structuredName": {
                            "firstName": "Junjie",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junjie Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50056360"
                        ],
                        "name": "William W. Cohen",
                        "slug": "William-W.-Cohen",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Cohen",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William W. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10370694,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7ffc8f44f7dafd7f51e4e7500842ec406b8e239",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous work combines word-level and character-level representations using concatenation or scalar weighting, which is suboptimal for high-level tasks like reading comprehension. We present a fine-grained gating mechanism to dynamically combine word-level and character-level representations based on properties of the words. We also extend the idea of fine-grained gating to modeling the interaction between questions and paragraphs for reading comprehension. Experiments show that our approach can improve the performance on reading comprehension tasks, achieving new state-of-the-art results on the Children's Book Test dataset. To demonstrate the generality of our gating mechanism, we also show improved results on a social media tag prediction task."
            },
            "slug": "Words-or-Characters-Fine-grained-Gating-for-Reading-Yang-Dhingra",
            "title": {
                "fragments": [],
                "text": "Words or Characters? Fine-grained Gating for Reading Comprehension"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A fine-grained gating mechanism to dynamically combine word-level and character-level representations based on properties of the words is presented, which can improve the performance on reading comprehension tasks and show improved results on a social media tag prediction task."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3295092"
                        ],
                        "name": "S. Chopra",
                        "slug": "S.-Chopra",
                        "structuredName": {
                            "firstName": "Sumit",
                            "lastName": "Chopra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chopra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713934"
                        ],
                        "name": "Antoine Bordes",
                        "slug": "Antoine-Bordes",
                        "structuredName": {
                            "firstName": "Antoine",
                            "lastName": "Bordes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antoine Bordes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "n successfully extended to machine comprehension (attention between the query and the context paragraph) and visual question answering (attention between the query and image). These mechanisms (e.g., Weston et al. (2015); Antol et al. (2015); Xiong et al. (2016)) typically have one or more of the following characteristics. First, the computed attention weight vectors are often used to combine the constituents into a "
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "dels, BIDAF does not summarize the two modalities in the attention layer and instead lets the attention vectors \ufb02ow into the modeling layer. The third group (considered as variants of Memory Network (Weston et al., 2015)) repeats computing an attention vector between the query and the context through multiple layers, typically referred to as multi-hop. (e.g., Sordoni et al. (2016); Dhingra et al. (2016)). Shen et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 25
                            }
                        ],
                        "text": "We use a Long Short-Term Memory Network (LSTM) (Hochreiter & Schmidhuber, 1997) on top of the embeddings provided by the previous layers to model the temporal interactions between words."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 48
                            }
                        ],
                        "text": "Unlike previously popular attention mechanisms (Weston et al., 2015; Hill et al., 2016; Sordoni et al., 2016; Shen et al., 2016), the attention flow layer is not used to summarize the query and context into single feature vectors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 236
                            }
                        ],
                        "text": "\u2026the advancement has been the use of neural attention mechanism, which enables the system to focus on a targeted area within a context paragraph (for MC) or within an image (for Visual QA), that is most relevant to answer the question (Weston et al., 2015; Antol et al., 2015; Xiong et al., 2016a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 59
                            }
                        ],
                        "text": "The third group (considered as variants of Memory Network (Weston et al., 2015)) repeats computing an attention vector between the query and the context through multiple layers, typically referred to as multi-hop (Sordoni et al., 2016; Dhingra et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 27
                            }
                        ],
                        "text": "Shen et al. (2016) combine Memory Networks with Reinforcement Learning in order to dynamically control the number of hops."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2926851,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71ae756c75ac89e2d731c9c79649562b5768ff39",
            "isKey": true,
            "numCitedBy": 1146,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract: We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs."
            },
            "slug": "Memory-Networks-Weston-Chopra",
            "title": {
                "fragments": [],
                "text": "Memory Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "This work describes a new class of learning models called memory networks, which reason with inference components combined with a long-term memory component; they learn how to use these jointly."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2135128997"
                        ],
                        "name": "Akira Fukui",
                        "slug": "Akira-Fukui",
                        "structuredName": {
                            "firstName": "Akira",
                            "lastName": "Fukui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Akira Fukui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422202"
                        ],
                        "name": "Dong Huk Park",
                        "slug": "Dong-Huk-Park",
                        "structuredName": {
                            "firstName": "Dong Huk",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Huk Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422876"
                        ],
                        "name": "Daylen Yang",
                        "slug": "Daylen-Yang",
                        "structuredName": {
                            "firstName": "Daylen",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daylen Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34721166"
                        ],
                        "name": "Anna Rohrbach",
                        "slug": "Anna-Rohrbach",
                        "structuredName": {
                            "firstName": "Anna",
                            "lastName": "Rohrbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anna Rohrbach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34849128"
                        ],
                        "name": "Marcus Rohrbach",
                        "slug": "Marcus-Rohrbach",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Rohrbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus Rohrbach"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 176
                            }
                        ],
                        "text": "Several approaches to constructing the attention matrix have been used including element-wise product, element-wise sum, concatenation and Multimodal Compact Bilinear Pooling (Fukui et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2840197,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fddc15480d086629b960be5bff96232f967f2252",
            "isKey": false,
            "numCitedBy": 1086,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years. However, tasks such as visual question answering require combining these vector representations with each other. Approaches to multimodal pooling include element-wise product or sum, as well as concatenation of the visual and textual representations. We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors. As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multimodal features. We extensively evaluate MCB on the visual question answering and grounding tasks. We consistently show the benefit of MCB over ablations without MCB. For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation. This model outperforms the state-of-the-art on the Visual7W dataset and the VQA challenge."
            },
            "slug": "Multimodal-Compact-Bilinear-Pooling-for-Visual-and-Fukui-Park",
            "title": {
                "fragments": [],
                "text": "Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This work extensively evaluates Multimodal Compact Bilinear pooling (MCB) on the visual question answering and grounding tasks and consistently shows the benefit of MCB over ablations without MCB."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2801949"
                        ],
                        "name": "Aishwarya Agrawal",
                        "slug": "Aishwarya-Agrawal",
                        "structuredName": {
                            "firstName": "Aishwarya",
                            "lastName": "Agrawal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aishwarya Agrawal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8553015"
                        ],
                        "name": "Jiasen Lu",
                        "slug": "Jiasen-Lu",
                        "structuredName": {
                            "firstName": "Jiasen",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiasen Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1963421"
                        ],
                        "name": "Stanislaw Antol",
                        "slug": "Stanislaw-Antol",
                        "structuredName": {
                            "firstName": "Stanislaw",
                            "lastName": "Antol",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stanislaw Antol"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067793408"
                        ],
                        "name": "Margaret Mitchell",
                        "slug": "Margaret-Mitchell",
                        "structuredName": {
                            "firstName": "Margaret",
                            "lastName": "Mitchell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Margaret Mitchell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153432684"
                        ],
                        "name": "Devi Parikh",
                        "slug": "Devi-Parikh",
                        "structuredName": {
                            "firstName": "Devi",
                            "lastName": "Parikh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Devi Parikh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746610"
                        ],
                        "name": "Dhruv Batra",
                        "slug": "Dhruv-Batra",
                        "structuredName": {
                            "firstName": "Dhruv",
                            "lastName": "Batra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dhruv Batra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 275,
                                "start": 257
                            }
                        ],
                        "text": "\u2026the advancement has been the use of neural attention mechanism, which enables the system to focus on a targeted area within a context paragraph (for MC) or within an image (for Visual QA), that is most relevant to answer the question (Weston et al., 2015; Antol et al., 2015; Xiong et al., 2016a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 166
                            }
                        ],
                        "text": "Early works on visual question answering (VQA) involved encoding the question using an RNN, encoding the image using a CNN and combining them to answer the question (Antol et al., 2015; Malinowski et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 55
                            }
                        ],
                        "text": "Second, in the text domain, they are often temporally dynamic, whereby the attention weights at the current time step are a function of the attended vector at the previous time step."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3180429,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db",
            "isKey": false,
            "numCitedBy": 2887,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing $$\\sim $$\u223c0.25\u00a0M images, $$\\sim $$\u223c0.76\u00a0M questions, and $$\\sim $$\u223c10\u00a0M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines and methods for VQA are provided and compared with human performance. Our VQA demo is available on CloudCV (http://cloudcv.org/vqa)."
            },
            "slug": "VQA:-Visual-Question-Answering-Agrawal-Lu",
            "title": {
                "fragments": [],
                "text": "VQA: Visual Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "The task of free-form and open-ended Visual Question Answering (VQA) is proposed, given an image and a natural language question about the image, the task is to provide an accurate natural language answer."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145478807"
                        ],
                        "name": "Mateusz Malinowski",
                        "slug": "Mateusz-Malinowski",
                        "structuredName": {
                            "firstName": "Mateusz",
                            "lastName": "Malinowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mateusz Malinowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34849128"
                        ],
                        "name": "Marcus Rohrbach",
                        "slug": "Marcus-Rohrbach",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Rohrbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus Rohrbach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739548"
                        ],
                        "name": "Mario Fritz",
                        "slug": "Mario-Fritz",
                        "structuredName": {
                            "firstName": "Mario",
                            "lastName": "Fritz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mario Fritz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 186
                            }
                        ],
                        "text": "Early works on visual question answering (VQA) involved encoding the question using an RNN, encoding the image using a CNN and combining them to answer the question (Antol et al., 2015; Malinowski et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 738850,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd7bd1d2945a58cdcc1797ba9698b8810fe68f60",
            "isKey": false,
            "numCitedBy": 519,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We address a question answering task on real-world images that is set up as a Visual Turing Test. By combining latest advances in image representation and natural language processing, we propose Neural-Image-QA, an end-to-end formulation to this problem for which all parts are trained jointly. In contrast to previous efforts, we are facing a multi-modal problem where the language output (answer) is conditioned on visual and natural language input (image and question). Our approach Neural-Image-QA doubles the performance of the previous best approach on this problem. We provide additional insights into the problem by analyzing how much information is contained only in the language part for which we provide a new human baseline. To study human consensus, which is related to the ambiguities inherent in this challenging task, we propose two novel metrics and collect additional answers which extends the original DAQUAR dataset to DAQUAR-Consensus."
            },
            "slug": "Ask-Your-Neurons:-A-Neural-Based-Approach-to-about-Malinowski-Rohrbach",
            "title": {
                "fragments": [],
                "text": "Ask Your Neurons: A Neural-Based Approach to Answering Questions about Images"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This work addresses a question answering task on real-world images that is set up as a Visual Turing Test by combining latest advances in image representation and natural language processing and proposes Neural-Image-QA, an end-to-end formulation to this problem for which all parts are trained jointly."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2910877"
                        ],
                        "name": "K. Hermann",
                        "slug": "K.-Hermann",
                        "structuredName": {
                            "firstName": "Karl",
                            "lastName": "Hermann",
                            "middleNames": [
                                "Moritz"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2367821"
                        ],
                        "name": "Tom\u00e1s Kocisk\u00fd",
                        "slug": "Tom\u00e1s-Kocisk\u00fd",
                        "structuredName": {
                            "firstName": "Tom\u00e1s",
                            "lastName": "Kocisk\u00fd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom\u00e1s Kocisk\u00fd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1864353"
                        ],
                        "name": "Edward Grefenstette",
                        "slug": "Edward-Grefenstette",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Grefenstette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward Grefenstette"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2311318"
                        ],
                        "name": "Lasse Espeholt",
                        "slug": "Lasse-Espeholt",
                        "structuredName": {
                            "firstName": "Lasse",
                            "lastName": "Espeholt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lasse Espeholt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2062879616"
                        ],
                        "name": "Will Kay",
                        "slug": "Will-Kay",
                        "structuredName": {
                            "firstName": "Will",
                            "lastName": "Kay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Will Kay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2573615"
                        ],
                        "name": "Mustafa Suleyman",
                        "slug": "Mustafa-Suleyman",
                        "structuredName": {
                            "firstName": "Mustafa",
                            "lastName": "Suleyman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mustafa Suleyman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685771"
                        ],
                        "name": "P. Blunsom",
                        "slug": "P.-Blunsom",
                        "structuredName": {
                            "firstName": "Phil",
                            "lastName": "Blunsom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Blunsom"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 115
                            }
                        ],
                        "text": "We also evaluate our model on the task of cloze-style reading comprehension using the CNN and Daily Mail datasets (Hermann et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 46
                            }
                        ],
                        "text": "Massive cloze test datasets (CNN/DailyMail by Hermann et al. (2015) and Childrens Book Test by Hill et al. (2016)), enabled the application of deep neural architectures to this task."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Hermann et al. (2015) have recently compiled a massive Cloze-style comprehension dataset, consisting of 300k/4k/3k and 879k/65k/53k (train/dev/test) examples from CNN and DailyMail news articles, respectively."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 174
                            }
                        ],
                        "text": "The two significant contributors towards this progress are the advent of deep neural architectures and the availability of massive datasets on the order of 100,000 questions (Rajpurkar et al., 2016; Hermann et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 55
                            }
                        ],
                        "text": "Linear mapping after perceptron layer has been used by Hermann et al. (2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 49
                            }
                        ],
                        "text": "CNN DailyMail val test val test Attentive Reader (Hermann et al., 2015) 61."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Hermann et al. (2015) argue that the dynamic attention model performs better than using a single fixed query vector to attend on context words on CNN & DailyMail datasets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 114
                            }
                        ],
                        "text": "We also evaluate our model on the task of Cloze-style reading comprehension using the CNN and Daily Mail datasets (Hermann et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6203757,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1505c6123c102e53eb19dff312cb25cea840b72",
            "isKey": true,
            "numCitedBy": 2432,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure."
            },
            "slug": "Teaching-Machines-to-Read-and-Comprehend-Hermann-Kocisk\u00fd",
            "title": {
                "fragments": [],
                "text": "Teaching Machines to Read and Comprehend"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new methodology is defined that resolves this bottleneck and provides large scale supervised reading comprehension data that allows a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure to be developed."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145783676"
                        ],
                        "name": "Felix Hill",
                        "slug": "Felix-Hill",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Hill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Felix Hill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713934"
                        ],
                        "name": "Antoine Bordes",
                        "slug": "Antoine-Bordes",
                        "structuredName": {
                            "firstName": "Antoine",
                            "lastName": "Bordes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antoine Bordes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3295092"
                        ],
                        "name": "S. Chopra",
                        "slug": "S.-Chopra",
                        "structuredName": {
                            "firstName": "Sumit",
                            "lastName": "Chopra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chopra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 37
                            }
                        ],
                        "text": "Inspired by the window-based method (Hill et al., 2016), we split each article into short sentences where each sentence is a 19-word window around each entity (hence the same word might appear in multiple sentences)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 95
                            }
                        ],
                        "text": "Massive cloze test datasets (CNN/DailyMail by Hermann et al. (2015) and Childrens Book Test by Hill et al. (2016)), enabled the application of deep neural architectures to this task."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 36
                            }
                        ],
                        "text": "Inspired by the window-based method (Hill et al., 2016), in order to speed up the training process and reduce GPU memory usage, we only consider words within 7 word-distance of any entity, which we found to be sufficient in most cases."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 69
                            }
                        ],
                        "text": "Unlike previously popular attention mechanisms (Weston et al., 2015; Hill et al., 2016; Sordoni et al., 2016; Shen et al., 2016), the attention flow layer is not used to summarize the query and context into single feature vectors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 83
                            }
                        ],
                        "text": "Dot product has been used for the measurement of similarity between two vectors by Hill et al. (2016)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14915449,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "35b91b365ceb016fb3e022577cec96fb9b445dc5",
            "isKey": true,
            "numCitedBy": 530,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new test of how well language models capture meaning in children's books. Unlike standard language modelling benchmarks, it distinguishes the task of predicting syntactic function words from that of predicting lower-frequency words, which carry greater semantic content. We compare a range of state-of-the-art models, each with a different way of encoding what has been previously read. We show that models which store explicit representations of long-term contexts outperform state-of-the-art neural language models at predicting semantic content words, although this advantage is not observed for syntactic function words. Interestingly, we find that the amount of text encoded in a single memory representation is highly influential to the performance: there is a sweet-spot, not too big and not too small, between single words and full sentences that allows the most meaningful information in a text to be effectively retained and recalled. Further, the attention over such window-based memories can be trained effectively through self-supervision. We then assess the generality of this principle by applying it to the CNN QA benchmark, which involves identifying named entities in paraphrased summaries of news articles, and achieve state-of-the-art performance."
            },
            "slug": "The-Goldilocks-Principle:-Reading-Children's-Books-Hill-Bordes",
            "title": {
                "fragments": [],
                "text": "The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "There is a sweet-spot, not too big and not too small, between single words and full sentences that allows the most meaningful information in a text to be effectively retained and recalled, and models which store explicit representations of long-term contexts outperform state-of-the-art neural language models at predicting semantic content words."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143845796"
                        ],
                        "name": "Jeffrey Pennington",
                        "slug": "Jeffrey-Pennington",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Pennington",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey Pennington"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 40
                            }
                        ],
                        "text": "We use pre-trained word vectors, GloVe (Pennington et al., 2014), to obtain the fixed word embedding of each word."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 94
                            }
                        ],
                        "text": "A significant contributor to the advancement of MC models has been the availability of large datasets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1957433,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f37e1b62a767a307c046404ca96bc140b3e68cb5",
            "isKey": false,
            "numCitedBy": 22536,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition."
            },
            "slug": "GloVe:-Global-Vectors-for-Word-Representation-Pennington-Socher",
            "title": {
                "fragments": [],
                "text": "GloVe: Global Vectors for Word Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods and produces a vector space with meaningful substructure."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3335364"
                        ],
                        "name": "Dzmitry Bahdanau",
                        "slug": "Dzmitry-Bahdanau",
                        "structuredName": {
                            "firstName": "Dzmitry",
                            "lastName": "Bahdanau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dzmitry Bahdanau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1979489"
                        ],
                        "name": "Kyunghyun Cho",
                        "slug": "Kyunghyun-Cho",
                        "structuredName": {
                            "firstName": "Kyunghyun",
                            "lastName": "Cho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyunghyun Cho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 67
                            }
                        ],
                        "text": "That is, while we iteratively compute attention through time as in Bahdanau et al. (2015), the attention at each time step is a function of only the query and the context paragraph at the current time step and does not directly depend on the attention at the previous time step."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 6
                            }
                        ],
                        "text": "Attention mechanisms in previous works typically have one or more of the following characteristics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 37
                            }
                        ],
                        "text": "The first group (largely inspired by Bahdanau et al. (2015)) uses a dynamic attention mechanism, in which the attention weights are updated dynamically given the query and the context as well as the previous attention."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 170
                            }
                        ],
                        "text": "To evaluate the attention flow, we study a dynamic attention model, where the attention is dynamically computed within the modeling layer\u2019s LSTM, following previous work (Bahdanau et al., 2015; Wang & Jiang, 2016)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11212020,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
            "isKey": true,
            "numCitedBy": 19342,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition."
            },
            "slug": "Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho",
            "title": {
                "fragments": [],
                "text": "Neural Machine Translation by Jointly Learning to Align and Translate"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and it is proposed to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38367242"
                        ],
                        "name": "Yoon Kim",
                        "slug": "Yoon-Kim",
                        "structuredName": {
                            "firstName": "Yoon",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoon Kim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 2
                            }
                        ],
                        "text": "Word Embedding Layer maps each word to a vector space using a pre-trained word embedding model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 10
                            }
                        ],
                        "text": "Following Kim (2014), we obtain the characterlevel embedding of each word using Convolutional Neural Networks (CNN)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9672033,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba",
            "isKey": false,
            "numCitedBy": 10065,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification."
            },
            "slug": "Convolutional-Neural-Networks-for-Sentence-Kim",
            "title": {
                "fragments": [],
                "text": "Convolutional Neural Networks for Sentence Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification, and are proposed to allow for the use of both task-specific and static vectors."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308557"
                        ],
                        "name": "S. Hochreiter",
                        "slug": "S.-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hochreiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "We use two layers of bi-directional LSTM, with the output size of d for each direction."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 264
                            }
                        ],
                        "text": "C L\n] 2\n4 Fe\nb 20\n17\nModeling Layer\nOutput Layer\nAttention Flow Layer\nContextual Embed Layer\nWord Embed Layer\nx1 x2 x3 xT q1 qJ\nLS TM\nLS TM\nLS TM\nLS TM\nStart End\nh1 h2 hT\nu1 u2\nuJ\nSo ftm\nax\nh1 h2 hT\nu1 u2\nuJ\nM ax\nSoftmax\nContext2Query\nQuery2Context\nh1 h2 hT u1 uJ\nLSTM + SoftmaxDense + Softmax\nContext Query\nQuery2Context and Context2Query Attention\nWord Embedding\nGLOVE Char-CNN\nCharacter Embed Layer\nCharacter Embedding\ng1 g2 gT\nm1 m2 mT\nFigure 1: BiDirectional Attention Flow Model (best viewed in color)\nquery-aware context representation (the output of the attention layer)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 139
                            }
                        ],
                        "text": "To evaluate the attention flow, we study a dynamic attention model, where the attention is dynamically computed within the modeling layer\u2019s LSTM, following previous work (Bahdanau et al., 2015; Wang & Jiang, 2016)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 48
                            }
                        ],
                        "text": "We use a Long Short-Term Memory Network (LSTM) (Hochreiter & Schmidhuber, 1997) on top of the embeddings provided by the previous layers to model the temporal interactions between words."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 79
                            }
                        ],
                        "text": "In this section, we evaluate our model on the task of question answering using the recently released SQuAD (Rajpurkar et al., 2016), which has gained a huge attention over a few months."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 72
                            }
                        ],
                        "text": "Since the output dimension of \u03b2 changes, the input dimension of the first LSTM of the modeling layer will change as well."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 224
                            }
                        ],
                        "text": "Hence we obtain H \u2208 R2d\u00d7T from the context word vectors X, and U \u2208 R2d\u00d7J from query word vectors Q. Note that each column vector of H and U is 2d-dimensional because of the concatenation of the outputs of the forward and backward LSTMs, each with d-dimensional output."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 12
                            }
                        ],
                        "text": "We place an LSTM in both directions, and concatenate the outputs of the two LSTMs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "For the end index of the answer phrase, we pass M to another bidirectional LSTM layer and obtain M2 \u2208 R2d\u00d7T ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 154
                            }
                        ],
                        "text": "For ablating C2Q attention, we replace the attended question vector U\u0303 with the average of the output vectors of the question\u2019s contextual embedding layer (LSTM)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 331,
                                "start": 327
                            }
                        ],
                        "text": "We define the training loss (to be minimized) as the sum of the negative log probabilities of the true start and end indices by the predicted distributions, averaged over all examples:\nL(\u03b8) = \u2212 1 N N\u2211 i log(p1y1i ) + log(p2y2i ) (5)\nwhere \u03b8 is the set of all trainable weights in the model (the weights and biases of CNN filters and LSTM cells, w(S), w(p1) and w(p2)), N is the number of examples in the dataset, y1i and y 2 i are the true start and end indices of the i-th example, respectively, and pk indicates the k-th value of the vector p.\nTest."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "A dropout (Srivastava et al., 2014) rate of 0.2 is used for the CNN, all LSTM layers, and the linear transformation before the softmax for the answers."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1915014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44d2abe2175df8153f465f6c39b68b76a0d40ab9",
            "isKey": true,
            "numCitedBy": 51694,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms."
            },
            "slug": "Long-Short-Term-Memory-Hochreiter-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Long Short-Term Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel, efficient, gradient based method called long short-term memory (LSTM) is introduced, which can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8536286"
                        ],
                        "name": "A. Bosselut",
                        "slug": "A.-Bosselut",
                        "structuredName": {
                            "firstName": "Antoine",
                            "lastName": "Bosselut",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bosselut"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39455775"
                        ],
                        "name": "Omer Levy",
                        "slug": "Omer-Levy",
                        "structuredName": {
                            "firstName": "Omer",
                            "lastName": "Levy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omer Levy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14487640"
                        ],
                        "name": "Ari Holtzman",
                        "slug": "Ari-Holtzman",
                        "structuredName": {
                            "firstName": "Ari",
                            "lastName": "Holtzman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ari Holtzman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40479850"
                        ],
                        "name": "C. Ennis",
                        "slug": "C.-Ennis",
                        "structuredName": {
                            "firstName": "Corin",
                            "lastName": "Ennis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Ennis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145197953"
                        ],
                        "name": "D. Fox",
                        "slug": "D.-Fox",
                        "structuredName": {
                            "firstName": "Dieter",
                            "lastName": "Fox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Fox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699545"
                        ],
                        "name": "Yejin Choi",
                        "slug": "Yejin-Choi",
                        "structuredName": {
                            "firstName": "Yejin",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yejin Choi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 51540074,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bcd857d75841aa3e92cd4284a8818aba9f6c0c3f",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Understanding procedural language requires anticipating the causal effects of actions, even when they are not explicitly stated. In this work, we introduce Neural Process Networks to understand procedural text through (neural) simulation of action dynamics. Our model complements existing memory architectures with dynamic entity tracking by explicitly modeling actions as state transformers. The model updates the states of the entities by executing learned action operators. Empirical results demonstrate that our proposed model can reason about the unstated causal effects of actions, allowing it to provide more accurate contextual information for understanding and generating procedural text, all while offering more interpretable internal representations than existing alternatives."
            },
            "slug": "Published-as-a-conference-paper-at-ICLR-2018-S-A-D-Bosselut-Levy",
            "title": {
                "fragments": [],
                "text": "Published as a conference paper at ICLR 2018 S IMULATING A CTION D YNAMICS WITH N EURAL P ROCESS N ETWORKS"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work introduces Neural Process Networks to understand procedural text through (neural) simulation of action dynamics, and complements existing memory architectures with dynamic entity tracking by explicitly modeling actions as state transformers."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2897313"
                        ],
                        "name": "Nitish Srivastava",
                        "slug": "Nitish-Srivastava",
                        "structuredName": {
                            "firstName": "Nitish",
                            "lastName": "Srivastava",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nitish Srivastava"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 11
                            }
                        ],
                        "text": "A dropout (Srivastava et al., 2014) rate of 0.2 is used for the CNN, all LSTM layers, and the linear transformation before the softmax for the answers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6844431,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "34f25a8704614163c4095b3ee2fc969b60de4698",
            "isKey": true,
            "numCitedBy": 28149,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \"thinned\" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets."
            },
            "slug": "Dropout:-a-simple-way-to-prevent-neural-networks-Srivastava-Hinton",
            "title": {
                "fragments": [],
                "text": "Dropout: a simple way to prevent neural networks from overfitting"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1425082935"
                        ],
                        "name": "Xinyun Chen",
                        "slug": "Xinyun-Chen",
                        "structuredName": {
                            "firstName": "Xinyun",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinyun Chen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17735339,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "94e3e7bc3d23276f0ee2d1cb8f9d14aa19668d5f",
            "isKey": false,
            "numCitedBy": 488,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to find, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the first time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack Clarifai.com, which is a black-box image classification system."
            },
            "slug": "Under-Review-as-a-Conference-Paper-at-Iclr-2017-Ex-Chen",
            "title": {
                "fragments": [],
                "text": "Under Review as a Conference Paper at Iclr 2017 Delving into Transferable Adversarial Ex- Amples and Black-box Attacks"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This work is the first to conduct an extensive study of the transferability over large models and a large scale dataset, and it is also theFirst to study the transferabilities of targeted adversarial examples with their target labels."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48799969"
                        ],
                        "name": "Matthew D. Zeiler",
                        "slug": "Matthew-D.-Zeiler",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Zeiler",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew D. Zeiler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7365802,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8729441d734782c3ed532a7d2d9611b438c0a09a",
            "isKey": false,
            "numCitedBy": 5464,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
            },
            "slug": "ADADELTA:-An-Adaptive-Learning-Rate-Method-Zeiler",
            "title": {
                "fragments": [],
                "text": "ADADELTA: An Adaptive Learning Rate Method"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A novel per-dimension learning rate method for gradient descent called ADADELTA that dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent is presented."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 33
                            }
                        ],
                        "text": "Second, in the text domain, they are often temporally dynamic, whereby the attention weights at the current time step are a function of the attended vector at the previous time step."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 25
                            }
                        ],
                        "text": "We use a Long Short-Term Memory Network (LSTM) (Hochreiter & Schmidhuber, 1997) on top of the embeddings provided by the previous layers to model the temporal interactions between words."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 48
                            }
                        ],
                        "text": "Unlike previously popular attention mechanisms (Weston et al., 2015; Hill et al., 2016; Sordoni et al., 2016; Shen et al., 2016), the attention flow layer is not used to summarize the query and context into single feature vectors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 236
                            }
                        ],
                        "text": "\u2026the advancement has been the use of neural attention mechanism, which enables the system to focus on a targeted area within a context paragraph (for MC) or within an image (for Visual QA), that is most relevant to answer the question (Weston et al., 2015; Antol et al., 2015; Xiong et al., 2016a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 59
                            }
                        ],
                        "text": "The third group (considered as variants of Memory Network (Weston et al., 2015)) repeats computing an attention vector between the query and the context through multiple layers, typically referred to as multi-hop (Sordoni et al., 2016; Dhingra et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 27
                            }
                        ],
                        "text": "Shen et al. (2016) combine Memory Networks with Reinforcement Learning in order to dynamically control the number of hops."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Memory networks. In ICLR"
            },
            "venue": {
                "fragments": [],
                "text": "Memory networks. In ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152847920"
                        ],
                        "name": "Yang Yu",
                        "slug": "Yang-Yu",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Wei Zhang",
                        "slug": "Wei-Zhang",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064634"
                        ],
                        "name": "K. Hasan",
                        "slug": "K.-Hasan",
                        "structuredName": {
                            "firstName": "Kazi",
                            "lastName": "Hasan",
                            "middleNames": [
                                "Saidul"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hasan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2482533"
                        ],
                        "name": "Mo Yu",
                        "slug": "Mo-Yu",
                        "structuredName": {
                            "firstName": "Mo",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mo Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144028698"
                        ],
                        "name": "Bing Xiang",
                        "slug": "Bing-Xiang",
                        "structuredName": {
                            "firstName": "Bing",
                            "lastName": "Xiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bing Xiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145218984"
                        ],
                        "name": "Bowen Zhou",
                        "slug": "Bowen-Zhou",
                        "structuredName": {
                            "firstName": "Bowen",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bowen Zhou"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 40
                            }
                        ],
                        "text": ", 2016)(1), (Wang & Jiang, 2016)(2) and (Yu et al., 2016)(3) on the SQuAD test set."
                    },
                    "intents": []
                }
            ],
            "corpusId": 13094204,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "a8c33413a626bafc67d46029ed28c2a28cc08899",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "End-to-End-Reading-Comprehension-with-Dynamic-Chunk-Yu-Zhang",
            "title": {
                "fragments": [],
                "text": "End-to-End Reading Comprehension with Dynamic Answer Chunk Ranking"
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In NIPS"
            },
            "venue": {
                "fragments": [],
                "text": "Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In NIPS"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "2016) 71.3 72.9 - - Iterative Attention"
            },
            "venue": {
                "fragments": [],
                "text": ") 68.6 69.5 75.0 73.9 DER Network"
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ours) 76.3 76.9 80.3 79.6 MemNN * (Hill et al., 2016) 66.2 69"
            },
            "venue": {
                "fragments": [],
                "text": "2016) 74.5 75.7 - - GA Reader *"
            },
            "year": 2016
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 17,
            "methodology": 18
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 39,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Bidirectional-Attention-Flow-for-Machine-Seo-Kembhavi/3a7b63b50c64f4ec3358477790e84cbd6be2a0b4?sort=total-citations"
}