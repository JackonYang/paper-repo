{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40219976"
                        ],
                        "name": "Zhi Tian",
                        "slug": "Zhi-Tian",
                        "structuredName": {
                            "firstName": "Zhi",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi Tian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780381"
                        ],
                        "name": "Chunhua Shen",
                        "slug": "Chunhua-Shen",
                        "structuredName": {
                            "firstName": "Chunhua",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunhua Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2029503517"
                        ],
                        "name": "Hao Chen",
                        "slug": "Hao-Chen",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118328320"
                        ],
                        "name": "Tong He",
                        "slug": "Tong-He",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong He"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[42], where we consider the locations positive as long as they are in a ground-truth box."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "In our preliminary version [42], this issue is addressed by multiplying a learnable scalar to the convolutional layer\u2019s outputs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 88
                            }
                        ],
                        "text": "performance than the IoU loss in UnitBox [50], which is used in our preliminary version [42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "Compared to our conference version [42], the gap is relatively smaller since we make use of the center sampling"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 91184137,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e2751a898867ce6687e08a5cc7bdb562e999b841",
            "isKey": false,
            "numCitedBy": 1475,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to semantic segmentation. Almost all state-of-the-art object detectors such as RetinaNet, SSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast, our proposed detector FCOS is anchor box free, as well as proposal free. By eliminating the pre-defined set of anchor boxes, FCOS completely avoids the complicated computation related to anchor boxes such as calculating overlapping during training. More importantly, we also avoid all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance. With the only post-processing non-maximum suppression (NMS), FCOS with ResNeXt-64x4d-101 achieves 44.7% in AP with single-model and single-scale testing, surpassing previous one-stage detectors with the advantage of being much simpler. For the first time, we demonstrate a much simpler and flexible detection framework achieving improved detection accuracy. We hope that the proposed FCOS framework can serve as a simple and strong alternative for many other instance-level tasks. Code is available at: https://tinyurl.com/FCOSv1"
            },
            "slug": "FCOS:-Fully-Convolutional-One-Stage-Object-Tian-Shen",
            "title": {
                "fragments": [],
                "text": "FCOS: Fully Convolutional One-Stage Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "For the first time, a much simpler and flexible detection framework achieving improved detection accuracy is demonstrated, and it is hoped that the proposed FCOS framework can serve as a simple and strong alternative for many other instance-level tasks."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150167366"
                        ],
                        "name": "Jiahui Yu",
                        "slug": "Jiahui-Yu",
                        "structuredName": {
                            "firstName": "Jiahui",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiahui Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117772986"
                        ],
                        "name": "Yuning Jiang",
                        "slug": "Yuning-Jiang",
                        "structuredName": {
                            "firstName": "Yuning",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuning Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2969311"
                        ],
                        "name": "Zhangyang Wang",
                        "slug": "Zhangyang-Wang",
                        "structuredName": {
                            "firstName": "Zhangyang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhangyang Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2695115"
                        ],
                        "name": "Zhimin Cao",
                        "slug": "Zhimin-Cao",
                        "structuredName": {
                            "firstName": "Zhimin",
                            "lastName": "Cao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhimin Cao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "By replacing GIoU [33] with the origin IoU loss in [50], the performance drops by 0."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 161
                            }
                        ],
                        "text": "Besides, more significantly, these methods are mainly used in special domain objection detection such as scene text detection [16], [53] or face detection [18], [50], since it is believed that these methods do not work well when applied to generic object detection with highly overlapped bounding boxes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "w/ IoU: using IoU loss in [50] instead of GIoU."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 83
                            }
                        ],
                        "text": "As shown in experiments, the GIoU loss has better performance than the IoU loss in UnitBox [50], which is used in our preliminary version [42]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "performance than the IoU loss in UnitBox [50], which is used in our preliminary version [42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "Another family of anchor-free detectors such as [50] are based on DenseBox [18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15883006,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "22264e60f1dfbc7d0b52549d1de560993dd96e46",
            "isKey": true,
            "numCitedBy": 484,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "In present object detection systems, the deep convolutional neural networks (CNNs) are utilized to predict bounding boxes of object candidates, and have gained performance advantages over the traditional region proposal methods. However, existing deep CNN methods assume the object bounds to be four independent variables, which could be regressed by the l2 loss separately. Such an oversimplified assumption is contrary to the well-received observation, that those variables are correlated, resulting to less accurate localization. To address the issue, we firstly introduce a novel Intersection over Union (IoU) loss function for bounding box prediction, which regresses the four bounds of a predicted box as a whole unit. By taking the advantages of IoU loss and deep fully convolutional networks, the UnitBox is introduced, which performs accurate and efficient localization, shows robust to objects of varied shapes and scales, and converges fast. We apply UnitBox on face detection task and achieve the best performance among all published methods on the FDDB benchmark."
            },
            "slug": "UnitBox:-An-Advanced-Object-Detection-Network-Yu-Jiang",
            "title": {
                "fragments": [],
                "text": "UnitBox: An Advanced Object Detection Network"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A novel Intersection over Union (IoU) loss function for bounding box prediction, which regresses the four bounds of a predicted box as a whole unit, and introduces the UnitBox, which performs accurate and efficient localization, shows robust to objects of varied shapes and scales, and converges fast."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Multimedia"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2029503517"
                        ],
                        "name": "Hao Chen",
                        "slug": "Hao-Chen",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121253154"
                        ],
                        "name": "Kunyang Sun",
                        "slug": "Kunyang-Sun",
                        "structuredName": {
                            "firstName": "Kunyang",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kunyang Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069520672"
                        ],
                        "name": "Zhi Tian",
                        "slug": "Zhi-Tian",
                        "structuredName": {
                            "firstName": "Zhi",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi Tian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12459603"
                        ],
                        "name": "Chunhua Shen",
                        "slug": "Chunhua-Shen",
                        "structuredName": {
                            "firstName": "Chunhua",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunhua Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48355817"
                        ],
                        "name": "Yongming Huang",
                        "slug": "Yongming-Huang",
                        "structuredName": {
                            "firstName": "Yongming",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yongming Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2376903"
                        ],
                        "name": "Youliang Yan",
                        "slug": "Youliang-Yan",
                        "structuredName": {
                            "firstName": "Youliang",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Youliang Yan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 193
                            }
                        ],
                        "text": "More importantly, due to its simple design, FCOS can be easily extended to solve other instance-level recognition tasks with minimal modification, as already evidenced by instance segmentation [2], [20], [47], [51], keypoint detection [39], text spotting [26], and tracking [13], [44]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 29
                            }
                        ],
                        "text": "Thus, following BlendMask-RT [2], we remove P6 and P7, further reducing the inference time."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 209531988,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "02acc835a3d5e115d86cb6885a28d9bab78c86d9",
            "isKey": false,
            "numCitedBy": 158,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Instance segmentation is one of the fundamental vision tasks. Recently, fully convolutional instance segmentation methods have drawn much attention as they are often simpler and more efficient than two-stage approaches like Mask R-CNN. To date, almost all such approaches fall behind the two-stage Mask R-CNN method in mask precision when models have similar computation complexity, leaving great room for improvement. In this work, we achieve improved mask prediction by effectively combining instance-level information with semantic information with lower-level fine-granularity. Our main contribution is a blender module which draws inspiration from both top-down and bottom-up instance segmentation approaches. The proposed BlendMask can effectively predict dense per-pixel position-sensitive instance features with very few channels, and learn attention maps for each instance with merely one convolution layer, thus being fast in inference. BlendMask can be easily incorporate with the state-of-the-art one-stage detection frameworks and outperforms Mask R-CNN under the same training schedule while being faster. A light-weight version of BlendMask achieves 36.0 mAP at 27 FPS evaluated on a single 1080Ti. Because of its simplicity and efficacy, we hope that our BlendMask could serve as a simple yet strong baseline for a wide range of instance-wise prediction tasks."
            },
            "slug": "BlendMask:-Top-Down-Meets-Bottom-Up-for-Instance-Chen-Sun",
            "title": {
                "fragments": [],
                "text": "BlendMask: Top-Down Meets Bottom-Up for Instance Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The proposed BlendMask can effectively predict dense per-pixel position-sensitive instance features with very few channels, and learn attention maps for each instance with merely one convolution layer, thus being fast in inference."
            },
            "venue": {
                "fragments": [],
                "text": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2032184078"
                        ],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 48
                            }
                        ],
                        "text": "All current mainstream detectors such as Faster R-CNN [32], SSD [25] and YOLOv2, v3 [31] rely on a set of pre-defined anchor boxes and it has\nAuthors are with The University of Adelaide, Australia."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 113
                            }
                        ],
                        "text": "It is worth noting that FCOS has 9\u00d7 fewer network output variables than the popular anchor-based detectors [22], [32] with 9 anchor boxes per location, which is of great importance when FCOS is applied to keypoint detection [39] or instance segmentation [41]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 135
                            }
                        ],
                        "text": "With deep learning, detection has been largely shifted to the use of fully convolutional networks (FCNs) since the invention of Faster R-CNN [32]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "The design of anchor boxes are popularized by Faster R-CNN in its RPNs [32], SSD [25] and YOLOv2 [30], and has become the"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "Following the common practice [21], [22], [32], we use the COCO train2017 split (115K images) for training and val2017 split (5K images) as validation for our ablation study."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 123
                            }
                        ],
                        "text": "Despite their great success, it is important to note that anchor-based detectors suffer some drawbacks:\n\u2022 As shown in Faster R-CNN and RetinaNet [22], detection performance is sensitive to the sizes, aspect ratios and number of anchor boxes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 137
                            }
                        ],
                        "text": "\u2022 With considerably reduced design complexity, our proposed detector outperforms previous strong baseline detectors such as Faster R-CNN [32], RetinaNet [22], YOLOv3 [31] and SSD [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 103
                            }
                        ],
                        "text": "Quantitative results are shown in\n9 Method Backbone AP AP50 AP75 APS APM APL Two-stage methods: Faster R-CNN+++ [14] ResNet-101 34.9 55.7 37.4 15.6 38.7 50.9 Faster R-CNN w/ FPN [21] ResNet-101-FPN 36.2 59.1 39.0 18.2 39.0 48.2 Faster R-CNN by G-RMI [17] Inception-ResNet-v2 [37] 34.7 55.5 36.7 13.5 38.1 52.0 Faster R-CNN w/ TDM [36] Inception-ResNet-v2-TDM 36.8 57.7 39.2 16.2 39.8 52.1 One-stage methods: YOLOv2 [30] DarkNet-19 [30] 21.6 44.0 19.2 5.0 22.4 35.5 SSD513 [25] ResNet-101-SSD 31.2 50.4 33.3 10.2 34.5 49.8 YOLOv3 608\u00d7 608 [31] Darknet-53 33.0 57.9 34.4 18.3 35.4 41.9 DSSD513 [10] ResNet-101-DSSD 33.2 53.3 35.2 13.0 35.4 51.1 RetinaNet [22] ResNet-101-FPN 39.1 59.1 42.3 21.8 42.7 50.2 CornerNet [19] Hourglass-104 40.5 56.5 43.1 19.4 42.7 53.9 FSAF [54] ResNeXt-64x4d-101-FPN 42.9 63.8 46.3 26.6 46.2 52.7 CenterNet511 [9] Hourglass-104 44.9 62.4 48.1 25.6 47.4 57.4 FCOS ResNet-101-FPN 43.2 62.4 46.8 26.1 46.2 52.8 FCOS ResNeXt-32x8d-101-FPN 44.1 63.7 47.9 27.4 46.8 53.7 FCOS ResNeXt-64x4d-101-FPN 44.8 64.4 48.5 27.7 47.4 55.0 FCOS w/ deform. conv. v2 [55] ResNeXt-32x8d-101-FPN 46.6 65.9 50.8 28.6 49.1 58.6 FCOS ResNet-101-BiFPN [38] 45.0 63.6 48.7 27.0 47.9 55.9 FCOS ResNeXt-32x8d-101-BiFPN 46.2 65.2 50.0 28.7 49.1 56.5 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-BiFPN 47.9 66.9 51.9 30.2 50.3 59.9 w/ test-time augmentation: FCOS ResNet-101-FPN 45.9 64.5 50.4 29.4 48.3 56.1 FCOS ResNeXt-32x8d-101-FPN 47.0 66.0 51.6 30.7 49.4 57.1 FCOS ResNeXt-64x4d-101-FPN 47.5 66.4 51.9 31.4 49.7 58.2 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-FPN 49.1 68.0 53.9 31.7 51.6 61.0 FCOS ResNet-101-BiFPN 47.9 65.9 52.5 31.0 50.7 59.7 FCOS ResNeXt-32x8d-101-BiFPN 49.0 67.4 53.6 32.0 51.7 60.5 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-BiFPN 50.4 68.9 55.0 33.2 53.0 62.7\nTABLE 8 FCOS vs. other state-of-the-art two-stage or one-stage detectors (single-model results)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 115
                            }
                        ],
                        "text": "Anchor-based detectors inherit the ideas from traditional sliding-window and proposal based detectors such as Fast R-CNN [11]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 53
                            }
                        ],
                        "text": "The design of anchor boxes are popularized by Faster R-CNN in its RPNs [32], SSD [25] and YOLOv2 [30], and has become the convention in a modern detector."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "All current mainstream detectors such as Faster R-CNN [32], SSD [25] and YOLOv2, v3 [31] rely on a set of pre-defined anchor boxes and it has"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 83
                            }
                        ],
                        "text": "Moreover, FCOS also surpasses the classical two-stage anchor-based detector Faster R-CNN by a large margin (43.2% vs. 36.2%)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10328909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "424561d8585ff8ebce7d5d07de8dbf7aae5e7270",
            "isKey": true,
            "numCitedBy": 32561,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available"
            },
            "slug": "Faster-R-CNN:-Towards-Real-Time-Object-Detection-Ren-He",
            "title": {
                "fragments": [],
                "text": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work introduces a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals and further merge RPN and Fast R-CNN into a single network by sharing their convolutionAL features."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3445691"
                        ],
                        "name": "Youngwan Lee",
                        "slug": "Youngwan-Lee",
                        "structuredName": {
                            "firstName": "Youngwan",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Youngwan Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2800227"
                        ],
                        "name": "Jongyoul Park",
                        "slug": "Jongyoul-Park",
                        "structuredName": {
                            "firstName": "Jongyoul",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jongyoul Park"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 208075876,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f5669fca3ce3699476b6562019d950059c00f1e",
            "isKey": false,
            "numCitedBy": 183,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a simple yet efficient anchor-free instance segmentation, called CenterMask, that adds a novel spatial attention-guided mask (SAG-Mask) branch to anchor-free one stage object detector (FCOS) in the same vein with Mask R-CNN. Plugged into the FCOS object detector, the SAG-Mask branch predicts a segmentation mask on each box with the spatial attention map that helps to focus on informative pixels and suppress noise. We also present an improved backbone networks, VoVNetV2, with two effective strategies: (1) residual connection for alleviating the optimization problem of larger VoVNet \\cite{lee2019energy} and (2) effective Squeeze-Excitation (eSE) dealing with the channel information loss problem of original SE. With SAG-Mask and VoVNetV2, we deign CenterMask and CenterMask-Lite that are targeted to large and small models, respectively. Using the same ResNet-101-FPN backbone, CenterMask achieves 38.3%, surpassing all previous state-of-the-art methods while at a much faster speed. CenterMask-Lite also outperforms the state-of-the-art by large margins at over 35fps on Titan Xp. We hope that CenterMask and VoVNetV2 can serve as a solid baseline of real-time instance segmentation and backbone network for various vision tasks, respectively. The Code is available at https://github.com/youngwanLEE/CenterMask."
            },
            "slug": "CenterMask:-Real-Time-Anchor-Free-Instance-Lee-Park",
            "title": {
                "fragments": [],
                "text": "CenterMask: Real-Time Anchor-Free Instance Segmentation"
            },
            "venue": {
                "fragments": [],
                "text": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33493200"
                        ],
                        "name": "Tsung-Yi Lin",
                        "slug": "Tsung-Yi-Lin",
                        "structuredName": {
                            "firstName": "Tsung-Yi",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tsung-Yi Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47316088"
                        ],
                        "name": "Priya Goyal",
                        "slug": "Priya-Goyal",
                        "structuredName": {
                            "firstName": "Priya",
                            "lastName": "Goyal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Priya Goyal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 107
                            }
                        ],
                        "text": "It is worth noting that FCOS has 9\u00d7 fewer network output variables than the popular anchor-based detectors [22], [32] with 9 anchor boxes per location, which is of great importance when FCOS is applied to keypoint detection [39] or instance segmentation [41]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Following [22], we choose the location with px,y > 0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 116
                            }
                        ],
                        "text": "Despite their great success, it is important to note that anchor-based detectors suffer some drawbacks:\n\u2022 As shown in Faster R-CNN and RetinaNet [22], detection performance is sensitive to the sizes, aspect ratios and number of anchor boxes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 108
                            }
                        ],
                        "text": "Speed/accuracy trade-off between FCOS and several recent methods: CenterNet [52], YOLOv3 [31] and RetinaNet [22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 153
                            }
                        ],
                        "text": "\u2022 With considerably reduced design complexity, our proposed detector outperforms previous strong baseline detectors such as Faster R-CNN [32], RetinaNet [22], YOLOv3 [31] and SSD [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "Similar to [22], we add two branches, respectively with four convolutional layers (exclude the final prediction layers) after the feature maps produced by FPNs for classification and regression tasks, respectively."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Following [22], instead of training a multi-class classifier, we train C binary classifiers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "\u2022 As shown in Faster R-CNN and RetinaNet [22], detection performance is sensitive to the sizes, aspect ratios and number of anchor boxes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "where Lcls is focal loss as in [22] and Lreg is the GIoU loss [33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "Finally, following [21], [22], we share the heads between different feature levels, not only making the detector parameter-efficient but also improving the detection performance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 174
                            }
                        ],
                        "text": "Here, we empirically show that even with a large stride, FCOS is still able to produce a good BPR, and it can even better than the BPR of the anchor-based detector RetinaNet [22] in the official implementation Detectron [12] (refer to Table 1)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 47252984,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "72564a69bf339ff1d16a639c86a764db2321caab",
            "isKey": true,
            "numCitedBy": 8230,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron."
            },
            "slug": "Focal-Loss-for-Dense-Object-Detection-Lin-Goyal",
            "title": {
                "fragments": [],
                "text": "Focal Loss for Dense Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes to address the extreme foreground-background class imbalance encountered during training of dense detectors by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples, and develops a novel Focal Loss, which focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36686543"
                        ],
                        "name": "Kaiwen Duan",
                        "slug": "Kaiwen-Duan",
                        "structuredName": {
                            "firstName": "Kaiwen",
                            "lastName": "Duan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiwen Duan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47651566"
                        ],
                        "name": "S. Bai",
                        "slug": "S.-Bai",
                        "structuredName": {
                            "firstName": "Song",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3041937"
                        ],
                        "name": "Lingxi Xie",
                        "slug": "Lingxi-Xie",
                        "structuredName": {
                            "firstName": "Lingxi",
                            "lastName": "Xie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lingxi Xie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144097734"
                        ],
                        "name": "H. Qi",
                        "slug": "H.-Qi",
                        "structuredName": {
                            "firstName": "Honggang",
                            "lastName": "Qi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Qi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689702"
                        ],
                        "name": "Qingming Huang",
                        "slug": "Qingming-Huang",
                        "structuredName": {
                            "firstName": "Qingming",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qingming Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144876831"
                        ],
                        "name": "Q. Tian",
                        "slug": "Q.-Tian",
                        "structuredName": {
                            "firstName": "Qi",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Q. Tian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 119296375,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "52e7190540745960333ab483623099edf1641257",
            "isKey": false,
            "numCitedBy": 807,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "In object detection, keypoint-based approaches often experience the drawback of a large number of incorrect object bounding boxes, arguably due to the lack of an additional assessment inside cropped regions. This paper presents an efficient solution that explores the visual patterns within individual cropped regions with minimal costs. We build our framework upon a representative one-stage keypoint-based detector named CornerNet. Our approach, named CenterNet, detects each object as a triplet, rather than a pair, of keypoints, which improves both precision and recall. Accordingly, we design two customized modules, cascade corner pooling, and center pooling, that enrich information collected by both the top-left and bottom-right corners and provide more recognizable information from the central regions. On the MS-COCO dataset, CenterNet achieves an AP of 47.0 %, outperforming all existing one-stage detectors by at least 4.9%. Furthermore, with a faster inference speed than the top-ranked two-stage detectors, CenterNet demonstrates a comparable performance to these detectors. Code is available at https://github.com/Duankaiwen/CenterNet."
            },
            "slug": "CenterNet:-Keypoint-Triplets-for-Object-Detection-Duan-Bai",
            "title": {
                "fragments": [],
                "text": "CenterNet: Keypoint Triplets for Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper presents an efficient solution that explores the visual patterns within individual cropped regions with minimal costs, and builds the framework upon a representative one-stage keypoint-based detector named CornerNet, which improves both precision and recall."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33493200"
                        ],
                        "name": "Tsung-Yi Lin",
                        "slug": "Tsung-Yi-Lin",
                        "structuredName": {
                            "firstName": "Tsung-Yi",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tsung-Yi Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790580"
                        ],
                        "name": "Bharath Hariharan",
                        "slug": "Bharath-Hariharan",
                        "structuredName": {
                            "firstName": "Bharath",
                            "lastName": "Hariharan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bharath Hariharan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 30
                            }
                        ],
                        "text": "Following the common practice [21], [22], [32], we use the COCO train2017 split (115K images) for training and val2017 split (5K images) as validation for our ablation study."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 114
                            }
                        ],
                        "text": "2, P3, P4 and P5 are produced by the backbone CNNs\u2019 feature maps C3, C4 and C5 with the topdown connections as in [21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 116
                            }
                        ],
                        "text": "Here we show that how two possible issues of the proposed FCOS can be resolved with multi-level prediction with FPN [21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "Specifically, following FPN [21], we detect different size objects on different feature map levels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "Moreover, with multi-level FPN prediction [21], the BPR can be improved further to match the best BPR the anchor-based RetinaNet can achieve."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "First, we experiment with the assigning strategy when FPN [21] assigns the object proposals (i."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "Finally, following [21], [22], we share the heads between different feature levels, not only making the detector parameter-efficient but also improving the detection performance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "9 Faster R-CNN w/ FPN [21] ResNet-101-FPN 36."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 64
                            }
                        ],
                        "text": ", more than 180K anchor boxes in feature pyramid networks (FPN) [21] for an image with its shorter side being 800)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10716717,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9b4e05faa194e5022edd9eb9dd07e3d675c2b36",
            "isKey": true,
            "numCitedBy": 9352,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But pyramid representations have been avoided in recent object detectors that are based on deep convolutional networks, partially because they are slow to compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available."
            },
            "slug": "Feature-Pyramid-Networks-for-Object-Detection-Lin-Doll\u00e1r",
            "title": {
                "fragments": [],
                "text": "Feature Pyramid Networks for Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper exploits the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost and achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40497777"
                        ],
                        "name": "Joseph Redmon",
                        "slug": "Joseph-Redmon",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Redmon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph Redmon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2038685"
                        ],
                        "name": "S. Divvala",
                        "slug": "S.-Divvala",
                        "structuredName": {
                            "firstName": "Santosh",
                            "lastName": "Divvala",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Divvala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143787583"
                        ],
                        "name": "Ali Farhadi",
                        "slug": "Ali-Farhadi",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Farhadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ali Farhadi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206594738,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8e79ac0ea341056ef20f2616628b3e964764cfd",
            "isKey": false,
            "numCitedBy": 16502,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork."
            },
            "slug": "You-Only-Look-Once:-Unified,-Real-Time-Object-Redmon-Divvala",
            "title": {
                "fragments": [],
                "text": "You Only Look Once: Unified, Real-Time Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background, and outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47033130"
                        ],
                        "name": "Lichao Huang",
                        "slug": "Lichao-Huang",
                        "structuredName": {
                            "firstName": "Lichao",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lichao Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2143686789"
                        ],
                        "name": "Yi Yang",
                        "slug": "Yi-Yang",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30639994"
                        ],
                        "name": "Yafeng Deng",
                        "slug": "Yafeng-Deng",
                        "structuredName": {
                            "firstName": "Yafeng",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yafeng Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2119041121"
                        ],
                        "name": "Yinan Yu",
                        "slug": "Yinan-Yu",
                        "structuredName": {
                            "firstName": "Yinan",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yinan Yu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": "However, to handle the bounding boxes with different sizes, DenseBox [18] crops and resizes training images to a fixed scale."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 155
                            }
                        ],
                        "text": "Besides, more significantly, these methods are mainly used in special domain objection detection such as scene text detection [16], [53] or face detection [18], [50], since it is believed that these methods do not work well when applied to generic object detection with highly overlapped bounding boxes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 115
                            }
                        ],
                        "text": "In the literature, some works attempted to leverage the FCNs-based framework for object detection such as DenseBox [18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 5
                            }
                        ],
                        "text": "Thus DenseBox has to perform detection on image pyramids, which is against FCN\u2019s philosophy of computing all convolutions once."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "Another family of anchor-free detectors such as [50] are based on DenseBox [18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14710847,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aeb86c216e32b39c8716fe5a832aa7b135164a87",
            "isKey": true,
            "numCitedBy": 308,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "How can a single fully convolutional neural network (FCN) perform on object detection? We introduce DenseBox, a unified end-to-end FCN framework that directly predicts bounding boxes and object class confidences through all locations and scales of an image. Our contribution is two-fold. First, we show that a single FCN, if designed and optimized carefully, can detect multiple different objects extremely accurately and efficiently. Second, we show that when incorporating with landmark localization during multi-task learning, DenseBox further improves object detection accuray. We present experimental results on public benchmark datasets including MALF face detection and KITTI car detection, that indicate our DenseBox is the state-of-the-art system for detecting challenging objects such as faces and cars."
            },
            "slug": "DenseBox:-Unifying-Landmark-Localization-with-End-Huang-Yang",
            "title": {
                "fragments": [],
                "text": "DenseBox: Unifying Landmark Localization with End to End Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "DenseBox is introduced, a unified end-to-end FCN framework that directly predicts bounding boxes and object class confidences through all locations and scales of an image and shows that when incorporating with landmark localization during multi-task learning, DenseBox further improves object detection accuray."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36686543"
                        ],
                        "name": "Kaiwen Duan",
                        "slug": "Kaiwen-Duan",
                        "structuredName": {
                            "firstName": "Kaiwen",
                            "lastName": "Duan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiwen Duan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3041937"
                        ],
                        "name": "Lingxi Xie",
                        "slug": "Lingxi-Xie",
                        "structuredName": {
                            "firstName": "Lingxi",
                            "lastName": "Xie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lingxi Xie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144097734"
                        ],
                        "name": "H. Qi",
                        "slug": "H.-Qi",
                        "structuredName": {
                            "firstName": "Honggang",
                            "lastName": "Qi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Qi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47651566"
                        ],
                        "name": "S. Bai",
                        "slug": "S.-Bai",
                        "structuredName": {
                            "firstName": "Song",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689702"
                        ],
                        "name": "Qingming Huang",
                        "slug": "Qingming-Huang",
                        "structuredName": {
                            "firstName": "Qingming",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qingming Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400120070"
                        ],
                        "name": "Qi Tian",
                        "slug": "Qi-Tian",
                        "structuredName": {
                            "firstName": "Qi",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qi Tian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "to get the target boxes. In contrast, FCOS is a concise and very straightforward method for box detection. After the submission of this work, some new anchorfree detectors appeared. For instance, CPN [36] replaces the region proposal networks (RPNs) in two-stage detectors with bottom-up anchor-free networks, achieving improved results. HoughNet [37] employs a voting mechanism to improve the performanc"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 220831338,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9641a861cea30c1b4b92f1fb01748811597db022",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of object detection is to determine the class and location of objects in an image. This paper proposes a novel anchor-free, two-stage framework which first extracts a number of object proposals by finding potential corner keypoint combinations and then assigns a class label to each proposal by a standalone classification stage. We demonstrate that these two stages are effective solutions for improving recall and precision, respectively, and they can be integrated into an end-to-end network. Our approach, dubbed Corner Proposal Network (CPN), enjoys the ability to detect objects of various scales and also avoids being confused by a large number of false-positive proposals. On the MS-COCO dataset, CPN achieves an AP of 49.2% which is competitive among state-of-the-art object detection methods. CPN also fits the scenario of computational efficiency, which achieves an AP of 41.6%/39.7% at 26.2/43.3 FPS, surpassing most competitors with the same inference speed. Code is available at this https URL"
            },
            "slug": "Corner-Proposal-Network-for-Anchor-free,-Two-stage-Duan-Xie",
            "title": {
                "fragments": [],
                "text": "Corner Proposal Network for Anchor-free, Two-stage Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A novel anchor-free, two-stage framework which first extracts a number of object proposals by finding potential corner keypoint combinations and then assigns a class label to each proposal by a standalone classification stage is proposed, which is competitive among state-of-the-art object detection methods."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46641573"
                        ],
                        "name": "W. Liu",
                        "slug": "W.-Liu",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1838674"
                        ],
                        "name": "Dragomir Anguelov",
                        "slug": "Dragomir-Anguelov",
                        "structuredName": {
                            "firstName": "Dragomir",
                            "lastName": "Anguelov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dragomir Anguelov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761978"
                        ],
                        "name": "D. Erhan",
                        "slug": "D.-Erhan",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Erhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Erhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574060"
                        ],
                        "name": "Christian Szegedy",
                        "slug": "Christian-Szegedy",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Szegedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Szegedy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144828948"
                        ],
                        "name": "Scott E. Reed",
                        "slug": "Scott-E.-Reed",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Reed",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott E. Reed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2084646762"
                        ],
                        "name": "Cheng-Yang Fu",
                        "slug": "Cheng-Yang-Fu",
                        "structuredName": {
                            "firstName": "Cheng-Yang",
                            "lastName": "Fu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheng-Yang Fu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "detectors such as SSD [25] and DSSD [10], we also achieve much better performance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "The design of anchor boxes are popularized by Faster R-CNN in its RPNs [32], SSD [25] and YOLOv2 [30], and has become the"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 179
                            }
                        ],
                        "text": "\u2022 With considerably reduced design complexity, our proposed detector outperforms previous strong baseline detectors such as Faster R-CNN [32], RetinaNet [22], YOLOv3 [31] and SSD [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "For these experiments, following previous works [22], [25], we make use of multi-scale training."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 64
                            }
                        ],
                        "text": "All current mainstream detectors such as Faster R-CNN [32], SSD [25] and YOLOv2, v3 [31] rely on a set of pre-defined anchor boxes and it has"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2141740,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d7a9197433acbfb24ef0e9d0f33ed1699e4a5b0",
            "isKey": true,
            "numCitedBy": 15423,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For $300\\times 300$ input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for $500\\times 500$ input, SSD achieves 75.1% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at this https URL ."
            },
            "slug": "SSD:-Single-Shot-MultiBox-Detector-Liu-Anguelov",
            "title": {
                "fragments": [],
                "text": "SSD: Single Shot MultiBox Detector"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "The approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location, which makes SSD easy to train and straightforward to integrate into systems that require a detection component."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807746211"
                        ],
                        "name": "Rufeng Zhang",
                        "slug": "Rufeng-Zhang",
                        "structuredName": {
                            "firstName": "Rufeng",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rufeng Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069520672"
                        ],
                        "name": "Zhi Tian",
                        "slug": "Zhi-Tian",
                        "structuredName": {
                            "firstName": "Zhi",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi Tian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12459603"
                        ],
                        "name": "Chunhua Shen",
                        "slug": "Chunhua-Shen",
                        "structuredName": {
                            "firstName": "Chunhua",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunhua Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34647494"
                        ],
                        "name": "Mingyu You",
                        "slug": "Mingyu-You",
                        "structuredName": {
                            "firstName": "Mingyu",
                            "lastName": "You",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mingyu You"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2376903"
                        ],
                        "name": "Youliang Yan",
                        "slug": "Youliang-Yan",
                        "structuredName": {
                            "firstName": "Youliang",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Youliang Yan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 214667915,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ec1e7746b74aeb915d7edd201f52e7bf349b305",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "To date, instance segmentation is dominated by two-stage methods, as pioneered by Mask R-CNN. In contrast, one-stage alternatives cannot compete with Mask R-CNN in mask AP, mainly due to the difficulty of compactly representing masks, making the design of one-stage methods very challenging. In this work, we propose a simple single-shot instance segmentation framework, termed mask encoding based instance segmentation (MEInst). Instead of predicting the two-dimensional mask directly, MEInst distills it into a compact and fixed-dimensional representation vector, which allows the instance segmentation task to be incorporated into one-stage bounding-box detectors and results in a simple yet efficient instance segmentation framework. The proposed one-stage MEInst achieves 36.4% in mask AP with single-model (ResNeXt-101-FPN backbone) and single-scale testing on the MS-COCO benchmark. We show that the much simpler and flexible one-stage instance segmentation method, can also achieve competitive performance. This framework can be easily adapted for other instance-level recognition tasks. Code is available at: git.io/AdelaiDet"
            },
            "slug": "Mask-Encoding-for-Single-Shot-Instance-Segmentation-Zhang-Tian",
            "title": {
                "fragments": [],
                "text": "Mask Encoding for Single Shot Instance Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Instead of predicting the two-dimensional mask directly, MEInst distills it into a compact and fixed-dimensional representation vector, which allows the instance segmentation task to be incorporated into one-stage bounding-box detectors and results in a simple yet efficient instance segmentations framework."
            },
            "venue": {
                "fragments": [],
                "text": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120805419"
                        ],
                        "name": "Mingxing Tan",
                        "slug": "Mingxing-Tan",
                        "structuredName": {
                            "firstName": "Mingxing",
                            "lastName": "Tan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mingxing Tan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34320634"
                        ],
                        "name": "Ruoming Pang",
                        "slug": "Ruoming-Pang",
                        "structuredName": {
                            "firstName": "Ruoming",
                            "lastName": "Pang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ruoming Pang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 51
                            }
                        ],
                        "text": "For the real-time models, we also replace FPN with BiFPN as in Section 4.3, resulting 1.8% AP improvement (from 40.3% to 42.1"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 13
                            }
                        ],
                        "text": "As a result, BiFPN generally improves all FCOS models by \u223c 2% AP and pushes the performance of the best model to 47.9%."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "We make use of BiFPN in D3 model in [38]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1151,
                                "start": 1146
                            }
                        ],
                        "text": "Quantitative results are shown in\n9 Method Backbone AP AP50 AP75 APS APM APL Two-stage methods: Faster R-CNN+++ [14] ResNet-101 34.9 55.7 37.4 15.6 38.7 50.9 Faster R-CNN w/ FPN [21] ResNet-101-FPN 36.2 59.1 39.0 18.2 39.0 48.2 Faster R-CNN by G-RMI [17] Inception-ResNet-v2 [37] 34.7 55.5 36.7 13.5 38.1 52.0 Faster R-CNN w/ TDM [36] Inception-ResNet-v2-TDM 36.8 57.7 39.2 16.2 39.8 52.1 One-stage methods: YOLOv2 [30] DarkNet-19 [30] 21.6 44.0 19.2 5.0 22.4 35.5 SSD513 [25] ResNet-101-SSD 31.2 50.4 33.3 10.2 34.5 49.8 YOLOv3 608\u00d7 608 [31] Darknet-53 33.0 57.9 34.4 18.3 35.4 41.9 DSSD513 [10] ResNet-101-DSSD 33.2 53.3 35.2 13.0 35.4 51.1 RetinaNet [22] ResNet-101-FPN 39.1 59.1 42.3 21.8 42.7 50.2 CornerNet [19] Hourglass-104 40.5 56.5 43.1 19.4 42.7 53.9 FSAF [54] ResNeXt-64x4d-101-FPN 42.9 63.8 46.3 26.6 46.2 52.7 CenterNet511 [9] Hourglass-104 44.9 62.4 48.1 25.6 47.4 57.4 FCOS ResNet-101-FPN 43.2 62.4 46.8 26.1 46.2 52.8 FCOS ResNeXt-32x8d-101-FPN 44.1 63.7 47.9 27.4 46.8 53.7 FCOS ResNeXt-64x4d-101-FPN 44.8 64.4 48.5 27.7 47.4 55.0 FCOS w/ deform. conv. v2 [55] ResNeXt-32x8d-101-FPN 46.6 65.9 50.8 28.6 49.1 58.6 FCOS ResNet-101-BiFPN [38] 45.0 63.6 48.7 27.0 47.9 55.9 FCOS ResNeXt-32x8d-101-BiFPN 46.2 65.2 50.0 28.7 49.1 56.5 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-BiFPN 47.9 66.9 51.9 30.2 50.3 59.9 w/ test-time augmentation: FCOS ResNet-101-FPN 45.9 64.5 50.4 29.4 48.3 56.1 FCOS ResNeXt-32x8d-101-FPN 47.0 66.0 51.6 30.7 49.4 57.1 FCOS ResNeXt-64x4d-101-FPN 47.5 66.4 51.9 31.4 49.7 58.2 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-FPN 49.1 68.0 53.9 31.7 51.6 61.0 FCOS ResNet-101-BiFPN 47.9 65.9 52.5 31.0 50.7 59.7 FCOS ResNeXt-32x8d-101-BiFPN 49.0 67.4 53.6 32.0 51.7 60.5 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-BiFPN 50.4 68.9 55.0 33.2 53.0 62.7\nTABLE 8 FCOS vs. other state-of-the-art two-stage or one-stage detectors (single-model results)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 63
                            }
                        ],
                        "text": "In addition, we also attempt to replace FPN in FCOS with BiFPN [38]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 30
                            }
                        ],
                        "text": "Note that unlike the original BiFPN, we do not employ depthwise separable convolutions in it."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 35
                            }
                        ],
                        "text": "To be specific, the single cell of BiFPN is repeated 6 times and the number of its output channels is set to 160."
                    },
                    "intents": []
                }
            ],
            "corpusId": 208175544,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41c67d04be2d1632c0d3b0880c21c9fe797cdab8",
            "isKey": true,
            "numCitedBy": 1172,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Model efficiency has become increasingly important in computer vision. In this paper, we systematically study neural network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multi-scale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and EfficientNet backbones, we have developed a new family of object detectors, called EfficientDet, which consistently achieve much better efficiency than prior art across a wide spectrum of resource constraints. In particular, with single-model and single-scale, our EfficientDetD7 achieves state-of-the-art 52.2 AP on COCO test-dev with 52M parameters and 325B FLOPs, being 4x \u2013 9x smaller and using 13x \u2013 42x fewer FLOPs than previous detector."
            },
            "slug": "EfficientDet:-Scalable-and-Efficient-Object-Tan-Pang",
            "title": {
                "fragments": [],
                "text": "EfficientDet: Scalable and Efficient Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This paper systematically study neural network architecture design choices for object detection and proposes a weighted bi-directional feature pyramid network (BiFPN) and a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time."
            },
            "venue": {
                "fragments": [],
                "text": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41020000"
                        ],
                        "name": "Enze Xie",
                        "slug": "Enze-Xie",
                        "structuredName": {
                            "firstName": "Enze",
                            "lastName": "Xie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Enze Xie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075416446"
                        ],
                        "name": "Pei Sun",
                        "slug": "Pei-Sun",
                        "structuredName": {
                            "firstName": "Pei",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pei Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118943253"
                        ],
                        "name": "Xiaoge Song",
                        "slug": "Xiaoge-Song",
                        "structuredName": {
                            "firstName": "Xiaoge",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoge Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71074736"
                        ],
                        "name": "Wenhai Wang",
                        "slug": "Wenhai-Wang",
                        "structuredName": {
                            "firstName": "Wenhai",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenhai Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1390611971"
                        ],
                        "name": "Xuebo Liu",
                        "slug": "Xuebo-Liu",
                        "structuredName": {
                            "firstName": "Xuebo",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuebo Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152335674"
                        ],
                        "name": "Ding Liang",
                        "slug": "Ding-Liang",
                        "structuredName": {
                            "firstName": "Ding",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ding Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12459603"
                        ],
                        "name": "Chunhua Shen",
                        "slug": "Chunhua-Shen",
                        "structuredName": {
                            "firstName": "Chunhua",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunhua Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47571885"
                        ],
                        "name": "Ping Luo",
                        "slug": "Ping-Luo",
                        "structuredName": {
                            "firstName": "Ping",
                            "lastName": "Luo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ping Luo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 204
                            }
                        ],
                        "text": "More importantly, due to its simple design, FCOS can be easily extended to solve other instance-level recognition tasks with minimal modification, as already evidenced by instance segmentation [2], [20], [47], [51], keypoint detection [39], text spotting [26], and tracking [13], [44]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 203593662,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d158f43fc1dfe148f63ad2c7162b51ee7e743cc",
            "isKey": false,
            "numCitedBy": 215,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we introduce an anchor-box free and single shot instance segmentation method, which is conceptually simple, fully convolutional and can be used by easily embedding it into most off-the-shelf detection methods. Our method, termed PolarMask, formulates the instance segmentation problem as predicting contour of instance through instance center classification and dense distance regression in a polar coordinate. Moreover, we propose two effective approaches to deal with sampling high-quality center examples and optimization for dense distance regression, respectively, which can significantly improve the performance and simplify the training process. Without any bells and whistles, PolarMask achieves 32.9% in mask mAP with single-model and single-scale training/testing on the challenging COCO dataset. For the first time, we show that the complexity of instance segmentation, in terms of both design and computation complexity, can be the same as bounding box object detection and this much simpler and flexible instance segmentation framework can achieve competitive accuracy. We hope that the proposed PolarMask framework can serve as a fundamental and strong baseline for single shot instance segmentation task."
            },
            "slug": "PolarMask:-Single-Shot-Instance-Segmentation-With-Xie-Sun",
            "title": {
                "fragments": [],
                "text": "PolarMask: Single Shot Instance Segmentation With Polar Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "This paper introduces an anchor-box free and single shot instance segmentation method, which is conceptually simple, fully convolutional and can be used by easily embedding it into most off-the-shelf detection methods."
            },
            "venue": {
                "fragments": [],
                "text": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143624054"
                        ],
                        "name": "Ze Yang",
                        "slug": "Ze-Yang",
                        "structuredName": {
                            "firstName": "Ze",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ze Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2119051209"
                        ],
                        "name": "Shaohui Liu",
                        "slug": "Shaohui-Liu",
                        "structuredName": {
                            "firstName": "Shaohui",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaohui Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1823518756"
                        ],
                        "name": "Han Hu",
                        "slug": "Han-Hu",
                        "structuredName": {
                            "firstName": "Han",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Han Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24952249"
                        ],
                        "name": "Liwei Wang",
                        "slug": "Liwei-Wang",
                        "structuredName": {
                            "firstName": "Liwei",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liwei Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145676588"
                        ],
                        "name": "Stephen Lin",
                        "slug": "Stephen-Lin",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "RepPoints [35] represents the boxes by a set of points and uses converting functions to get the target boxes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 131775182,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e3a2d59acd8fdab805b465c80b5fca443cd0bff",
            "isKey": false,
            "numCitedBy": 299,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "Modern object detectors rely heavily on rectangular bounding boxes, such as anchors, proposals and the final predictions, to represent objects at various recognition stages. The bounding box is convenient to use but provides only a coarse localization of objects and leads to a correspondingly coarse extraction of object features. In this paper, we present \\textbf{RepPoints} (representative points), a new finer representation of objects as a set of sample points useful for both localization and recognition. Given ground truth localization and recognition targets for training, RepPoints learn to automatically arrange themselves in a manner that bounds the spatial extent of an object and indicates semantically significant local areas. They furthermore do not require the use of anchors to sample a space of bounding boxes. We show that an anchor-free object detector based on RepPoints can be as effective as the state-of-the-art anchor-based detection methods, with 46.5 AP and 67.4 $AP_{50}$ on the COCO test-dev detection benchmark, using ResNet-101 model. Code is available at \\href{https://github.com/microsoft/RepPoints}{\\color{cyan}{https://github.com/microsoft/RepPoints}}."
            },
            "slug": "RepPoints:-Point-Set-Representation-for-Object-Yang-Liu",
            "title": {
                "fragments": [],
                "text": "RepPoints: Point Set Representation for Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that an anchor-free object detector based on RepPoints can be as effective as the state-of-the-art anchor-based detection methods, with 46.5 AP and 67.4 $AP_{50}$ on the COCO test-dev detection benchmark, using ResNet-101 model."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2004175"
                        ],
                        "name": "Dongyan Guo",
                        "slug": "Dongyan-Guo",
                        "structuredName": {
                            "firstName": "Dongyan",
                            "lastName": "Guo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dongyan Guo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152812753"
                        ],
                        "name": "Jun Wang",
                        "slug": "Jun-Wang",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690877"
                        ],
                        "name": "Ying Cui",
                        "slug": "Ying-Cui",
                        "structuredName": {
                            "firstName": "Ying",
                            "lastName": "Cui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ying Cui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108329894"
                        ],
                        "name": "Zhenhua Wang",
                        "slug": "Zhenhua-Wang",
                        "structuredName": {
                            "firstName": "Zhenhua",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhenhua Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788427"
                        ],
                        "name": "Shengyong Chen",
                        "slug": "Shengyong-Chen",
                        "structuredName": {
                            "firstName": "Shengyong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shengyong Chen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 278,
                                "start": 274
                            }
                        ],
                        "text": "More importantly, due to its simple design, FCOS can be easily extended to solve other instance-level recognition tasks with minimal modification, as already evidenced by instance segmentation [2], [20], [47], [51], keypoint detection [39], text spotting [26], and tracking [13], [44]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 208138555,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "738165f33c50b059e87b14d8b4a129230e14eacd",
            "isKey": false,
            "numCitedBy": 121,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "By decomposing the visual tracking task into two subproblems as classification for pixel category and regression for object bounding box at this pixel, we propose a novel fully convolutional Siamese network to solve visual tracking end-to-end in a per-pixel manner. The proposed framework SiamCAR consists of two simple subnetworks: one Siamese subnetwork for feature extraction and one classification-regression subnetwork for bounding box prediction. Different from state-of-the-art trackers like Siamese-RPN, SiamRPN++ and SPM, which are based on region proposal, the proposed framework is both proposal and anchor free. Consequently, we are able to avoid the tricky hyper-parameter tuning of anchors and reduce human intervention. The proposed framework is simple, neat and effective. Extensive experiments and comparisons with state-of-the-art trackers are conducted on challenging benchmarks including GOT-10K, LaSOT, UAV123 and OTB-50. Without bells and whistles, our SiamCAR achieves the leading performance with a considerable real-time speed. The code is available at https://github.com/ohhhyeahhh/SiamCAR."
            },
            "slug": "SiamCAR:-Siamese-Fully-Convolutional-Classification-Guo-Wang",
            "title": {
                "fragments": [],
                "text": "SiamCAR: Siamese Fully Convolutional Classification and Regression for Visual Tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A novel fully convolutional Siamese network to solve visual tracking end-to-end in a per-pixel manner by decomposing the visual tracking task into two subproblems as classification for pixel category and regression for object bounding box at this pixel is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2136435893"
                        ],
                        "name": "Jonathan Huang",
                        "slug": "Jonathan-Huang",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1382126732"
                        ],
                        "name": "V. Rathod",
                        "slug": "V.-Rathod",
                        "structuredName": {
                            "firstName": "Vivek",
                            "lastName": "Rathod",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Rathod"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1491624845"
                        ],
                        "name": "Chen Sun",
                        "slug": "Chen-Sun",
                        "structuredName": {
                            "firstName": "Chen",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chen Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2717876"
                        ],
                        "name": "Menglong Zhu",
                        "slug": "Menglong-Zhu",
                        "structuredName": {
                            "firstName": "Menglong",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Menglong Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34786378"
                        ],
                        "name": "A. Balan",
                        "slug": "A.-Balan",
                        "structuredName": {
                            "firstName": "Anoop",
                            "lastName": "Balan",
                            "middleNames": [
                                "Korattikara"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Balan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50706340"
                        ],
                        "name": "A. Fathi",
                        "slug": "A.-Fathi",
                        "structuredName": {
                            "firstName": "Alireza",
                            "lastName": "Fathi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fathi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33091759"
                        ],
                        "name": "Ian S. Fischer",
                        "slug": "Ian-S.-Fischer",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Fischer",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian S. Fischer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3282833"
                        ],
                        "name": "Z. Wojna",
                        "slug": "Z.-Wojna",
                        "structuredName": {
                            "firstName": "Zbigniew",
                            "lastName": "Wojna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Wojna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157997231"
                        ],
                        "name": "Yang Song",
                        "slug": "Yang-Song",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687120"
                        ],
                        "name": "S. Guadarrama",
                        "slug": "S.-Guadarrama",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Guadarrama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Guadarrama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702318"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "2 Faster R-CNN by G-RMI [17] Inception-ResNet-v2 [37] 34."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 244
                            }
                        ],
                        "text": "Quantitative results are shown in\n9 Method Backbone AP AP50 AP75 APS APM APL Two-stage methods: Faster R-CNN+++ [14] ResNet-101 34.9 55.7 37.4 15.6 38.7 50.9 Faster R-CNN w/ FPN [21] ResNet-101-FPN 36.2 59.1 39.0 18.2 39.0 48.2 Faster R-CNN by G-RMI [17] Inception-ResNet-v2 [37] 34.7 55.5 36.7 13.5 38.1 52.0 Faster R-CNN w/ TDM [36] Inception-ResNet-v2-TDM 36.8 57.7 39.2 16.2 39.8 52.1 One-stage methods: YOLOv2 [30] DarkNet-19 [30] 21.6 44.0 19.2 5.0 22.4 35.5 SSD513 [25] ResNet-101-SSD 31.2 50.4 33.3 10.2 34.5 49.8 YOLOv3 608\u00d7 608 [31] Darknet-53 33.0 57.9 34.4 18.3 35.4 41.9 DSSD513 [10] ResNet-101-DSSD 33.2 53.3 35.2 13.0 35.4 51.1 RetinaNet [22] ResNet-101-FPN 39.1 59.1 42.3 21.8 42.7 50.2 CornerNet [19] Hourglass-104 40.5 56.5 43.1 19.4 42.7 53.9 FSAF [54] ResNeXt-64x4d-101-FPN 42.9 63.8 46.3 26.6 46.2 52.7 CenterNet511 [9] Hourglass-104 44.9 62.4 48.1 25.6 47.4 57.4 FCOS ResNet-101-FPN 43.2 62.4 46.8 26.1 46.2 52.8 FCOS ResNeXt-32x8d-101-FPN 44.1 63.7 47.9 27.4 46.8 53.7 FCOS ResNeXt-64x4d-101-FPN 44.8 64.4 48.5 27.7 47.4 55.0 FCOS w/ deform. conv. v2 [55] ResNeXt-32x8d-101-FPN 46.6 65.9 50.8 28.6 49.1 58.6 FCOS ResNet-101-BiFPN [38] 45.0 63.6 48.7 27.0 47.9 55.9 FCOS ResNeXt-32x8d-101-BiFPN 46.2 65.2 50.0 28.7 49.1 56.5 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-BiFPN 47.9 66.9 51.9 30.2 50.3 59.9 w/ test-time augmentation: FCOS ResNet-101-FPN 45.9 64.5 50.4 29.4 48.3 56.1 FCOS ResNeXt-32x8d-101-FPN 47.0 66.0 51.6 30.7 49.4 57.1 FCOS ResNeXt-64x4d-101-FPN 47.5 66.4 51.9 31.4 49.7 58.2 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-FPN 49.1 68.0 53.9 31.7 51.6 61.0 FCOS ResNet-101-BiFPN 47.9 65.9 52.5 31.0 50.7 59.7 FCOS ResNeXt-32x8d-101-BiFPN 49.0 67.4 53.6 32.0 51.7 60.5 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-BiFPN 50.4 68.9 55.0 33.2 53.0 62.7\nTABLE 8 FCOS vs. other state-of-the-art two-stage or one-stage detectors (single-model results)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206595627,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a312a573ef81793d56401e932ef6c9498791a3d1",
            "isKey": true,
            "numCitedBy": 1999,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of this paper is to serve as a guide for selecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end, we investigate various ways to trade accuracy for speed and memory usage in modern convolutional object detection systems. A number of successful systems have been proposed in recent years, but apples-toapples comparisons are difficult due to different base feature extractors (e.g., VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms. We present a unified implementation of the Faster R-CNN [30], R-FCN [6] and SSD [25] systems, which we view as meta-architectures and trace out the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures. On one extreme end of this spectrum where speed and memory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device. On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance measured on the COCO detection task."
            },
            "slug": "Speed/Accuracy-Trade-Offs-for-Modern-Convolutional-Huang-Rathod",
            "title": {
                "fragments": [],
                "text": "Speed/Accuracy Trade-Offs for Modern Convolutional Object Detectors"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A unified implementation of the Faster R-CNN, R-FCN and SSD systems is presented and the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures is traced out."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2084646762"
                        ],
                        "name": "Cheng-Yang Fu",
                        "slug": "Cheng-Yang-Fu",
                        "structuredName": {
                            "firstName": "Cheng-Yang",
                            "lastName": "Fu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheng-Yang Fu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46641573"
                        ],
                        "name": "W. Liu",
                        "slug": "W.-Liu",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "22748016"
                        ],
                        "name": "A. Ranga",
                        "slug": "A.-Ranga",
                        "structuredName": {
                            "firstName": "Ananth",
                            "lastName": "Ranga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ranga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36183106"
                        ],
                        "name": "A. Tyagi",
                        "slug": "A.-Tyagi",
                        "structuredName": {
                            "firstName": "Ambrish",
                            "lastName": "Tyagi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tyagi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "detectors such as SSD [25] and DSSD [10], we also achieve much better performance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 587,
                                "start": 583
                            }
                        ],
                        "text": "Quantitative results are shown in\n9 Method Backbone AP AP50 AP75 APS APM APL Two-stage methods: Faster R-CNN+++ [14] ResNet-101 34.9 55.7 37.4 15.6 38.7 50.9 Faster R-CNN w/ FPN [21] ResNet-101-FPN 36.2 59.1 39.0 18.2 39.0 48.2 Faster R-CNN by G-RMI [17] Inception-ResNet-v2 [37] 34.7 55.5 36.7 13.5 38.1 52.0 Faster R-CNN w/ TDM [36] Inception-ResNet-v2-TDM 36.8 57.7 39.2 16.2 39.8 52.1 One-stage methods: YOLOv2 [30] DarkNet-19 [30] 21.6 44.0 19.2 5.0 22.4 35.5 SSD513 [25] ResNet-101-SSD 31.2 50.4 33.3 10.2 34.5 49.8 YOLOv3 608\u00d7 608 [31] Darknet-53 33.0 57.9 34.4 18.3 35.4 41.9 DSSD513 [10] ResNet-101-DSSD 33.2 53.3 35.2 13.0 35.4 51.1 RetinaNet [22] ResNet-101-FPN 39.1 59.1 42.3 21.8 42.7 50.2 CornerNet [19] Hourglass-104 40.5 56.5 43.1 19.4 42.7 53.9 FSAF [54] ResNeXt-64x4d-101-FPN 42.9 63.8 46.3 26.6 46.2 52.7 CenterNet511 [9] Hourglass-104 44.9 62.4 48.1 25.6 47.4 57.4 FCOS ResNet-101-FPN 43.2 62.4 46.8 26.1 46.2 52.8 FCOS ResNeXt-32x8d-101-FPN 44.1 63.7 47.9 27.4 46.8 53.7 FCOS ResNeXt-64x4d-101-FPN 44.8 64.4 48.5 27.7 47.4 55.0 FCOS w/ deform. conv. v2 [55] ResNeXt-32x8d-101-FPN 46.6 65.9 50.8 28.6 49.1 58.6 FCOS ResNet-101-BiFPN [38] 45.0 63.6 48.7 27.0 47.9 55.9 FCOS ResNeXt-32x8d-101-BiFPN 46.2 65.2 50.0 28.7 49.1 56.5 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-BiFPN 47.9 66.9 51.9 30.2 50.3 59.9 w/ test-time augmentation: FCOS ResNet-101-FPN 45.9 64.5 50.4 29.4 48.3 56.1 FCOS ResNeXt-32x8d-101-FPN 47.0 66.0 51.6 30.7 49.4 57.1 FCOS ResNeXt-64x4d-101-FPN 47.5 66.4 51.9 31.4 49.7 58.2 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-FPN 49.1 68.0 53.9 31.7 51.6 61.0 FCOS ResNet-101-BiFPN 47.9 65.9 52.5 31.0 50.7 59.7 FCOS ResNeXt-32x8d-101-BiFPN 49.0 67.4 53.6 32.0 51.7 60.5 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-BiFPN 50.4 68.9 55.0 33.2 53.0 62.7\nTABLE 8 FCOS vs. other state-of-the-art two-stage or one-stage detectors (single-model results)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "Compared to other one-stage detectors such as SSD [25] and DSSD [10], we also achieve much better performance."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7691159,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e94183191183a368bf07eb544654bae4b3cbf407",
            "isKey": true,
            "numCitedBy": 1204,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The main contribution of this paper is an approach for introducing additional context into state-of-the-art general object detection. To achieve this we first combine a state-of-the-art classifier (Residual-101[14]) with a fast detection framework (SSD[18]). We then augment SSD+Residual-101 with deconvolution layers to introduce additional large-scale context in object detection and improve accuracy, especially for small objects, calling our resulting system DSSD for deconvolutional single shot detector. While these two contributions are easily described at a high-level, a naive implementation does not succeed. Instead we show that carefully adding additional stages of learned transformations, specifically a module for feed-forward connections in deconvolution and a new output module, enables this new approach and forms a potential way forward for further detection research. Results are shown on both PASCAL VOC and COCO detection. Our DSSD with $513 \\times 513$ input achieves 81.5% mAP on VOC2007 test, 80.0% mAP on VOC2012 test, and 33.2% mAP on COCO, outperforming a state-of-the-art method R-FCN[3] on each dataset."
            },
            "slug": "DSSD-:-Deconvolutional-Single-Shot-Detector-Fu-Liu",
            "title": {
                "fragments": [],
                "text": "DSSD : Deconvolutional Single Shot Detector"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper combines a state-of-the-art classifier with a fast detection framework and augments SSD+Residual-101 with deconvolution layers to introduce additional large-scale context in object detection and improve accuracy, especially for small objects."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40219976"
                        ],
                        "name": "Zhi Tian",
                        "slug": "Zhi-Tian",
                        "structuredName": {
                            "firstName": "Zhi",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi Tian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12459603"
                        ],
                        "name": "Chunhua Shen",
                        "slug": "Chunhua-Shen",
                        "structuredName": {
                            "firstName": "Chunhua",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunhua Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2029503517"
                        ],
                        "name": "Hao Chen",
                        "slug": "Hao-Chen",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Chen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 258,
                                "start": 254
                            }
                        ],
                        "text": "It is worth noting that FCOS has 9\u00d7 fewer network output variables than the popular anchor-based detectors [22], [32] with 9 anchor boxes per location, which is of great importance when FCOS is applied to keypoint detection [39] or instance segmentation [41]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 212675968,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "862f2e2e5ba7b8b83f42226170c634ecb02834e4",
            "isKey": false,
            "numCitedBy": 178,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a simple yet effective instance segmentation framework, termed CondInst (conditional convolutions for instance segmentation). Top-performing instance segmentation methods such as Mask R-CNN rely on ROI operations (typically ROIPool or ROIAlign) to obtain the final instance masks. In contrast, we propose to solve instance segmentation from a new perspective. Instead of using instance-wise ROIs as inputs to a network of fixed weights, we employ dynamic instance-aware networks, conditioned on instances. CondInst enjoys two advantages: 1) Instance segmentation is solved by a fully convolutional network, eliminating the need for ROI cropping and feature alignment. 2) Due to the much improved capacity of dynamically-generated conditional convolutions, the mask head can be very compact (e.g., 3 conv. layers, each having only 8 channels), leading to significantly faster inference. We demonstrate a simpler instance segmentation method that can achieve improved performance in both accuracy and inference speed. On the COCO dataset, we outperform a few recent methods including well-tuned Mask RCNN baselines, without longer training schedules needed. \nCode is available: this https URL"
            },
            "slug": "Conditional-Convolutions-for-Instance-Segmentation-Tian-Shen",
            "title": {
                "fragments": [],
                "text": "Conditional Convolutions for Instance Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A simpler instance segmentation method that can achieve improved performance in both accuracy and inference speed on the COCO dataset, and outperform a few recent methods including well-tuned Mask RCNN baselines, without longer training schedules needed."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781242"
                        ],
                        "name": "Abhinav Shrivastava",
                        "slug": "Abhinav-Shrivastava",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Shrivastava",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abhinav Shrivastava"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694199"
                        ],
                        "name": "R. Sukthankar",
                        "slug": "R.-Sukthankar",
                        "structuredName": {
                            "firstName": "Rahul",
                            "lastName": "Sukthankar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sukthankar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13123786,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47a1fa91816f50266aef3c31b6bd38895df63b00",
            "isKey": false,
            "numCitedBy": 266,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years, we have seen tremendous progress in the field of object detection. Most of the recent improvements have been achieved by targeting deeper feedforward networks. However, many hard object categories such as bottle, remote, etc. require representation of fine details and not just coarse, semantic representations. But most of these fine details are lost in the early convolutional layers. What we need is a way to incorporate finer details from lower layers into the detection architecture. Skip connections have been proposed to combine high-level and low-level features, but we argue that selecting the right features from low-level requires top-down contextual information. Inspired by the human visual pathway, in this paper we propose top-down modulations as a way to incorporate fine details into the detection framework. Our approach supplements the standard bottom-up, feedforward ConvNet with a top-down modulation (TDM) network, connected using lateral connections. These connections are responsible for the modulation of lower layer filters, and the top-down network handles the selection and integration of contextual information and low-level features. The proposed TDM architecture provides a significant boost on the COCO testdev benchmark, achieving 28.6 AP for VGG16, 35.2 AP for ResNet101, and 37.3 for InceptionResNetv2 network, without any bells and whistles (e.g., multi-scale, iterative box refinement, etc.)."
            },
            "slug": "Beyond-Skip-Connections:-Top-Down-Modulation-for-Shrivastava-Sukthankar",
            "title": {
                "fragments": [],
                "text": "Beyond Skip Connections: Top-Down Modulation for Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Inspired by the human visual pathway, this paper proposes top-down modulations as a way to incorporate fine details into the detection framework, and supplements the standard bottom-up, feedforward ConvNet with a top- down modulation (TDM) network, connected using lateral connections."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32109772"
                        ],
                        "name": "Benjin Zhu",
                        "slug": "Benjin-Zhu",
                        "structuredName": {
                            "firstName": "Benjin",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjin Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110257737"
                        ],
                        "name": "Jianfeng Wang",
                        "slug": "Jianfeng-Wang",
                        "structuredName": {
                            "firstName": "Jianfeng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianfeng Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50676465"
                        ],
                        "name": "Zhengkai Jiang",
                        "slug": "Zhengkai-Jiang",
                        "structuredName": {
                            "firstName": "Zhengkai",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhengkai Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796301107"
                        ],
                        "name": "Fuhang Zong",
                        "slug": "Fuhang-Zong",
                        "structuredName": {
                            "firstName": "Fuhang",
                            "lastName": "Zong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fuhang Zong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144363412"
                        ],
                        "name": "Songtao Liu",
                        "slug": "Songtao-Liu",
                        "structuredName": {
                            "firstName": "Songtao",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Songtao Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6000385"
                        ],
                        "name": "Zeming Li",
                        "slug": "Zeming-Li",
                        "structuredName": {
                            "firstName": "Zeming",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zeming Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 220380967,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3a699a33a0458d7e7de1a9177ab7cb4ea136583b",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose an anchor-free object detector with a fully differentiable label assignment strategy, named AutoAssign. It automatically determines positive/negative samples by generating positive and negative weight maps to modify each location's prediction dynamically. Specifically, we present a center weighting module to adjust the category-specific prior distributions and a confidence weighting module to adapt the specific assign strategy of each instance. The entire label assignment process is differentiable and requires no additional modification to transfer to different datasets and tasks. Extensive experiments on MS COCO show that our method steadily surpasses other best sampling strategies by $ \\sim $ 1\\% AP with various backbones. Moreover, our best model achieves 52.1\\% AP, outperforming all existing one-stage detectors. Besides, experiments on other datasets, \\emph{e.g.}, PASCAL VOC, Objects365, and WiderFace, demonstrate the broad applicability of AutoAssign."
            },
            "slug": "AutoAssign:-Differentiable-Label-Assignment-for-Zhu-Wang",
            "title": {
                "fragments": [],
                "text": "AutoAssign: Differentiable Label Assignment for Dense Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "An anchor-free object detector with a fully differentiable label assignment strategy, named AutoAssign, that automatically determines positive/negative samples by generating positive and negative weight maps to modify each location's prediction dynamically."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40313071"
                        ],
                        "name": "Nermin Samet",
                        "slug": "Nermin-Samet",
                        "structuredName": {
                            "firstName": "Nermin",
                            "lastName": "Samet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nermin Samet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060717"
                        ],
                        "name": "Samet Hicsonmez",
                        "slug": "Samet-Hicsonmez",
                        "structuredName": {
                            "firstName": "Samet",
                            "lastName": "Hicsonmez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samet Hicsonmez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2132749"
                        ],
                        "name": "Emre Akbas",
                        "slug": "Emre-Akbas",
                        "structuredName": {
                            "firstName": "Emre",
                            "lastName": "Akbas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Emre Akbas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "HoughNet [37] employs a voting mechanism to"
                    },
                    "intents": []
                }
            ],
            "corpusId": 220363921,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5736970f7d7cc84ba7164ad962a13eb1c7293159",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents HoughNet, a one-stage, anchor-free, voting-based, bottom-up object detection method. Inspired by the Generalized Hough Transform, HoughNet determines the presence of an object at a certain location by the sum of the votes cast on that location. Votes are collected from both near and long-distance locations based on a log-polar vote field. Thanks to this voting mechanism, HoughNet is able to integrate both near and long-range, class-conditional evidence for visual recognition, thereby generalizing and enhancing current object detection methodology, which typically relies on only local evidence. On the COCO dataset, HoughNet's best model achieves 46.4 $AP$ (and 65.1 $AP_{50}$), performing on par with the state-of-the-art in bottom-up object detection and outperforming most major one-stage and two-stage methods. We further validate the effectiveness of our proposal in another task, namely, \"labels to photo\" image generation by integrating the voting module of HoughNet to two different GAN models and showing that the accuracy is significantly improved in both cases. Code is available at this https URL."
            },
            "slug": "HoughNet:-Integrating-near-and-long-range-evidence-Samet-Hicsonmez",
            "title": {
                "fragments": [],
                "text": "HoughNet: Integrating near and long-range evidence for bottom-up object detection"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This paper presents HoughNet, a one-stage, anchor-free, voting-based, bottom-up object detection method, inspired by the Generalized Hough Transform, performing on par with the state-of-the-art in bottom- up object detection and outperforming most major one- stage and two-stage methods."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069520672"
                        ],
                        "name": "Zhi Tian",
                        "slug": "Zhi-Tian",
                        "structuredName": {
                            "firstName": "Zhi",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi Tian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2029503517"
                        ],
                        "name": "Hao Chen",
                        "slug": "Hao-Chen",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12459603"
                        ],
                        "name": "Chunhua Shen",
                        "slug": "Chunhua-Shen",
                        "structuredName": {
                            "firstName": "Chunhua",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunhua Shen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 235
                            }
                        ],
                        "text": "More importantly, due to its simple design, FCOS can be easily extended to solve other instance-level recognition tasks with minimal modification, as already evidenced by instance segmentation [2], [20], [47], [51], keypoint detection [39], text spotting [26], and tracking [13], [44]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 224
                            }
                        ],
                        "text": "It is worth noting that FCOS has 9\u00d7 fewer network output variables than the popular anchor-based detectors [22], [32] with 9 anchor boxes per location, which is of great importance when FCOS is applied to keypoint detection [39] or instance segmentation [41]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 208139223,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "389df5fbf21aa7ad13985a13dc02220f96725af0",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose the first direct end-to-end multi-person pose estimation framework, termed DirectPose. Inspired by recent anchor-free object detectors, which directly regress the two corners of target bounding-boxes, the proposed framework directly predicts instance-aware keypoints for all the instances from a raw input image, eliminating the need for heuristic grouping in bottom-up methods or bounding-box detection and RoI operations in top-down ones. We also propose a novel Keypoint Alignment (KPAlign) mechanism, which overcomes the main difficulty: lack of the alignment between the convolutional features and predictions in this end-to-end framework. KPAlign improves the framework's performance by a large margin while still keeping the framework end-to-end trainable. With the only postprocessing non-maximum suppression (NMS), our proposed framework can detect multi-person keypoints with or without bounding-boxes in a single shot. Experiments demonstrate that the end-to-end paradigm can achieve competitive or better performance than previous strong baselines, in both bottom-up and top-down methods. We hope that our end-to-end approach can provide a new perspective for the human pose estimation task."
            },
            "slug": "DirectPose:-Direct-End-to-End-Multi-Person-Pose-Tian-Chen",
            "title": {
                "fragments": [],
                "text": "DirectPose: Direct End-to-End Multi-Person Pose Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "The first direct end-to-end multi-person pose estimation framework, termed DirectPose, is proposed, which directly predicts instance-aware keypoints for all the instances from a raw input image, eliminating the need for heuristic grouping in bottom-up methods or bounding-box detection and RoI operations in top-down ones."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145388313"
                        ],
                        "name": "Guangting Wang",
                        "slug": "Guangting-Wang",
                        "structuredName": {
                            "firstName": "Guangting",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guangting Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2820418"
                        ],
                        "name": "Chong Luo",
                        "slug": "Chong-Luo",
                        "structuredName": {
                            "firstName": "Chong",
                            "lastName": "Luo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chong Luo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145200611"
                        ],
                        "name": "Xiaoyan Sun",
                        "slug": "Xiaoyan-Sun",
                        "structuredName": {
                            "firstName": "Xiaoyan",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoyan Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2352456"
                        ],
                        "name": "Zhiwei Xiong",
                        "slug": "Zhiwei-Xiong",
                        "structuredName": {
                            "firstName": "Zhiwei",
                            "lastName": "Xiong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhiwei Xiong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1634494276"
                        ],
                        "name": "Wenjun Zeng",
                        "slug": "Wenjun-Zeng",
                        "structuredName": {
                            "firstName": "Wenjun",
                            "lastName": "Zeng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenjun Zeng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 280
                            }
                        ],
                        "text": "More importantly, due to its simple design, FCOS can be easily extended to solve other instance-level recognition tasks with minimal modification, as already evidenced by instance segmentation [2], [20], [47], [51], keypoint detection [39], text spotting [26], and tracking [13], [44]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 214774780,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "af985dea540bd489397e7d28affa10f32a4d7167",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the tracking problem as a special type of object detection problem, which we call instance detection. With proper initialization, a detector can be quickly converted into a tracker by learning the new instance from a single image. We find that model-agnostic meta-learning (MAML) offers a strategy to initialize the detector that satisfies our needs. We propose a principled three-step approach to build a high-performance tracker. First, pick any modern object detector trained with gradient descent. Second, conduct offline training (or initialization) with MAML. Third, perform domain adaptation using the initial frame. We follow this procedure to build two trackers, named Retina-MAML and FCOS-MAML, based on two modern detectors RetinaNet and FCOS. Evaluations on four benchmarks show that both trackers are competitive against state-of-the-art trackers. On OTB-100, Retina-MAML achieves the highest ever AUC of 0.712. On TrackingNet, FCOS-MAML ranks the first on the leader board with an AUC of 0.757 and the normalized precision of 0.822. Both trackers run in real-time at 40 FPS."
            },
            "slug": "Tracking-by-Instance-Detection:-A-Meta-Learning-Wang-Luo",
            "title": {
                "fragments": [],
                "text": "Tracking by Instance Detection: A Meta-Learning Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes a principled three-step approach to build a high-performance tracker based on two modern detectors RetinaNet and FCOS, and finds that model-agnostic meta-learning (MAML) offers a strategy to initialize the detector that satisfies the authors' needs."
            },
            "venue": {
                "fragments": [],
                "text": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47894545"
                        ],
                        "name": "Chenchen Zhu",
                        "slug": "Chenchen-Zhu",
                        "structuredName": {
                            "firstName": "Chenchen",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chenchen Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39838894"
                        ],
                        "name": "Yihui He",
                        "slug": "Yihui-He",
                        "structuredName": {
                            "firstName": "Yihui",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yihui He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794486"
                        ],
                        "name": "M. Savvides",
                        "slug": "M.-Savvides",
                        "structuredName": {
                            "firstName": "Marios",
                            "lastName": "Savvides",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Savvides"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "Recently, FSAF [54] was proposed to employ an anchor-free detection branch as a complement to an anchor-based detection branch since they consider that a totally anchor-free detector cannot achieve good performance."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 208
                            }
                        ],
                        "text": "However, in this work, we surprisingly show that the totally anchor-free detector can actually obtain better performance than its anchorbased counterpart, without the need for the feature selection module in FSAF."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 765,
                                "start": 761
                            }
                        ],
                        "text": "Quantitative results are shown in\n9 Method Backbone AP AP50 AP75 APS APM APL Two-stage methods: Faster R-CNN+++ [14] ResNet-101 34.9 55.7 37.4 15.6 38.7 50.9 Faster R-CNN w/ FPN [21] ResNet-101-FPN 36.2 59.1 39.0 18.2 39.0 48.2 Faster R-CNN by G-RMI [17] Inception-ResNet-v2 [37] 34.7 55.5 36.7 13.5 38.1 52.0 Faster R-CNN w/ TDM [36] Inception-ResNet-v2-TDM 36.8 57.7 39.2 16.2 39.8 52.1 One-stage methods: YOLOv2 [30] DarkNet-19 [30] 21.6 44.0 19.2 5.0 22.4 35.5 SSD513 [25] ResNet-101-SSD 31.2 50.4 33.3 10.2 34.5 49.8 YOLOv3 608\u00d7 608 [31] Darknet-53 33.0 57.9 34.4 18.3 35.4 41.9 DSSD513 [10] ResNet-101-DSSD 33.2 53.3 35.2 13.0 35.4 51.1 RetinaNet [22] ResNet-101-FPN 39.1 59.1 42.3 21.8 42.7 50.2 CornerNet [19] Hourglass-104 40.5 56.5 43.1 19.4 42.7 53.9 FSAF [54] ResNeXt-64x4d-101-FPN 42.9 63.8 46.3 26.6 46.2 52.7 CenterNet511 [9] Hourglass-104 44.9 62.4 48.1 25.6 47.4 57.4 FCOS ResNet-101-FPN 43.2 62.4 46.8 26.1 46.2 52.8 FCOS ResNeXt-32x8d-101-FPN 44.1 63.7 47.9 27.4 46.8 53.7 FCOS ResNeXt-64x4d-101-FPN 44.8 64.4 48.5 27.7 47.4 55.0 FCOS w/ deform. conv. v2 [55] ResNeXt-32x8d-101-FPN 46.6 65.9 50.8 28.6 49.1 58.6 FCOS ResNet-101-BiFPN [38] 45.0 63.6 48.7 27.0 47.9 55.9 FCOS ResNeXt-32x8d-101-BiFPN 46.2 65.2 50.0 28.7 49.1 56.5 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-BiFPN 47.9 66.9 51.9 30.2 50.3 59.9 w/ test-time augmentation: FCOS ResNet-101-FPN 45.9 64.5 50.4 29.4 48.3 56.1 FCOS ResNeXt-32x8d-101-FPN 47.0 66.0 51.6 30.7 49.4 57.1 FCOS ResNeXt-64x4d-101-FPN 47.5 66.4 51.9 31.4 49.7 58.2 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-FPN 49.1 68.0 53.9 31.7 51.6 61.0 FCOS ResNet-101-BiFPN 47.9 65.9 52.5 31.0 50.7 59.7 FCOS ResNeXt-32x8d-101-BiFPN 49.0 67.4 53.6 32.0 51.7 60.5 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-BiFPN 50.4 68.9 55.0 33.2 53.0 62.7\nTABLE 8 FCOS vs. other state-of-the-art two-stage or one-stage detectors (single-model results)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 103
                            }
                        ],
                        "text": "Even more surprisingly, it can outperform the combination of anchor-free and anchor-based detectors in FSAF."
                    },
                    "intents": []
                }
            ],
            "corpusId": 67855967,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9721f9b2d5a77b878f6ffc6badd3470e1ade9415",
            "isKey": true,
            "numCitedBy": 374,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We motivate and present feature selective anchor-free (FSAF) module, a simple and effective building block for single-shot object detectors. It can be plugged into single-shot detectors with feature pyramid structure. The FSAF module addresses two limitations brought up by the conventional anchor-based detection: 1) heuristic-guided feature selection; 2) overlap-based anchor sampling. The general concept of the FSAF module is online feature selection applied to the training of multi-level anchor-free branches. Specifically, an anchor-free branch is attached to each level of the feature pyramid, allowing box encoding and decoding in the anchor-free manner at an arbitrary level. During training, we dynamically assign each instance to the most suitable feature level. At the time of inference, the FSAF module can work independently or jointly with anchor-based branches. We instantiate this concept with simple implementations of anchor-free branches and online feature selection strategy. Experimental results on the COCO detection track show that our FSAF module performs better than anchor-based counterparts while being faster. When working jointly with anchor-based branches, the FSAF module robustly improves the baseline RetinaNet by a large margin under various settings, while introducing nearly free inference overhead. And the resulting best model can achieve a state-of-the-art 44.6% mAP, outperforming all existing single-shot detectors on COCO."
            },
            "slug": "Feature-Selective-Anchor-Free-Module-for-Object-Zhu-He",
            "title": {
                "fragments": [],
                "text": "Feature Selective Anchor-Free Module for Single-Shot Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The FSAF module robustly improves the baseline RetinaNet by a large margin under various settings, while introducing nearly free inference overhead, and the resulting best model can achieve a state-of-the-art 44.6% mAP, outperforming all existing single-shot detectors on COCO."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148927556"
                        ],
                        "name": "Xinyu Zhou",
                        "slug": "Xinyu-Zhou",
                        "structuredName": {
                            "firstName": "Xinyu",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinyu Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146721"
                        ],
                        "name": "C. Yao",
                        "slug": "C.-Yao",
                        "structuredName": {
                            "firstName": "Cong",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075348632"
                        ],
                        "name": "He Wen",
                        "slug": "He-Wen",
                        "structuredName": {
                            "firstName": "He",
                            "lastName": "Wen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "He Wen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47905836"
                        ],
                        "name": "Yuzhi Wang",
                        "slug": "Yuzhi-Wang",
                        "structuredName": {
                            "firstName": "Yuzhi",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuzhi Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132667"
                        ],
                        "name": "Shuchang Zhou",
                        "slug": "Shuchang-Zhou",
                        "structuredName": {
                            "firstName": "Shuchang",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuchang Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2416953"
                        ],
                        "name": "Weiran He",
                        "slug": "Weiran-He",
                        "structuredName": {
                            "firstName": "Weiran",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weiran He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1387852255"
                        ],
                        "name": "Jiajun Liang",
                        "slug": "Jiajun-Liang",
                        "structuredName": {
                            "firstName": "Jiajun",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiajun Liang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 132
                            }
                        ],
                        "text": "Besides, more significantly, these methods are mainly used in special domain objection detection such as scene text detection [16], [53] or face detection [18], [50], since it is believed that these methods do not work well when applied to generic object detection with highly overlapped bounding boxes."
                    },
                    "intents": []
                }
            ],
            "corpusId": 706860,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f1630b4485027eb99ae59b745372ef1f3699c16",
            "isKey": false,
            "numCitedBy": 904,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous approaches for scene text detection have already achieved promising performances across various benchmarks. However, they usually fall short when dealing with challenging scenarios, even when equipped with deep neural network models, because the overall performance is determined by the interplay of multiple stages and components in the pipelines. In this work, we propose a simple yet powerful pipeline that yields fast and accurate text detection in natural scenes. The pipeline directly predicts words or text lines of arbitrary orientations and quadrilateral shapes in full images, eliminating unnecessary intermediate steps (e.g., candidate aggregation and word partitioning), with a single neural network. The simplicity of our pipeline allows concentrating efforts on designing loss functions and neural network architecture. Experiments on standard datasets including ICDAR 2015, COCO-Text and MSRA-TD500 demonstrate that the proposed algorithm significantly outperforms state-of-the-art methods in terms of both accuracy and efficiency. On the ICDAR 2015 dataset, the proposed algorithm achieves an F-score of 0.7820 at 13.2fps at 720p resolution."
            },
            "slug": "EAST:-An-Efficient-and-Accurate-Scene-Text-Detector-Zhou-Yao",
            "title": {
                "fragments": [],
                "text": "EAST: An Efficient and Accurate Scene Text Detector"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work proposes a simple yet powerful pipeline that yields fast and accurate text detection in natural scenes, and significantly outperforms state-of-the-art methods in terms of both accuracy and efficiency."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33493200"
                        ],
                        "name": "Tsung-Yi Lin",
                        "slug": "Tsung-Yi-Lin",
                        "structuredName": {
                            "firstName": "Tsung-Yi",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tsung-Yi Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145854440"
                        ],
                        "name": "M. Maire",
                        "slug": "M.-Maire",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Maire",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Maire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48966748"
                        ],
                        "name": "James Hays",
                        "slug": "James-Hays",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hays",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Hays"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "Following the common practice [21], [22], [32], we use the COCO train2017 split (115K images) for training and val2017 split (5K images) as validation for our ablation study."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "For example, in RetinaNet, varying these hyper-parameters affects the performance up to 4% in AP on the COCO benchmark [23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "As shown in Table 6, r = 1.5 has the best performance on COCO val split."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "On COCO, both schemes result in similar performance."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "Here, we compare FCOS with its anchor-based counterpart RetinaNet on the challenging benchmark COCO, demonstrating that the much simpler anchor-free FCOS is superior."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 205
                            }
                        ],
                        "text": "The center area of a box centered at (cx, cy) is defined as the sub-box (cx \u2212 rs, cy \u2212 rs, cx + rs, cy + rs), where s is the total stride until the current feature maps and r is a hyper-parameter being 1.5 on COCO."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 85
                            }
                        ],
                        "text": "We compare FCOS with other state-of-the-art object detectors on test-dev split of MS-COCO benchmark."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 74
                            }
                        ],
                        "text": "Our experiments are conducted on the large-scale detection benchmark COCO [23]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "C is the number of classes, which is 80 for the MS-COCO dataset."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 101
                            }
                        ],
                        "text": "Qualitative results on the CrowdHuman val set with the ResNet-50-FPN backbone.\nous sample ratio than COCO, it is expected that FCOS will have inferior performance on the highly crowded dataset."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 225
                            }
                        ],
                        "text": "RetinaNet X Not used 88.16 RetinaNet X \u2265 0.4 91.94 RetinaNet X All 99.32 FCOS - 96.34 FCOS X - 98.95\nTABLE 1 The best possible recall (BPR) of anchor-based RetinaNet under a variety of matching rules and the BPR of FCOS on the COCO val2017 split."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 88
                            }
                        ],
                        "text": "Second, when a location is supposed to be associated\nto multiple ground-truth boxes, on COCO, we choose the object with minimal area as the target for the location."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 141
                            }
                        ],
                        "text": "In the section, we show that the concern is not necessary by comparing BPR of FCOS and that of its anchor-based counterpart RetinaNet on the COCO val2017 split."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "Other settings are the same as that of COCO."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14113767,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71b7178df5d2b112d07e45038cb5637208659ff7",
            "isKey": true,
            "numCitedBy": 19778,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model."
            },
            "slug": "Microsoft-COCO:-Common-Objects-in-Context-Lin-Maire",
            "title": {
                "fragments": [],
                "text": "Microsoft COCO: Common Objects in Context"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding by gathering images of complex everyday scenes containing common objects in their natural context."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49660254"
                        ],
                        "name": "Han Qiu",
                        "slug": "Han-Qiu",
                        "structuredName": {
                            "firstName": "Han",
                            "lastName": "Qiu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Han Qiu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1628250729"
                        ],
                        "name": "Yuchen Ma",
                        "slug": "Yuchen-Ma",
                        "structuredName": {
                            "firstName": "Yuchen",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuchen Ma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6000385"
                        ],
                        "name": "Zeming Li",
                        "slug": "Zeming-Li",
                        "structuredName": {
                            "firstName": "Zeming",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zeming Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144363412"
                        ],
                        "name": "Songtao Liu",
                        "slug": "Songtao-Liu",
                        "structuredName": {
                            "firstName": "Songtao",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Songtao Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "y measure the network latency for all detectors. RetinaNet results are from Detectron2. FCOS achieves competitive performance compared with recent methods including anchor-based ones. built upon FCOS [38], [39], [40], [41]. These works enhance the detection features, loss functions or the assigning strategy of FCOS, further boosting the anchor-free detector\u2019s performance. 2 OUR APPROACH In this sectio"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 220686718,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5800afc4ca06a309d74baedd65a0b0666b3716d1",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Dense object detectors rely on the sliding-window paradigm that predicts the object over a regular grid of image. Meanwhile, the feature maps on the point of the grid are adopted to generate the bounding box predictions. The point feature is convenient to use but may lack the explicit border information for accurate localization. In this paper, We propose a simple and efficient operator called Border-Align to extract \"border features\" from the extreme point of the border to enhance the point feature. Based on the BorderAlign, we design a novel detection architecture called BorderDet, which explicitly exploits the border information for stronger classification and more accurate localization. With ResNet-50 backbone, our method improves single-stage detector FCOS by 2.8 AP gains (38.6 v.s. 41.4). With the ResNeXt-101-DCN backbone, our BorderDet obtains 50.3 AP, outperforming the existing state-of-the-art approaches. The code is available at (this https URL)."
            },
            "slug": "BorderDet:-Border-Feature-for-Dense-Object-Qiu-Ma",
            "title": {
                "fragments": [],
                "text": "BorderDet: Border Feature for Dense Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper designs a novel detection architecture called BorderDet, which explicitly exploits the border information for stronger classification and more accurate localization and proposes a simple and efficient operator called Border-Align to extract \"border features\" from the extreme point of the border to enhance the point feature."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3304536"
                        ],
                        "name": "Jifeng Dai",
                        "slug": "Jifeng-Dai",
                        "structuredName": {
                            "firstName": "Jifeng",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jifeng Dai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7217794"
                        ],
                        "name": "Haozhi Qi",
                        "slug": "Haozhi-Qi",
                        "structuredName": {
                            "firstName": "Haozhi",
                            "lastName": "Qi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haozhi Qi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3372084"
                        ],
                        "name": "Yuwen Xiong",
                        "slug": "Yuwen-Xiong",
                        "structuredName": {
                            "firstName": "Yuwen",
                            "lastName": "Xiong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuwen Xiong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2153682629"
                        ],
                        "name": "Yi Li",
                        "slug": "Yi-Li",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46266081"
                        ],
                        "name": "Guodong Zhang",
                        "slug": "Guodong-Zhang",
                        "structuredName": {
                            "firstName": "Guodong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guodong Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1823518756"
                        ],
                        "name": "Han Hu",
                        "slug": "Han-Hu",
                        "structuredName": {
                            "firstName": "Han",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Han Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732264"
                        ],
                        "name": "Yichen Wei",
                        "slug": "Yichen-Wei",
                        "structuredName": {
                            "firstName": "Yichen",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yichen Wei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 57
                            }
                        ],
                        "text": "As shown in Table 8, by applying deformable convolutions [5], [55] to ResNeXt-32x8d-101-FPN based FCOS, the performance is improved from 44."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4028864,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4a73a1840945e87583d89ca0216a2c449d50a4a3",
            "isKey": false,
            "numCitedBy": 2320,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the fixed geometric structures in their building modules. In this work, we introduce two new modules to enhance the transformation modeling capability of CNNs, namely, deformable convolution and deformable RoI pooling. Both are based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from the target tasks, without additional supervision. The new modules can readily replace their plain counterparts in existing CNNs and can be easily trained end-to-end by standard back-propagation, giving rise to deformable convolutional networks. Extensive experiments validate the performance of our approach. For the first time, we show that learning dense spatial transformation in deep CNNs is effective for sophisticated vision tasks such as object detection and semantic segmentation. The code is released at https://github.com/msracver/Deformable-ConvNets."
            },
            "slug": "Deformable-Convolutional-Networks-Dai-Qi",
            "title": {
                "fragments": [],
                "text": "Deformable Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This work introduces two new modules to enhance the transformation modeling capability of CNNs, namely, deformable convolution and deformable RoI pooling, based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from the target tasks, without additional supervision."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782282"
                        ],
                        "name": "Evan Shelhamer",
                        "slug": "Evan-Shelhamer",
                        "structuredName": {
                            "firstName": "Evan",
                            "lastName": "Shelhamer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Evan Shelhamer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117314646"
                        ],
                        "name": "Jonathan Long",
                        "slug": "Jonathan-Long",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Long",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Long"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "With deep learning, detection has been largely shifted to the use of fully convolutional networks (FCNs) since the invention of Faster R-CNN [32]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Recently, FCNs [28] have achieved tremendous success in dense prediction tasks such as semantic segmentation [15], [28], [40], depth estimation [24], [48], keypoint detection [3] and counting."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 177
                            }
                        ],
                        "text": "In other words, our detector directly views locations as training samples instead of anchor boxes in anchor-based detectors, which is the same as FCNs for semantic segmentation [28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "\u2022 Detection is now unified with many other FCNsolvable tasks such as semantic segmentation, making it easier to re-use ideas from those tasks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "In the literature, some works attempted to leverage the FCNs-based framework for object detection such as DenseBox [18]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "in dense prediction tasks such as semantic segmentation [15], [28], [40], depth estimation [24], [48], keypoint detection [3] and counting."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "Thus DenseBox has to perform detection on image pyramids, which is against FCN\u2019s philosophy of computing all convolutions once."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "These frameworks are similar to the FCNs for semantic segmentation, except that each location is required to regress a 4D continuous vector."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "Recently, FCNs [28] have achieved tremendous success"
                    },
                    "intents": []
                }
            ],
            "corpusId": 1629541,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "317aee7fc081f2b137a85c4f20129007fd8e717e",
            "isKey": true,
            "numCitedBy": 15652,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build \u201cfully convolutional\u201d networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional networks achieve improved segmentation of PASCAL VOC (30% relative improvement to 67.2% mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image."
            },
            "slug": "Fully-Convolutional-Networks-for-Semantic-Shelhamer-Long",
            "title": {
                "fragments": [],
                "text": "Fully Convolutional Networks for Semantic Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "It is shown that convolutional networks by themselves, trained end- to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118328320"
                        ],
                        "name": "Tong He",
                        "slug": "Tong-He",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40219976"
                        ],
                        "name": "Zhi Tian",
                        "slug": "Zhi-Tian",
                        "structuredName": {
                            "firstName": "Zhi",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi Tian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49015548"
                        ],
                        "name": "Weilin Huang",
                        "slug": "Weilin-Huang",
                        "structuredName": {
                            "firstName": "Weilin",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weilin Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780381"
                        ],
                        "name": "Chunhua Shen",
                        "slug": "Chunhua-Shen",
                        "structuredName": {
                            "firstName": "Chunhua",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunhua Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143970608"
                        ],
                        "name": "Y. Qiao",
                        "slug": "Y.-Qiao",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Qiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Qiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755278"
                        ],
                        "name": "Changming Sun",
                        "slug": "Changming-Sun",
                        "structuredName": {
                            "firstName": "Changming",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Changming Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 126
                            }
                        ],
                        "text": "Besides, more significantly, these methods are mainly used in special domain objection detection such as scene text detection [16], [53] or face detection [18], [50], since it is believed that these methods do not work well when applied to generic object detection with highly overlapped bounding boxes."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3801827,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89642d3bacccbe543e224ea139b69986048915ef",
            "isKey": false,
            "numCitedBy": 143,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Text detection and recognition in natural images have long been considered as two separate tasks that are processed sequentially. Jointly training two tasks is non-trivial due to significant differences in learning difficulties and convergence rates. In this work, we present a conceptually simple yet efficient framework that simultaneously processes the two tasks in a united framework. Our main contributions are three-fold: (1) we propose a novel text-alignment layer that allows it to precisely compute convolutional features of a text instance in arbitrary orientation, which is the key to boost the performance; (2) a character attention mechanism is introduced by using character spatial information as explicit supervision, leading to large improvements in recognition; (3) two technologies, together with a new RNN branch for word recognition, are integrated seamlessly into a single model which is end-to-end trainable. This allows the two tasks to work collaboratively by sharing convolutional features, which is critical to identify challenging text instances. Our model obtains impressive results in end-to-end recognition on the ICDAR 2015 [19], significantly advancing the most recent results [2], with improvements of F-measure from (0.54, 0.51, 0.47) to (0.82, 0.77, 0.63), by using a strong, weak and generic lexicon respectively. Thanks to joint training, our method can also serve as a good detector by achieving a new state-of-the-art detection performance on related benchmarks. Code is available at https://github.com/tonghe90/textspotter."
            },
            "slug": "An-End-to-End-TextSpotter-with-Explicit-Alignment-He-Tian",
            "title": {
                "fragments": [],
                "text": "An End-to-End TextSpotter with Explicit Alignment and Attention"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel text-alignment layer is proposed that allows it to precisely compute convolutional features of a text instance in arbitrary orientation, which is the key to boost the performance of the model on the ICDAR 2015."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1585443206"
                        ],
                        "name": "Xuangeng Chu",
                        "slug": "Xuangeng-Chu",
                        "structuredName": {
                            "firstName": "Xuangeng",
                            "lastName": "Chu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuangeng Chu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "28964350"
                        ],
                        "name": "Anlin Zheng",
                        "slug": "Anlin-Zheng",
                        "structuredName": {
                            "firstName": "Anlin",
                            "lastName": "Zheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anlin Zheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771551"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 25
                            }
                        ],
                        "text": "Furthermore, as shown in [4], it is more reasonable to let one proposal make multiple predictions under the highly crowded scenario (i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 46
                            }
                        ],
                        "text": "Following previous works on crowded benchmark [4], [34], we use AP, long-average Miss Rate on False Positive Per Image in [10\u22122, 100] (MR\u22122) [8] and Jaccard Index (JI) as the evaluation metrics."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 10
                            }
                        ],
                        "text": "Following [4], all experiments here are trained on the train split for 30 epochs with batch size 16 and then evaluated on the val split."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 112
                            }
                        ],
                        "text": "set NMS\u201d: Multiple Instance Prediction, which predicts multiple instances from a single location as proposed by [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 35
                            }
                        ],
                        "text": "As shown in Table 10, with MIP and Set NMS, improved performance is achieved under all the three metrics."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 43
                            }
                        ],
                        "text": "Finally, the results are merged by Set NMS [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 52
                            }
                        ],
                        "text": "After that, these predictions are merged by Set NMS [4], which skips the suppression for the boxes from the same location."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 214605673,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c255c8911593d6d5f22b4d7542811bfb06e788e2",
            "isKey": true,
            "numCitedBy": 53,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a simple yet effective proposal-based object detector, aiming at detecting highly-overlapped instances in crowded scenes. The key of our approach is to let each proposal predict a set of correlated instances rather than a single one in previous proposal-based frameworks. Equipped with new techniques such as EMD Loss and Set NMS, our detector can effectively handle the difficulty of detecting highly overlapped objects. On a FPN-Res50 baseline, our detector can obtain 4.9\\% AP gains on challenging CrowdHuman dataset and 1.0\\% $\\text{MR}^{-2}$ improvements on CityPersons dataset, without bells and whistles. Moreover, on less crowed datasets like COCO, our approach can still achieve moderate improvement, suggesting the proposed method is robust to crowdedness."
            },
            "slug": "Detection-in-Crowded-Scenes:-One-Proposal,-Multiple-Chu-Zheng",
            "title": {
                "fragments": [],
                "text": "Detection in Crowded Scenes: One Proposal, Multiple Predictions"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The key of the approach is to let each proposal predict a set of correlated instances rather than a single one in previous proposal-based frameworks, which can effectively handle the difficulty of detecting highly overlapped objects."
            },
            "venue": {
                "fragments": [],
                "text": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32647956"
                        ],
                        "name": "Hei Law",
                        "slug": "Hei-Law",
                        "structuredName": {
                            "firstName": "Hei",
                            "lastName": "Law",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hei Law"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153302678"
                        ],
                        "name": "Jia Deng",
                        "slug": "Jia-Deng",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Deng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "CornerNet [19] is a recently proposed one-stage anchorfree detector, which detects a pair of corners of a bounding box and groups them to form the final detected bounding box."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 66
                            }
                        ],
                        "text": "Moreover, FCOS also outperforms the previous anchor-free detector CornerNet [19] and CenterNet [9] while being much simpler since they requires to group corners with embedding vectors, which needs special design for the detector."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "anchor-free detector CornerNet [19] and CenterNet [9] while being much simpler since they requires to group corners"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 711,
                                "start": 702
                            }
                        ],
                        "text": "Quantitative results are shown in\n9 Method Backbone AP AP50 AP75 APS APM APL Two-stage methods: Faster R-CNN+++ [14] ResNet-101 34.9 55.7 37.4 15.6 38.7 50.9 Faster R-CNN w/ FPN [21] ResNet-101-FPN 36.2 59.1 39.0 18.2 39.0 48.2 Faster R-CNN by G-RMI [17] Inception-ResNet-v2 [37] 34.7 55.5 36.7 13.5 38.1 52.0 Faster R-CNN w/ TDM [36] Inception-ResNet-v2-TDM 36.8 57.7 39.2 16.2 39.8 52.1 One-stage methods: YOLOv2 [30] DarkNet-19 [30] 21.6 44.0 19.2 5.0 22.4 35.5 SSD513 [25] ResNet-101-SSD 31.2 50.4 33.3 10.2 34.5 49.8 YOLOv3 608\u00d7 608 [31] Darknet-53 33.0 57.9 34.4 18.3 35.4 41.9 DSSD513 [10] ResNet-101-DSSD 33.2 53.3 35.2 13.0 35.4 51.1 RetinaNet [22] ResNet-101-FPN 39.1 59.1 42.3 21.8 42.7 50.2 CornerNet [19] Hourglass-104 40.5 56.5 43.1 19.4 42.7 53.9 FSAF [54] ResNeXt-64x4d-101-FPN 42.9 63.8 46.3 26.6 46.2 52.7 CenterNet511 [9] Hourglass-104 44.9 62.4 48.1 25.6 47.4 57.4 FCOS ResNet-101-FPN 43.2 62.4 46.8 26.1 46.2 52.8 FCOS ResNeXt-32x8d-101-FPN 44.1 63.7 47.9 27.4 46.8 53.7 FCOS ResNeXt-64x4d-101-FPN 44.8 64.4 48.5 27.7 47.4 55.0 FCOS w/ deform. conv. v2 [55] ResNeXt-32x8d-101-FPN 46.6 65.9 50.8 28.6 49.1 58.6 FCOS ResNet-101-BiFPN [38] 45.0 63.6 48.7 27.0 47.9 55.9 FCOS ResNeXt-32x8d-101-BiFPN 46.2 65.2 50.0 28.7 49.1 56.5 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-BiFPN 47.9 66.9 51.9 30.2 50.3 59.9 w/ test-time augmentation: FCOS ResNet-101-FPN 45.9 64.5 50.4 29.4 48.3 56.1 FCOS ResNeXt-32x8d-101-FPN 47.0 66.0 51.6 30.7 49.4 57.1 FCOS ResNeXt-64x4d-101-FPN 47.5 66.4 51.9 31.4 49.7 58.2 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-FPN 49.1 68.0 53.9 31.7 51.6 61.0 FCOS ResNet-101-BiFPN 47.9 65.9 52.5 31.0 50.7 59.7 FCOS ResNeXt-32x8d-101-BiFPN 49.0 67.4 53.6 32.0 51.7 60.5 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-BiFPN 50.4 68.9 55.0 33.2 53.0 62.7\nTABLE 8 FCOS vs. other state-of-the-art two-stage or one-stage detectors (single-model results)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 0
                            }
                        ],
                        "text": "CornerNet requires much more complicated postprocessing to group the pairs of corners belonging to the same instance."
                    },
                    "intents": []
                }
            ],
            "corpusId": 51923817,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "16028acc465c434a43599f0f900c91f38d690e02",
            "isKey": true,
            "numCitedBy": 1416,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose CornerNet, a new approach to object detection where we detect an object bounding box as a pair of keypoints, the top-left corner and the bottom-right corner, using a single convolution neural network. By detecting objects as paired keypoints, we eliminate the need for designing a set of anchor boxes commonly used in prior single-stage detectors. In addition to our novel formulation, we introduce corner pooling, a new type of pooling layer that helps the network better localize corners. Experiments show that CornerNet achieves a 42.2% AP on MS COCO, outperforming all existing one-stage detectors."
            },
            "slug": "CornerNet:-Detecting-Objects-as-Paired-Keypoints-Law-Deng",
            "title": {
                "fragments": [],
                "text": "CornerNet: Detecting Objects as Paired Keypoints"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "CornerNet, a new approach to object detection where an object bounding box is detected as a pair of keypoints, the top-left corner and the bottom-right corner, using a single convolution neural network, is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144439048"
                        ],
                        "name": "Xiang Li",
                        "slug": "Xiang-Li",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiang Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71074736"
                        ],
                        "name": "Wenhai Wang",
                        "slug": "Wenhai-Wang",
                        "structuredName": {
                            "firstName": "Wenhai",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenhai Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47767791"
                        ],
                        "name": "Lijun Wu",
                        "slug": "Lijun-Wu",
                        "structuredName": {
                            "firstName": "Lijun",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lijun Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15841516"
                        ],
                        "name": "Shuo Chen",
                        "slug": "Shuo-Chen",
                        "structuredName": {
                            "firstName": "Shuo",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuo Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109753669"
                        ],
                        "name": "Xiaolin Hu",
                        "slug": "Xiaolin-Hu",
                        "structuredName": {
                            "firstName": "Xiaolin",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaolin Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152748444"
                        ],
                        "name": "Jun Li",
                        "slug": "Jun-Li",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8053308"
                        ],
                        "name": "Jinhui Tang",
                        "slug": "Jinhui-Tang",
                        "structuredName": {
                            "firstName": "Jinhui",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jinhui Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146236917"
                        ],
                        "name": "Jian Yang",
                        "slug": "Jian-Yang",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "built upon FCOS [38], [39], [40], [41]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 219531292,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da60e046aac895b5775ed34bde45beb86aad0fe8",
            "isKey": false,
            "numCitedBy": 136,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "One-stage detector basically formulates object detection as dense classification and localization. The classification is usually optimized by Focal Loss and the box location is commonly learned under Dirac delta distribution. A recent trend for one-stage detectors is to introduce an individual prediction branch to estimate the quality of localization, where the predicted quality facilitates the classification to improve detection performance. This paper delves into the representations of the above three fundamental elements: quality estimation, classification and localization. Two problems are discovered in existing practices, including (1) the inconsistent usage of the quality estimation and classification between training and inference and (2) the inflexible Dirac delta distribution for localization when there is ambiguity and uncertainty in complex scenes. To address the problems, we design new representations for these elements. Specifically, we merge the quality estimation into the class prediction vector to form a joint representation of localization quality and classification, and use a vector to represent arbitrary distribution of box locations. The improved representations eliminate the inconsistency risk and accurately depict the flexible distribution in real data, but contain continuous labels, which is beyond the scope of Focal Loss. We then propose Generalized Focal Loss (GFL) that generalizes Focal Loss from its discrete form to the continuous version for successful optimization. On COCO test-dev, GFL achieves 45.0\\% AP using ResNet-101 backbone, surpassing state-of-the-art SAPD (43.5\\%) and ATSS (43.6\\%) with higher or comparable inference speed, under the same backbone and training settings. Notably, our best model can achieve a single-model single-scale AP of 48.2\\%, at 10 FPS on a single 2080Ti GPU. Code and models are available at this https URL."
            },
            "slug": "Generalized-Focal-Loss:-Learning-Qualified-and-for-Li-Wang",
            "title": {
                "fragments": [],
                "text": "Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Improved representations of quality estimation into the class prediction vector to form a joint representation of localization quality and classification, and use a vector to represent arbitrary distribution of box locations are designed."
            },
            "venue": {
                "fragments": [],
                "text": "NeurIPS"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1390555183"
                        ],
                        "name": "Xingyi Zhou",
                        "slug": "Xingyi-Zhou",
                        "structuredName": {
                            "firstName": "Xingyi",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xingyi Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2774612"
                        ],
                        "name": "Dequan Wang",
                        "slug": "Dequan-Wang",
                        "structuredName": {
                            "firstName": "Dequan",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dequan Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2562966"
                        ],
                        "name": "Philipp Kr\u00e4henb\u00fchl",
                        "slug": "Philipp-Kr\u00e4henb\u00fchl",
                        "structuredName": {
                            "firstName": "Philipp",
                            "lastName": "Kr\u00e4henb\u00fchl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philipp Kr\u00e4henb\u00fchl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 85
                            }
                        ],
                        "text": "Moreover, FCOS also outperforms the previous anchor-free detector CornerNet [19] and CenterNet [9] while being much simpler since they requires to group corners with embedding vectors, which needs special design for the detector."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "Speed/accuracy trade-off between FCOS and several recent methods: CenterNet [52], YOLOv3 [31] and RetinaNet [22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "In order to compare with CenterNet [52], we share the towers (i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 832,
                                "start": 823
                            }
                        ],
                        "text": "Quantitative results are shown in\n9 Method Backbone AP AP50 AP75 APS APM APL Two-stage methods: Faster R-CNN+++ [14] ResNet-101 34.9 55.7 37.4 15.6 38.7 50.9 Faster R-CNN w/ FPN [21] ResNet-101-FPN 36.2 59.1 39.0 18.2 39.0 48.2 Faster R-CNN by G-RMI [17] Inception-ResNet-v2 [37] 34.7 55.5 36.7 13.5 38.1 52.0 Faster R-CNN w/ TDM [36] Inception-ResNet-v2-TDM 36.8 57.7 39.2 16.2 39.8 52.1 One-stage methods: YOLOv2 [30] DarkNet-19 [30] 21.6 44.0 19.2 5.0 22.4 35.5 SSD513 [25] ResNet-101-SSD 31.2 50.4 33.3 10.2 34.5 49.8 YOLOv3 608\u00d7 608 [31] Darknet-53 33.0 57.9 34.4 18.3 35.4 41.9 DSSD513 [10] ResNet-101-DSSD 33.2 53.3 35.2 13.0 35.4 51.1 RetinaNet [22] ResNet-101-FPN 39.1 59.1 42.3 21.8 42.7 50.2 CornerNet [19] Hourglass-104 40.5 56.5 43.1 19.4 42.7 53.9 FSAF [54] ResNeXt-64x4d-101-FPN 42.9 63.8 46.3 26.6 46.2 52.7 CenterNet511 [9] Hourglass-104 44.9 62.4 48.1 25.6 47.4 57.4 FCOS ResNet-101-FPN 43.2 62.4 46.8 26.1 46.2 52.8 FCOS ResNeXt-32x8d-101-FPN 44.1 63.7 47.9 27.4 46.8 53.7 FCOS ResNeXt-64x4d-101-FPN 44.8 64.4 48.5 27.7 47.4 55.0 FCOS w/ deform. conv. v2 [55] ResNeXt-32x8d-101-FPN 46.6 65.9 50.8 28.6 49.1 58.6 FCOS ResNet-101-BiFPN [38] 45.0 63.6 48.7 27.0 47.9 55.9 FCOS ResNeXt-32x8d-101-BiFPN 46.2 65.2 50.0 28.7 49.1 56.5 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-BiFPN 47.9 66.9 51.9 30.2 50.3 59.9 w/ test-time augmentation: FCOS ResNet-101-FPN 45.9 64.5 50.4 29.4 48.3 56.1 FCOS ResNeXt-32x8d-101-FPN 47.0 66.0 51.6 30.7 49.4 57.1 FCOS ResNeXt-64x4d-101-FPN 47.5 66.4 51.9 31.4 49.7 58.2 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-FPN 49.1 68.0 53.9 31.7 51.6 61.0 FCOS ResNet-101-BiFPN 47.9 65.9 52.5 31.0 50.7 59.7 FCOS ResNeXt-32x8d-101-BiFPN 49.0 67.4 53.6 32.0 51.7 60.5 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-BiFPN 50.4 68.9 55.0 33.2 53.0 62.7\nTABLE 8 FCOS vs. other state-of-the-art two-stage or one-stage detectors (single-model results)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 182
                            }
                        ],
                        "text": "Here (x(i)0 , y (i) 0 ) and (x (i) 1 y (i) 1 ) denote the coordinates of the left-top and right-bottom corners of the\n4 20 30 40 50 60 70 FPS 28 30 32 34 36 38 40 42 AP FCOS-RT FCOS CenterNet YOLOv3 RetinaNet\nFig."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "However, as shown in Table 9, the model still outperforms CenterNet [52] by 1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 58
                            }
                        ],
                        "text": "However, as shown in Table 9, the model still outperforms CenterNet [52] by 1.7% AP at the same speed."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 25
                            }
                        ],
                        "text": "In order to compare with CenterNet [52], we share the towers (i.e., 4\u00d7 conv. layers shown in Fig."
                    },
                    "intents": []
                }
            ],
            "corpusId": 118714035,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a2e2fd1b5bb11224daef98b3fb6d029f68a73f2",
            "isKey": true,
            "numCitedBy": 1095,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "Detection identifies objects as axis-aligned boxes in an image. Most successful object detectors enumerate a nearly exhaustive list of potential object locations and classify each. This is wasteful, inefficient, and requires additional post-processing. In this paper, we take a different approach. We model an object as a single point --- the center point of its bounding box. Our detector uses keypoint estimation to find center points and regresses to all other object properties, such as size, 3D location, orientation, and even pose. Our center point based approach, CenterNet, is end-to-end differentiable, simpler, faster, and more accurate than corresponding bounding box based detectors. CenterNet achieves the best speed-accuracy trade-off on the MS COCO dataset, with 28.1% AP at 142 FPS, 37.4% AP at 52 FPS, and 45.1% AP with multi-scale testing at 1.4 FPS. We use the same approach to estimate 3D bounding box in the KITTI benchmark and human pose on the COCO keypoint dataset. Our method performs competitively with sophisticated multi-stage methods and runs in real-time."
            },
            "slug": "Objects-as-Points-Zhou-Wang",
            "title": {
                "fragments": [],
                "text": "Objects as Points"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The center point based approach, CenterNet, is end-to-end differentiable, simpler, faster, and more accurate than corresponding bounding box based detectors and performs competitively with sophisticated multi-stage methods and runs in real-time."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108353180"
                        ],
                        "name": "Yuliang Liu",
                        "slug": "Yuliang-Liu",
                        "structuredName": {
                            "firstName": "Yuliang",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuliang Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2029503517"
                        ],
                        "name": "Hao Chen",
                        "slug": "Hao-Chen",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12459603"
                        ],
                        "name": "Chunhua Shen",
                        "slug": "Chunhua-Shen",
                        "structuredName": {
                            "firstName": "Chunhua",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunhua Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118328320"
                        ],
                        "name": "Tong He",
                        "slug": "Tong-He",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144838978"
                        ],
                        "name": "Lianwen Jin",
                        "slug": "Lianwen-Jin",
                        "structuredName": {
                            "firstName": "Lianwen",
                            "lastName": "Jin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lianwen Jin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144691690"
                        ],
                        "name": "Liangwei Wang",
                        "slug": "Liangwei-Wang",
                        "structuredName": {
                            "firstName": "Liangwei",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liangwei Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 259,
                                "start": 255
                            }
                        ],
                        "text": "More importantly, due to its simple design, FCOS can be easily extended to solve other instance-level recognition tasks with minimal modification, as already evidenced by instance segmentation [2], [20], [47], [51], keypoint detection [39], text spotting [26], and tracking [13], [44]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 211258587,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3da15c04db020c629aeb71b856cfdc7127b677ff",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Scene text detection and recognition has received increasing research attention. Existing methods can be roughly categorized into two groups: character-based and segmentation-based. These methods either are costly for character annotation or need to maintain a complex pipeline, which is often not suitable for real-time applications. Here we address the problem by proposing the Adaptive Bezier-Curve Network (\\BeCan). Our contributions are three-fold: 1) For the first time, we adaptively fit oriented or curved text by a parameterized Bezier curve. 2) We design a novel BezierAlign layer for extracting accurate convolution features of a text instance with arbitrary shapes, significantly improving the precision compared with previous methods. 3) Compared with standard bounding box detection, our Bezier curve detection introduces negligible computation overhead, resulting in superiority of our method in both efficiency and accuracy. Experiments on oriented or curved benchmark datasets, namely Total-Text and CTW1500, demonstrate that \\BeCan achieves state-of-the-art accuracy, meanwhile significantly improving the speed. In particular, on Total-Text, our real-time version is over 10 times faster than recent state-of-the-art methods with a competitive recognition accuracy. Code is available at \\url{https://git.io/AdelaiDet}."
            },
            "slug": "ABCNet:-Real-Time-Scene-Text-Spotting-With-Adaptive-Liu-Chen",
            "title": {
                "fragments": [],
                "text": "ABCNet: Real-Time Scene Text Spotting With Adaptive Bezier-Curve Network"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "For the first time, a novel BezierAlign layer is designed for extracting accurate convolution features of a text instance with arbitrary shapes, significantly improving the precision compared with previous methods and introducing negligible computation overhead."
            },
            "venue": {
                "fragments": [],
                "text": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 48
                            }
                        ],
                        "text": "All current mainstream detectors such as Faster R-CNN [32], SSD [25] and YOLOv2, v3 [31] rely on a set of pre-defined anchor boxes and it has\nAuthors are with The University of Adelaide, Australia."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 135
                            }
                        ],
                        "text": "With deep learning, detection has been largely shifted to the use of fully convolutional networks (FCNs) since the invention of Faster R-CNN [32]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 123
                            }
                        ],
                        "text": "Despite their great success, it is important to note that anchor-based detectors suffer some drawbacks:\n\u2022 As shown in Faster R-CNN and RetinaNet [22], detection performance is sensitive to the sizes, aspect ratios and number of anchor boxes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 130
                            }
                        ],
                        "text": "\u2022 With considerably reduced design complexity, our proposed detector outperforms previous strong baseline detectors such as Faster R-CNN [32], RetinaNet [22], YOLOv3 [31] and SSD [25]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 103
                            }
                        ],
                        "text": "Quantitative results are shown in\n9 Method Backbone AP AP50 AP75 APS APM APL Two-stage methods: Faster R-CNN+++ [14] ResNet-101 34.9 55.7 37.4 15.6 38.7 50.9 Faster R-CNN w/ FPN [21] ResNet-101-FPN 36.2 59.1 39.0 18.2 39.0 48.2 Faster R-CNN by G-RMI [17] Inception-ResNet-v2 [37] 34.7 55.5 36.7 13.5 38.1 52.0 Faster R-CNN w/ TDM [36] Inception-ResNet-v2-TDM 36.8 57.7 39.2 16.2 39.8 52.1 One-stage methods: YOLOv2 [30] DarkNet-19 [30] 21.6 44.0 19.2 5.0 22.4 35.5 SSD513 [25] ResNet-101-SSD 31.2 50.4 33.3 10.2 34.5 49.8 YOLOv3 608\u00d7 608 [31] Darknet-53 33.0 57.9 34.4 18.3 35.4 41.9 DSSD513 [10] ResNet-101-DSSD 33.2 53.3 35.2 13.0 35.4 51.1 RetinaNet [22] ResNet-101-FPN 39.1 59.1 42.3 21.8 42.7 50.2 CornerNet [19] Hourglass-104 40.5 56.5 43.1 19.4 42.7 53.9 FSAF [54] ResNeXt-64x4d-101-FPN 42.9 63.8 46.3 26.6 46.2 52.7 CenterNet511 [9] Hourglass-104 44.9 62.4 48.1 25.6 47.4 57.4 FCOS ResNet-101-FPN 43.2 62.4 46.8 26.1 46.2 52.8 FCOS ResNeXt-32x8d-101-FPN 44.1 63.7 47.9 27.4 46.8 53.7 FCOS ResNeXt-64x4d-101-FPN 44.8 64.4 48.5 27.7 47.4 55.0 FCOS w/ deform. conv. v2 [55] ResNeXt-32x8d-101-FPN 46.6 65.9 50.8 28.6 49.1 58.6 FCOS ResNet-101-BiFPN [38] 45.0 63.6 48.7 27.0 47.9 55.9 FCOS ResNeXt-32x8d-101-BiFPN 46.2 65.2 50.0 28.7 49.1 56.5 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-BiFPN 47.9 66.9 51.9 30.2 50.3 59.9 w/ test-time augmentation: FCOS ResNet-101-FPN 45.9 64.5 50.4 29.4 48.3 56.1 FCOS ResNeXt-32x8d-101-FPN 47.0 66.0 51.6 30.7 49.4 57.1 FCOS ResNeXt-64x4d-101-FPN 47.5 66.4 51.9 31.4 49.7 58.2 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-FPN 49.1 68.0 53.9 31.7 51.6 61.0 FCOS ResNet-101-BiFPN 47.9 65.9 52.5 31.0 50.7 59.7 FCOS ResNeXt-32x8d-101-BiFPN 49.0 67.4 53.6 32.0 51.7 60.5 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-BiFPN 50.4 68.9 55.0 33.2 53.0 62.7\nTABLE 8 FCOS vs. other state-of-the-art two-stage or one-stage detectors (single-model results)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 121
                            }
                        ],
                        "text": "Anchor-based detectors inherit the ideas from traditional sliding-window and proposal based detectors such as Fast R-CNN [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 53
                            }
                        ],
                        "text": "The design of anchor boxes are popularized by Faster R-CNN in its RPNs [32], SSD [25] and YOLOv2 [30], and has become the convention in a modern detector."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 83
                            }
                        ],
                        "text": "Moreover, FCOS also surpasses the classical two-stage anchor-based detector Faster R-CNN by a large margin (43.2% vs. 36.2%)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206770307,
            "fieldsOfStudy": [
                "Computer Science",
                "Environmental Science"
            ],
            "id": "7ffdbc358b63378f07311e883dddacc9faeeaf4b",
            "isKey": true,
            "numCitedBy": 14072,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn."
            },
            "slug": "Fast-R-CNN-Girshick",
            "title": {
                "fragments": [],
                "text": "Fast R-CNN"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection that builds on previous work to efficiently classify object proposals using deep convolutional networks."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48527910"
                        ],
                        "name": "P. Doll\u00e1r",
                        "slug": "P.-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2340109"
                        ],
                        "name": "C. Wojek",
                        "slug": "C.-Wojek",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Wojek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Wojek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 141
                            }
                        ],
                        "text": "Following previous works on crowded benchmark [4], [34], we use AP, long-average Miss Rate on False Positive Per Image in [10\u22122, 100] (MR\u22122) [8] and Jaccard Index (JI) as the evaluation metrics."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 36
                            }
                        ],
                        "text": "We find that it has large impact on MR\u22122 and JI."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 16
                            }
                        ],
                        "text": "Note that lower MR\u22122 is better."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": "The higher MR\u22122 of FCOS denotes that FCOS might have a large number of false positives with high confidence."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 26
                            }
                        ],
                        "text": "By using the center-ness, MR\u22122 can be significantly reduced from 59.04% to 51.34%."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206764948,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "34e0ba2daabfa4d3d22913ade8265aff50b5f917",
            "isKey": true,
            "numCitedBy": 2761,
            "numCiting": 116,
            "paperAbstract": {
                "fragments": [],
                "text": "Pedestrian detection is a key problem in computer vision, with several applications that have the potential to positively impact quality of life. In recent years, the number of approaches to detecting pedestrians in monocular images has grown steadily. However, multiple data sets and widely varying evaluation protocols are used, making direct comparisons difficult. To address these shortcomings, we perform an extensive evaluation of the state of the art in a unified framework. We make three primary contributions: 1) We put together a large, well-annotated, and realistic monocular pedestrian detection data set and study the statistics of the size, position, and occlusion patterns of pedestrians in urban scenes, 2) we propose a refined per-frame evaluation methodology that allows us to carry out probing and informative comparisons, including measuring performance in relation to scale and occlusion, and 3) we evaluate the performance of sixteen pretrained state-of-the-art detectors across six data sets. Our study allows us to assess the state of the art and provides a framework for gauging future efforts. Our experiments show that despite significant progress, performance still has much room for improvement. In particular, detection is disappointing at low resolutions and for partially occluded pedestrians."
            },
            "slug": "Pedestrian-Detection:-An-Evaluation-of-the-State-of-Doll\u00e1r-Wojek",
            "title": {
                "fragments": [],
                "text": "Pedestrian Detection: An Evaluation of the State of the Art"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An extensive evaluation of the state of the art in a unified framework of monocular pedestrian detection using sixteen pretrained state-of-the-art detectors across six data sets and proposes a refined per-frame evaluation methodology."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118328320"
                        ],
                        "name": "Tong He",
                        "slug": "Tong-He",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780381"
                        ],
                        "name": "Chunhua Shen",
                        "slug": "Chunhua-Shen",
                        "structuredName": {
                            "firstName": "Chunhua",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunhua Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40219976"
                        ],
                        "name": "Zhi Tian",
                        "slug": "Zhi-Tian",
                        "structuredName": {
                            "firstName": "Zhi",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi Tian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145542268"
                        ],
                        "name": "Dong Gong",
                        "slug": "Dong-Gong",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Gong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755278"
                        ],
                        "name": "Changming Sun",
                        "slug": "Changming-Sun",
                        "structuredName": {
                            "firstName": "Changming",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Changming Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2376903"
                        ],
                        "name": "Youliang Yan",
                        "slug": "Youliang-Yan",
                        "structuredName": {
                            "firstName": "Youliang",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Youliang Yan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "in dense prediction tasks such as semantic segmentation [15], [28], [40], depth estimation [24], [48], keypoint detection [3] and counting."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 75137175,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7e2c8c52c5a0202c11d9d6b3fe3427976608c1b",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Both accuracy and efficiency are of significant importance to the task of semantic segmentation. Existing deep FCNs suffer from heavy computations due to a series of high-resolution feature maps for preserving the detailed knowledge in dense estimation. Although reducing the feature map resolution (i.e., applying a large overall stride) via subsampling operations (e.g., polling and convolution striding) can instantly increase the efficiency, it dramatically decreases the estimation accuracy. To tackle this dilemma, we propose a knowledge distillation method tailored for semantic segmentation to improve the performance of the compact FCNs with large overall stride. To handle the inconsistency between the features of the student and teacher network, we optimize the feature similarity in a transferred latent domain formulated by utilizing a pre-trained autoencoder. Moreover, an affinity distillation module is proposed to capture the long-range dependency by calculating the non local interactions across the whole image. To validate the effectiveness of our proposed method, extensive experiments have been conducted on three popular benchmarks: Pascal VOC, Cityscapes and Pascal Context. Built upon a highly competitive baseline, our proposed method can improve the performance of a student network by 2.5% (mIOU boosts from 70.2 to 72.7 on the cityscapes test set) and can train a better compact model with only 8% float operations (FLOPS) of a model that achieves comparable performances."
            },
            "slug": "Knowledge-Adaptation-for-Efficient-Semantic-He-Shen",
            "title": {
                "fragments": [],
                "text": "Knowledge Adaptation for Efficient Semantic Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work proposes a knowledge distillation method tailored for semantic segmentation to improve the performance of the compact FCNs with large overall stride and optimize the feature similarity in a transferred latent domain formulated by utilizing a pre-trained autoencoder."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40219976"
                        ],
                        "name": "Zhi Tian",
                        "slug": "Zhi-Tian",
                        "structuredName": {
                            "firstName": "Zhi",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi Tian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118328320"
                        ],
                        "name": "Tong He",
                        "slug": "Tong-He",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780381"
                        ],
                        "name": "Chunhua Shen",
                        "slug": "Chunhua-Shen",
                        "structuredName": {
                            "firstName": "Chunhua",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunhua Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2376903"
                        ],
                        "name": "Youliang Yan",
                        "slug": "Youliang-Yan",
                        "structuredName": {
                            "firstName": "Youliang",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Youliang Yan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "in dense prediction tasks such as semantic segmentation [15], [28], [40], depth estimation [24], [48], keypoint detection [3] and counting."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 70349981,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "52dd94bed56a9d8bfdad352a4aa42377a873e99f",
            "isKey": false,
            "numCitedBy": 121,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent semantic segmentation methods exploit encoder-decoder architectures to produce the desired pixel-wise segmentation prediction. The last layer of the decoders is typically a bilinear upsampling procedure to recover the final pixel-wise prediction. We empirically show that this oversimple and data-independent bilinear upsampling may lead to sub-optimal results. In this work, we propose a data-dependent upsampling (DUpsampling) to replace bilinear, which takes advantages of the redundancy in the label space of semantic segmentation and is able to recover the pixel-wise prediction from low-resolution outputs of CNNs. The main advantage of the new upsampling layer lies in that with a relatively lower-resolution feature map such as 1/16 or 1/32 of the input size, we can achieve even better segmentation accuracy, significantly reducing computation complexity. This is made possible by 1) the new upsampling layer's much improved reconstruction capability; and more importantly 2) the DUpsampling based decoder's flexibility in leveraging almost arbitrary combinations of the CNN encoders' features. Experiments on PASCAL VOC demonstrate that with much less computation complexity, our decoder outperforms the state-of-the-art decoder. Finally, without any post-processing, the framework equipped with our proposed decoder achieves new state-of-the-art performance on two datasets: 88.1% mIOU on PASCAL VOC with 30% computation of the previously best model; and 52.5% mIOU on PASCAL Context."
            },
            "slug": "Decoders-Matter-for-Semantic-Segmentation:-Decoding-Tian-He",
            "title": {
                "fragments": [],
                "text": "Decoders Matter for Semantic Segmentation: Data-Dependent Decoding Enables Flexible Feature Aggregation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes a data-dependent upsampling (DUpsampling) to replace bilinear, which takes advantages of the redundancy in the label space of semantic segmentation and is able to recover the pixel-wise prediction from low-resolution outputs of CNNs."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48527910"
                        ],
                        "name": "P. Doll\u00e1r",
                        "slug": "P.-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060858809"
                        ],
                        "name": "Ron Appel",
                        "slug": "Ron-Appel",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Appel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ron Appel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 72
                            }
                        ],
                        "text": "Prior to deep learning, the sliding-window approach was the main method [7], [35], [43], which exhaustively classifies every possible location, thus requiring"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 219903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "84e0d68e41788644c78cfdc3f4ac3cbea7854a5c",
            "isKey": false,
            "numCitedBy": 1707,
            "numCiting": 80,
            "paperAbstract": {
                "fragments": [],
                "text": "Multi-resolution image features may be approximated via extrapolation from nearby scales, rather than being computed explicitly. This fundamental insight allows us to design object detection algorithms that are as accurate, and considerably faster, than the state-of-the-art. The computational bottleneck of many modern detectors is the computation of features at every scale of a finely-sampled image pyramid. Our key insight is that one may compute finely sampled feature pyramids at a fraction of the cost, without sacrificing performance: for a broad family of features we find that features computed at octave-spaced scale intervals are sufficient to approximate features on a finely-sampled pyramid. Extrapolation is inexpensive as compared to direct feature computation. As a result, our approximation yields considerable speedups with negligible loss in detection accuracy. We modify three diverse visual recognition systems to use fast feature pyramids and show results on both pedestrian detection (measured on the Caltech, INRIA, TUD-Brussels and ETH data sets) and general object detection (measured on the PASCAL VOC). The approach is general and is widely applicable to vision algorithms requiring fine-grained multi-scale analysis. Our approximation is valid for images with broad spectra (most natural images) and fails for images with narrow band-pass spectra (e.g., periodic textures)."
            },
            "slug": "Fast-Feature-Pyramids-for-Object-Detection-Doll\u00e1r-Appel",
            "title": {
                "fragments": [],
                "text": "Fast Feature Pyramids for Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "For a broad family of features, this work finds that features computed at octave-spaced scale intervals are sufficient to approximate features on a finely-sampled pyramid, and this approximation yields considerable speedups with negligible loss in detection accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40497777"
                        ],
                        "name": "Joseph Redmon",
                        "slug": "Joseph-Redmon",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Redmon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph Redmon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143787583"
                        ],
                        "name": "Ali Farhadi",
                        "slug": "Ali-Farhadi",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Farhadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ali Farhadi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 133
                            }
                        ],
                        "text": "However, since only points near the center are used to predict bounding boxes, YOLOv1 suffers from low recall as mentioned in YOLOv2 [30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "As a result, YOLOv2 [30] employs anchor boxes as well."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": "The design of anchor boxes are popularized by Faster R-CNN in its RPNs [32], SSD [25] and YOLOv2 [30], and has become the"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "1 One-stage methods: YOLOv2 [30] DarkNet-19 [30] 21."
                    },
                    "intents": []
                }
            ],
            "corpusId": 786357,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d39d69b23424446f0400ef603b2e3e22d0309d6",
            "isKey": true,
            "numCitedBy": 7933,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that dont have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. YOLO9000 predicts detections for more than 9000 different object categories, all in real-time."
            },
            "slug": "YOLO9000:-Better,-Faster,-Stronger-Redmon-Farhadi",
            "title": {
                "fragments": [],
                "text": "YOLO9000: Better, Faster, Stronger"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories, is introduced and a method to jointly train on object detection and classification is proposed, both novel and drawn from prior work."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731948"
                        ],
                        "name": "Paul A. Viola",
                        "slug": "Paul-A.-Viola",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Viola",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul A. Viola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111328101"
                        ],
                        "name": "Michael Jones",
                        "slug": "Michael-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 83
                            }
                        ],
                        "text": "Prior to deep learning, the sliding-window approach was the main method [7], [35], [43], which exhaustively classifies every possible location, thus requiring"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 235084,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4cb4d685b47001652b29dc41c1b3e786277e7647",
            "isKey": false,
            "numCitedBy": 4016,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a visual object detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the \u201cIntegral Image\u201d which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features and yields extremely efficient classifiers [4]. The third contribution is a method for combining classifiers in a \u201ccascade\u201d which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. A set of experiments in the domain of face detection are presented. The system yields face detection performance comparable to the best previous systems [16, 11, 14, 10, 1]. Implemented on a conventional desktop, face detection proceeds at 15 frames per second. Author email: fPaul.Viola,Mike.J.Jonesg@compaq.com c Compaq Computer Corporation, 2001 This work may not be copied or reproduced in whole or in part for any commercial purpose. Permission to copy in whole or in part without payment of fee is granted for nonprofit educational and research purposes provided that all such whole or partial copies include the following: a notice that such copying is by permission of the Cambridge Research Laboratory of Compaq Computer Corporation in Cambridge, Massachusetts; an acknowledgment of the authors and individual contributors to the work; and all applicable portions of the copyright notice. Copying, reproducing, or republishing for any other purpose shall require a license with payment of fee to the Cambridge Research Laboratory. All rights reserved. CRL Technical reports are available on the CRL\u2019s web page at http://crl.research.compaq.com. Compaq Computer Corporation Cambridge Research Laboratory One Cambridge Center Cambridge, Massachusetts 02142 USA"
            },
            "slug": "Robust-Real-time-Object-Detection-Viola-Jones",
            "title": {
                "fragments": [],
                "text": "Robust Real-time Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A visual object detection framework that is capable of processing images extremely rapidly while achieving high detection rates is described, with the introduction of a new image representation called the \u201cIntegral Image\u201d which allows the features used by the detector to be computed very quickly."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2578924"
                        ],
                        "name": "Xizhou Zhu",
                        "slug": "Xizhou-Zhu",
                        "structuredName": {
                            "firstName": "Xizhou",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xizhou Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1823518756"
                        ],
                        "name": "Han Hu",
                        "slug": "Han-Hu",
                        "structuredName": {
                            "firstName": "Han",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Han Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145676588"
                        ],
                        "name": "Stephen Lin",
                        "slug": "Stephen-Lin",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3304536"
                        ],
                        "name": "Jifeng Dai",
                        "slug": "Jifeng-Dai",
                        "structuredName": {
                            "firstName": "Jifeng",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jifeng Dai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "As shown in Table 8, by applying deformable convolutions [5], [55] to ResNeXt-32x8d-101-FPN based FCOS, the performance is improved from 44."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 53745820,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "987b2db58fbe0bda771f11a046cd23de1ce92b39",
            "isKey": false,
            "numCitedBy": 683,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The superior performance of Deformable Convolutional Networks arises from its ability to adapt to the geometric variations of objects. Through an examination of its adaptive behavior, we observe that while the spatial support for its neural features conforms more closely than regular ConvNets to object structure, this support may nevertheless extend well beyond the region of interest, causing features to be influenced by irrelevant image content. To address this problem, we present a reformulation of Deformable ConvNets that improves its ability to focus on pertinent image regions, through increased modeling power and stronger training. The modeling power is enhanced through a more comprehensive integration of deformable convolution within the network, and by introducing a modulation mechanism that expands the scope of deformation modeling. To effectively harness this enriched modeling capability, we guide network training via a proposed feature mimicking scheme that helps the network to learn features that reflect the object focus and classification power of R-CNN features. With the proposed contributions, this new version of Deformable ConvNets yields significant performance gains over the original model and produces leading results on the COCO benchmark for object detection and instance segmentation."
            },
            "slug": "Deformable-ConvNets-V2:-More-Deformable,-Better-Zhu-Hu",
            "title": {
                "fragments": [],
                "text": "Deformable ConvNets V2: More Deformable, Better Results"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This work presents a reformulation of Deformable Convolutional Networks that improves its ability to focus on pertinent image regions, through increased modeling power and stronger training, and guides network training via a proposed feature mimicking scheme that helps the network to learn features that reflect the object focus and classification power of R-CNN features."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145599121"
                        ],
                        "name": "Shifeng Zhang",
                        "slug": "Shifeng-Zhang",
                        "structuredName": {
                            "firstName": "Shifeng",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shifeng Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143700133"
                        ],
                        "name": "Cheng Chi",
                        "slug": "Cheng-Chi",
                        "structuredName": {
                            "firstName": "Cheng",
                            "lastName": "Chi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheng Chi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110253898"
                        ],
                        "name": "Yongqiang Yao",
                        "slug": "Yongqiang-Yao",
                        "structuredName": {
                            "firstName": "Yongqiang",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yongqiang Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145754448"
                        ],
                        "name": "Zhen Lei",
                        "slug": "Zhen-Lei",
                        "structuredName": {
                            "firstName": "Zhen",
                            "lastName": "Lei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhen Lei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1390908654"
                        ],
                        "name": "Stan Z. Li",
                        "slug": "Stan-Z.-Li",
                        "structuredName": {
                            "firstName": "Stan Z.",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stan Z. Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "built upon FCOS [38], [39], [40], [41]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 208637257,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "448529da2bf004cf79084401ad3cbd6b511e4969",
            "isKey": false,
            "numCitedBy": 363,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "Object detection has been dominated by anchor-based detectors for several years. Recently, anchor-free detectors have become popular due to the proposal of FPN and Focal Loss. In this paper, we first point out that the essential difference between anchor-based and anchor-free detection is actually how to define positive and negative training samples, which leads to the performance gap between them. If they adopt the same definition of positive and negative samples during training, there is no obvious difference in the final performance, no matter regressing from a box or a point. This shows that how to select positive and negative training samples is important for current object detectors. Then, we propose an Adaptive Training Sample Selection (ATSS) to automatically select positive and negative samples according to statistical characteristics of object. It significantly improves the performance of anchor-based and anchor-free detectors and bridges the gap between them. Finally, we discuss the necessity of tiling multiple anchors per location on the image to detect objects. Extensive experiments conducted on MS COCO support our aforementioned analysis and conclusions. With the newly introduced ATSS, we improve state-of-the-art detectors by a large margin to 50.7% AP without introducing any overhead. The code is available at https://github.com/sfzhang15/ATSS."
            },
            "slug": "Bridging-the-Gap-Between-Anchor-Based-and-Detection-Zhang-Chi",
            "title": {
                "fragments": [],
                "text": "Bridging the Gap Between Anchor-Based and Anchor-Free Detection via Adaptive Training Sample Selection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An Adaptive Training Sample Selection (ATSS) to automatically select positive and negative samples according to statistical characteristics of object significantly improves the performance of anchor-based and anchor-free detectors and bridges the gap between them."
            },
            "venue": {
                "fragments": [],
                "text": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "98264506"
                        ],
                        "name": "Yuxin Wu",
                        "slug": "Yuxin-Wu",
                        "structuredName": {
                            "firstName": "Yuxin",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuxin Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "As shown Table 5, removing group normalization (GN) [45] in both the classification and regression heads drops the performance by 1% AP."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 85
                            }
                        ],
                        "text": "RetinaNet with the universal improvements in FCOS including Group Normalization (GN) [45], GIoU loss [1] and scalars in regression, using P5 instead of C5 and NMS threshold 0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4076251,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f7919a5677290ab2eca4fa8056bdbbf7c0b11d6",
            "isKey": false,
            "numCitedBy": 1620,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems\u2014BN\u2019s error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN\u2019s usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN\u2019s computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BN-based counterparts for object detection and segmentation in COCO ( https://github.com/facebookresearch/Detectron/blob/master/projects/GN ), and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries."
            },
            "slug": "Group-Normalization-Wu-He",
            "title": {
                "fragments": [],
                "text": "Group Normalization"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Group Normalization (GN) is presented as a simple alternative to BN that can outperform its BN-based counterparts for object detection and segmentation in COCO, and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2143993776"
                        ],
                        "name": "Shuai Shao",
                        "slug": "Shuai-Shao",
                        "structuredName": {
                            "firstName": "Shuai",
                            "lastName": "Shao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuai Shao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117928374"
                        ],
                        "name": "Zijian Zhao",
                        "slug": "Zijian-Zhao",
                        "structuredName": {
                            "firstName": "Zijian",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zijian Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2143356219"
                        ],
                        "name": "Boxun Li",
                        "slug": "Boxun-Li",
                        "structuredName": {
                            "firstName": "Boxun",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Boxun Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15727192"
                        ],
                        "name": "Tete Xiao",
                        "slug": "Tete-Xiao",
                        "structuredName": {
                            "firstName": "Tete",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tete Xiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116565951"
                        ],
                        "name": "Gang Yu",
                        "slug": "Gang-Yu",
                        "structuredName": {
                            "firstName": "Gang",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gang Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50875121"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "Xiangyu",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "Following previous works on crowded benchmark [4], [34], we use AP, long-average Miss Rate on False Positive Per Image in [10\u22122, 100] (MR\u22122) [8] and Jaccard Index (JI) as the evaluation metrics."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 19933351,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03a65d274dc6caea94f6ab344e0b4969575327e3",
            "isKey": false,
            "numCitedBy": 222,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Human detection has witnessed impressive progress in recent years. However, the occlusion issue of detecting human in highly crowded environments is far from solved. To make matters worse, crowd scenarios are still under-represented in current human detection benchmarks. In this paper, we introduce a new dataset, called CrowdHuman, to better evaluate detectors in crowd scenarios. The CrowdHuman dataset is large, rich-annotated and contains high diversity. There are a total of $470K$ human instances from the train and validation subsets, and $~22.6$ persons per image, with various kinds of occlusions in the dataset. Each human instance is annotated with a head bounding-box, human visible-region bounding-box and human full-body bounding-box. Baseline performance of state-of-the-art detection frameworks on CrowdHuman is presented. The cross-dataset generalization results of CrowdHuman dataset demonstrate state-of-the-art performance on previous dataset including Caltech-USA, CityPersons, and Brainwash without bells and whistles. We hope our dataset will serve as a solid baseline and help promote future research in human detection tasks."
            },
            "slug": "CrowdHuman:-A-Benchmark-for-Detecting-Human-in-a-Shao-Zhao",
            "title": {
                "fragments": [],
                "text": "CrowdHuman: A Benchmark for Detecting Human in a Crowd"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "The cross-dataset generalization results of CrowdHuman dataset demonstrate state-of-the-art performance on previous dataset including Caltech-USA, CityPersons, and Brainwash without bells and whistles."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47559011"
                        ],
                        "name": "Y. Chen",
                        "slug": "Y.-Chen",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12459603"
                        ],
                        "name": "Chunhua Shen",
                        "slug": "Chunhua-Shen",
                        "structuredName": {
                            "firstName": "Chunhua",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunhua Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2126047"
                        ],
                        "name": "Xiu-Shen Wei",
                        "slug": "Xiu-Shen-Wei",
                        "structuredName": {
                            "firstName": "Xiu-Shen",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiu-Shen Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2161037"
                        ],
                        "name": "Lingqiao Liu",
                        "slug": "Lingqiao-Liu",
                        "structuredName": {
                            "firstName": "Lingqiao",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lingqiao Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46478067"
                        ],
                        "name": "Jingqing Yang",
                        "slug": "Jingqing-Yang",
                        "structuredName": {
                            "firstName": "Jingqing",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jingqing Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 122
                            }
                        ],
                        "text": "in dense prediction tasks such as semantic segmentation [15], [28], [40], depth estimation [24], [48], keypoint detection [3] and counting."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206770818,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d66f875913ff61bccedee35d554bde5408ef4461",
            "isKey": false,
            "numCitedBy": 233,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "For human pose estimation in monocular images, joint occlusions and overlapping upon human bodies often result in deviated pose predictions. Under these circumstances, biologically implausible pose predictions may be produced. In contrast, human vision is able to predict poses by exploiting geometric constraints of joint inter-connectivity. To address the problem by incorporating priors about the structure of human bodies, we propose a novel structure-aware convolutional network to implicitly take such priors into account during training of the deep network. Explicit learning of such constraints is typically challenging. Instead, we design discriminators to distinguish the real poses from the fake ones (such as biologically implausible ones). If the pose generator (G) generates results that the discriminator fails to distinguish from real ones, the network successfully learns the priors.,,To better capture the structure dependency of human body joints, the generator G is designed in a stacked multi-task manner to predict poses as well as occlusion heatmaps. Then, the pose and occlusion heatmaps are sent to the discriminators to predict the likelihood of the pose being real. Training of the network follows the strategy of conditional Generative Adversarial Networks (GANs). The effectiveness of the proposed network is evaluated on two widely used human pose estimation benchmark datasets. Our approach significantly outperforms the state-of-the-art methods and almost always generates plausible human pose predictions."
            },
            "slug": "Adversarial-PoseNet:-A-Structure-Aware-Network-for-Chen-Shen",
            "title": {
                "fragments": [],
                "text": "Adversarial PoseNet: A Structure-Aware Convolutional Network for Human Pose Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes a novel structure-aware convolutional network to implicitly take priors about the structure of human bodies into account during training of the deep network and significantly outperforms the state-of-the-art methods and almost always generates plausible human pose predictions."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771551"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "ResNet-50 [14] is used as our backbone networks and the same hyper-parameters with Reti-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 96
                            }
                        ],
                        "text": "Quantitative results are shown in\n9 Method Backbone AP AP50 AP75 APS APM APL Two-stage methods: Faster R-CNN+++ [14] ResNet-101 34.9 55.7 37.4 15.6 38.7 50.9 Faster R-CNN w/ FPN [21] ResNet-101-FPN 36.2 59.1 39.0 18.2 39.0 48.2 Faster R-CNN by G-RMI [17] Inception-ResNet-v2 [37] 34.7 55.5 36.7 13.5 38.1 52.0 Faster R-CNN w/ TDM [36] Inception-ResNet-v2-TDM 36.8 57.7 39.2 16.2 39.8 52.1 One-stage methods: YOLOv2 [30] DarkNet-19 [30] 21.6 44.0 19.2 5.0 22.4 35.5 SSD513 [25] ResNet-101-SSD 31.2 50.4 33.3 10.2 34.5 49.8 YOLOv3 608\u00d7 608 [31] Darknet-53 33.0 57.9 34.4 18.3 35.4 41.9 DSSD513 [10] ResNet-101-DSSD 33.2 53.3 35.2 13.0 35.4 51.1 RetinaNet [22] ResNet-101-FPN 39.1 59.1 42.3 21.8 42.7 50.2 CornerNet [19] Hourglass-104 40.5 56.5 43.1 19.4 42.7 53.9 FSAF [54] ResNeXt-64x4d-101-FPN 42.9 63.8 46.3 26.6 46.2 52.7 CenterNet511 [9] Hourglass-104 44.9 62.4 48.1 25.6 47.4 57.4 FCOS ResNet-101-FPN 43.2 62.4 46.8 26.1 46.2 52.8 FCOS ResNeXt-32x8d-101-FPN 44.1 63.7 47.9 27.4 46.8 53.7 FCOS ResNeXt-64x4d-101-FPN 44.8 64.4 48.5 27.7 47.4 55.0 FCOS w/ deform. conv. v2 [55] ResNeXt-32x8d-101-FPN 46.6 65.9 50.8 28.6 49.1 58.6 FCOS ResNet-101-BiFPN [38] 45.0 63.6 48.7 27.0 47.9 55.9 FCOS ResNeXt-32x8d-101-BiFPN 46.2 65.2 50.0 28.7 49.1 56.5 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-BiFPN 47.9 66.9 51.9 30.2 50.3 59.9 w/ test-time augmentation: FCOS ResNet-101-FPN 45.9 64.5 50.4 29.4 48.3 56.1 FCOS ResNeXt-32x8d-101-FPN 47.0 66.0 51.6 30.7 49.4 57.1 FCOS ResNeXt-64x4d-101-FPN 47.5 66.4 51.9 31.4 49.7 58.2 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-FPN 49.1 68.0 53.9 31.7 51.6 61.0 FCOS ResNet-101-BiFPN 47.9 65.9 52.5 31.0 50.7 59.7 FCOS ResNeXt-32x8d-101-BiFPN 49.0 67.4 53.6 32.0 51.7 60.5 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-BiFPN 50.4 68.9 55.0 33.2 53.0 62.7\nTABLE 8 FCOS vs. other state-of-the-art two-stage or one-stage detectors (single-model results)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "Method Backbone AP AP50 AP75 APS APM APL Two-stage methods: Faster R-CNN+++ [14] ResNet-101 34."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206594692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "isKey": true,
            "numCitedBy": 95314,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
            },
            "slug": "Deep-Residual-Learning-for-Image-Recognition-He-Zhang",
            "title": {
                "fragments": [],
                "text": "Deep Residual Learning for Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work presents a residual learning framework to ease the training of networks that are substantially deeper than those used previously, and provides comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2254178"
                        ],
                        "name": "Fayao Liu",
                        "slug": "Fayao-Liu",
                        "structuredName": {
                            "firstName": "Fayao",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fayao Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780381"
                        ],
                        "name": "Chunhua Shen",
                        "slug": "Chunhua-Shen",
                        "structuredName": {
                            "firstName": "Chunhua",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunhua Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2604251"
                        ],
                        "name": "Guosheng Lin",
                        "slug": "Guosheng-Lin",
                        "structuredName": {
                            "firstName": "Guosheng",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guosheng Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145950884"
                        ],
                        "name": "I. Reid",
                        "slug": "I.-Reid",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Reid",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Reid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "in dense prediction tasks such as semantic segmentation [15], [28], [40], depth estimation [24], [48], keypoint detection [3] and counting."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15774646,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9c21b3ffdac5f2450b82dd6660ac69f72bb9018b",
            "isKey": false,
            "numCitedBy": 876,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "In this article, we tackle the problem of depth estimation from single monocular images. Compared with depth estimation using multiple images such as stereo depth perception, depth from monocular images is much more challenging. Prior work typically focuses on exploiting geometric priors or additional sources of information, most using hand-crafted features. Recently, there is mounting evidence that features from deep convolutional neural networks (CNN) set new records for various vision applications. On the other hand, considering the continuous characteristic of the depth values, depth estimation can be naturally formulated as a continuous conditional random field (CRF) learning problem. Therefore, here we present a deep convolutional neural field model for estimating depths from single monocular images, aiming to jointly explore the capacity of deep CNN and continuous CRF. In particular, we propose a deep structured learning scheme which learns the unary and pairwise potentials of continuous CRF in a unified deep CNN framework. We then further propose an equally effective model based on fully convolutional networks and a novel superpixel pooling method, which is about 10 times faster, to speedup the patch-wise convolutions in the deep model. With this more efficient model, we are able to design deeper networks to pursue better performance. Our proposed method can be used for depth estimation of general scenes with no geometric priors nor any extra information injected. In our case, the integral of the partition function can be calculated in a closed form such that we can exactly solve the log-likelihood maximization. Moreover, solving the inference problem for predicting depths of a test image is highly efficient as closed-form solutions exist. Experiments on both indoor and outdoor scene datasets demonstrate that the proposed method outperforms state-of-the-art depth estimation approaches."
            },
            "slug": "Learning-Depth-from-Single-Monocular-Images-Using-Liu-Shen",
            "title": {
                "fragments": [],
                "text": "Learning Depth from Single Monocular Images Using Deep Convolutional Neural Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A deep convolutional neural field model for estimating depths from single monocular images, aiming to jointly explore the capacity of deep CNN and continuous CRF is presented, and a deep structured learning scheme which learns the unary and pairwise potentials of continuousCRF in a unified deep CNN framework is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574060"
                        ],
                        "name": "Christian Szegedy",
                        "slug": "Christian-Szegedy",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Szegedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Szegedy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054165706"
                        ],
                        "name": "S. Ioffe",
                        "slug": "S.-Ioffe",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Ioffe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ioffe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2657155"
                        ],
                        "name": "Vincent Vanhoucke",
                        "slug": "Vincent-Vanhoucke",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Vanhoucke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent Vanhoucke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122113652"
                        ],
                        "name": "Alexander A. Alemi",
                        "slug": "Alexander-A.-Alemi",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Alemi",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander A. Alemi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 49
                            }
                        ],
                        "text": "2 Faster R-CNN by G-RMI [17] Inception-ResNet-v2 [37] 34."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 255
                            }
                        ],
                        "text": "Quantitative results are shown in\n9 Method Backbone AP AP50 AP75 APS APM APL Two-stage methods: Faster R-CNN+++ [14] ResNet-101 34.9 55.7 37.4 15.6 38.7 50.9 Faster R-CNN w/ FPN [21] ResNet-101-FPN 36.2 59.1 39.0 18.2 39.0 48.2 Faster R-CNN by G-RMI [17] Inception-ResNet-v2 [37] 34.7 55.5 36.7 13.5 38.1 52.0 Faster R-CNN w/ TDM [36] Inception-ResNet-v2-TDM 36.8 57.7 39.2 16.2 39.8 52.1 One-stage methods: YOLOv2 [30] DarkNet-19 [30] 21.6 44.0 19.2 5.0 22.4 35.5 SSD513 [25] ResNet-101-SSD 31.2 50.4 33.3 10.2 34.5 49.8 YOLOv3 608\u00d7 608 [31] Darknet-53 33.0 57.9 34.4 18.3 35.4 41.9 DSSD513 [10] ResNet-101-DSSD 33.2 53.3 35.2 13.0 35.4 51.1 RetinaNet [22] ResNet-101-FPN 39.1 59.1 42.3 21.8 42.7 50.2 CornerNet [19] Hourglass-104 40.5 56.5 43.1 19.4 42.7 53.9 FSAF [54] ResNeXt-64x4d-101-FPN 42.9 63.8 46.3 26.6 46.2 52.7 CenterNet511 [9] Hourglass-104 44.9 62.4 48.1 25.6 47.4 57.4 FCOS ResNet-101-FPN 43.2 62.4 46.8 26.1 46.2 52.8 FCOS ResNeXt-32x8d-101-FPN 44.1 63.7 47.9 27.4 46.8 53.7 FCOS ResNeXt-64x4d-101-FPN 44.8 64.4 48.5 27.7 47.4 55.0 FCOS w/ deform. conv. v2 [55] ResNeXt-32x8d-101-FPN 46.6 65.9 50.8 28.6 49.1 58.6 FCOS ResNet-101-BiFPN [38] 45.0 63.6 48.7 27.0 47.9 55.9 FCOS ResNeXt-32x8d-101-BiFPN 46.2 65.2 50.0 28.7 49.1 56.5 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-BiFPN 47.9 66.9 51.9 30.2 50.3 59.9 w/ test-time augmentation: FCOS ResNet-101-FPN 45.9 64.5 50.4 29.4 48.3 56.1 FCOS ResNeXt-32x8d-101-FPN 47.0 66.0 51.6 30.7 49.4 57.1 FCOS ResNeXt-64x4d-101-FPN 47.5 66.4 51.9 31.4 49.7 58.2 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-FPN 49.1 68.0 53.9 31.7 51.6 61.0 FCOS ResNet-101-BiFPN 47.9 65.9 52.5 31.0 50.7 59.7 FCOS ResNeXt-32x8d-101-BiFPN 49.0 67.4 53.6 32.0 51.7 60.5 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-BiFPN 50.4 68.9 55.0 33.2 53.0 62.7\nTABLE 8 FCOS vs. other state-of-the-art two-stage or one-stage detectors (single-model results)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1023605,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5c26ab8767d046cb6e32d959fdf726aee89bb62",
            "isKey": true,
            "numCitedBy": 8045,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "\n \n Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08% top-5 error on the test set of the ImageNet classification (CLS) challenge.\n \n"
            },
            "slug": "Inception-v4,-Inception-ResNet-and-the-Impact-of-on-Szegedy-Ioffe",
            "title": {
                "fragments": [],
                "text": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning"
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "73768948"
                        ],
                        "name": "S. H. Rezatofighi",
                        "slug": "S.-H.-Rezatofighi",
                        "structuredName": {
                            "firstName": "Seyed",
                            "lastName": "Rezatofighi",
                            "middleNames": [
                                "Hamid"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. H. Rezatofighi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39282796"
                        ],
                        "name": "Nathan Tsoi",
                        "slug": "Nathan-Tsoi",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Tsoi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathan Tsoi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39813007"
                        ],
                        "name": "JunYoung Gwak",
                        "slug": "JunYoung-Gwak",
                        "structuredName": {
                            "firstName": "JunYoung",
                            "lastName": "Gwak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "JunYoung Gwak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145759966"
                        ],
                        "name": "Amir Sadeghian",
                        "slug": "Amir-Sadeghian",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Sadeghian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amir Sadeghian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145950884"
                        ],
                        "name": "I. Reid",
                        "slug": "I.-Reid",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Reid",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Reid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702137"
                        ],
                        "name": "S. Savarese",
                        "slug": "S.-Savarese",
                        "structuredName": {
                            "firstName": "Silvio",
                            "lastName": "Savarese",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Savarese"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "RetinaNet with the universal improvements in FCOS including Group Normalization (GN) [45], GIoU loss [1] and scalars in regression, using P5 instead of C5 and NMS threshold 0.6 instead of 0.5."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "By replacing GIoU [33] with the origin IoU loss in [50], the performance drops by 0."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 79
                            }
                        ],
                        "text": "We would like to thank the author of [1] for the tricks of center sampling and GIoU."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "By replacing GIoU [33] with the origin IoU loss in [50], the performance drops by 0.3% AP."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 530,
                                "start": 526
                            }
                        ],
                        "text": "As shown in the figure, after applying the centerness scores to the classification scores, the boxes with low IoU scores but high confidence scores are largely eliminated\n8 AP AP50 AP75 APS APM APL Baseline 38.9 57.5 42.2 23.1 42.7 50.2 w/o GN 37.9 56.4 40.9 22.1 41.8 48.8 w/ IoU 38.6 57.2 41.9 22.4 42.1 49.8 w/ C5 38.5 57.4 41.7 22.8 42.1 49.3\nTABLE 5 Ablation study for design choices in FCOS. w/o GN: without using Group Normalization (GN) for the convolutional layers in heads. w/ IoU: using IoU loss in [50] instead of GIoU. w/ C5: using C5 instead of P5.\nr AP AP50 AP75 APS APM APL 1.0 38.5 57.2 41.5 22.6 42.3 49.7 1.5 38.9 57.5 42.2 23.1 42.7 50.2 2.0 38.8 57.7 41.7 22.7 42.6 49.9\nTABLE 6 Ablation study for the radius r of positive sample regions (defined in Section 3.1)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 193
                            }
                        ],
                        "text": "We define our training loss function as follows:\nL({px,y}, {tx,y}) = 1\nNpos \u2211 x,y Lcls(px,y, c \u2217 x,y)\n+ \u03bb\nNpos \u2211 x,y 1{c\u2217x,y>0}Lreg(tx,y, t \u2217 x,y), (2)\nwhere Lcls is focal loss as in [22] and Lreg is the GIoU loss [33]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "As shown in experiments, the GIoU loss has better performance than the IoU loss in UnitBox [50], which is used in our preliminary version [42]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "where Lcls is focal loss as in [22] and Lreg is the GIoU loss [33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 67855581,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "889c81b4d7b7ed43a3f69f880ea60b0572e02e27",
            "isKey": true,
            "numCitedBy": 973,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Intersection over Union (IoU) is the most popular evaluation metric used in the object detection benchmarks. However, there is a gap between optimizing the commonly used distance losses for regressing the parameters of a bounding box and maximizing this metric value. The optimal objective for a metric is the metric itself. In the case of axis-aligned 2D bounding boxes, it can be shown that IoU can be directly used as a regression loss. However, IoU has a plateau making it infeasible to optimize in the case of non-overlapping bounding boxes. In this paper, we address the this weakness by introducing a generalized version of IoU as both a new loss and a new metric. By incorporating this generalized IoU ( GIoU) as a loss into the state-of-the art object detection frameworks, we show a consistent improvement on their performance using both the standard, IoU based, and new, GIoU based, performance measures on popular object detection benchmarks such as PASCAL VOC and MS COCO."
            },
            "slug": "Generalized-Intersection-Over-Union:-A-Metric-and-a-Rezatofighi-Tsoi",
            "title": {
                "fragments": [],
                "text": "Generalized Intersection Over Union: A Metric and a Loss for Bounding Box Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper introduces a generalized version of IoU ( GIoU) as a loss into the state-of-the art object detection frameworks, and shows a consistent improvement on their performance using both the standard, IoU based, and new, GIo U based, performance measures on popular object detection benchmarks."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731948"
                        ],
                        "name": "Paul A. Viola",
                        "slug": "Paul-A.-Viola",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Viola",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul A. Viola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111328101"
                        ],
                        "name": "Michael Jones",
                        "slug": "Michael-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2796017,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b09ec0b350f8352bce46a2f5bf7ae97c83a7b9ca",
            "isKey": false,
            "numCitedBy": 11227,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a face detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the \u201cIntegral Image\u201d which allows the features used by our detector to be computed very quickly. The second is a simple and efficient classifier which is built using the AdaBoost learning algorithm (Freund and Schapire, 1995) to select a small number of critical visual features from a very large set of potential features. The third contribution is a method for combining classifiers in a \u201ccascade\u201d which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions. A set of experiments in the domain of face detection is presented. The system yields face detection performance comparable to the best previous systems (Sung and Poggio, 1998; Rowley et al., 1998; Schneiderman and Kanade, 2000; Roth et al., 2000). Implemented on a conventional desktop, face detection proceeds at 15 frames per second."
            },
            "slug": "Robust-Real-Time-Face-Detection-Viola-Jones",
            "title": {
                "fragments": [],
                "text": "Robust Real-Time Face Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new image representation called the \u201cIntegral Image\u201d is introduced which allows the features used by the detector to be computed very quickly and a method for combining classifiers in a \u201ccascade\u201d which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2006490058"
                        ],
                        "name": "Wei Yin",
                        "slug": "Wei-Yin",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Yin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49421615"
                        ],
                        "name": "Yifan Liu",
                        "slug": "Yifan-Liu",
                        "structuredName": {
                            "firstName": "Yifan",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yifan Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12459603"
                        ],
                        "name": "Chunhua Shen",
                        "slug": "Chunhua-Shen",
                        "structuredName": {
                            "firstName": "Chunhua",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunhua Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2376903"
                        ],
                        "name": "Youliang Yan",
                        "slug": "Youliang-Yan",
                        "structuredName": {
                            "firstName": "Youliang",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Youliang Yan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": "in dense prediction tasks such as semantic segmentation [15], [28], [40], depth estimation [24], [48], keypoint detection [3] and counting."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 198968133,
            "fieldsOfStudy": [
                "Computer Science",
                "Geology"
            ],
            "id": "b7a2d31bf89ae5d9204dd2ce75db9dfa23e8b1ac",
            "isKey": false,
            "numCitedBy": 189,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Monocular depth prediction plays a crucial role in understanding 3D scene geometry. Although recent methods have achieved impressive progress in evaluation metrics such as the pixel-wise relative error, most methods neglect the geometric constraints in the 3D space. In this work, we show the importance of the high-order 3D geometric constraints for depth prediction. By designing a loss term that enforces one simple type of geometric constraints, namely, virtual normal directions determined by randomly sampled three points in the reconstructed 3D space, we can considerably improve the depth prediction accuracy. Furthermore, we can not only predict accurate depth but also achieve high-quality other 3D information from the depth without retraining new parameters, Significantly, the byproduct of this predicted depth being sufficiently accurate is that we are now able to recover good 3D structures of the scene such as the point cloud and surface normal directly from the depth, eliminating the necessity of training new sub-models as was previously done. Experiments on two challenging benchmarks: NYU Depth-V2 and KITTI demonstrate the effectiveness of our method and state-of-the-art performance."
            },
            "slug": "Enforcing-Geometric-Constraints-of-Virtual-Normal-Yin-Liu",
            "title": {
                "fragments": [],
                "text": "Enforcing Geometric Constraints of Virtual Normal for Depth Prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work shows the importance of the high-order 3D geometric constraints for depth prediction by designing a loss term that enforces one simple type of geometric constraints, namely, virtual normal directions determined by randomly sampled three points in the reconstructed 3D space, to considerably improve the depth prediction accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807197"
                        ],
                        "name": "F. Yu",
                        "slug": "F.-Yu",
                        "structuredName": {
                            "firstName": "Fisher",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2774612"
                        ],
                        "name": "Dequan Wang",
                        "slug": "Dequan-Wang",
                        "structuredName": {
                            "firstName": "Dequan",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dequan Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "We further replace ResNet-50 with the backbone DLA-34 [49], which results in a better speed/accuracy trade-off (40."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 47
                            }
                        ],
                        "text": "We further replace ResNet-50 with the backbone DLA-34 [49], which results in a better speed/accuracy trade-off (40.3% AP at 46 FPS)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 30834643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37c5a90619eb4bfd7c41b674e4fdf2317622e0f7",
            "isKey": true,
            "numCitedBy": 623,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual recognition requires rich representations that span levels from low to high, scales from small to large, and resolutions from fine to coarse. Even with the depth of features in a convolutional network, a layer in isolation is not enough: compounding and aggregating these representations improves inference of what and where. Architectural efforts are exploring many dimensions for network backbones, designing deeper or wider architectures, but how to best aggregate layers and blocks across a network deserves further attention. Although skip connections have been incorporated to combine layers, these connections have been \"shallow\" themselves, and only fuse by simple, one-step operations. We augment standard architectures with deeper aggregation to better fuse information across layers. Our deep layer aggregation structures iteratively and hierarchically merge the feature hierarchy to make networks with better accuracy and fewer parameters. Experiments across architectures and tasks show that deep layer aggregation improves recognition and resolution compared to existing branching and merging schemes."
            },
            "slug": "Deep-Layer-Aggregation-Yu-Wang",
            "title": {
                "fragments": [],
                "text": "Deep Layer Aggregation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work augments standard architectures with deeper aggregation to better fuse information across layers and iteratively and hierarchically merge the feature hierarchy to make networks with better accuracy and fewer parameters."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49421615"
                        ],
                        "name": "Yifan Liu",
                        "slug": "Yifan-Liu",
                        "structuredName": {
                            "firstName": "Yifan",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yifan Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1410691403"
                        ],
                        "name": "Changyong Shun",
                        "slug": "Changyong-Shun",
                        "structuredName": {
                            "firstName": "Changyong",
                            "lastName": "Shun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Changyong Shun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688516"
                        ],
                        "name": "Jingdong Wang",
                        "slug": "Jingdong-Wang",
                        "structuredName": {
                            "firstName": "Jingdong",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jingdong Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12459603"
                        ],
                        "name": "Chunhua Shen",
                        "slug": "Chunhua-Shen",
                        "structuredName": {
                            "firstName": "Chunhua",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunhua Shen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "An example is shown in [27], where a structured knowl-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 208109903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "252d5cf2a9ff20eb526321e8fbe5a2b790a42a12",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work, we consider transferring the structure information from large networks to compact ones for dense prediction tasks in computer vision. Previous knowledge distillation strategies used for dense prediction tasks often directly borrow the distillation scheme for image classification and perform knowledge distillation for each pixel separately, leading to sub-optimal performance. Here we propose to distill structured knowledge from large networks to compact networks, taking into account the fact that dense predictions a structured prediction problem. Specifically, we study two structured distillation schemes: i)pair-wise distillation that distills the pair-wise similarities by building a static graph; and ii) holistic distillation that uses adversarial training to distill holistic knowledge. The effectiveness of our knowledge distillation approaches is demonstrated by experiments on three dense prediction tasks: semantic segmentation, depth estimation and object detection. Code is available at: https://git.io/StructKD}."
            },
            "slug": "Structured-Knowledge-Distillation-for-Dense-Liu-Shun",
            "title": {
                "fragments": [],
                "text": "Structured Knowledge Distillation for Dense Prediction."
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Two structured distillation schemes are studied:pair-wise distillation that distills the pair-wise similarities by building a static graph; and holisticdistillation that uses adversarial training to distill holistic knowledge."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE transactions on pattern analysis and machine intelligence"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780381"
                        ],
                        "name": "Chunhua Shen",
                        "slug": "Chunhua-Shen",
                        "structuredName": {
                            "firstName": "Chunhua",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunhua Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144282676"
                        ],
                        "name": "Peng Wang",
                        "slug": "Peng-Wang",
                        "structuredName": {
                            "firstName": "Peng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peng Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1892724"
                        ],
                        "name": "S. Paisitkriangkrai",
                        "slug": "S.-Paisitkriangkrai",
                        "structuredName": {
                            "firstName": "Sakrapee",
                            "lastName": "Paisitkriangkrai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Paisitkriangkrai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5546141"
                        ],
                        "name": "A. V. Hengel",
                        "slug": "A.-V.-Hengel",
                        "structuredName": {
                            "firstName": "Anton",
                            "lastName": "Hengel",
                            "middleNames": [
                                "van",
                                "den"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. V. Hengel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "Prior to deep learning, the sliding-window approach was the main method [7], [35], [43], which exhaustively classifies every possible location, thus requiring"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16285836,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "26bea6f9fe9b829d412dfd0d8dd01dc392c2ea5c",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "Cascade classifiers are widely used in real-time object detection. Different from conventional classifiers that are designed for a low overall classification error rate, a classifier in each node of the cascade is required to achieve an extremely high detection rate and moderate false positive rate. Although there are a few reported methods addressing this requirement in the context of object detection, there is no principled feature selection method that explicitly takes into account this asymmetric node learning objective. We provide such an algorithm here. We show that a special case of the biased minimax probability machine has the same formulation as the linear asymmetric classifier (LAC) of Wu et al. (linear asymmetric classifier for cascade detectors, 2005). We then design a new boosting algorithm that directly optimizes the cost function of LAC. The resulting totally-corrective boosting algorithm is implemented by the column generation technique in convex optimization. Experimental results on object detection verify the effectiveness of the proposed boosting algorithm as a node classifier in cascade object detection, and show performance better than that of the current state-of-the-art."
            },
            "slug": "Training-Effective-Node-Classifiers-for-Cascade-Shen-Wang",
            "title": {
                "fragments": [],
                "text": "Training Effective Node Classifiers for Cascade Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work shows that a special case of the biased minimax probability machine has the same formulation as the linear asymmetric classifier (LAC) of Wu et al. (linear asymmetricclassifier for cascade detectors, 2005), and designs a new boosting algorithm that directly optimizes the cost function of LAC."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40497777"
                        ],
                        "name": "Joseph Redmon",
                        "slug": "Joseph-Redmon",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Redmon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph Redmon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143787583"
                        ],
                        "name": "Ali Farhadi",
                        "slug": "Ali-Farhadi",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Farhadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ali Farhadi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 104
                            }
                        ],
                        "text": "settings, we reduce the shorter side of input images from Method FPS AP AP test-dev YOLOv3 (Darknet-53) [31] 26 \u2212 33."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "Speed/accuracy trade-off between FCOS and several recent methods: CenterNet [52], YOLOv3 [31] and RetinaNet [22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 166
                            }
                        ],
                        "text": "\u2022 With considerably reduced design complexity, our proposed detector outperforms previous strong baseline detectors such as Faster R-CNN [32], RetinaNet [22], YOLOv3 [31] and SSD [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 528,
                                "start": 522
                            }
                        ],
                        "text": "Quantitative results are shown in\n9 Method Backbone AP AP50 AP75 APS APM APL Two-stage methods: Faster R-CNN+++ [14] ResNet-101 34.9 55.7 37.4 15.6 38.7 50.9 Faster R-CNN w/ FPN [21] ResNet-101-FPN 36.2 59.1 39.0 18.2 39.0 48.2 Faster R-CNN by G-RMI [17] Inception-ResNet-v2 [37] 34.7 55.5 36.7 13.5 38.1 52.0 Faster R-CNN w/ TDM [36] Inception-ResNet-v2-TDM 36.8 57.7 39.2 16.2 39.8 52.1 One-stage methods: YOLOv2 [30] DarkNet-19 [30] 21.6 44.0 19.2 5.0 22.4 35.5 SSD513 [25] ResNet-101-SSD 31.2 50.4 33.3 10.2 34.5 49.8 YOLOv3 608\u00d7 608 [31] Darknet-53 33.0 57.9 34.4 18.3 35.4 41.9 DSSD513 [10] ResNet-101-DSSD 33.2 53.3 35.2 13.0 35.4 51.1 RetinaNet [22] ResNet-101-FPN 39.1 59.1 42.3 21.8 42.7 50.2 CornerNet [19] Hourglass-104 40.5 56.5 43.1 19.4 42.7 53.9 FSAF [54] ResNeXt-64x4d-101-FPN 42.9 63.8 46.3 26.6 46.2 52.7 CenterNet511 [9] Hourglass-104 44.9 62.4 48.1 25.6 47.4 57.4 FCOS ResNet-101-FPN 43.2 62.4 46.8 26.1 46.2 52.8 FCOS ResNeXt-32x8d-101-FPN 44.1 63.7 47.9 27.4 46.8 53.7 FCOS ResNeXt-64x4d-101-FPN 44.8 64.4 48.5 27.7 47.4 55.0 FCOS w/ deform. conv. v2 [55] ResNeXt-32x8d-101-FPN 46.6 65.9 50.8 28.6 49.1 58.6 FCOS ResNet-101-BiFPN [38] 45.0 63.6 48.7 27.0 47.9 55.9 FCOS ResNeXt-32x8d-101-BiFPN 46.2 65.2 50.0 28.7 49.1 56.5 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-BiFPN 47.9 66.9 51.9 30.2 50.3 59.9 w/ test-time augmentation: FCOS ResNet-101-FPN 45.9 64.5 50.4 29.4 48.3 56.1 FCOS ResNeXt-32x8d-101-FPN 47.0 66.0 51.6 30.7 49.4 57.1 FCOS ResNeXt-64x4d-101-FPN 47.5 66.4 51.9 31.4 49.7 58.2 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-FPN 49.1 68.0 53.9 31.7 51.6 61.0 FCOS ResNet-101-BiFPN 47.9 65.9 52.5 31.0 50.7 59.7 FCOS ResNeXt-32x8d-101-BiFPN 49.0 67.4 53.6 32.0 51.7 60.5 FCOS w/ deform. conv. v2 ResNeXt-32x8d-101-BiFPN 50.4 68.9 55.0 33.2 53.0 62.7\nTABLE 8 FCOS vs. other state-of-the-art two-stage or one-stage detectors (single-model results)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 192
                            }
                        ],
                        "text": "Here (x(i)0 , y (i) 0 ) and (x (i) 1 y (i) 1 ) denote the coordinates of the left-top and right-bottom corners of the\n4 20 30 40 50 60 70 FPS 28 30 32 34 36 38 40 42 AP FCOS-RT FCOS CenterNet YOLOv3 RetinaNet\nFig."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 66
                            }
                        ],
                        "text": "We measure the FPS of YOLOv3 on 1080Ti using the code released by [31]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 84
                            }
                        ],
                        "text": "All current mainstream detectors such as Faster R-CNN [32], SSD [25] and YOLOv2, v3 [31] rely on a set of pre-defined anchor boxes and it has"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4714433,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e4845fb1e624965d4f036d7fd32e8dcdd2408148",
            "isKey": true,
            "numCitedBy": 8616,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at this https URL"
            },
            "slug": "YOLOv3:-An-Incremental-Improvement-Redmon-Farhadi",
            "title": {
                "fragments": [],
                "text": "YOLOv3: An Incremental Improvement"
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153302678"
                        ],
                        "name": "Jia Deng",
                        "slug": "Jia-Deng",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144847596"
                        ],
                        "name": "Wei Dong",
                        "slug": "Wei-Dong",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Dong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Dong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2040091191"
                        ],
                        "name": "Li-Jia Li",
                        "slug": "Li-Jia-Li",
                        "structuredName": {
                            "firstName": "Li-Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li-Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "94451829"
                        ],
                        "name": "K. Li",
                        "slug": "K.-Li",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 77
                            }
                        ],
                        "text": "We initialize our backbone networks with the weights pre-trained on ImageNet [6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 57246310,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b47265245e8db53a553049dcb27ed3e495fd625",
            "isKey": false,
            "numCitedBy": 27402,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called \u201cImageNet\u201d, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond."
            },
            "slug": "ImageNet:-A-large-scale-hierarchical-image-database-Deng-Dong",
            "title": {
                "fragments": [],
                "text": "ImageNet: A large-scale hierarchical image database"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new database called \u201cImageNet\u201d is introduced, a large-scale ontology of images built upon the backbone of the WordNet structure, much larger in scale and diversity and much more accurate than the current image datasets."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 94
                            }
                        ],
                        "text": "The BPR is much higher than the BPR of 91.94% of the RetinaNet in the official implementation Detectron [12], where only the low-quality matches with IOU\u2265 0.4 are used."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 128
                            }
                        ],
                        "text": "FCOS has very similar BPR to the best anchor-based one and has much higher recall than the official implementation in Detectron [12], where only low-quality matches with IOU \u2265 0."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 27
                            }
                        ],
                        "text": "RetinaNet results are from Detectron2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 46
                            }
                        ],
                        "text": "RetinaNet (#A=9): the original RetinaNet from Detectron2 [46]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 118
                            }
                        ],
                        "text": "FCOS has very similar BPR to the best anchor-based one and has much higher recall than the official implementation in Detectron [12], where only low-quality matches with IOU \u2265 0.4 are considered.\nexcept that we use NMS threshold 0.6 instead of 0.5 in RetinaNet."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 220
                            }
                        ],
                        "text": "Here, we empirically show that even with a large stride, FCOS is still able to produce a good BPR, and it can even better than the BPR of the anchor-based detector RetinaNet [22] in the official implementation Detectron [12] (refer to Table 1)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "94% of the RetinaNet in the official implementation Detectron [12], where only the low-quality matches with IOU\u2265 0."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Detectron,\u201d. https://github.com/ facebookresearch/detectron, 2018"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2018
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Detectron"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2018
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "RetinaNet (#A=9): the original RetinaNet from Detectron2 [46]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 27
                            }
                        ],
                        "text": "RetinaNet results are from Detectron2."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Detectron2,\u201d. https://github.com/ facebookresearch/detectron2, 2019"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2019
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Detectron2"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2019
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 25,
            "methodology": 30
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 64,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/FCOS:-A-Simple-and-Strong-Anchor-Free-Object-Tian-Shen/554363727a07d39b23ad5b4e32f556685afd332a?sort=total-citations"
}