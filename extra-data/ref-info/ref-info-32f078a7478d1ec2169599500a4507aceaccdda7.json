{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751569"
                        ],
                        "name": "Samy Bengio",
                        "slug": "Samy-Bengio",
                        "structuredName": {
                            "firstName": "Samy",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samy Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 143
                            }
                        ],
                        "text": "\u2026a powerful framework for deriving tractable distribution estimators is the family of fully visible Bayesian networks (e.g. Frey et al., 1996; Bengio and Bengio, 2000), which decompose the observation\u2019s probability distribution as follows:\np(v) = D\u220f\ni=1\np(vi|vparents(i)) , (3)\nwhere all\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Bengio and Bengio (2000) also proposed using neural networks for the conditionals of a Bayesian network."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11221889,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e53014dce003ae69ec30db3e1b820eec868c31e",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The curse of dimensionality is severe when modeling high-dimensional discrete data: the number of possible combinations of the variables explodes exponentially. In this paper, we propose a new architecture for modeling high-dimensional data that requires resources (parameters and computations) that grow at most as the square of the number of variables, using a multilayer neural network to represent the joint distribution of the variables as the product of conditional distributions. The neural network can be interpreted as a graphical model without hidden random variables, but in which the conditional distributions are tied through the hidden units. The connectivity of the neural network can be pruned by using dependency tests between the variables (thus reducing significantly the number of parameters). Experiments on modeling the distribution of several discrete data sets show statistically significant improvements over other methods such as naive Bayes and comparable Bayesian networks and show that significant improvements can be obtained by pruning the network."
            },
            "slug": "Taking-on-the-curse-of-dimensionality-in-joint-Bengio-Bengio",
            "title": {
                "fragments": [],
                "text": "Taking on the curse of dimensionality in joint distributions using neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper proposes a new architecture for modeling high-dimensional data that requires resources that grow at most as the square of the number of variables, using a multilayer neural network to represent the joint distribution of the variables as the product of conditional distributions."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks Learn. Syst."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153160559"
                        ],
                        "name": "Joseph P. Turian",
                        "slug": "Joseph-P.-Turian",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Turian",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph P. Turian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17251892,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "495ffba7d66c365403cb8bb84c61b4b5425b251c",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the problem of estimating the density function of multivariate binary data. In particular, we focus on models for which computing the estimated probability of any data point is tractable. In such a setting, previous work has mostly concentrated on mixture modeling approaches. We argue that for the problem of tractable density estimation, the restricted Boltzmann machine (RBM) provides a competitive framework for multivariate binary density modeling. With this in mind, we also generalize the RBM framework and present the restricted Boltzmann forest (RBForest), which replaces the binary variables in the hidden layer of RBMs with groups of tree-structured binary variables. This extension allows us to obtain models that have more modeling capacity but remain tractable. In experiments on several data sets, we demonstrate the competitiveness of this approach and study some of its properties."
            },
            "slug": "Tractable-Multivariate-Binary-Density-Estimation-Larochelle-Bengio",
            "title": {
                "fragments": [],
                "text": "Tractable Multivariate Binary Density Estimation and the Restricted Boltzmann Forest"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work argues that for the problem of tractable density estimation, the restricted Boltzmann machine (RBM) provides a competitive framework for multivariate binary density modeling and presents the restricted Bolzmann forest (RBForest), which replaces the binary variables in the hidden layer of RBMs with groups of tree-structured binary variables."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145797336"
                        ],
                        "name": "Iain Murray",
                        "slug": "Iain-Murray",
                        "structuredName": {
                            "firstName": "Iain",
                            "lastName": "Murray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iain Murray"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This version was used by Salakhutdinov and Murray (2008) to train RBMs with different versions of contrastive divergence and evaluate them as distribution estimators."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "64 average test log-likelihoods respectively (taken from Salakhutdinov and Murray (2008))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 9
                            }
                        ],
                        "text": "However, Salakhutdinov and Murray (2008) showed that using annealed importance sampling, it is possible to obtain a reasonable approximation to Z."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 178
                            }
                        ],
                        "text": "Again, it also improves over mixtures of Bernoullis which, with 10, 100 and 500 components obtain \u2212168.95, \u2212142.63 and \u2212137.64 average test log-likelihoods respectively (taken from Salakhutdinov and Murray (2008))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 189
                            }
                        ],
                        "text": "\u2026isn\u2019t possible to meaningfully upper-bound the partition function from these results: the true test log-likelihood averages could be much smaller than the values and error bars reported by Salakhutdinov and Murray (2008), although their approximations were shown to be accurate in a tractable case."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 25
                            }
                        ],
                        "text": "This version was used by Salakhutdinov and Murray (2008) to\ntrain RBMs with different versions of contrastive divergence and evaluate them as distribution estimators."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 458722,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "08d0ea90b53aba0008d25811268fe46562cfb38c",
            "isKey": false,
            "numCitedBy": 459,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep Belief Networks (DBN's) are generative models that contain many layers of hidden variables. Efficient greedy algorithms for learning and approximate inference have allowed these models to be applied successfully in many application domains. The main building block of a DBN is a bipartite undirected graphical model called a restricted Boltzmann machine (RBM). Due to the presence of the partition function, model selection, complexity control, and exact maximum likelihood learning in RBM's are intractable. We show that Annealed Importance Sampling (AIS) can be used to efficiently estimate the partition function of an RBM, and we present a novel AIS scheme for comparing RBM's with different architectures. We further show how an AIS estimator, along with approximate inference, can be used to estimate a lower bound on the log-probability that a DBN model with multiple hidden layers assigns to the test data. This is, to our knowledge, the first step towards obtaining quantitative results that would allow us to directly assess the performance of Deep Belief Networks as generative models of data."
            },
            "slug": "On-the-quantitative-analysis-of-deep-belief-Salakhutdinov-Murray",
            "title": {
                "fragments": [],
                "text": "On the quantitative analysis of deep belief networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that Annealed Importance Sampling (AIS) can be used to efficiently estimate the partition function of an RBM, and a novel AIS scheme for comparing RBM's with different architectures is presented."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751569"
                        ],
                        "name": "Samy Bengio",
                        "slug": "Samy-Bengio",
                        "structuredName": {
                            "firstName": "Samy",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samy Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 143
                            }
                        ],
                        "text": "\u2026framework for deriving tractable distribution estimators is the family of fully visible Bayesian networks (e.g. Frey et al., 1996; Bengio and Bengio, 2000), which decompose the observation\u2019s probability distribution as follows:\np(v) = D\u220f\ni=1\np(vi|vparents(i)) , (3)\nwhere all observation\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 11
                            }
                        ],
                        "text": "Bengio and Bengio (2000) also proposed using neural networks for the conditionals of a Bayesian network."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9580239,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "190e4800c67ef445e4bd0944a55debaccebcf43f",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The curse of dimensionality is severe when modeling high-dimensional discrete data: the number of possible combinations of the variables explodes exponentially. In this paper we propose a new architecture for modeling high-dimensional data that requires resources (parameters and computations) that grow only at most as the square of the number of variables, using a multi-layer neural network to represent the joint distribution of the variables as the product of conditional distributions. The neural network can be interpreted as a graphical model without hidden random variables, but in which the conditional distributions are tied through the hidden units. The connectivity of the neural network can be pruned by using dependency tests between the variables. Experiments on modeling the distribution of several discrete data sets show statistically significant improvements over other methods such as naive Bayes and comparable Bayesian networks, and show that significant improvements can be obtained by pruning the network."
            },
            "slug": "Modeling-High-Dimensional-Discrete-Data-with-Neural-Bengio-Bengio",
            "title": {
                "fragments": [],
                "text": "Modeling High-Dimensional Discrete Data with Multi-Layer Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A new architecture for modeling high-dimensional data that requires resources that grow only at most as the square of the number of variables is proposed, using a multi-layer neural network to represent the joint distribution of the variables as the product of conditional distributions."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 15
                            }
                        ],
                        "text": "Recent work by Salakhutdinov and Larochelle (2010) proposed a neural net training procedure that found weights whereby one mean-field iteration would give an output close to the result of running the mean-field procedure to convergence."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9383489,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "00cd1dab559a9671b692f39f14c1573ab2d1416b",
            "isKey": false,
            "numCitedBy": 343,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new approximate inference algorithm for Deep Boltzmann Machines (DBM\u2019s), a generative model with many layers of hidden variables. The algorithm learns a separate \u201crecognition\u201d model that is used to quickly initialize, in a single bottom-up pass, the values of the latent variables in all hidden layers. We show that using such a recognition model, followed by a combined top-down and bottom-up pass, it is possible to efficiently learn a good generative model of high-dimensional highly-structured sensory input. We show that the additional computations required by incorporating a top-down feedback plays a critical role in the performance of a DBM, both as a generative and discriminative model. Moreover, inference is only at most three times slower compared to the approximate inference in a Deep Belief Network (DBN), making large-scale learning of DBM\u2019s practical. Finally, we demonstrate that the DBM\u2019s trained using the proposed approximate inference algorithm perform well compared to DBN\u2019s and SVM\u2019s on the MNIST handwritten digit, OCR English letters, and NORB visual object recognition tasks."
            },
            "slug": "Efficient-Learning-of-Deep-Boltzmann-Machines-Salakhutdinov-Larochelle",
            "title": {
                "fragments": [],
                "text": "Efficient Learning of Deep Boltzmann Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A new approximate inference algorithm for Deep Boltzmann Machines (DBM\u2019s), a generative model with many layers of hidden variables, that learns a separate \u201crecognition\u201d model that is used to quickly initialize, in a single bottom-up pass, the values of the latent variables in all hidden layers."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 117
                            }
                        ],
                        "text": "Still, the general principle of mean-field has been shown to work well in practice for RBMs (Welling & Hinton, 2002; Salakhutdinov & Hinton, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 58
                            }
                        ],
                        "text": "By assuming this approximation corresponds to the true value of Z, we can then make such quantitative comparisons, with the caveat that there are no strong guarantees on the quality of the estimate of Z."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 877639,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "85021c84383d18a7a4434d76dc8135fc6bdc0aa6",
            "isKey": false,
            "numCitedBy": 2024,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and dataindependent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer \u201cpre-training\u201d phase that allows variational inference to be initialized with a single bottomup pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks."
            },
            "slug": "Deep-Boltzmann-Machines-Salakhutdinov-Hinton",
            "title": {
                "fragments": [],
                "text": "Deep Boltzmann Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A new learning algorithm for Boltzmann machines that contain many layers of hidden variables that is made more efficient by using a layer-by-layer \u201cpre-training\u201d phase that allows variational inference to be initialized with a single bottomup pass."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35988982"
                        ],
                        "name": "Asja Fischer",
                        "slug": "Asja-Fischer",
                        "structuredName": {
                            "firstName": "Asja",
                            "lastName": "Fischer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Asja Fischer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748824"
                        ],
                        "name": "C. Igel",
                        "slug": "C.-Igel",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Igel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Igel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18401632,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dd135a89b5075af5cbef5becaf419457cdd77cc9",
            "isKey": false,
            "numCitedBy": 483,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Restricted Boltzmann machines (RBMs) are probabilistic graphical models that can be interpreted as stochastic neural networks. The increase in computational power and the development of faster learning algorithms have made them applicable to relevant machine learning problems. They attracted much attention recently after being proposed as building blocks of multi-layer learning systems called deep belief networks. This tutorial introduces RBMs as undirected graphical models. The basic concepts of graphical models are introduced first, however, basic knowledge in statistics is presumed. Different learning algorithms for RBMs are discussed. As most of them are based on Markov chain Monte Carlo (MCMC) methods, an introduction to Markov chains and the required MCMC techniques is provided."
            },
            "slug": "An-Introduction-to-Restricted-Boltzmann-Machines-Fischer-Igel",
            "title": {
                "fragments": [],
                "text": "An Introduction to Restricted Boltzmann Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This tutorial introduces RBMs as undirected graphical models as building blocks of multi-layer learning systems called deep belief networks based on Markov chain Monte Carlo methods."
            },
            "venue": {
                "fragments": [],
                "text": "CIARP"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 103
                            }
                        ],
                        "text": "Still, the general principle of mean-field has been shown to work well in practice for RBMs (Welling & Hinton, 2002; Salakhutdinov & Hinton, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 99
                            }
                        ],
                        "text": "In recent years, the restricted Boltzmann machine (RBM) (Smolensky, 1986; Freund & Haussler, 1992; Hinton, 2002) has frequently been used as a feature extractor."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 137
                            }
                        ],
                        "text": "However, in this particular case the relative partition functions of the RBMs have successfully been approximated to build a classifier (Hinton, 2002; Schmah et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 137
                            }
                        ],
                        "text": "For this reason, training under an RBM requires approximating the loglikelihood gradient on the parameters, with contrastive divergence (Hinton, 2002) being the most popular approach."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207596505,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9360e5ce9c98166bb179ad479a9d2919ff13d022",
            "isKey": true,
            "numCitedBy": 4571,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual expert models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called contrastive divergence whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data."
            },
            "slug": "Training-Products-of-Experts-by-Minimizing-Hinton",
            "title": {
                "fragments": [],
                "text": "Training Products of Experts by Minimizing Contrastive Divergence"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A product of experts (PoE) is an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary because it is hard even to approximate the derivatives of the renormalization term in the combination rule."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 167
                            }
                        ],
                        "text": "If the observations can be decomposed into an input x and a target y, then an RBM trained on such pairs can also be used to predict the missing target for new inputs (Larochelle & Bengio, 2008; Tieleman, 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15584821,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a53da9916b87fa295837617c16ef2ca6462cafb8",
            "isKey": false,
            "numCitedBy": 808,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, many applications for Restricted Boltzmann Machines (RBMs) have been developed for a large variety of learning problems. However, RBMs are usually used as feature extractors for another learning algorithm or to provide a good initialization for deep feed-forward neural network classifiers, and are not considered as a standalone solution to classification problems. In this paper, we argue that RBMs provide a self-contained framework for deriving competitive non-linear classifiers. We present an evaluation of different learning algorithms for RBMs which aim at introducing a discriminative component to RBM training and improve their performance as classifiers. This approach is simple in that RBMs are used directly to build a classifier, rather than as a stepping stone. Finally, we demonstrate how discriminative RBMs can also be successfully employed in a semi-supervised setting."
            },
            "slug": "Classification-using-discriminative-restricted-Larochelle-Bengio",
            "title": {
                "fragments": [],
                "text": "Classification using discriminative restricted Boltzmann machines"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper presents an evaluation of different learning algorithms for RBMs which aim at introducing a discriminative component to RBM training and improve their performance as classifiers, and demonstrates how discriminating RBMs can also be successfully employed in a semi-supervised setting."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 272,
                                "start": 253
                            }
                        ],
                        "text": "By training an RBM to learn the distribution of observations, it is then possible to use the posterior over these hidden variables given some observation as learned features, which can be composed into several layers and fine-tuned for some given task (Hinton et al., 2006; Bengio et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2309950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8978cf7574ceb35f4c3096be768c7547b28a35d0",
            "isKey": false,
            "numCitedBy": 13411,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind."
            },
            "slug": "A-Fast-Learning-Algorithm-for-Deep-Belief-Nets-Hinton-Osindero",
            "title": {
                "fragments": [],
                "text": "A Fast Learning Algorithm for Deep Belief Nets"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A fast, greedy algorithm is derived that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3021654"
                        ],
                        "name": "Daniel Lowd",
                        "slug": "Daniel-Lowd",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Lowd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Lowd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740213"
                        ],
                        "name": "Pedro M. Domingos",
                        "slug": "Pedro-M.-Domingos",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Domingos",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro M. Domingos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Lowd and Domingos (2005) argued that it improves over a Bayesian network with probabilistic decision trees for each conditional."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207158142,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2e5a7515d76675961462db97c70f4d2b592acd6",
            "isKey": false,
            "numCitedBy": 294,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Naive Bayes models have been widely used for clustering and classification. However, they are seldom used for general probabilistic learning and inference (i.e., for estimating and computing arbitrary joint, conditional and marginal distributions). In this paper we show that, for a wide range of benchmark datasets, naive Bayes models learned using EM have accuracy and learning time comparable to Bayesian networks with context-specific independence. Most significantly, naive Bayes inference is orders of magnitude faster than Bayesian network inference using Gibbs sampling and belief propagation. This makes naive Bayes models a very attractive alternative to Bayesian networks for general probability estimation, particularly in large or real-time domains."
            },
            "slug": "Naive-Bayes-models-for-probability-estimation-Lowd-Domingos",
            "title": {
                "fragments": [],
                "text": "Naive Bayes models for probability estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown that, for a wide range of benchmark datasets, naive Bayes models learned using EM have accuracy and learning time comparable to Bayesian networks with context-specific independence."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2373952"
                        ],
                        "name": "J. Louradour",
                        "slug": "J.-Louradour",
                        "structuredName": {
                            "firstName": "J\u00e9r\u00f4me",
                            "lastName": "Louradour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Louradour"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3087941"
                        ],
                        "name": "Pascal Lamblin",
                        "slug": "Pascal-Lamblin",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Lamblin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Lamblin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 144
                            }
                        ],
                        "text": "While the filters seem a bit noisy compared to those learned by an RBM, they are a clear improvement on those learned by a regular autoencoder (Larochelle et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 996073,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "05fd1da7b2e34f86ec7f010bef068717ae964332",
            "isKey": false,
            "numCitedBy": 1027,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization often appears to get stuck in poor solutions. Hinton et al. recently proposed a greedy layer-wise unsupervised learning procedure relying on the training algorithm of restricted Boltzmann machines (RBM) to initialize the parameters of a deep belief network (DBN), a generative model with many layers of hidden causal variables. This was followed by the proposal of another greedy layer-wise procedure, relying on the usage of autoassociator networks. In the context of the above optimization problem, we study these algorithms empirically to better understand their success. Our experiments confirm the hypothesis that the greedy layer-wise unsupervised training strategy helps the optimization by initializing weights in a region near a good local minimum, but also implicitly acts as a sort of regularization that brings better generalization and encourages internal distributed representations that are high-level abstractions of the input. We also present a series of experiments aimed at evaluating the link between the performance of deep neural networks and practical aspects of their topology, for example, demonstrating cases where the addition of more depth helps. Finally, we empirically explore simple variants of these training algorithms, such as the use of different RBM input unit distributions, a simple way of combining gradient estimators to improve performance, as well as on-line versions of those algorithms."
            },
            "slug": "Exploring-Strategies-for-Training-Deep-Neural-Larochelle-Bengio",
            "title": {
                "fragments": [],
                "text": "Exploring Strategies for Training Deep Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "These experiments confirm the hypothesis that the greedy layer-wise unsupervised training strategy helps the optimization by initializing weights in a region near a good local minimum, but also implicitly acts as a sort of regularization that brings better generalization and encourages internal distributed representations that are high-level abstractions of the input."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3087941"
                        ],
                        "name": "Pascal Lamblin",
                        "slug": "Pascal-Lamblin",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Lamblin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Lamblin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32384143"
                        ],
                        "name": "D. Popovici",
                        "slug": "D.-Popovici",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Popovici",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Popovici"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 274
                            }
                        ],
                        "text": "By training an RBM to learn the distribution of observations, it is then possible to use the posterior over these hidden variables given some observation as learned features, which can be composed into several layers and fine-tuned for some given task (Hinton et al., 2006; Bengio et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14201947,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "355d44f53428b1ac4fb2ab468d593c720640e5bd",
            "isKey": false,
            "numCitedBy": 3434,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization."
            },
            "slug": "Greedy-Layer-Wise-Training-of-Deep-Networks-Bengio-Lamblin",
            "title": {
                "fragments": [],
                "text": "Greedy Layer-Wise Training of Deep Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "These experiments confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 147
                            }
                        ],
                        "text": "One such model which has been shown to be a good model of multivariate discrete distributions is the fully visible sigmoid belief network (FVSBN) (Neal, 1992; Frey et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14290328,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a120c05ad7cd4ce2eb8fb9697e16c7c4877208a5",
            "isKey": false,
            "numCitedBy": 601,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Connectionist-Learning-of-Belief-Networks-Neal",
            "title": {
                "fragments": [],
                "text": "Connectionist Learning of Belief Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2957517"
                        ],
                        "name": "T. Tieleman",
                        "slug": "T.-Tieleman",
                        "structuredName": {
                            "firstName": "Tijmen",
                            "lastName": "Tieleman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tieleman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 194
                            }
                        ],
                        "text": "If the observations can be decomposed into an input x and a target y, then an RBM trained on such pairs can also be used to predict the missing target for new inputs (Larochelle & Bengio, 2008; Tieleman, 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7330145,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "73d6a26f407db77506959fdf3f7b853e44f3844a",
            "isKey": false,
            "numCitedBy": 902,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "A new algorithm for training Restricted Boltzmann Machines is introduced. The algorithm, named Persistent Contrastive Divergence, is different from the standard Contrastive Divergence algorithms in that it aims to draw samples from almost exactly the model distribution. It is compared to some standard Contrastive Divergence and Pseudo-Likelihood algorithms on the tasks of modeling and classifying various types of data. The Persistent Contrastive Divergence algorithm outperforms the other algorithms, and is equally fast and simple."
            },
            "slug": "Training-restricted-Boltzmann-machines-using-to-the-Tieleman",
            "title": {
                "fragments": [],
                "text": "Training restricted Boltzmann machines using approximations to the likelihood gradient"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A new algorithm for training Restricted Boltzmann Machines is introduced, which is compared to some standard Contrastive Divergence and Pseudo-Likelihood algorithms on the tasks of modeling and classifying various types of data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207178999,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e60ff004dde5c13ec53087872cfcdd12e85beb57",
            "isKey": false,
            "numCitedBy": 7558,
            "numCiting": 345,
            "paperAbstract": {
                "fragments": [],
                "text": "Theoretical results strongly suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one needs deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult optimization task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This paper discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks."
            },
            "slug": "Learning-Deep-Architectures-for-AI-Bengio",
            "title": {
                "fragments": [],
                "text": "Learning Deep Architectures for AI"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer modelssuch as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Found. Trends Mach. Learn."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "RBMs model the distribution of observations using binary hidden variables."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 243
                            }
                        ],
                        "text": "However, an RBM typically does not provide a tractable distribution estimator, since evaluating the probability it assigns to some given observation requires the computation of the so-called partition function, which itself is intractable for RBMs of even moderate size."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "While RBMs of moderate size are intractable as distribution estimators, they can be made small enough to be tractable."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 228
                            }
                        ],
                        "text": "To approximate the conditional p(vi|v<i) under an RBM, we first find an approximation q(vi,v>i,h|v<i) for p(vi,v>i,h|v<i), such that q(vi|v<i) can be easily obtained. Such a choice for q(vi,v>i,h|v<i) and a popular approach for RBMs in general is the mean-field distribution, where a factorial decomposition is assumed:\nq(vi,v>i,h|v<i) = \u00b5i(i)vi(1\u2212 \u00b5i(i))1\u2212vi\u220f j>i\n\u00b5j(i)vj (1\u2212 \u00b5j(i))1\u2212vj\u220f k \u03c4k(i)hk(1\u2212 \u03c4k(i))1\u2212hk , (6)\nwhere \u00b5j(i) is the marginal probability of observation vj being equal to 1, given v<i. Similarly, \u03c4k(i) is the marginal probability of hidden variable hk being equal to 1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 66
                            }
                        ],
                        "text": "Indeed, Larochelle et al. (2010) have shown that small, tractable RBMs can outperform standard mixture models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 145
                            }
                        ],
                        "text": "In future work, we would like to investigate the use of NADE on problems other than distribution estimation, in particular on problems for which RBMs and autoencoders are often considered."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "The RBM\u2019s intractable partition function reduces its use as a modeling tool that can be incorporated into some other larger probabilistic system."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "However, in this particular case the relative partition functions of the RBMs have successfully been approximated to build a classifier (Hinton, 2002; Schmah et al., 2009)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "Generative versus discriminative training of RBMs for classification of fMRI images."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "Unfortunately, for RBMs with even just a few hidden variables (i.e. more than 30), computing this partition function becomes intractable."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 66
                            }
                        ],
                        "text": "This version was used by Salakhutdinov and Murray (2008) to\ntrain RBMs with different versions of contrastive divergence and evaluate them as distribution estimators."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "RBMs with 500 hidden units were reported to obtain \u2212125.53, \u2212105.50 and \u221286.34 in average test loglikelihood when trained using contrastive divergence with 1, 3 and 25 steps of Gibbs sampling, respectively."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 66
                            }
                        ],
                        "text": "This is almost as good as the best RBM claim and much better than RBMs trained with just a few steps of Gibbs sampling."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 93
                            }
                        ],
                        "text": "Still, the general principle of mean-field has been shown to work well in practice for RBMs (Welling & Hinton, 2002; Salakhutdinov & Hinton, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "Even small RBMs are equivalent to very large mixture models with constrained parameters."
                    },
                    "intents": []
                }
            ],
            "corpusId": 18600461,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7b5bea7b4d40003a6887794652ea07196a97134",
            "isKey": true,
            "numCitedBy": 138,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new learning algorithm for Mean Field Boltzmann Machines based on the contrastive divergence optimization criterion. In addition to minimizing the divergence between the data distribution and the equilibrium distribution, we maximize the divergence between one-step reconstructions of the data and the equilibrium distribution. This eliminates the need to estimate equilibrium statistics, so we do not need to approximate the multimodal probability distribution of the free network with the unimodal mean field distribution. We test the learning algorithm on the classification of digits."
            },
            "slug": "A-New-Learning-Algorithm-for-Mean-Field-Boltzmann-Welling-Hinton",
            "title": {
                "fragments": [],
                "text": "A New Learning Algorithm for Mean Field Boltzmann Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A new learning algorithm for Mean Field Boltzmann Machines based on the contrastive divergence optimization criterion that eliminates the need to estimate equilibrium statistics, so it does not need to approximate the multimodal probability distribution of the free network with the unimodal mean field distribution."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783426"
                        ],
                        "name": "T. Schmah",
                        "slug": "T.-Schmah",
                        "structuredName": {
                            "firstName": "Tanya",
                            "lastName": "Schmah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Schmah"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756977"
                        ],
                        "name": "S. Small",
                        "slug": "S.-Small",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Small",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Small"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2790050"
                        ],
                        "name": "S. Strother",
                        "slug": "S.-Strother",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Strother",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Strother"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 151
                            }
                        ],
                        "text": "However, in this particular case the relative partition functions of the RBMs have successfully been approximated to build a classifier (Hinton, 2002; Schmah et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9695661,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4e3aaac4439825650480f9cb914aa895d55d1e13",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Neuroimaging datasets often have a very large number of voxels and a very small number of training cases, which means that overfitting of models for this data can become a very serious problem. Working with a set of fMRI images from a study on stroke recovery, we consider a classification task for which logistic regression performs poorly, even when L1- or L2- regularized. We show that much better discrimination can be achieved by fitting a generative model to each separate condition and then seeing which model is most likely to have generated the data. We compare discriminative training of exactly the same set of models, and we also consider convex blends of generative and discriminative training."
            },
            "slug": "Generative-versus-discriminative-training-of-RBMs-Schmah-Hinton",
            "title": {
                "fragments": [],
                "text": "Generative versus discriminative training of RBMs for classification of fMRI images"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown that much better discrimination can be achieved by fitting a generative model to each separate condition and then seeing which model is most likely to have generated the data."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120247189"
                        ],
                        "name": "Pascal Vincent",
                        "slug": "Pascal-Vincent",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Vincent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798462"
                        ],
                        "name": "Pierre-Antoine Manzagol",
                        "slug": "Pierre-Antoine-Manzagol",
                        "structuredName": {
                            "firstName": "Pierre-Antoine",
                            "lastName": "Manzagol",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre-Antoine Manzagol"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 42
                            }
                        ],
                        "text": "In particular, the denoising autoencoder (Vincent et al., 2008) is a variation where a proportion of the inputs are \u201cdestroyed\u201d (i.e. set to 0) but must be reconstructed at the output based on the other inputs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 41
                            }
                        ],
                        "text": "In particular, the denoising autoencoder (Vincent et al., 2008) is a variation where a proportion of the inputs are \u201cdestroyed\u201d (i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207168299,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "843959ffdccf31c6694d135fad07425924f785b1",
            "isKey": false,
            "numCitedBy": 5471,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite."
            },
            "slug": "Extracting-and-composing-robust-features-with-Vincent-Larochelle",
            "title": {
                "fragments": [],
                "text": "Extracting and composing robust features with denoising autoencoders"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work introduces and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47577047"
                        ],
                        "name": "Na Li",
                        "slug": "Na-Li",
                        "structuredName": {
                            "firstName": "Na",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Na Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145761702"
                        ],
                        "name": "M. Stephens",
                        "slug": "M.-Stephens",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Stephens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stephens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 147
                            }
                        ],
                        "text": "Finally, the general approach of decomposing the probability of an intractable model and approximating the conditionals was previously explored by Li and Stephens (2003) for a model of linkage disequilibrium in genetic data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 51
                            }
                        ],
                        "text": "Also, using a separate set of weights means that we can invoke the universal approximation theorems for neural networks for each conditional, since V could be such that each hidden unit only has non-zero connections to one\nof the conditional outputs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17193500,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "97de476f5d3485f8fe29548ed061355d596f9d81",
            "isKey": false,
            "numCitedBy": 871,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new statistical model for patterns of linkage disequilibrium (LD) among multiple SNPs in a population sample. The model overcomes limitations of existing approaches to understanding, summarizing, and interpreting LD by (i) relating patterns of LD directly to the underlying recombination process; (ii) considering all loci simultaneously, rather than pairwise; (iii) avoiding the assumption that LD necessarily has a \"block-like\" structure; and (iv) being computationally tractable for huge genomic regions (up to complete chromosomes). We examine in detail one natural application of the model: estimation of underlying recombination rates from population data. Using simulation, we show that in the case where recombination is assumed constant across the region of interest, recombination rate estimates based on our model are competitive with the very best of current available methods. More importantly, we demonstrate, on real and simulated data, the potential of the model to help identify and quantify fine-scale variation in recombination rate from population data. We also outline how the model could be useful in other contexts, such as in the development of more efficient haplotype-based methods for LD mapping."
            },
            "slug": "Modeling-linkage-disequilibrium-and-identifying-Li-Stephens",
            "title": {
                "fragments": [],
                "text": "Modeling linkage disequilibrium and identifying recombination hotspots using single-nucleotide polymorphism data."
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The model overcomes limitations of existing approaches to understanding, summarizing, and interpreting LD by relating patterns of LD directly to the underlying recombination process and is competitive with the very best of current available methods for recombination rate estimates."
            },
            "venue": {
                "fragments": [],
                "text": "Genetics"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706809"
                        ],
                        "name": "Marc'Aurelio Ranzato",
                        "slug": "Marc'Aurelio-Ranzato",
                        "structuredName": {
                            "firstName": "Marc'Aurelio",
                            "lastName": "Ranzato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc'Aurelio Ranzato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778989"
                        ],
                        "name": "M. Szummer",
                        "slug": "M.-Szummer",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Szummer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Szummer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 286,
                                "start": 263
                            }
                        ],
                        "text": "\u2026one could model other distributions by adjusting the output non-linearity accordingly: a linear output for Gaussian distributions, a so-called softmax output for multinomial distributions or an exponentiated output to yield Exponential or Poisson distributions (Ranzato & Szummer, 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2151537,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "18862760ac708a589afa5848ab55931996db1b28",
            "isKey": false,
            "numCitedBy": 219,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Finding good representations of text documents is crucial in information retrieval and classification systems. Today the most popular document representation is based on a vector of word counts in the document. This representation neither captures dependencies between related words, nor handles synonyms or polysemous words. In this paper, we propose an algorithm to learn text document representations based on semi-supervised autoencoders that are stacked to form a deep network. The model can be trained efficiently on partially labeled corpora, producing very compact representations of documents, while retaining as much class information and joint word statistics as possible. We show that it is advantageous to exploit even a few labeled samples during training."
            },
            "slug": "Semi-supervised-learning-of-compact-document-with-Ranzato-Szummer",
            "title": {
                "fragments": [],
                "text": "Semi-supervised learning of compact document representations with deep networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An algorithm to learn text document representations based on semi-supervised autoencoders that are stacked to form a deep network that can be trained efficiently on partially labeled corpora, producing very compact representations of documents, while retaining as much class information and joint word statistics as possible."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145704247"
                        ],
                        "name": "James Martens",
                        "slug": "James-Martens",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Martens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Martens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 128
                            }
                        ],
                        "text": "Moreover NADE will be able to take advantage of future non-linear optimization methods, currently an active area research (e.g. Martens, 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11154521,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4c46347fbc272b21468efe3d9af34b4b2bad6684",
            "isKey": false,
            "numCitedBy": 845,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a 2nd-order optimization method based on the \"Hessian-free\" approach, and apply it to training deep auto-encoders. Without using pre-training, we obtain results superior to those reported by Hinton & Salakhutdinov (2006) on the same tasks they considered. Our method is practical, easy to use, scales nicely to very large datasets, and isn't limited in applicability to auto-encoders, or any specific model class. We also discuss the issue of \"pathological curvature\" as a possible explanation for the difficulty of deep-learning and how 2nd-order optimization, and our method in particular, effectively deals with it."
            },
            "slug": "Deep-learning-via-Hessian-free-optimization-Martens",
            "title": {
                "fragments": [],
                "text": "Deep learning via Hessian-free optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A 2nd-order optimization method based on the \"Hessian-free\" approach is developed, and applied to training deep auto-encoders, and results superior to those reported by Hinton & Salakhutdinov (2006) are obtained."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061635449"
                        ],
                        "name": "J. Ott",
                        "slug": "J.-Ott",
                        "structuredName": {
                            "firstName": "J\u00fcrg",
                            "lastName": "Ott",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ott"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11827568"
                        ],
                        "name": "R. Kronmal",
                        "slug": "R.-Kronmal",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Kronmal",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kronmal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 141
                            }
                        ],
                        "text": "Though this comparison excludes intractable models such as factor models in general or models based on orthogonal expansions of Walsh bases (Ott & Kronmal, 1976; Chen et al., 1989), we also provide a comparison with a large and intractable RBM which suggests that NADE has comparable modeling power."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 103
                            }
                        ],
                        "text": ")i,\u00b7hi ) (9)\nhi = sigm (c + W\u00b7, iv i) , (10)\nwhich corresponds to a feed-forward neural network with a single hidden layer, and tied weighted connections going in and out of the hidden layer."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123084404,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5efe95b6603833595a19308192bb91dd577022cd",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Four new methods for classification of multivariate binary data are presented, based on an orthogonal expansion of the density in terms of discrete Fourier series. The performance of these methods in 11 populations of various structures was measured in terms of mean error of misclassification and was compared to three well-known methods. Also, performance in density estimation was measured for the appropriate methods. In general, the new methods seem to be superior for classification as well as for density estimation."
            },
            "slug": "Some-Classification-Procedures-for-Multivariate-Ott-Kronmal",
            "title": {
                "fragments": [],
                "text": "Some Classification Procedures for Multivariate Binary Data Using Orthogonal Functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748557"
                        ],
                        "name": "P. Smolensky",
                        "slug": "P.-Smolensky",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Smolensky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Smolensky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 57
                            }
                        ],
                        "text": "In recent years, the restricted Boltzmann machine (RBM) (Smolensky, 1986; Freund & Haussler, 1992; Hinton, 2002) has frequently been used as a feature extractor."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 533055,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4f7476037408ac3d993f5088544aab427bc319c1",
            "isKey": false,
            "numCitedBy": 1949,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : At this early stage in the development of cognitive science, methodological issues are both open and central. There may have been times when developments in neuroscience, artificial intelligence, or cognitive psychology seduced researchers into believing that their discipline was on the verge of discovering the secret of intelligence. But a humbling history of hopes disappointed has produced the realization that understanding the mind will challenge the power of all these methodologies combined. The work reported in this chapter rests on the conviction that a methodology that has a crucial role to play in the development of cognitive science is mathematical analysis. The success of cognitive science, like that of many other sciences, will, I believe, depend upon the construction of a solid body of theoretical results: results that express in a mathematical language the conceptual insights of the field; results that squeeze all possible implications out of those insights by exploiting powerful mathematical techniques. This body of results, which I will call the theory of information processing, exists because information is a concept that lends itself to mathematical formalization. One part of the theory of information processing is already well-developed. The classical theory of computation provides powerful and elegant results about the notion of effective procedure, including languages for precisely expressing them and theoretical machines for realizing them."
            },
            "slug": "Information-processing-in-dynamical-systems:-of-Smolensky",
            "title": {
                "fragments": [],
                "text": "Information processing in dynamical systems: foundations of harmony theory"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The work reported in this chapter rests on the conviction that a methodology that has a crucial role to play in the development of cognitive science is mathematical analysis."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49794603"
                        ],
                        "name": "X. Chen",
                        "slug": "X.-Chen",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Chen",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48104572"
                        ],
                        "name": "P. Krishnaiah",
                        "slug": "P.-Krishnaiah",
                        "structuredName": {
                            "firstName": "Paruchuri",
                            "lastName": "Krishnaiah",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Krishnaiah"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153667762"
                        ],
                        "name": "W. Liang",
                        "slug": "W.-Liang",
                        "structuredName": {
                            "firstName": "Wen-Qi",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Liang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 162
                            }
                        ],
                        "text": "Though this comparison excludes intractable models such as factor models in general or models based on orthogonal expansions of Walsh bases (Ott & Kronmal, 1976; Chen et al., 1989), we also provide a comparison with a large and intractable RBM which suggests that NADE has comparable modeling power."
                    },
                    "intents": []
                }
            ],
            "corpusId": 121421089,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "dd5f1f7043ad510b53dc07525d66404a060a01b3",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Estimation-of-multivariate-binary-density-using-Chen-Krishnaiah",
            "title": {
                "fragments": [],
                "text": "Estimation of multivariate binary density using orthogonal functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749650"
                        ],
                        "name": "B. Frey",
                        "slug": "B.-Frey",
                        "structuredName": {
                            "firstName": "Brendan",
                            "lastName": "Frey",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Frey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 201
                            }
                        ],
                        "text": "Better Bayesian networks can be obtained by using log-linear logistic regressors for the conditionals, also known as fully visible sigmoid belief networks or logistic autoregressive Bayesian networks (Frey, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62488180,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "629cc74dcaf655feea40f64cd74617ac884ed0f8",
            "isKey": false,
            "numCitedBy": 621,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Probabilistic inference in graphical models pattern classification unsupervised learning data compression channel coding future research directions."
            },
            "slug": "Graphical-Models-for-Machine-Learning-and-Digital-Frey",
            "title": {
                "fragments": [],
                "text": "Graphical Models for Machine Learning and Digital Communication"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "Probabilistic inference in graphical models pattern classification unsupervised learning data compression channel coding future research directions and how this affects research directions is investigated."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 40
                            }
                        ],
                        "text": "On the remaining dataset, both NADE and FVSBN are the best performing models with statistically indistinguishable results."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 7
                            }
                        ],
                        "text": "In the FVSBN, the acyclic graph is obtained by defining the parents of vi as all variables that are to its left1, or vparents(i) = v i, where v i refers to the subvector containing all variables vj such that j   i (see Figure 1 for an illustration)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 66
                            }
                        ],
                        "text": "NADE is the sole best performing model on 6 of the datasets, with FVSBN outperforming NADE on only 1 dataset."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 87
                            }
                        ],
                        "text": "No other regularization technique (e.g. priors on the parameters) were used for either FVSBN or NADE."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 5
                            }
                        ],
                        "text": "Both FVSBN and NADE were trained by stochastic gradient descent."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 159
                            }
                        ],
                        "text": "One such model which has been shown to be a good model of multivariate discrete distributions is the fully visible sigmoid belief network (FVSBN) (Neal, 1992; Frey et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 146
                            }
                        ],
                        "text": "\u2026been shown to provide a powerful framework for deriving tractable distribution estimators is the family of fully visible Bayesian networks (e.g. Frey et al., 1996; Bengio and Bengio, 2000), which decompose the observation\u2019s probability distribution as follows:\np(v) = D\u220f\ni=1\np(vi|vparents(i)) ,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 132
                            }
                        ],
                        "text": "\u00b1 0.06 \u00b1 0.03 \u00b1 0.40 \u00b1 0.05 \u00b1 1.06 \u00b1 0.29 \u00b1 0.11 \u00b1 0.21 RBForest 4.12 0.59 1.39 0.04 12.61 3.78 0.56 -0.15 \u00b1 0.06 \u00b1 0.02 \u00b1 0.49 \u00b1 0.07 \u00b1 1.07 \u00b1 0.28 \u00b1 0.11 \u00b1 0.21 FVSBN 7.27 11.02 14.55 4.19 13.14 1.26 -2.24 0.81 \u00b1 0.04 \u00b1 0.01 \u00b1 0.50 \u00b1 0.05 \u00b1 0.98 \u00b1 0.23 \u00b1 0.11 \u00b1 0.20 NADE 7.25 11.42 13.38 4.65 16.94 13.34 0.93 1.77 \u00b1 0.05 \u00b1 0.01 \u00b1 0.57 \u00b1 0.04 \u00b1 1.11 \u00b1 0.21 \u00b1 0.11 \u00b1 0.20 Normalization -20.44 -23.41 -98.19 -14.46 -290.02 -40.56 -47.59 -30.16\nTo measure the sensitivity of NADE to the ordering of the observations we trained a dozen separate models for the mushrooms, dna and nips-0-12 datasets using different random shufflings."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 158
                            }
                        ],
                        "text": "While the choice of log-linear conditionals probably yields a misspecified model in most cases, these networks do tend to perform better than mixture models (Frey et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 269
                            }
                        ],
                        "text": "It was proposed by Larochelle et al. (2010) to allow the number of parameters of the RBM to grow while maintaining tractability;\n\u2022 RBForest: an RBM where the activation of hidden units within a group obey tree constraints (see Larochelle et al. (2010) for more details);\n\u2022 FVSBN: a fully visible sigmoid belief network."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 9
                            }
                        ],
                        "text": "Finally, FVSBN trained by stochastic gradient descent achieves \u221297.45 and improves on the mixture models but not on NADE."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 140
                            }
                        ],
                        "text": "A single random ordering of the observation variables was used to decompose p(v) into conditionals, and the same ordering was used for both FVSBN and NADE."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Does the wake-sleep algorithm learn good density estimators? Advances in Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 40
                            }
                        ],
                        "text": "On the remaining dataset, both NADE and FVSBN are the best performing models with statistically indistinguishable results."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 7
                            }
                        ],
                        "text": "In the FVSBN, the acyclic graph is obtained by defining the parents of vi as all variables that are to its left1, or vparents(i) = v i, where v i refers to the subvector containing all variables vj such that j   i (see Figure 1 for an illustration)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 66
                            }
                        ],
                        "text": "NADE is the sole best performing model on 6 of the datasets, with FVSBN outperforming NADE on only 1 dataset."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 87
                            }
                        ],
                        "text": "No other regularization technique (e.g. priors on the parameters) were used for either FVSBN or NADE."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 5
                            }
                        ],
                        "text": "Both FVSBN and NADE were trained by stochastic gradient descent."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 159
                            }
                        ],
                        "text": "One such model which has been shown to be a good model of multivariate discrete distributions is the fully visible sigmoid belief network (FVSBN) (Neal, 1992; Frey et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 146
                            }
                        ],
                        "text": "\u2026been shown to provide a powerful framework for deriving tractable distribution estimators is the family of fully visible Bayesian networks (e.g. Frey et al., 1996; Bengio and Bengio, 2000), which decompose the observation\u2019s probability distribution as follows:\np(v) = D\u220f\ni=1\np(vi|vparents(i)) ,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 132
                            }
                        ],
                        "text": "\u00b1 0.06 \u00b1 0.03 \u00b1 0.40 \u00b1 0.05 \u00b1 1.06 \u00b1 0.29 \u00b1 0.11 \u00b1 0.21 RBForest 4.12 0.59 1.39 0.04 12.61 3.78 0.56 -0.15 \u00b1 0.06 \u00b1 0.02 \u00b1 0.49 \u00b1 0.07 \u00b1 1.07 \u00b1 0.28 \u00b1 0.11 \u00b1 0.21 FVSBN 7.27 11.02 14.55 4.19 13.14 1.26 -2.24 0.81 \u00b1 0.04 \u00b1 0.01 \u00b1 0.50 \u00b1 0.05 \u00b1 0.98 \u00b1 0.23 \u00b1 0.11 \u00b1 0.20 NADE 7.25 11.42 13.38 4.65 16.94 13.34 0.93 1.77 \u00b1 0.05 \u00b1 0.01 \u00b1 0.57 \u00b1 0.04 \u00b1 1.11 \u00b1 0.21 \u00b1 0.11 \u00b1 0.20 Normalization -20.44 -23.41 -98.19 -14.46 -290.02 -40.56 -47.59 -30.16\nTo measure the sensitivity of NADE to the ordering of the observations we trained a dozen separate models for the mushrooms, dna and nips-0-12 datasets using different random shufflings."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 158
                            }
                        ],
                        "text": "While the choice of log-linear conditionals probably yields a misspecified model in most cases, these networks do tend to perform better than mixture models (Frey et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 269
                            }
                        ],
                        "text": "It was proposed by Larochelle et al. (2010) to allow the number of parameters of the RBM to grow while maintaining tractability;\n\u2022 RBForest: an RBM where the activation of hidden units within a group obey tree constraints (see Larochelle et al. (2010) for more details);\n\u2022 FVSBN: a fully visible sigmoid belief network."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 9
                            }
                        ],
                        "text": "Finally, FVSBN trained by stochastic gradient descent achieves \u221297.45 and improves on the mixture models but not on NADE."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 140
                            }
                        ],
                        "text": "A single random ordering of the observation variables was used to decompose p(v) into conditionals, and the same ordering was used for both FVSBN and NADE."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Does the wake-sleep algorithm learn good density estimators? Advances in Neural Information Processing Systems 8 (NIPS\u201995) (pp. 661\u2013670)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 130
                            }
                        ],
                        "text": "Indeed, computing probabilities of observations or sampling new observations from the model can be done exactly and efficiently under NADE."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 159
                            }
                        ],
                        "text": "One such model which has been shown to be a good model of multivariate discrete distributions is the fully visible sigmoid belief network (FVSBN) (Neal, 1992; Frey et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 146
                            }
                        ],
                        "text": "\u2026been shown to provide a powerful framework for deriving tractable distribution estimators is the family of fully visible Bayesian networks (e.g. Frey et al., 1996; Bengio and Bengio, 2000), which decompose the observation\u2019s probability distribution as follows:\np(v) = D\u220f\ni=1\np(vi|vparents(i)) ,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We also show that its performance is very close to that of a large but intractable RBM whose partition\nfunction has been approximated."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 158
                            }
                        ],
                        "text": "While the choice of log-linear conditionals probably yields a misspecified model in most cases, these networks do tend to perform better than mixture models (Frey et al., 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 15
                            }
                        ],
                        "text": "The RBM\u2019s intractable partition function reduces its use as a modeling tool that can be incorporated into some other larger probabilistic system."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Does the wake-sleep algorithm learn good density estimators"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems 8 (NIPS'95)"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 176
                            }
                        ],
                        "text": "Another type of model which has also been shown to provide a powerful framework for deriving tractable distribution estimators is the family of fully visible Bayesian networks (e.g. Frey et al., 1996; Bengio and Bengio, 2000), which decompose the observation\u2019s probability distribution as follows:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 143
                            }
                        ],
                        "text": "\u2026a powerful framework for deriving tractable distribution estimators is the family of fully visible Bayesian networks (e.g. Frey et al., 1996; Bengio and Bengio, 2000), which decompose the observation\u2019s probability distribution as follows:\np(v) = D\u220f\ni=1\np(vi|vparents(i)) , (3)\nwhere all\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Bengio and Bengio (2000) also proposed using neural networks for the conditionals of a Bayesian network."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Modeling highdimensional discrete data with multi-layer neural networks. Advances in Neural Information Processing Systems 12 (NIPS\u201999) (pp. 400\u2013406)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The neural autoregressive distribution estimator"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 74
                            }
                        ],
                        "text": "In recent years, the restricted Boltzmann machine (RBM) (Smolensky, 1986; Freund & Haussler, 1992; Hinton, 2002) has frequently been used as a feature extractor."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 50
                            }
                        ],
                        "text": "We show that this new model outperforms other multivariate binary distribution estimators on several datasets and performs similarly to a large (but intractable) RBM."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A fast and exact learning rule for a restricted class of Boltzmann machines"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems 4 (NIPS'91)"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 83
                            }
                        ],
                        "text": "In recent years, the restricted Boltzmann machine (RBM) (Smolensky, 1986; Freund & Haussler, 1992; Hinton, 2002) has frequently been used as a feature extractor."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A fast and exact learning rule for a restricted class of Boltzmann machines"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems 4 ( NIPS \u2019 91 )"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 143
                            }
                        ],
                        "text": "\u2026a powerful framework for deriving tractable distribution estimators is the family of fully visible Bayesian networks (e.g. Frey et al., 1996; Bengio and Bengio, 2000), which decompose the observation\u2019s probability distribution as follows:\np(v) = D\u220f\ni=1\np(vi|vparents(i)) , (3)\nwhere all\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Bengio and Bengio (2000) also proposed using neural networks for the conditionals of a Bayesian network."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Modeling highdimensional discrete data with multi-layer neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems 12 (NIPS'99)"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Does the wake - sleep algorithm learn good density estimators ?"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The neural autoregressive distribution estimator.\" Proceedings of the fourteenth international conference on artificial intelligence and statistics"
            },
            "venue": {
                "fragments": [],
                "text": "JMLR Workshop and Conference Proceedings,"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 74
                            }
                        ],
                        "text": "In recent years, the restricted Boltzmann machine (RBM) (Smolensky, 1986; Freund & Haussler, 1992; Hinton, 2002) has frequently been used as a feature extractor."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A fast and exact learning rule for a restricted class of Boltzmann machines. Advances in Neural Information Processing Systems 4 (NIPS\u201991) (pp. 912\u2013919)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 19,
            "methodology": 15,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 37,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/The-Neural-Autoregressive-Distribution-Estimator-Larochelle-Murray/32f078a7478d1ec2169599500a4507aceaccdda7?sort=total-citations"
}