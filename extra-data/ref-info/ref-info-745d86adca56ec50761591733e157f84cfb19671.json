{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747648"
                        ],
                        "name": "William Schuler",
                        "slug": "William-Schuler",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Schuler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William Schuler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3975984"
                        ],
                        "name": "Stephen T Wu",
                        "slug": "Stephen-T-Wu",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Wu",
                            "middleNames": [
                                "T"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen T Wu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60548486,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ad9b8e92f303cebfd4c9a97ca99ac09ace82fcc",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 104,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis aims to define and extend a line of computational models for text comprehension that are humanly plausible. Since natural language is human by nature, computational models of human language will always be just that \u2014 models. To the degree that they miss out on information that humans would tap into, they may be improved by considering the human process of language processing in a linguistic, psychological, and cognitive light. \nApproaches to constructing vectorial semantic spaces often begin with the distributional hypothesis, i.e., that words can be judged \u2018by the company they keep.\u2019 Typically, words that occur in the same documents are similar, and will have similar vectorial meaning representations. However, this does not in itself provide a way for two distinct meanings to be composed, and it ignores syntactic context. \nBoth of these problems are solved in Structured Vectorial Semantics (SVS), a new framework that fully unifies vectorial semantics with syntactic parsing. Most approaches that try to combine syntactic and semantic information will either lack a cohesive semantic component or a full-fledged parser, but SVS integrates both. Thus, in the SVS framework, interpretation is interactive, considering both syntax and semantics simultaneously. \nCognitively-plausible language models would also be incremental, support linear-time inference, and operate in only a bounded store of short-term memory. Each of these characteristics is supported by right-corner Hierarchical Hidden Markov Model (HHMM) parsing; therefore, SVS will be transformed into right-corner form and mapped to an HHMM parser. The resulting representation will then encode a psycholinguistically plausible incremental SVS language model."
            },
            "slug": "Vectorial-representations-of-meaning-for-a-model-of-Schuler-Wu",
            "title": {
                "fragments": [],
                "text": "Vectorial representations of meaning for a computational model of language comprehension"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This thesis aims to define and extend a line of computational models for text comprehension that are humanly plausible, and introduces Structured Vectorial Semantics (SVS), a new framework that fully unifies vectorial semantics with syntactic parsing."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708114"
                        ],
                        "name": "Katrin Erk",
                        "slug": "Katrin-Erk",
                        "structuredName": {
                            "firstName": "Katrin",
                            "lastName": "Erk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katrin Erk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708581"
                        ],
                        "name": "Sebastian Pad\u00f3",
                        "slug": "Sebastian-Pad\u00f3",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Pad\u00f3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Pad\u00f3"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The model of Erk and Pado (2008) attempts to overcome this issue by creating vector representations not only of a target word itself, but also of its dependencies."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This operation of combining multiple constituent parts into a single whole is the key characteristic of composition and sets it apart from the problem of, say, modelling how the meaning of individual words are modified or selected in context (Erk and Pado, 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Instead, Erk and Pado (2008) present their method as model of word meaning in context, in other words, how an individual word\u2019s meaning is modulated by the other words around it."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1588782,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cb9cc883bdd08d58feee5c7da01acff6fdb4ad78",
            "isKey": true,
            "numCitedBy": 387,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the task of computing vector space representations for the meaning of word occurrences, which can vary widely according to context. This task is a crucial step towards a robust, vector-based compositional account of sentence meaning. We argue that existing models for this task do not take syntactic structure sufficiently into account. \n \nWe present a novel structured vector space model that addresses these issues by incorporating the selectional preferences for words' argument positions. This makes it possible to integrate syntax into the computation of word meaning in context. In addition, the model performs at and above the state of the art for modeling the contextual adequacy of paraphrases."
            },
            "slug": "A-Structured-Vector-Space-Model-for-Word-Meaning-in-Erk-Pad\u00f3",
            "title": {
                "fragments": [],
                "text": "A Structured Vector Space Model for Word Meaning in Context"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel structured vector space model is presented that makes it possible to integrate syntax into the computation of word meaning in context and performs at and above the state of the art for modeling the contextual adequacy of paraphrases."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34902160"
                        ],
                        "name": "Jeff Mitchell",
                        "slug": "Jeff-Mitchell",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Mitchell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Mitchell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 221
                            }
                        ],
                        "text": "\u2026Lapata and Lascarides (2003) present an experiment where participants rate whether adjective\u2013noun combinations and their paraphrases have similar meanings, whereas other work (Li, McLean, Bandar, O\u2019Shea, & Crockett, 2006; Mitchell & Lapata, 2008) elicits similarity judgments for sentence pairs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18597583,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5d67d1dc671bce42a9daac0c3605adb3fcfc697",
            "isKey": false,
            "numCitedBy": 730,
            "numCiting": 133,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a framework for representing the meaning of phrases and sentences in vector space. Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions. Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task. Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments."
            },
            "slug": "Vector-based-Models-of-Semantic-Composition-Mitchell-Lapata",
            "title": {
                "fragments": [],
                "text": "Vector-based Models of Semantic Composition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Under this framework, a wide range of composition models are introduced which are evaluated empirically on a sentence similarity task and demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144523372"
                        ],
                        "name": "S. Clark",
                        "slug": "S.-Clark",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3326718"
                        ],
                        "name": "B. Coecke",
                        "slug": "B.-Coecke",
                        "structuredName": {
                            "firstName": "Bob",
                            "lastName": "Coecke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Coecke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "80045331"
                        ],
                        "name": "Mehrnoosh",
                        "slug": "Mehrnoosh",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Mehrnoosh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mehrnoosh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1480,
                                "start": 0
                            }
                        ],
                        "text": "Clark et al. (2008) do not suggest concrete methods for constructing or estimating the various tensors involved in their model. Instead, they are more interested in its formal properties and do not report any empirical tests of this approach. Unfortunately, comparisons across vector composition models have been few and far between. The merits of different approaches are illustrated with special purpose examples, and large-scale evaluations are uniformly absent. For instance, Kintsch (2001) demonstrates how his own composition algorithm works intuitively on a few hand-selected examples but does not provide a comprehensive test set (for a criticism of Kintsch\u2019s 2001 evaluation standards, see Frank, Koppen, Noordman, & Vonk, 2008). In a similar vein, Widdows (2008) explores the potential of vector product operations for modeling compositional phenomena in natural language, again on a small number of hand-picked examples. Our work goes beyond these isolated proposals; we present a framework for vector composition which allows us to explore a range of potential composition functions, their properties, and relations. Under this framework, we reconceptualize existing composition models as well as introduce novel ones. Our experiments make use of conventional semantic vectors built from co-occurrence data. However, our compositional models are not tied to a specific representation and could be used with the holographic vectors proposed in Jones and Mewhort (2007) or with random indexing; however, we leave this to future work."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 773,
                                "start": 0
                            }
                        ],
                        "text": "Clark et al. (2008) do not suggest concrete methods for constructing or estimating the various tensors involved in their model. Instead, they are more interested in its formal properties and do not report any empirical tests of this approach. Unfortunately, comparisons across vector composition models have been few and far between. The merits of different approaches are illustrated with special purpose examples, and large-scale evaluations are uniformly absent. For instance, Kintsch (2001) demonstrates how his own composition algorithm works intuitively on a few hand-selected examples but does not provide a comprehensive test set (for a criticism of Kintsch\u2019s 2001 evaluation standards, see Frank, Koppen, Noordman, & Vonk, 2008). In a similar vein, Widdows (2008) explores the potential of vector product operations for modeling compositional phenomena in natural language, again on a small number of hand-picked examples."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 495,
                                "start": 0
                            }
                        ],
                        "text": "Clark et al. (2008) do not suggest concrete methods for constructing or estimating the various tensors involved in their model. Instead, they are more interested in its formal properties and do not report any empirical tests of this approach. Unfortunately, comparisons across vector composition models have been few and far between. The merits of different approaches are illustrated with special purpose examples, and large-scale evaluations are uniformly absent. For instance, Kintsch (2001) demonstrates how his own composition algorithm works intuitively on a few hand-selected examples but does not provide a comprehensive test set (for a criticism of Kintsch\u2019s 2001 evaluation standards, see Frank, Koppen, Noordman, & Vonk, 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Clark et al. (2008) suggest a scheme in which the structure of a representation depends on its syntactic type, such that, for example, if nouns are represented by plain vectors then adjectives, as modifiers of nouns, are represented by matrices."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 15
                            }
                        ],
                        "text": "Interestingly, Clark, Coecke, and Sadrzadeh (2008) try to construct a tensor product-based model of vector composition which makes an explicit connection to models of linguistic composition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 20
                            }
                        ],
                        "text": "This is essentially Clark et al.\u2019s (2008) approach to adjective\u2013noun composition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Clark et al. (2008) do not suggest concrete methods for constructing or estimating the various tensors involved in their model."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15293885,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f86ef3e7b856d61ade62e643d87d288fef8827dd",
            "isKey": true,
            "numCitedBy": 119,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a mathematical framework for a unification of the distributional theory of meaning in terms of vector space models, and a compositional theory for grammatical types, namely Lambek\u2019s pregroup semantics. A key observation is that the monoidal category of (finite dimensional) vector spaces, linear maps and the tensor product, as well as any pregroup, are examples of compact closed categories. Since, by definition, a pregroup is a compact closed category with trivial morphisms, its compositional content is reflected within the compositional structure of any non-degenerate compact closed category. The (slightly refined) category of vector spaces enables us to compute the meaning of a compound well-typed sentence from the meaning of its constituents, by \u2018lifting\u2019 the type reduction mechanisms of pregroup semantics to the whole category. These sentence meanings live in a single space, independent of the grammatical structure of the sentence. Hence we can use the inner-product to compare meanings of arbitrary sentences. A variation of this procedure which involves constraining the scalars of the vector spaces to the semiring of Booleans results in the well-known Montague semantics."
            },
            "slug": "A-Compositional-Distributional-Model-of-Meaning-Clark-Coecke",
            "title": {
                "fragments": [],
                "text": "A Compositional Distributional Model of Meaning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689647"
                        ],
                        "name": "Peter D. Turney",
                        "slug": "Peter-D.-Turney",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Turney",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D. Turney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990190"
                        ],
                        "name": "P. Pantel",
                        "slug": "P.-Pantel",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Pantel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pantel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 150
                            }
                        ],
                        "text": "\u2026consider that the values of their components are derived from event frequencies such as the number of times a given word appears in a given context (Turney & Pantel, 2010).1 Having this in mind, we present a general framework for vector-based composition that allows us to consider different\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 82
                            }
                        ],
                        "text": "A different type of semantic space is proposed in Lin and Pantel (2001) (see also Turney and Pantel, 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1500900,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3a0e788268fafb23ab20da0e98bb578b06830f7d",
            "isKey": false,
            "numCitedBy": 2722,
            "numCiting": 208,
            "paperAbstract": {
                "fragments": [],
                "text": "Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field."
            },
            "slug": "From-Frequency-to-Meaning:-Vector-Space-Models-of-Turney-Pantel",
            "title": {
                "fragments": [],
                "text": "From Frequency to Meaning: Vector Space Models of Semantics"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs, and to provide pointers into the literature for those who are less familiar with the field."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153592298"
                        ],
                        "name": "S. McDonald",
                        "slug": "S.-McDonald",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "McDonald",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. McDonald"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 55
                            }
                        ],
                        "text": "Our first model is a simple and popular semantic space (Bullinaria & Levy, 2007; Lowe, 2000; McDonald, 2000) that associates each vector component with a particular context word, and assigns it a value based on the strength of its co-occurrence with the target (i."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 154
                            }
                        ],
                        "text": "\u2026et al., 2007; Landauer & Dumais, 1997) such\nas those included in the Test of English as Foreign Language (TOEFL), reading times (Griffiths et al., 2007; McDonald, 2000), and judgments of semantic similarity (McDonald, 2000) and association (Denhire & Lemaire, 2004; Griffiths et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 93
                            }
                        ],
                        "text": "Our first model is a simple and popular semantic space (Bullinaria & Levy, 2007; Lowe, 2000; McDonald, 2000) that associates each vector component with a particular context word, and assigns it a value based on the strength of its co-occurrence with the target (i.e., the word for which a semantic\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 135
                            }
                        ],
                        "text": "28) due to its simplicity and good performance in simulating word similarity ratings (Bullinaria & Levy, 2007; Griffiths et al., 2007; McDonald, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 24
                            }
                        ],
                        "text": "Following previous work (Bullinaria & Levy, 2007; McDonald, 2000; Pad\u00f3 & Lapata, 2007), we then used correlation analysis to examine the relationship between the human ratings and their corresponding vector-based similarity values."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 62
                            }
                        ],
                        "text": ", 2007; McDonald, 2000), and judgments of semantic similarity (McDonald, 2000) and association (Denhire & Lemaire, 2004; Griffiths et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 50
                            }
                        ],
                        "text": "Following previous work (Bullinaria & Levy, 2007; McDonald, 2000; Pado\u0301 & Lapata, 2007), we then used correlation analysis to examine the relationship between the human ratings and their corresponding vector-based similarity values."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 84
                            }
                        ],
                        "text": "as those included in the Test of English as Foreign Language (TOEFL), reading times (Griffiths et al., 2007; McDonald, 2000), and judgments of semantic similarity (McDonald, 2000) and association (Denhire & Lemaire, 2004; Griffiths et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 30529950,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "6d8018bd8b288baca0c55522877efd1b49258747",
            "isKey": true,
            "numCitedBy": 74,
            "numCiting": 172,
            "paperAbstract": {
                "fragments": [],
                "text": "A central concern of psycholinguistic research is explaining the relative ease or difficulty involved in processing words. In this thesis, we explore the connection between lexical processing effort and measurable properties of the linguistic environment. Distributional information (information about a word\u2019s contexts of use) is easily extracted from large language corpora in the form of co-occurrence statistics. We claim that such simple distributional statistics can form the basis of a parsimonious model of lexical processing effort. Adopting the purposive style of explanation advocated by the recent rational analysis approach to understanding cognition, we propose that the primary function of the human language processor is to recover meaning from an utterance. We assume that for this task to be efficient, a useful processing strategy is to use prior knowledge in order to build expectations about the meaning of upcoming words. Processing effort can then be seen as reflecting the difference between \u2018expected\u2019 meaning and \u2018actual\u2019 meaning. Applying the tools of information theory to lexical representations constructed from simple distributional statistics, we show how this quantity can be estimated as the amount of information conveyed by a word about its contexts of use. The hypothesis that properties of the linguistic environment are relevant to lexical processing effort is evaluated against a wide range of empirical data, including both new experimental studies and computational reanalyses of published behavioural data. Phenomena accounted for using the current approach include: both singleword and multiple-word lexical priming, isolated word recognition, the effect of contextual constraint on eye movements during reading, sentence and \u2018feature\u2019 priming, and picture naming performance by Alzheimer\u2019s patients. Besides explaining a broad range of empirical findings, our model provides an integrated account of both context-dependent and context-independent processing behaviour, offers an objective alternative to the influential spreading activation model of contextual facilitation, and invites reinterpretation of a number of controversial issues in the literature, such as the word frequency effect and the need for distinct mechanisms to explain semantic and associative priming. We conclude by emphasising the important role of distributional information in explanations of lexical processing effort, and suggest that environmental factors in general should given a more prominent place in theories of human language processing."
            },
            "slug": "Environmental-Determinants-of-Lexical-Processing-McDonald",
            "title": {
                "fragments": [],
                "text": "Environmental Determinants of Lexical Processing Effort"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This model provides an integrated account of both context-dependent and context-independent processing behaviour, offers an objective alternative to the influential spreading activation model of contextual facilitation, and invites reinterpretation of a number of controversial issues in the literature."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3025325"
                        ],
                        "name": "W. Kintsch",
                        "slug": "W.-Kintsch",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Kintsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Kintsch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Kintsch (2001) attempts to model the composition of a predicate with its argument in a manner that distinguishes the role of these constituents, making use of the lexicon of semantic representations to identify the features of each constituent relevant to their combination."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 14
                            }
                        ],
                        "text": "For instance, Kintsch (2001) demonstrates how his own composition algorithm works intuitively on a few hand-selected examples but does not provide a comprehensive test set (for a criticism of Kintsch\u2019s 2001 evaluation standards, see Frank, Koppen, Noordman, & Vonk, 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Kintsch (2001) attempts to achieve this in his predication algorithm by modeling how the meaning of a predicate (e.g., run) varies depending on the arguments it operates upon (e.g., the horse ran vs. the color ran)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Kintsch (2001) considers only the m most similar neighbors to the predicate, from which he subsequently selects k, those most similar to its argument."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 219322170,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "040a6d4542111c1b6161533aa0818c0b025505a0",
            "isKey": true,
            "numCitedBy": 562,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Predication-Kintsch",
            "title": {
                "fragments": [],
                "text": "Predication"
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708581"
                        ],
                        "name": "Sebastian Pad\u00f3",
                        "slug": "Sebastian-Pad\u00f3",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Pad\u00f3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Pad\u00f3"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 123
                            }
                        ],
                        "text": "One alternative approach is to incorporate the syntactic structure of contexts into the procedure for constructing vectors (Grefenstette, 1994; Lin, 1998; Pad\u00f3 and Lapata, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 24
                            }
                        ],
                        "text": "Following previous work (Bullinaria and Levy, 2007; Pad\u00f3 and Lapata, 2007; McDonald, 2000), we then use correlation analysis to examine the relationship between the human ratings and the corresponding model predictions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 127
                            }
                        ],
                        "text": "One approach is to use word co-occurence counts that are further broken down by the syntactic dependencies between those words (Pad\u00f3 and Lapata, 2007; Lin, 1998; Grefenstette, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 111
                            }
                        ],
                        "text": "For example, they do so by defining context in terms of syntactic dependencies (Grefenstette, 1994; Lin, 1998; Pado\u0301 & Lapata, 2007) or by taking into account relational information about how roles and fillers combine to create specific factual knowledge (Dennis, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 17
                            }
                        ],
                        "text": "Syntactic models (Grefenstette, 1994; Lin, 1998; Pad\u00f3 and Lapata, 2007) take into account the syntactic relations between words in defining their contexts, and this can help improve their performance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 91
                            }
                        ],
                        "text": "The second task was based on a TOEFL task, commonly used to evaluate distributional models (Landauer and Dumais, 1996; Bullinaria and Levy, 2007; Pad\u00f3 and Lapata, 2007), and involved identifying synonyms from amongst a set of alternatives."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 66
                            }
                        ],
                        "text": "Following previous work (Bullinaria & Levy, 2007; McDonald, 2000; Pado\u0301 & Lapata, 2007), we then used correlation analysis to examine the relationship between the human ratings and their corresponding vector-based similarity values."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 276
                            }
                        ],
                        "text": "\u2026themselves (Lund & Burgess, 1996), larger linguistic units such as paragraphs or documents (Landauer & Dumais, 1997), or even more complex linguistic representations such as n-grams (Jones & Mewhort, 2007) and the argument slots of predicates (Grefenstette, 1994; Lin, 1998; Pado\u0301 & Lapata, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7747235,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "7441116c5b5a745708a9d7c5aa0ecf04e0c76c93",
            "isKey": false,
            "numCitedBy": 695,
            "numCiting": 103,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditionally, vector-based semantic space models use word co-occurrence counts from large corpora to represent lexical meaning. In this article we present a novel framework for constructing semantic spaces that takes syntactic relations into account. We introduce a formalization for this class of models, which allows linguistic knowledge to guide the construction process. We evaluate our framework on a range of tasks relevant for cognitive science and natural language processing: semantic priming, synonymy detection, and word sense disambiguation. In all cases, our framework obtains results that are comparable or superior to the state of the art."
            },
            "slug": "Dependency-Based-Construction-of-Semantic-Space-Pad\u00f3-Lapata",
            "title": {
                "fragments": [],
                "text": "Dependency-Based Construction of Semantic Space Models"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This article presents a novel framework for constructing semantic spaces that takes syntactic relations into account, and introduces a formalization for this class of models, which allows linguistic knowledge to guide the construction process."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1876168"
                        ],
                        "name": "A. Lascarides",
                        "slug": "A.-Lascarides",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Lascarides",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lascarides"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 13
                            }
                        ],
                        "text": "For example, Lapata and Lascarides (2003) present an experiment where participants rate whether adjective\u2013noun combinations and their paraphrases have similar meanings, whereas other work (Li, McLean, Bandar, O\u2019Shea, & Crockett, 2006; Mitchell & Lapata, 2008) elicits similarity judgments for sentence pairs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 13
                            }
                        ],
                        "text": "For example, Lapata and Lascarides (2003) present an experiment where participants rate whether adjective\u2013noun combinations and their paraphrases have similar meanings, whereas other work (Li, McLean, Bandar, O\u2019Shea, & Crockett, 2006; Mitchell & Lapata, 2008) elicits similarity judgments for\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5653822,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "f932caac89709a716a7d3e6632caf9f34d709518",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "In this article we investigate logical metonymy, that is, constructions in which the argument of a word in syntax appears to be different from that argument in logical form (e.g., enjoy the book means enjoy reading the book, and easy problem means a problem that is easy to solve). The systematic variation in the interpretation of such constructions suggests a rich and complex theory of composition on the syntax/semantics interface. Linguistic accounts of logical metonymy typically fail to describe exhaustively all the possible interpretations, or they don't rank those interpretations in terms of their likelihood. In view of this, we acquire the meanings of metonymic verbs and adjectives from a large corpus and propose a probabilistic model that provides a ranking on the set of possible interpretations. We identify the interpretations automatically by exploiting the consistent correspondences between surface syntactic cues and meaning. We evaluate our results against paraphrase judgments elicited experimentally from humans and show that the model's ranking of meanings correlates reliably with human intuitions."
            },
            "slug": "A-Probabilistic-Account-of-Logical-Metonymy-Lapata-Lascarides",
            "title": {
                "fragments": [],
                "text": "A Probabilistic Account of Logical Metonymy"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This article acquires the meanings of metonymic verbs and adjectives from a large corpus and proposes a probabilistic model that provides a ranking on the set of possible interpretations and identifies the interpretations automatically by exploiting the consistent correspondences between surface syntactic cues and meaning."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145948254"
                        ],
                        "name": "J. Bullinaria",
                        "slug": "J.-Bullinaria",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Bullinaria",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bullinaria"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144053576"
                        ],
                        "name": "J. Levy",
                        "slug": "J.-Levy",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Levy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Levy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 24
                            }
                        ],
                        "text": "Following previous work (Bullinaria and Levy, 2007; Pad\u00f3 and Lapata, 2007; McDonald, 2000), we then use correlation analysis to examine the relationship between the human ratings and the corresponding model predictions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 13
                            }
                        ],
                        "text": "In addition, Bullinaria and Levy (2007) found that these parameters perform well on a number of other tasks such as the synonymy task from the Test of English as a Foreign Language (TOEFL)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 377,
                                "start": 323
                            }
                        ],
                        "text": "The fundamental semantic information which can be extracted from distributional models is the similarity between pairs of representations and their evaluation is typically made in terms of tasks which rely on this property, for example priming (Lund and Burgess, 1996; Landuaer and Dumais, 1997) or synonymy identification (Bullinaria and Levy, 2007; Landauer and Dumais, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 91
                            }
                        ],
                        "text": "A large number of such measures have been proposed in the literature (for an overview, see Bullinaria & Levy, 2007; Weeds, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 56
                            }
                        ],
                        "text": "Our first model is a simple and popular semantic space (Bullinaria & Levy, 2007; Lowe, 2000; McDonald, 2000) that associates each vector component with a particular context word, and assigns it a value based on the strength of its co-occurrence with the target (i.e., the word for which a semantic\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 86
                            }
                        ],
                        "text": "28) due to its simplicity and good performance in simulating word similarity ratings (Bullinaria & Levy, 2007; Griffiths et al., 2007; McDonald, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 91
                            }
                        ],
                        "text": "The second task was based on a TOEFL task, commonly used to evaluate distributional models (Landauer and Dumais, 1996; Bullinaria and Levy, 2007; Pad\u00f3 and Lapata, 2007), and involved identifying synonyms from amongst a set of alternatives."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 25
                            }
                        ],
                        "text": "Following previous work (Bullinaria & Levy, 2007; McDonald, 2000; Pado\u0301 & Lapata, 2007), we then used correlation analysis to examine the relationship between the human ratings and their corresponding vector-based similarity values."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 13
                            }
                        ],
                        "text": "For example, Bullinaria and Levy (2007) consider a range of component types, the simplest being to transform the raw frequencies into conditional probabilities, p(ci | t)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 25
                            }
                        ],
                        "text": "Following previous work (Bullinaria & Levy, 2007), we optimized its parameters on a word-based semantic similarity task."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1025306,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "f76807536e7f6542a72609929a9630de802a597f",
            "isKey": true,
            "numCitedBy": 672,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "The idea that at least some aspects of word meaning can be induced from patterns of word co-occurrence is becoming increasingly popular. However, there is less agreement about the precise computations involved, and the appropriate tests to distinguish between the various possibilities. It is important that the effect of the relevant design choices and parameter values are understood if psychological models using these methods are to be reliably evaluated and compared. In this article, we present a systematic exploration of the principal computational possibilities for formulating and validating representations of word meanings from word co-occurrence statistics. We find that, once we have identified the best procedures, a very simple approach is surprisingly successful and robust over a range of psychologically relevant evaluation measures."
            },
            "slug": "Extracting-semantic-representations-from-word-A-Bullinaria-Levy",
            "title": {
                "fragments": [],
                "text": "Extracting semantic representations from word co-occurrence statistics: A computational study"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This article presents a systematic exploration of the principal computational possibilities for formulating and validating representations of word meanings from word co-occurrence statistics and finds that, once the best procedures are identified, a very simple approach is surprisingly successful and robust over a range of psychologically relevant evaluation measures."
            },
            "venue": {
                "fragments": [],
                "text": "Behavior research methods"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111327888"
                        ],
                        "name": "Michael N. Jones",
                        "slug": "Michael-N.-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael N. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1821878"
                        ],
                        "name": "D. Mewhort",
                        "slug": "D.-Mewhort",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Mewhort",
                            "middleNames": [
                                "J.",
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mewhort"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Jones and Mewhort (2007) propose a model that makes use of the linear order of words in a context."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 135
                            }
                        ],
                        "text": "However, our compositional models are not tied to a specific representation and could be used with the holographic vectors proposed in Jones and Mewhort (2007) or with random indexing; however, we leave this to future work."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 8
                            }
                        ],
                        "text": "Whereas Jones and Mewhort (2007) test this ability to memorize the linear structure of contexts in terms of predicting a target word correctly given a context, our\ncomposition models will be evaluated in terms of their ability to model semantic properties of simple phrases."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 184
                            }
                        ],
                        "text": "\u2026themselves (Lund & Burgess, 1996), larger linguistic units such as paragraphs or documents (Landauer & Dumais, 1997), or even more complex linguistic representations such as n-grams (Jones & Mewhort, 2007) and the argument slots of predicates (Grefenstette, 1994; Lin, 1998; Pado\u0301 & Lapata, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 8
                            }
                        ],
                        "text": "Whereas Jones and Mewhort (2007) test this ability to memorize the linear structure of contexts in terms of predicting a target word correctly given a context, our 1400 J."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7819391,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "d3cf28ab36ff7f7601a55c1e832736b2473a07f0",
            "isKey": true,
            "numCitedBy": 576,
            "numCiting": 164,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present a computational model that builds a holographic lexicon representing both word meaning and word order from unsupervised experience with natural language. The model uses simple convolution and superposition mechanisms (cf. B. B. Murdock, 1982) to learn distributed holographic representations for words. The structure of the resulting lexicon can account for empirical data from classic experiments studying semantic typicality, categorization, priming, and semantic constraint in sentence completions. Furthermore, order information can be retrieved from the holographic representations, allowing the model to account for limited word transitions without the need for built-in transition rules. The model demonstrates that a broad range of psychological data can be accounted for directly from the structure of lexical representations learned in this way, without the need for complexity to be built into either the processing mechanisms or the representations. The holographic representations are an appropriate knowledge representation to be used by higher order models of language comprehension, relieving the complexity required at the higher level."
            },
            "slug": "Representing-word-meaning-and-order-information-in-Jones-Mewhort",
            "title": {
                "fragments": [],
                "text": "Representing word meaning and order information in a composite holographic lexicon."
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A computational model that builds a holographic lexicon representing both word meaning and word order from unsupervised experience with natural language demonstrates that a broad range of psychological data can be accounted for directly from the structure of lexical representations learned in this way, without the need for complexity to be built into either the processing mechanisms or the representations."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799860"
                        ],
                        "name": "T. Griffiths",
                        "slug": "T.-Griffiths",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Griffiths",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Griffiths"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804885"
                        ],
                        "name": "M. Steyvers",
                        "slug": "M.-Steyvers",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Steyvers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Steyvers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 289,
                                "start": 267
                            }
                        ],
                        "text": "\u2026et al., 2007; Landauer & Dumais, 1997) such\nas those included in the Test of English as Foreign Language (TOEFL), reading times (Griffiths et al., 2007; McDonald, 2000), and judgments of semantic similarity (McDonald, 2000) and association (Denhire & Lemaire, 2004; Griffiths et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 786,
                                "start": 457
                            }
                        ],
                        "text": "Semantic space models (and the related topic models) have been successful at simulating a wide range of psycholinguistic phenomena, including semantic priming (Griffiths, Steyvers, & Tenenbaum, 2007; Landauer & Dumais, 1997; Lund & Burgess, 1996), discourse comprehension (Foltz, Kintsch, & Landauer, 1998; Landauer & Dumais, 1997), word categorization (Laham, 2000), judgments of essay quality (Landauer, Laham, Rehder, & Schreiner, 1997), synonymy tests (Griffiths et al., 2007; Landauer & Dumais, 1997) such Fig. 1. In a semantic space words are represented as points, and proximity indicates semantic association. Here, circumstance, situation, and condition are similar to each other and different from hand, arm, and hair. 1390 J. Mitchell, M. Lapata \u2044 Cognitive Science 34 (2010)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 505,
                                "start": 456
                            }
                        ],
                        "text": "Semantic space models (and the related topic models) have been successful at simulating a wide range of psycholinguistic phenomena, including semantic priming (Griffiths, Steyvers, & Tenenbaum, 2007; Landauer & Dumais, 1997; Lund & Burgess, 1996), discourse comprehension (Foltz, Kintsch, & Landauer, 1998; Landauer & Dumais, 1997), word categorization (Laham, 2000), judgments of essay quality (Landauer, Laham, Rehder, & Schreiner, 1997), synonymy tests (Griffiths et al., 2007; Landauer & Dumais, 1997) such Fig."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 59
                            }
                        ],
                        "text": "Our model adopts the Gibbs sampling procedure discussed in Griffiths et al. (2007). Under this model, constructing a semantic representation for a target word amounts to estimating the topic proportions for that word."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 111
                            }
                        ],
                        "text": "28) due to its simplicity and good performance in simulating word similarity ratings (Bullinaria & Levy, 2007; Griffiths et al., 2007; McDonald, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 89
                            }
                        ],
                        "text": "Although several variants have been proposed in the literature (e.g., Blei et al., 2003; Griffiths et al., 2007), they are all based on the same fundamental idea: Documents are mixtures of topics where a topic is a probability distribution over words."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 142
                            }
                        ],
                        "text": "\u2026& Dumais, 1997), word categorization (Laham, 2000), judgments of essay quality (Landauer, Laham, Rehder, & Schreiner, 1997), synonymy tests (Griffiths et al., 2007; Landauer & Dumais, 1997) such\nas those included in the Test of English as Foreign Language (TOEFL), reading times (Griffiths\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 95
                            }
                        ],
                        "text": ", 2007; McDonald, 2000), and judgments of semantic similarity (McDonald, 2000) and association (Denhire & Lemaire, 2004; Griffiths et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 147
                            }
                        ],
                        "text": "\u2026tests (Griffiths et al., 2007; Landauer & Dumais, 1997) such\nas those included in the Test of English as Foreign Language (TOEFL), reading times (Griffiths et al., 2007; McDonald, 2000), and judgments of semantic similarity (McDonald, 2000) and association (Denhire & Lemaire, 2004; Griffiths et\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 84
                            }
                        ],
                        "text": "as those included in the Test of English as Foreign Language (TOEFL), reading times (Griffiths et al., 2007; McDonald, 2000), and judgments of semantic similarity (McDonald, 2000) and association (Denhire & Lemaire, 2004; Griffiths et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 59
                            }
                        ],
                        "text": "Our model adopts the Gibbs sampling procedure discussed in Griffiths et al. (2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5715561,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "509a2ca90a85c62d66a16b37e0de28715dd4e89f",
            "isKey": true,
            "numCitedBy": 1014,
            "numCiting": 140,
            "paperAbstract": {
                "fragments": [],
                "text": "Processing language requires the retrieval of concepts from memory in response to an ongoing stream of information. This retrieval is facilitated if one can infer the gist of a sentence, conversation, or document and use that gist to predict related concepts and disambiguate words. This article analyzes the abstract computational problem underlying the extraction and use of gist, formulating this problem as a rational statistical inference. This leads to a novel approach to semantic representation in which word meanings are represented in terms of a set of probabilistic topics. The topic model performs well in predicting word association and the effects of semantic association and ambiguity on a variety of language-processing and memory tasks. It also provides a foundation for developing more richly structured statistical models of language, as the generative process assumed in the topic model can easily be extended to incorporate other kinds of semantic and syntactic structure."
            },
            "slug": "Topics-in-semantic-representation.-Griffiths-Steyvers",
            "title": {
                "fragments": [],
                "text": "Topics in semantic representation."
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This article analyzes the abstract computational problem underlying the extraction and use of gist, formulating this problem as a rational statistical inference that leads to a novel approach to semantic representation in which word meanings are represented in terms of a set of probabilistic topics."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144514944"
                        ],
                        "name": "W. Lowe",
                        "slug": "W.-Lowe",
                        "structuredName": {
                            "firstName": "Will",
                            "lastName": "Lowe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Lowe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Firth (1957) paraphrased this approach as you shall know a word by the company it keeps and this has become a standard slogan invoked by those working on distributional representations of meaning (e.g., Weeds, 2003; Lowe, 2001; Jones and Mewhort, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15089423,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "12afb4e89aefd688f8c73d52b44cebca49221f3e",
            "isKey": false,
            "numCitedBy": 152,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Towards a Theory of Semantic Space Will Lowe (wlowe02@tufts.edu) Center for Cognitive Studies Tufts University; MA 21015 USA Abstract This paper adds some theory to the growing literature of semantic space models. We motivate semantic space models from the perspective of distributional linguistics and show how an explicit mathematical formulation can provide a better understanding of existing models and suggest changes and improvements. In addition to pro- viding a theoretical framework for current models, we consider the implications of statistical aspects of language data that have not been addressed in the psychological modeling literature. Statistical approaches to language must deal principally with count data, and this data will typically have a highly skewed frequency distribution due to Zipf\u2019s law. We consider the consequences of these facts for the construction of semantic space models, and present methods for removing frequency biases from se- mantic space models. Introduction There is a growing literature on the empirical adequacy of semantic space models across a wide range of sub- ject domains (Burgess et al., 1998; Landauer et al., 1998; Foltz et al., 1998; McDonald and Lowe, 1998; Lowe and McDonald, 2000). However, semantic space mod- els are typically structured and parameterized differently by each researcher. Levy and Bullinaria (2000) have ex- plored the implications of parameter changes empirically by running multiple simulations, but there has up until now been no work that places semantic space models in an overarching theoretical framework; consequently there there are few statements of how semantic spaces ought to be structured in the light of their intended pur- pose. In this paper we attempt to develop a theoretical framework for semantic space models by synthesizing theoretical analyses from vector space information re- trieval and categorical data analysis with new basic re- search. The structure of the paper is as follows. The next sec- tion brie\u00a4y motivates semantic space models using ideas from distributional linguistics. We then review Zipf\u2019s law and its consequences the distributional character of linguistic data. The \u00a3nal section presents a formal de\u00a3- nition of semantic space models and considers what ef- fects different choices of component have on the result- ing models. Motivating Semantic Space Firth (1968) observed that \u201cyou shall know a word by the company it keeps\u201d. If we interpret company as lex- ical company, the words that occur near to it in text or speech, then two related claims are possible. The \u00a3rst is unexceptional: we come to know about the syntactic character of a word by examining the other words that may and may not occur around it in text. Syntactic theory then postulates latent variables e.g. parts of speech and branching structure, that control the distributional prop- erties of words and restrictions on their contexts of occur- rence. The second claim is that we come to know about the semantic character of a word by examining the other words that may and may not occur around it in text. The intuition for this distributional characterization of semantics is that whatever makes words similar or dis- similar in meaning, it must show up distributionally, in the lexical company of the word. Otherwise the suppos- edly semantic difference is not available to hearers and it is not easy to see how it may be learned. If words are similar to the extent that they occur in the similar contexts then we may de\u00a3ne a statistical re- placement test (Finch, 1993) which tests the meaning- fulness of the result of switching one word for another in a sentence. When a corpus of meaningful sentences is available the test may be reversed (Lowe, 2000a), and un- der a suitable representation of lexical context, we may hold each word constant and estimate its typical sur- rounding context. A semantic space model is a way of representing similarity of typical context in a Euclidean space with axes determined by local word co-occurrence counts. Counting the co-occurrence of a target word with a \u00a3xed set of D other words makes it possible to position the target in a space of dimension D. A target\u2019s position with respect to other words then expresses similarity of lexical context. Since the basic notion from distributional linguistics is \u2018intersubstitutability in context\u2019, a semantic space model is effective to the extent it realizes this idea accurately. Zipf\u2019s Law The frequency of a word is (approximately) proportional to the reciprocal of its rank in a frequency list (Zipf, 1949; Mandelbrot, 1954). This is Zipf\u2019s Law. Zipf\u2019s law ensures dramatically skewed distributions for almost"
            },
            "slug": "Towards-a-Theory-of-Semantic-Space-Lowe",
            "title": {
                "fragments": [],
                "text": "Towards a Theory of Semantic Space"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A theoretical framework for semantic space models is developed by synthesizing theoretical analyses from vector space information re- trieval and categorical data analysis with new basic re- search."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733593"
                        ],
                        "name": "J. Curran",
                        "slug": "J.-Curran",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Curran",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Curran"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "While considerable research has been directed at optimising the construction of these word level representations (e.g., Bullinaria and Levy, 2007; Weeds, 2003; Curran, 2003), less attention has been focused on the question of how to combine them."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 227290,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c538b52e2868c3d6fe4490fff1da9eb65d0a2c4e",
            "isKey": false,
            "numCitedBy": 296,
            "numCiting": 235,
            "paperAbstract": {
                "fragments": [],
                "text": "Lexical-semantic resources, including thesauri and WORDNET, have been successfully incorporated into a wide range of applications in Natural Language Processing. However they are very difficult and expensive to create and maintain, and their usefulness has been severely hampered by their limited coverage, bias and inconsistency. Automated and semi-automated methods for developing such resources are therefore crucial for further resource development and improved application performance. Systems that extract thesauri often identify similar words using the distributional hypothesis that similar words appear in similar contexts. This approach involves using corpora to examine the contexts each word appears in and then calculating the similarity between context distributions. Different definitions of context can be used, and I begin by examining how different types of extracted context influence similarity. To be of most benefit these systems must be capable of finding synonyms for rare words. Reliable context counts for rare events can only be extracted from vast collections of text. In this dissertation I describe how to extract contexts from a corpus of over 2 billion words. I describe techniques for processing text on this scale and examine the trade-off between context accuracy, information content and quantity of text analysed. Distributional similarity is at best an approximation to semantic similarity. I develop improved approximations motivated by the intuition that some events in the context distribution are more indicative of meaning than others. For instance, the object-of-verb context wear is far more indicative of a clothing noun than get. However, existing distributional techniques do not effectively utilise this information. The new context-weighted similarity metric I propose in this dissertation significantly outperforms every distributional similarity metric described in the literature. Nearest-neighbour similarity algorithms scale poorly with vocabulary and context vector size. To overcome this problem I introduce a new context-weighted approximation algorithm with bounded complexity in context vector size that significantly reduces the system runtime with only a minor performance penalty. I also describe a parallelized version of the system that runs on a Beowulf cluster for the 2 billion word experiments. To evaluate the context-weighted similarity measure I compare ranked similarity lists against gold-standard resources using precision and recall-based measures from Information Retrieval,"
            },
            "slug": "From-distributional-to-semantic-similarity-Curran",
            "title": {
                "fragments": [],
                "text": "From distributional to semantic similarity"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This dissertation describes how to extract contexts from a corpus of over 2 billion words and introduces a new context-weighted approximation algorithm with bounded complexity in context vector size that significantly reduces the system runtime with only a minor performance penalty."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40591835"
                        ],
                        "name": "K. Lund",
                        "slug": "K.-Lund",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Lund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Lund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50611682"
                        ],
                        "name": "C. Burgess",
                        "slug": "C.-Burgess",
                        "structuredName": {
                            "firstName": "Curt",
                            "lastName": "Burgess",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burgess"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 152
                            }
                        ],
                        "text": "These models construct a representation for the meaning of a word from its pattern of usage, and have been used, for example, to model semantic priming (Landuaer and Dumais, 1997; Lund and Burgess, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 248
                            }
                        ],
                        "text": "Building on this work and the well-known vector space model in information retrieval (Deerwester, Dumais, Landauer, Furnas, & Harshman, 1990; Salton, Wong, & Yang, 1975), more recent semantic space models, such as LSA (Landauer & Dumais, 1997) and HAL (Lund & Burgess, 1996), overcome this limitation by constructing semantic representations indirectly from real language corpora."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 144
                            }
                        ],
                        "text": "Practical implementations of this idea range from ad-hoc approaches for turning word co-occurrence statistics into vector based representations (Lund and Burgess, 1996) to sophisticated generative models of the distribution of words across the documents in a corpus (Blei et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 59
                            }
                        ],
                        "text": "For example, the Hyperspace Analog to Language model (HAL, Lund & Burgess, 1996) represents each word by a vector where each element of the vector corresponds to a weighted co-occurrence value of that word with some other word."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 58
                            }
                        ],
                        "text": "Both LSA and HAL have been used to model semantic priming (Landuaer and Dumais, 1997; Lund and Burgess, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 38
                            }
                        ],
                        "text": ", 2007) or modelling semantic priming (Lund and Burgess, 1996; Landuaer and Dumais, 1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 77
                            }
                        ],
                        "text": "Priming, for example, has been modelled using distributional representations (Landauer et al., 1997; Lund and Burgess, 1996) on the assumption that priming strength depends on the similarity of target and prime."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 19
                            }
                        ],
                        "text": "To some extent HAL (Lund and Burgess, 1996) already explots this structure, albeit simplistically, with its weighting by distance between context and target words."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 103
                            }
                        ],
                        "text": "These semantic similarities can then be used in variety of tasks, including modelling semantic priming (Landuaer and Dumais, 1997; Lund and Burgess, 1996) and human similarity judgments (McDonald, 2000), automatic thesaurus extraction (Grefenstette, 1994) and word sense discrimination (Sch\u00fctze, 1998) and disambiguation (McCarthy et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 36
                            }
                        ],
                        "text": "The latter can be words themselves (Lund & Burgess, 1996), larger linguistic units such as paragraphs or documents (Landauer & Dumais, 1997), or even more complex linguistic representations such as n-grams (Jones & Mewhort, 2007) and the argument slots of predicates (Grefenstette, 1994; Lin, 1998;\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 244
                            }
                        ],
                        "text": "The fundamental semantic information which can be extracted from distributional models is the similarity between pairs of representations and their evaluation is typically made in terms of tasks which rely on this property, for example priming (Lund and Burgess, 1996; Landuaer and Dumais, 1997) or synonymy identification (Bullinaria and Levy, 2007; Landauer and Dumais, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 169
                            }
                        ],
                        "text": "\u2026(Deerwester, Dumais, Landauer, Furnas, & Harshman, 1990; Salton, Wong, & Yang, 1975), more recent semantic space models, such as LSA (Landauer & Dumais, 1997) and HAL (Lund & Burgess, 1996), overcome this limitation by constructing semantic representations indirectly from real language corpora."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 39
                            }
                        ],
                        "text": ", 2009) and modelling semantic priming (Lund and Burgess, 1996; Landuaer and Dumais, 1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 149
                            }
                        ],
                        "text": "\u2026simulating a wide range of psycholinguistic phenomena, including semantic priming (Griffiths, Steyvers, & Tenenbaum, 2007; Landauer & Dumais, 1997; Lund & Burgess, 1996), discourse comprehension (Foltz, Kintsch, & Landauer, 1998; Landauer & Dumais, 1997), word categorization (Laham, 2000),\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61090106,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c7093913b4daa0f34d9d58a41ceb0475cc3cc9f4",
            "isKey": true,
            "numCitedBy": 1721,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "A procedure that processes a corpus of text and produces numeric vectors containing information about its meanings for each word is presented. This procedure is applied to a large corpus of natural language text taken from Usenet, and the resulting vectors are examined to determine what information is contained within them. These vectors provide the coordinates in a high-dimensional space in which word relationships can be analyzed. Analyses of both vector similarity and multidimensional scaling demonstrate that there is significant semantic information carried in the vectors. A comparison of vector similarity with human reaction times in a single-word priming experiment is presented. These vectors provide the basis for a representational model of semantic memory, hyperspace analogue to language (HAL)."
            },
            "slug": "Producing-high-dimensional-semantic-spaces-from-Lund-Burgess",
            "title": {
                "fragments": [],
                "text": "Producing high-dimensional semantic spaces from lexical co-occurrence"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A procedure that processes a corpus of text and produces numeric vectors containing information about its meanings for each word, which provide the basis for a representational model of semantic memory, hyperspace analogue to language (HAL)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3022217"
                        ],
                        "name": "J. Spenader",
                        "slug": "J.-Spenader",
                        "structuredName": {
                            "firstName": "Jennifer",
                            "lastName": "Spenader",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Spenader"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2038941"
                        ],
                        "name": "Reinhard Blutner",
                        "slug": "Reinhard-Blutner",
                        "structuredName": {
                            "firstName": "Reinhard",
                            "lastName": "Blutner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Reinhard Blutner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 80
                            }
                        ],
                        "text": "Aside from the difficulties of determining what systematicity means in practice (Pullum and Scholz, 2007; Spenader and Blutner, 2007; Doumas and Hummel, 2005), it is worth noting that semantic transparency, the idea that words have meanings which remain unaffected by their context, contradicts Frege\u2019s (1884) claim that words only have definite meanings in context."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 146
                            }
                        ],
                        "text": "\u2026from the philosophical difficulties in precisely determining what systematicity means in practice (Doumas & Hummel, 2005; Pullum & Scholz, 2007; Spenader & Blutner, 2007), it is worth noting that semantic transparency, the idea that words have meanings which remain unaffected by their context,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10457370,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "abb83e7b45bfe3bc48cc504667c2da8fdf49c53d",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "A feasible model of natural language semantics should also account for the systematicity displayed by language understanding. The idea that compositionality alone is sufficient to guarantee systematic reasoning within a semantic system is rejected. An analysis of systematic inferences involving adjective\u2013noun combinations with intersective adjectives illustrates that more than a compositional semantics is required for their understanding, and that even simple concepts involve hidden structures. It is argued that the same processes used in determining context-dependent meaning are at work in understanding complex concepts where world-knowledge plays a key role. In the latter cases however accumulated experience with the given concept is argued to serve as a contrast set against which complex concepts are interpreted. 1 What is systematicity? The term \u2018systematicity\u2019 was first introduced in Fodor and Pylyshyn (1988) as a cognitive capacity on par with productivity which a compositional linguistic system is guaranteed to support. Fodor and Pylyshyn refer to systematicity as \u2018a feature of cognition, inferential coherence\u2019. In the following quote they describe what they mean (Fodor and Pylyshyn, 1988, p. 37): The ability to produce/understand some sentences is intrinsically connected to the ability to produce/understand certain others. However, Fodor and Pylyshyn give no formal definition (see van Gelder, 1990, for an attempt to work out the concept formally), instead choosing to illustrate the concept with examples. Here are examples of what types of systematic inferences we can make with language. When an agent understands the sentence \u2018John loves the girl\u2019, she understands the sentence \u2018The girl loves John\u2019 as well (Fodor and Pylyshyn, 1988, pp. 42, 48\u201349). (1) When an agent understands the expressions \u2018brown triangle\u2019 and \u2018black square\u2019, she understands the expressions \u2018brown square\u2019 and \u2018black triangle\u2019 as well. (Based on examples given in Fodor and Pylyshyn, 1988.) (2) Cognitive Foundations of Interpretation. Royal Netherlands Academy of Arts and Sciences, 2007 163 Initially, these two \u2018systematicity clauses\u2019 seem intuitively to be true. Surely understanding \u2018brown triangle\u2019 and \u2018black square\u2019 means that you will be able to infer what \u2018brown square\u2019 means. Systematicity is argued to follow from compositionality because understanding a complex concept like \u2018brown triangle\u2019 is achieved through an understanding of \u2018brown\u2019 and \u2018triangle\u2019, and therefore an agent must be able to use these lexical items in novel combinations. It is true that we understand all the above expressions, but how do we do it and what type of linguistic model will support these inferences? The ability to predict and confirm systematic inferences like these would seem to be a prerequisite for any language model that claims to model natural language understanding. Does understanding the results of the modification of one concept really insure the understanding of the same modification when applied to another concept? We will argue that the answer is not as simple as it first looks, not only for complex cases, but even for so-called simple compounds. Additionally, we will argue that the compositionality of a semantic theory is not sufficient to insure that the semantics predicts systematicity in language understanding. In Section 2 we examine several systematicity inferences that range from those requiring some reasoning to those that lead to compounds that seem only interpretable as idiomatic expressions, or even seem uninterpretable. Section 3 looks at simple adjective\u2013noun compounds involving intersective adjectives. The classic compositional approach is argued to be insufficient for correctly producing even the simplest systematicity clauses. Complex expressions with intersective adjectives are examined in more detail in Section 4. It is first shown that even the simplest concepts have hidden structures that are relevant to their interpretation. Second, it is shown that context affects both how adjective\u2013noun phrases are produced and how they are interpreted. We then argue that the mechanisms at work in cases where context-sets play a role in the meaning of adjective\u2013noun compounds, are the same mechanisms that were at work in determining in what way modification applies to a given object relative to other instances of that object in the world. Finally, Section 5 summarizes what implications this work has for other work in lexical semantics and natural language understanding. 2 Concepts do not relate systematically Lexical items evoke or refer to concepts, but these concepts don\u2019t necessarily relate in predictable ways. Consider the following examples of systematicity clauses where determining the actual meaning of each complex expression is not straightforwardly computable. An agent who understands \u2018good writer\u2019 and \u2018bad teacher\u2019 also understands \u2018bad writer\u2019 and \u2018good teacher\u2019. (3) An agent who understands \u2018within an hour\u2019 and \u2018without a watch\u2019 also understands \u2018without an hour\u2019 and \u2018within a watch\u2019. (4) An agent who understands \u2018Mom drives me to kindergarten\u2019 also understands \u2018I drive mom to kindergarten\u2019. (5) 164 Compositionality and systematicity Consider example (3). Do all who understand \u2018good writer\u2019 and \u2018bad teacher\u2019 also understand \u2018bad writer\u2019 and \u2018good teacher\u2019? The answer seems to be yes. However, determining the meaning of the new complex is a more involved process. For most of us, a writer is good or bad based on characteristics that are different from those under which a teacher is considered good or bad. Both characteristics can be identified from understanding the first two compound expressions. In order to understand the second two we seem to switch from the one end of the scale to the other. Example (4) seems unlikely. Do all who understand \u2018within an hour\u2019 and \u2018without a watch\u2019 also understand \u2018within a watch\u2019 and \u2018without an hour\u2019? (Szab\u00f3, 2000.) An interpretation requires some creativity, given that \u2018within a watch\u2019 does not seem to have any type of conventional meaning, and even if it did, it would not be part of combination of the meaning of the words \u2018within\u2019 and \u2018watch\u2019. On hearing it, listeners are more likely to assume that it has some sort of idiosyncratic, noncompositional meaning that they are unaware of. Finally, example (5) illustrates a case where the problem is conceptual. Would a child who understands \u2018Mom drives me to kindergarten\u2019 also be understand \u2018I drive mom to kindergarten\u2019? Here we are dealing with another type of example where there is a problem of conceptualization and likelihood. A child of kindergarten age may not even initially be able to conceive that he or she could drive the car. All three of these examples illustrate that systematic inferences seem to be heavily dependent on world knowledge, and not just linguistic knowledge. This raises the question what is actually involved in systematicity. If the systematicity that we sometimes find is a feature of language, entailed by virtue of language being a compositional system, then it should be in some ways independent of world knowledge. If however, systematic inferences are in part possible because of the way the world is, and are not really a reflection of linguistic structures, then whether or not your semantic system is compositional would seem to be beside the point. Consider what Clark (1993, p. 148) says: Instead of treating [systematicity] as a property to be directly induced by a canny choice of basic architecture, it may be fruitful to try to treat it as intrinsic to the knowledge we want a"
            },
            "slug": "Compositionality-and-Systematicity-Spenader-Blutner",
            "title": {
                "fragments": [],
                "text": "Compositionality and Systematicity"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An analysis of systematic inferences involving adjective\u2013noun combinations with intersective adjectives illustrates that more than a compositional semantics is required for their understanding, and that even simple concepts involve hidden structures."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145632116"
                        ],
                        "name": "N. Coccaro",
                        "slug": "N.-Coccaro",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Coccaro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Coccaro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746807"
                        ],
                        "name": "Dan Jurafsky",
                        "slug": "Dan-Jurafsky",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Jurafsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan Jurafsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 156
                            }
                        ],
                        "text": "For example, they have been used to model human similarity judgements (McDonald, 2000), enhance n-gram language models with long range semantic information (Bellegarda, 2000; Coccaro and Jurafsky, 1998) and quantify the effect of semantic constraint on reading times (Pynte et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 41
                            }
                        ],
                        "text": "A common method has been vector addition (Landauer et al., 1997; Foltz et al., 1998; Coccaro and Jurafsky, 1998), but this is unsatisfactory for a number of reasons."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 277,
                                "start": 157
                            }
                        ],
                        "text": "However, work on the composition of distributional representations to produce phrase and sentence representations has typically made unjustified assumptions (e.g., that addition is the appropriate approach: Landauer et al., 1997; Foltz et al., 1998; Coccaro and Jurafsky, 1998), or made only weak evaluations of the possible approaches (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 64
                            }
                        ],
                        "text": "The basic idea, which has been implemented in a variety of ways (Bellegarda, 2000; Coccaro and Jurafsky, 1998; Gildea and Hofmann, 1999), is that each new word in a sentence should be semantically similar to the prior context."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 273,
                                "start": 171
                            }
                        ],
                        "text": "A common approach to capturing these sort of effects in a language model is to base the probability of an upcoming word on its similarity to the history of previous words (Bellegarda, 2000; Coccaro and Jurafsky, 1998; Wandmacher and Antoine, 2007; Gildea and Hofmann, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 82
                            }
                        ],
                        "text": ", 1997) and enhancing n-gram language models with long range semantic information (Bellegarda, 2000; Coccaro and Jurafsky, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13185450,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b888cae7e6e288b108f9d119fc23b84b4d447029",
            "isKey": true,
            "numCitedBy": 104,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a number of techniques designed to help integrate semantic knowledge with N-gram language models for automatic speech recognition. Our techniques allow us to integrate Latent Semantic Analysis (LSA), a word-similarity algorithm based on word co-occurrence information, with N-gram models. While LSA is good at predicting content words which are coherent with the rest of a text, it is a bad predictor of frequent words, has a low dynamic range, and is inaccurate when combined linearly with N-grams. We show that modifying the dynamic range, applying a per-word con \ufb01 dence metric, and using geometric rather than linear combinations with N-grams produces a more robust language model which has a lower perplexity on a Wall Street Journal test-set than a baseline N-gram model."
            },
            "slug": "Towards-better-integration-of-semantic-predictors-Coccaro-Jurafsky",
            "title": {
                "fragments": [],
                "text": "Towards better integration of semantic predictors in statistical language modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that modifying the dynamic range, applying a per-word con \ufb01 dence metric, and using geometric rather than linear combinations with N-grams produces a more robust language model which has a lower perplexity on a Wall Street Journal test-set than a baseline N- gram model."
            },
            "venue": {
                "fragments": [],
                "text": "ICSLP"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802811"
                        ],
                        "name": "D. Widdows",
                        "slug": "D.-Widdows",
                        "structuredName": {
                            "firstName": "Dominic",
                            "lastName": "Widdows",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Widdows"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 127
                            }
                        ],
                        "text": "Tensor products have been recently proposed as an alternative to vector addition (Aerts & Czachor, 2004; Clark & Pulman, 2007; Widdows, 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 19
                            }
                        ],
                        "text": "In a similar vein, Widdows (2008) explores the potential of vector product operations for modeling compositional phenomena in natural language, again on a small number of hand-picked examples."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12044606,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7c69a90236f1c57348de858918c554a9420f1521",
            "isKey": false,
            "numCitedBy": 118,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic vector models have proven their worth in a number of natural language applications whose goals can be accomplished by modelling individual semantic concepts and measuring similarities between them. By comparison, the area of semantic compositionality in these models has so far remained underdeveloped. This will be a crucial hurdle for semantic vector models: in order to play a fuller part in the modelling of human language, these models will need some way of modelling the way in which single concepts are put together to form more complex conceptual structures. This paper explores some of the opportunities for using vector product operations to model compositional phenomena in natural language. These vector operations are all well-known and used in mathematics and physics, particularly in quantum mechanics. Instead of designing new vector composition operators, this paper gathers a list of existing operators, and a list of typical composition operations in natural language, and describes two small experiments that begin to investigate the use of certain vector operators to model certain"
            },
            "slug": "Semantic-Vector-Products:-Some-Initial-Widdows",
            "title": {
                "fragments": [],
                "text": "Semantic Vector Products: Some Initial Investigations"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper gathers a list of existing operators, an list of typical composition operations in natural language, and describes two small experiments that begin to investigate the use of certain vector operators to model certain compositional phenomena innatural language."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144523372"
                        ],
                        "name": "S. Clark",
                        "slug": "S.-Clark",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50419262"
                        ],
                        "name": "S. Pulman",
                        "slug": "S.-Pulman",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pulman",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Pulman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 105
                            }
                        ],
                        "text": "Tensor products have been recently proposed as an alternative to vector addition (Aerts & Czachor, 2004; Clark & Pulman, 2007; Widdows, 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 196
                            }
                        ],
                        "text": "us to differentiate between additive and multiplicative composition models and encompassed several existing proposals, including simple vector addition, Kintsch\u2019s (2001) model, the tensor product (Aerts and Czachor, 2004; Clark and Pulman, 2007; Widdows, 2008), and circular convolution (Widdows, 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 12
                            }
                        ],
                        "text": "Recent work (Aerts and Czachor, 2004; Clark and Pulman, 2007; Widdows, 2008), has examined this possibility in more depth, and in Chapter 5 we will evaluate the tensor product and circular convolution as models of semantic composition on their ability to predict similarity judgements for short phrases."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 17
                            }
                        ],
                        "text": "Some recent work (Aerts and Czachor, 2004; Clark and Pulman, 2007; Widdows, 2008) has drawn on this literature in addressing the problem of semantic composition within distributional models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 67
                            }
                        ],
                        "text": "As an alternative to simple vector addition, one set of approaches (Aerts and Czachor, 2004; Clark and Pulman, 2007; Widdows, 2008) has proposed using vector binding operations (Smolensky, 1990; Plate, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2280191,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "73e897104540642698321c106cc9c35af369fe12",
            "isKey": true,
            "numCitedBy": 144,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The are two main approaches to the representation of meaning in Computational Linguistics: a symbolic approach and a distributional approach. This paper considers the fundamental question of how these approaches might be combined. The proposal is to adapt a method from the Cognitive Science literature, in which symbolic and connectionist representations are combined using tensor products. Possible applications of this method for language processing are described. Finally, a potentially fruitful link between Quantum Mechanics, Computational Linguistics, and other related areas such as Information Retrieval and Machine Learning, is proposed."
            },
            "slug": "Combining-Symbolic-and-Distributional-Models-of-Clark-Pulman",
            "title": {
                "fragments": [],
                "text": "Combining Symbolic and Distributional Models of Meaning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A method is to be adapted from the Cognitive Science literature, in which symbolic and connectionist representations are combined using tensor products, to adapt a method for language processing."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI Spring Symposium: Quantum Interaction"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34902160"
                        ],
                        "name": "Jeff Mitchell",
                        "slug": "Jeff-Mitchell",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Mitchell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Mitchell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5741058,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e420f393bc23839a65a1a32778026bb6eae25fa2",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we propose a novel statistical language model to capture long-range semantic dependencies. Specifically, we apply the concept of semantic composition to the problem of constructing predictive history representations for upcoming words. We also examine the influence of the underlying semantic space on the composition task by comparing spatial semantic representations against topic-based ones. The composition models yield reductions in perplexity when combined with a standard n-gram language model over the n-gram model alone. We also obtain perplexity reductions when integrating our models with a structured language model."
            },
            "slug": "Language-Models-Based-on-Semantic-Composition-Mitchell-Lapata",
            "title": {
                "fragments": [],
                "text": "Language Models Based on Semantic Composition"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "A novel statistical language model is proposed to capture long-range semantic dependencies by applying the concept of semantic composition to the problem of constructing predictive history representations for upcoming words."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754586"
                        ],
                        "name": "Tonio Wandmacher",
                        "slug": "Tonio-Wandmacher",
                        "structuredName": {
                            "firstName": "Tonio",
                            "lastName": "Wandmacher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tonio Wandmacher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145928410"
                        ],
                        "name": "Jean-Yves Antoine",
                        "slug": "Jean-Yves-Antoine",
                        "structuredName": {
                            "firstName": "Jean-Yves",
                            "lastName": "Antoine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Yves Antoine"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Weeds (2003) considers various other similarity measures defined on simple conditional probability vectors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 51977123,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b414bafb3214a8461090ec18abf289153007f0da",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Most current word prediction systems make use of n-gram language models (LM) to estimate the probability of the following word in a phrase. In the past years there have been many attempts to enrich such language models with further syntactic or semantic information. We want to explore the predictive powers of Latent Semantic Analysis (LSA), a method that has been shown to provide reliable information on long-distance semantic dependencies between words in a context. We present and evaluate here several methods that integrate LSA-based information with a standard language model: a semantic cache , partial reranking , and different forms of interpolation. We found that all methods show significant improvements, compared to the 4gram baseline, and most of them to a simple cache model as well."
            },
            "slug": "Methods-to-Integrate-a-Language-Model-with-Semantic-Wandmacher-Antoine",
            "title": {
                "fragments": [],
                "text": "Methods to Integrate a Language Model with Semantic Information for a Word Prediction Component"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work explores the predictive powers of Latent Semantic Analysis (LSA), a method that has been shown to provide reliable information on long-distance semantic dependencies between words in a context, and presents and evaluates several methods that integrate LSA-based information with a standard language model: a semantic cache, partial reranking, and different forms of interpolation."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2119062122"
                        ],
                        "name": "Rong Zhang",
                        "slug": "Rong-Zhang",
                        "structuredName": {
                            "firstName": "Rong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rong Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783635"
                        ],
                        "name": "Alexander I. Rudnicky",
                        "slug": "Alexander-I.-Rudnicky",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Rudnicky",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander I. Rudnicky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 207
                            }
                        ],
                        "text": "Extensions on the basic semantic language models sketched above involve representing the history by multiple LSA models of varying granularity in an attempt to capture topic, subtopic, and local information (Zhang and Rudnicky, 2002); incorporating syntactic information by building the semantic space over words and their syntactic annotations (Kanejiya et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5648954,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8eeb7af25aae618b8d6ac2bd8b83d9c7208ef763",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an extension to the use of Latent Semantic Analysis (LSA) for language modeling. This technique makes it easier to exploit long distance relationships in natural language for which the traditional n-gram is unsuited. However, with the growth of length, the semantic representation of the history may be contaminated by irrelevant information, increasing the uncertainty in predicting the next word. To address this problem, we propose a multilevel framework dividing the history into three levels corresponding to document, paragraph and sentence. To combine the three levels of information with the n-gram, a Softmax network is used. We further present a statistical scheme that dynamically determines the unit scope in the generalization stage. The combination of all the techniques leads to a 14% perplexity reduction on a subset of Wall Street Journal, compared with the trigram model."
            },
            "slug": "Improve-latent-semantic-analysis-based-language-by-Zhang-Rudnicky",
            "title": {
                "fragments": [],
                "text": "Improve latent semantic analysis based language model by integrating multiple level knowledge"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A statistical scheme that dynamically determines the unit scope in the generalization stage and the combination of all the techniques leads to a 14% perplexity reduction on a subset of Wall Street Journal, compared with the trigram model."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2770201"
                        ],
                        "name": "G. Pullum",
                        "slug": "G.-Pullum",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Pullum",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Pullum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2917207"
                        ],
                        "name": "Barbara C. Scholz",
                        "slug": "Barbara-C.-Scholz",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Scholz",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barbara C. Scholz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 80
                            }
                        ],
                        "text": "Aside from the difficulties of determining what systematicity means in practice (Pullum and Scholz, 2007; Spenader and Blutner, 2007; Doumas and Hummel, 2005), it is worth noting that semantic transparency, the idea that words have meanings which remain unaffected by their context, contradicts Frege\u2019s (1884) claim that words only have definite meanings in context."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 128
                            }
                        ],
                        "text": "Aside from the philosophical difficulties in precisely determining what systematicity means in practice (Doumas & Hummel, 2005; Pullum & Scholz, 2007; Spenader & Blutner, 2007), it is worth noting that semantic transparency, the idea that words have meanings which remain unaffected by their\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 169556235,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "0e1a3b216c99c39506500013a5514748f8e2408d",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "A lengthy debate in the philosophy of the cognitive sciences has turned on whether the phenomenon known as 'systematicity' of language and thought shows that connectionist explanatory aspirations are misguided. We investigate the issue of just which phenomenon 'systematicity' is supposed to be. The much-rehearsed examples always suggest that being systematic has something to do with ways in which some parts of expressions in natural languages (and, more conjecturally, some parts of thoughts) can be substituted for others without altering well-formedness. We show that under one construal this yields a grossly weak claim that is not just compatible with a narrow version of associationist psychology but essentially coincides with a formalization of its descriptive power. Under another construal we get a claim (apparently unintended) that requires natural languages to fall within the context-free class, a claim that most linguists regard as too strong. Looking more closely at this proposed reconstruction of systematicity leads us to endorse, with further illustrations, the suggestion of Johnson (2004) that systematicity as a matter of substitutability of co-categorial constituents for one another does not appear to hold of natural languages at all. The appeal of the ill-delineated notion of systematicity may lie in the fact that within certain subclasses of lexical items mutual intersubstitutability does seem to hold, and the explanation for that lies in a limitation on human memory: we simply cannot learn separate privileges of syntactic distribution for all of the huge number of words and phrases that we know."
            },
            "slug": "Systematicity-and-Natural-Language-Syntax-Pullum-Scholz",
            "title": {
                "fragments": [],
                "text": "Systematicity and Natural Language Syntax"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2993688"
                        ],
                        "name": "Dharmendra Kanejiya",
                        "slug": "Dharmendra-Kanejiya",
                        "structuredName": {
                            "firstName": "Dharmendra",
                            "lastName": "Kanejiya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dharmendra Kanejiya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2119304006"
                        ],
                        "name": "Arun Kumar",
                        "slug": "Arun-Kumar",
                        "structuredName": {
                            "firstName": "Arun",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Arun Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145518506"
                        ],
                        "name": "S. Prasad",
                        "slug": "S.-Prasad",
                        "structuredName": {
                            "firstName": "Surendra",
                            "lastName": "Prasad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Prasad"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Extensions on the basic semantic language models sketched above involve representing the history by multiple LSA models of varying granularity in an attempt to capture topic, subtopic, and local information (Zhang and Rudnicky, 2002); incorporating syntactic information by building the semantic space over words and their syntactic annotations (Kanejiya et al., 2004); and treating the LSA similarity as a feature in a maximum entropy language model (Deng and Khundanpur, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6786749,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "fe0e2d5671ae8bea7914269708688c003201f729",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical language models using n-gram approach have been under the criticism of neglecting large-span syntactic-semantic information that influences the choice of the next word in a language. One of the approaches that helped recently is the use of latent semantic analysis to capture the semantic fabric of the document and enhance the n-gram model. Similarly there have been some approaches that used syntactic analysis to enhance the n-gram models. In this paper, we explain a framework called syntactically enhanced latent semantic analysis and its application in statistical language modeling. This approach augments each word with its syntactic descriptor in terms of the part-of-speech tag, phrase type or the supertag. We observe that given this syntactic knowledge, the model outperforms LSA based models significantly in terms of perplexity measure. We also present some observations on the effect of the knowledge of content or function word type in language modeling. This paper also poses the problem of better syntax prediction to achieve the benchmarks."
            },
            "slug": "Statistical-Language-Modeling-with-Performance-of-Kanejiya-Kumar",
            "title": {
                "fragments": [],
                "text": "Statistical Language Modeling with Performance Benchmarks using Various Levels of Syntactic-Semantic Information"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper explains a framework called syntactically enhanced latent semantic analysis and its application in statistical language modeling and observes that given this syntactic knowledge, the model outperforms LSA based models significantly in terms of perplexity measure."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3025325"
                        ],
                        "name": "W. Kintsch",
                        "slug": "W.-Kintsch",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Kintsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Kintsch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 72
                            }
                        ],
                        "text": "In contrast, the approach of Kintsch (2001) builds on an existing model (Kintsch, 1988) of how information is integrated during comprehension."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15246663,
            "fieldsOfStudy": [
                "Sociology"
            ],
            "id": "289d3a9562f57d0182d1aae9376b0e3793d80272",
            "isKey": false,
            "numCitedBy": 3132,
            "numCiting": 105,
            "paperAbstract": {
                "fragments": [],
                "text": "Publisher Summary This chapter discusses data concerning the time course of word identification in a discourse context. A simulation of arithmetic word-problem understanding provides a plausible account for some well-known phenomena. The current theories use representations with several mutually constraining layers. There is typically a linguistic level of representation, conceptual levels to represent both the local and global meaning and structure of a text, and a level at which the text itself has lost its individuality and its information content. Knowledge provides part of the context within which a discourse interpreted. The integration phase is the price the model pays for the necessary flexibility in the construction process."
            },
            "slug": "The-role-of-knowledge-in-discourse-comprehension:-a-Kintsch",
            "title": {
                "fragments": [],
                "text": "The role of knowledge in discourse comprehension: a construction-integration model."
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "This chapter discusses data concerning the time course of word identification in a discourse context and a simulation of arithmetic word-problem understanding provides a plausible account for some well-known phenomena."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143643017"
                        ],
                        "name": "R. Levy",
                        "slug": "R.-Levy",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Levy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Levy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6211281,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "2a92864d3f44e1b34da0fa17e13d5752d089eddf",
            "isKey": false,
            "numCitedBy": 1439,
            "numCiting": 146,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Expectation-based-syntactic-comprehension-Levy",
            "title": {
                "fragments": [],
                "text": "Expectation-based syntactic comprehension"
            },
            "venue": {
                "fragments": [],
                "text": "Cognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144655341"
                        ],
                        "name": "S. Frank",
                        "slug": "S.-Frank",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Frank",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Frank"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10187795,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "cacd01e1344245778ce1d40169942c4c3aec7a8c",
            "isKey": false,
            "numCitedBy": 71,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The 'unlexicalized surprisal' of a word in sentence context is defined as the negative logarithm of the probability of the word's part-of-speech given the sequence of previous parts-of-speech of the sentence. Unlexicalized surprisal is known to correlate with word reading time. Here, it is shown that this correlation grows stronger when surprisal values are estimated by a more accurate language model, indicating that readers make use of an objectively accurate probabilistic language model. Also, surprisals as estimated by a Simple Recurrent Network (SRN) were found to correlate more strongly with reading-time data than surprisals estimated by a Probabilistic Context-Free Grammar (PCFG). This suggests that the SRN forms a more accurate psycholinguistic model."
            },
            "slug": "Surprisal-based-comparison-between-a-symbolic-and-a-Frank",
            "title": {
                "fragments": [],
                "text": "Surprisal-based comparison between a symbolic and a connectionist model of sentence processing"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Unlexicalized surprisal values are shown to correlate more strongly with reading-time data than surprisals estimated by a Probabilistic Context-Free Grammar (PCFG), suggesting that the SRN forms a more accurate psycholinguistic model."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2444937"
                        ],
                        "name": "Darrell Laham",
                        "slug": "Darrell-Laham",
                        "structuredName": {
                            "firstName": "Darrell",
                            "lastName": "Laham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Darrell Laham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2197285"
                        ],
                        "name": "B. Rehder",
                        "slug": "B.-Rehder",
                        "structuredName": {
                            "firstName": "Bob",
                            "lastName": "Rehder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Rehder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144029655"
                        ],
                        "name": "M. E. Schreiner",
                        "slug": "M.-E.-Schreiner",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Schreiner",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. E. Schreiner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 52
                            }
                        ],
                        "text": "This is illustrated in the example below taken from Landauer et al. (1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 77
                            }
                        ],
                        "text": "Priming, for example, has been modelled using distributional representations (Landauer et al., 1997; Lund and Burgess, 1996) on the assumption that priming strength depends on the similarity of target and prime."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 41
                            }
                        ],
                        "text": "A common method has been vector addition (Landauer et al., 1997; Foltz et al., 1998; Coccaro and Jurafsky, 1998), but this is unsatisfactory for a number of reasons."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 52
                            }
                        ],
                        "text": "The applications of LSA have included essay grading (Landauer et al., 1997) and enhancing n-gram language models with long range semantic information (Bellegarda, 2000; Coccaro and Jurafsky, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 76
                            }
                        ],
                        "text": "On the other hand, distributional models have been applied to essay grading (Landauer et al., 1997), word sense discrimination (Sch\u00fctze, 1998), ontology extraction (Yamada et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14911179,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "0c074681e891ecd764bb844666011fa41cf7c8f1",
            "isKey": true,
            "numCitedBy": 344,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "How much of the meaning of a naturally occurring English passage is derivable from its combination of words without considering their order? An exploratory approach to this question was provided by asking humans to judge the quality and quantity of knowledge conveyed by short student essays on scientific topics and comparing the interrater reliability and predictive accuracy of their estimates with the performance of a corpus-based statistical model that takes no account of word order within an essay. There was surprisingly little difference between the human judges and the model. In the studies reported here, experts were asked to read short student essays about scientific topics with the goal of determining how much knowledge was accurately reflected in a given essay. We measured the readers\u2019 success by how well their ratings agreed with each other and how well they predicted scores on an objective test on the same subject. All current accounts of human discourse understanding"
            },
            "slug": "How-Well-Can-Passage-Meaning-be-Derived-without-A-Landauer-Laham",
            "title": {
                "fragments": [],
                "text": "How Well Can Passage Meaning be Derived without Using Word Order? A Comparison of Latent Semantic Analysis and Humans"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "An exploratory approach was provided by asking humans to judge the quality and quantity of knowledge conveyed by short student essays on scientific topics and comparing the interrater reliability and predictive accuracy of their estimates with the performance of a corpus-based statistical model that takes no account of word order within an essay."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36037226"
                        ],
                        "name": "R\u00e9jean Ducharme",
                        "slug": "R\u00e9jean-Ducharme",
                        "structuredName": {
                            "firstName": "R\u00e9jean",
                            "lastName": "Ducharme",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R\u00e9jean Ducharme"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120247189"
                        ],
                        "name": "Pascal Vincent",
                        "slug": "Pascal-Vincent",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Vincent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1909943744"
                        ],
                        "name": "Christian Janvin",
                        "slug": "Christian-Janvin",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Janvin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Janvin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In contrast, the concern of Bengio et al. (2003) is directed to the practical benefits of connectionist models, rather than their theoretical representational capacities."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 221275765,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c2b28f9354f667cd5bd07afc0471d8334430da7",
            "isKey": false,
            "numCitedBy": 6011,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts."
            },
            "slug": "A-Neural-Probabilistic-Language-Model-Bengio-Ducharme",
            "title": {
                "fragments": [],
                "text": "A Neural Probabilistic Language Model"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748407"
                        ],
                        "name": "J. Bellegarda",
                        "slug": "J.-Bellegarda",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Bellegarda",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bellegarda"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", see Bellegarda, 2004). For example, Kuhn and de Mori (1990) proposed interpolating the unadapted model with a model derived from a cache of recently observed words from the test corpus."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, they have been used to model human similarity judgements (McDonald, 2000), enhance n-gram language models with long range semantic information (Bellegarda, 2000; Coccaro and Jurafsky, 1998) and quantify the effect of semantic constraint on reading times (Pynte et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Bellegarda (2000) instead appeals to the mathematical foundations of the LSA procedure to represent the history as a pseudo-document, which is basically a weighted sum of the same word vectors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The basic idea, which has been implemented in a variety of ways (Bellegarda, 2000; Coccaro and Jurafsky, 1998; Gildea and Hofmann, 1999), is that each new word in a sentence should be semantically similar to the prior context."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A common approach to capturing these sort of effects in a language model is to base the probability of an upcoming word on its similarity to the history of previous words (Bellegarda, 2000; Coccaro and Jurafsky, 1998; Wandmacher and Antoine, 2007; Gildea and Hofmann, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", 1997) and enhancing n-gram language models with long range semantic information (Bellegarda, 2000; Coccaro and Jurafsky, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "mais, 1997) the obvious way to compare vectors is in terms of the cosine measure, and this is the approach taken by Bellegarda (2000) and Coccaro and Jurafsky (1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8238767,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a90c1ca6c335de94721d7445bb01b723c3d9a840",
            "isKey": true,
            "numCitedBy": 390,
            "numCiting": 108,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical language models used in large-vocabulary speech recognition must properly encapsulate the various constraints, both local and global, present in the language. While local constraints are readily captured through n-gram modeling, global constraints, such as long-term semantic dependencies, have been more difficult to handle within a data-driven formalism. This paper focuses on the use of latent semantic analysis, a paradigm that automatically uncovers the salient semantic relationships between words and documents in a given corpus. In this approach, (discrete) words and documents are mapped onto a (continuous) semantic vector space, in which familiar clustering techniques can be applied. This leads to the specification of a powerful framework for automatic semantic classification, as well as the derivation of several language model families with various smoothing properties. Because of their large-span nature, these language models are well suited to complement conventional n-grams. An integrative formulation is proposed for harnessing this synergy, in which the latent semantic information is used to adjust the standard n-gram probability. Such hybrid language modeling compares favorably with the corresponding n-gram baseline: experiments conducted on the Wall Street Journal domain show a reduction in average word error rate of over 20%. This paper concludes with a discussion of intrinsic tradeoffs, such as the influence of training data selection on the resulting performance."
            },
            "slug": "Exploiting-latent-semantic-information-in-language-Bellegarda",
            "title": {
                "fragments": [],
                "text": "Exploiting latent semantic information in statistical language modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper focuses on the use of latent semantic analysis, a paradigm that automatically uncovers the salient semantic relationships between words and documents in a given corpus, and proposes an integrative formulation for harnessing this synergy."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145811519"
                        ],
                        "name": "K. McRae",
                        "slug": "K.-McRae",
                        "structuredName": {
                            "firstName": "Ken",
                            "lastName": "McRae",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. McRae"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50851881"
                        ],
                        "name": "V. D. de Sa",
                        "slug": "V.-D.-de-Sa",
                        "structuredName": {
                            "firstName": "Virginia",
                            "lastName": "de Sa",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. de Sa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2246097"
                        ],
                        "name": "Mark S. Seidenberg",
                        "slug": "Mark-S.-Seidenberg",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Seidenberg",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark S. Seidenberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2109058,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "756fc297eb767565f3f0f130d351824159b7ec47",
            "isKey": false,
            "numCitedBy": 641,
            "numCiting": 147,
            "paperAbstract": {
                "fragments": [],
                "text": "Behavioral experiments and a connectionist model were used to explore the use of featural representations in the computation of word meaning. The research focused on the role of correlations among features, and differences between speeded and untimed tasks with respect to the use of featural information. The results indicate that featural representations are used in the initial computation of word meaning (as in an attractor network), patterns of feature correlations differ between artifacts and living things, and the degree to which features are intercorrelated plays an important role in the organization of semantic memory. The studies also suggest that it may be possible to predict semantic priming effects from independently motivated featural theories of semantic relatedness. Implications for related behavioral phenomena such as the semantic impairments associated with Alzheimer's disease (AD) are discussed."
            },
            "slug": "On-the-nature-and-scope-of-featural-representations-McRae-Sa",
            "title": {
                "fragments": [],
                "text": "On the nature and scope of featural representations of word meaning."
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "The results indicate that featural representations are used in the initial computation of word meaning, patterns of feature correlations differ between artifacts and living things, and the degree to which features are intercorrelated plays an important role in the organization of semantic memory."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of experimental psychology. General"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8754851,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3cd9fd8a36c8feb74bb20ae25817edb9c6a0518c",
            "isKey": false,
            "numCitedBy": 1401,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents context-group discrimination, a disambiguation algorithm based on clustering. Senses are interpreted as groups (or clusters) of similar contexts of the ambiguous word. Words, contexts, and senses are represented in Word Space, a high-dimensional, real-valued space in which closeness corresponds to semantic similarity. Similarity in Word Space is based on second-order co-occurrence: two tokens (or contexts) of the ambiguous word are assigned to the same sense cluster if the words they co-occur with in turn occur with similar words in a training corpus. The algorithm is automatic and unsupervised in both training and application: senses are induced from a corpus without labeled training instances or other external knowledge sources. The paper demonstrates good performance of context-group discrimination for a sample of natural and artificial ambiguous words."
            },
            "slug": "Automatic-Word-Sense-Discrimination-Sch\u00fctze",
            "title": {
                "fragments": [],
                "text": "Automatic Word Sense Discrimination"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This paper presents context-group discrimination, a disambiguation algorithm based on clustering that demonstrates good performance of context- group discrimination for a sample of natural and artificial ambiguous words."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689109"
                        ],
                        "name": "Magnus Sahlgren",
                        "slug": "Magnus-Sahlgren",
                        "structuredName": {
                            "firstName": "Magnus",
                            "lastName": "Sahlgren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Magnus Sahlgren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078819344"
                        ],
                        "name": "Anders Holst",
                        "slug": "Anders-Holst",
                        "structuredName": {
                            "firstName": "Anders",
                            "lastName": "Holst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anders Holst"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3298207"
                        ],
                        "name": "P. Kanerva",
                        "slug": "P.-Kanerva",
                        "structuredName": {
                            "firstName": "Pentti",
                            "lastName": "Kanerva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kanerva"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 0
                            }
                        ],
                        "text": "Sahlgren, Host, and Kanerva (2008) provide an alternative to convolution by showing that order information can also be captured by permuting the vector coordinates."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17182082,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "673b15746dd4ba0e70f038cf52f56dae1faeb744",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We show that sequence information can be encoded into high-dimensional fixed-width vectors using permutations of coordinates. Computational models of language often represent words with high-dimensional semantic vectors compiled from word-use statistics. A word's semantic vector usually encodes the contexts in which the word appears in a large body of text but ignores word order. However, word order often signals a word's grammatical role in a sentence and thus tells of the word's meaning. Jones and Mewhort (2007) show that word order can be included in the semantic vectors using holographic reduced representation and convolution. We show here that the order information can be captured also by permuting of vector coordinates, thus providing a general and computationally light alternative to convolution."
            },
            "slug": "Permutations-as-a-means-to-encode-order-in-word-Sahlgren-Holst",
            "title": {
                "fragments": [],
                "text": "Permutations as a means to encode order in word space"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "It is shown that sequence information can be encoded into high-dimensional fixed-width vectors using permutations of coordinates, thus providing a general and computationally light alternative to convolution."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2869436"
                        ],
                        "name": "V. Demberg",
                        "slug": "V.-Demberg",
                        "structuredName": {
                            "firstName": "Vera",
                            "lastName": "Demberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Demberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143694777"
                        ],
                        "name": "Frank Keller",
                        "slug": "Frank-Keller",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Keller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frank Keller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15473175,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0245c2e62ce5131b6179bdbbd81fc41f881056f7",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "A Computational Model of Prediction in Human Parsing: Unifying Locality and Surprisal Effects Vera Demberg (v.demberg@ed.ac.uk) and Frank Keller (keller@inf.ed.ac.uk) School of Informatics, University of Edinburgh 10 Crichton Street, Edinburgh EH8 9AB, UK necessary to build phrases whose lexical anchors (the words that they relate to) have not been encountered yet. Full con- nectedness ensures that a fully interpretable structure is avail- able at any point during incremental sentence processing. In this paper, we explore how these key psycholinguistic concepts (incrementality, connectedness, and prediction) can be realized within a new version of tree-adjoining grammar, which we call Psycholinguistically Motivated TAG (PLTAG). We propose a formalization of PLTAG and a linking theory that derives predictions of processing difficulty from it. We then present an implementation of this model and evaluate it against key experimental data relating to incrementality and prediction. The resulting model is shown to offer a unified framework that captures both locality effects and surprisal ef- fects in sentence processing. Abstract There is strong evidence that human sentence processing is in- cremental, i.e., that structures are built word by word. Recent experiments show that the processor also predicts upcoming linguistic material on the basis of previous input. We present a computational model of human parsing that is based on a vari- ant of tree-adjoining grammar and includes an explicit mecha- nism for generating and verifying predictions, while respecting incrementality and connectedness. An algorithm for deriving a lexicon from a treebank, a fully implemented parser, and a probability model for this formalism are also presented. We devise a linking function that explains processing difficulty as a combination of prefix probability (surprisal) and verification cost. The resulting model captures locality effects such as the subject/object relative clause asymmetry, as well as surprisal effects such as prediction in either . . . or constructions. Keywords: Sentence Processing; Incrementality; Prediction; Surprisal; Locality Effects; Tree-adjoining Grammar. Background Introduction Evidence from psycholinguistic research suggests that lan- guage comprehension is largely incremental, i.e., that com- prehenders build an interpretation of a sentence on a word- by-word basis. Evidence for incrementality comes from speech shadowing, self-paced reading, and eye-tracking stud- ies (Marslen-Wilson, 1973; Konieczny, 2000; Tanenhaus et al., 1995): as soon as readers or listeners perceive a word in a sentence, they integrate it as fully as possible into a repre- sentation of the sentence thus far. They experience differential processing difficulty during this integration process, depend- ing on the properties of the word and its relationship to the preceding context. There is also evidence for full connectivity in human lan- guage processing (Sturt & Lombardo, 2005). Full connec- tivity means that all words are connected by a single syn- tactic structure; the parser builds no unconnected tree frag- ments, even for the incomplete sentences (sentence prefixes) that arise during incremental processing. Furthermore, there is evidence that readers or listeners make predictions about upcoming material on the basis of sentence prefixes. Listeners can predict an upcoming post- verbal element, based on the semantics of the preceding verb (Kamide et al., 2003). Prediction effects can also be observed in reading. Staub & Clifton (2006) showed that following the word either readers predict or and the complement that fol- lows it; processing was facilitated compared to structures that include or without either. In an ERP study, van Berkum et al. (1999) found that listeners use contextual information to pre- dict specific lexical items and experience processing difficulty if the input is incompatible with the prediction. The concepts of incrementality, connectedness, and predic- tion are closely related: in order to guarantee that the syntac- tic structure of a sentence prefix is fully connected, it may be Among existing models of sentence processing, two stand out as potential candidates for accounting for prediction effects. One of them is Dependency Locality Theory (DLT), proposed by Gibson (1998). A central notion in DLT is integration cost, a distance-based measure of the amount of processing effort required when the head of a phrase is integrated with its syn- tactic dependents. In other words, dependents in DLT predict the existence of a subsequent head, and the verification of these predictions causes processing cost at the head, based on its distance from the dependents. A key experimental result captured by DLT is the fact that subject relative clauses (SRCs) as in (1a) are easier to process than object relative clauses (ORCs) as in (1b). Shorter reading times are observed on the verb attacked for SRCs compared to ORCs (King & Just, 1991). a. b. The reporter that attacked the senator admitted the error. The reporter that the senator attacked admitted the error. At the relative clause verb attacked, a dependency to the rel- ative pronoun that is constructed; in the SRC, this involves a distance of one, while in the ORC, the subject the senator intervenes, resulting in a distance of two, thus explaining the higher processing cost in DLT terms. DLT has been shown to also capture a range of other complexity results, including processing overload phenom- ena such as center embedding and cross-serial dependencies (Gibson, 1998). However, DLT is not a broad coverage the- ory: it captures the integration costs at main verbs and nouns, but makes no predictions for any other syntactic categories. This limits its usefulness in accounting for corpus data (Dem- berg & Keller, 2008a)."
            },
            "slug": "A-Computational-Model-of-Prediction-in-Human-and-Demberg-Keller",
            "title": {
                "fragments": [],
                "text": "A Computational Model of Prediction in Human Parsing: Unifying Locality and Surprisal Effects"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper presents a computational model of human parsing that is based on a vari- ant of tree-adjoining grammar and includes an explicit mecha- nism for generating and verifying predictions, while respecting incrementality and connectedness."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 152
                            }
                        ],
                        "text": "These models construct a representation for the meaning of a word from its pattern of usage, and have been used, for example, to model semantic priming (Landuaer and Dumais, 1997; Lund and Burgess, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 58
                            }
                        ],
                        "text": "Both LSA and HAL have been used to model semantic priming (Landuaer and Dumais, 1997; Lund and Burgess, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 38
                            }
                        ],
                        "text": ", 2007) or modelling semantic priming (Lund and Burgess, 1996; Landuaer and Dumais, 1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 84
                            }
                        ],
                        "text": "Commonly, however, distributional representations are combined in terms of addition (Landuaer and Dumais, 1997; Foltz et al., 1998), without any empirical evaluation of alternative choices."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 103
                            }
                        ],
                        "text": "These semantic similarities can then be used in variety of tasks, including modelling semantic priming (Landuaer and Dumais, 1997; Lund and Burgess, 1996) and human similarity judgments (McDonald, 2000), automatic thesaurus extraction (Grefenstette, 1994) and word sense discrimination (Sch\u00fctze, 1998) and disambiguation (McCarthy et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 244
                            }
                        ],
                        "text": "The fundamental semantic information which can be extracted from distributional models is the similarity between pairs of representations and their evaluation is typically made in terms of tasks which rely on this property, for example priming (Lund and Burgess, 1996; Landuaer and Dumais, 1997) or synonymy identification (Bullinaria and Levy, 2007; Landauer and Dumais, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 123
                            }
                        ],
                        "text": "Vector addition or averaging (which are equivalent under the cosine measure) is the most common form of vector combination (Landuaer and Dumais, 1997; Foltz et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 81
                            }
                        ],
                        "text": ", 2007) and have been shown to achieve human level performance on synonymy tests (Landuaer and Dumais, 1997; Griffiths et al., 2007) such as those included in the Test of English as Foreign Language (TOEFL)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 39
                            }
                        ],
                        "text": ", 2009) and modelling semantic priming (Lund and Burgess, 1996; Landuaer and Dumais, 1997)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 23
                            }
                        ],
                        "text": ", identifying synonyms (Landuaer and Dumais, 1997; Griffiths et al., 2007) or modelling semantic priming (Lund and Burgess, 1996; Landuaer and Dumais, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1144461,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68dd4b89ce1407372a29d05ca9e4e1a2e0513617",
            "isKey": true,
            "numCitedBy": 5788,
            "numCiting": 210,
            "paperAbstract": {
                "fragments": [],
                "text": "How do people know as much as they do with as little information as they get? The problem takes many forms; learning vocabulary from text is an especially dramatic and convenient case for research. A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena. By inducing global knowledge indirectly from local co-occurrence data in a large body of representative text, LSA acquired knowledge about the full vocabulary of English at a comparable rate to schoolchildren. LSA uses no prior linguistic or perceptual similarity knowledge; it is based solely on a general mathematical learning method that achieves powerful inductive effects by extracting the right number of dimensions (e.g., 300) to represent objects and contexts. Relations to other theories, phenomena, and problems are sketched."
            },
            "slug": "A-Solution-to-Plato's-Problem:-The-Latent-Semantic-Landauer-Dumais",
            "title": {
                "fragments": [],
                "text": "A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge."
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2036336"
                        ],
                        "name": "P. Foltz",
                        "slug": "P.-Foltz",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Foltz",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Foltz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3025325"
                        ],
                        "name": "W. Kintsch",
                        "slug": "W.-Kintsch",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Kintsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Kintsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1943,
                                "start": 124
                            }
                        ],
                        "text": "Vector addition or averaging (which are equivalent under the cosine measure) is the most common form of vector combination (Foltz et al., 1998; Landauer & Dumais, 1997). However, vector addition is not a suitable model of composition for at least two reasons. Firstly, it is insensitive to syntax and word order. As vector addition is commutative, it is essentially a bag-of-words model of composition: It assigns the same representation to any sentence containing the same constituents irrespective of their syntactic relations. However, there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville, Nichol, Barss, Forster, & Garrett, 1991; West & Stanovich, 1986). Secondly, addition simply blends together the content of all words involved to produce something in between them all. Ideally, we would like a model of semantic composition that generates novel meanings by selecting and modifying particular aspects of the constituents participating in the composition. Kintsch (2001) attempts to achieve this in his predication algorithm by modeling how the meaning of a predicate (e.g., run) varies depending on the arguments it operates upon (e.g., the horse ran vs. the color ran). The idea is to add not only the vectors representing the predicate and its argument but also the neighbors associated with both of them. The neighbors, Kintsch argues, can strengthen features of the predicate that are appropriate for the argument of the predication. Tensor products have been recently proposed as an alternative to vector addition (Aerts & Czachor, 2004; Clark & Pulman, 2007; Widdows, 2008). However, as illustrated in Fig. 2, these representations grow exponentially as more vectors are combined. This fact undermines not only their tractability in an artificial computational setting but also their J. Mitchell, M. Lapata \u2044 Cognitive Science 34 (2010) 1399"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 414,
                                "start": 137
                            }
                        ],
                        "text": "While vector averaging has been effective in some applications such as essay grading (Landauer & Dumais, 1997) and coherence assessment (Foltz et al., 1998), it is unfortunately insensitive to word order, and more generally syntactic structure, giving the same representation to any constructions that happen to share the same vocabulary. This is illustrated in the example below taken from Landauer et al. (1997). Sentences (1-a) and (1-b) contain exactly the same set of words, but their meaning is entirely different."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1070,
                                "start": 124
                            }
                        ],
                        "text": "Vector addition or averaging (which are equivalent under the cosine measure) is the most common form of vector combination (Foltz et al., 1998; Landauer & Dumais, 1997). However, vector addition is not a suitable model of composition for at least two reasons. Firstly, it is insensitive to syntax and word order. As vector addition is commutative, it is essentially a bag-of-words model of composition: It assigns the same representation to any sentence containing the same constituents irrespective of their syntactic relations. However, there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville, Nichol, Barss, Forster, & Garrett, 1991; West & Stanovich, 1986). Secondly, addition simply blends together the content of all words involved to produce something in between them all. Ideally, we would like a model of semantic composition that generates novel meanings by selecting and modifying particular aspects of the constituents participating in the composition. Kintsch (2001) attempts to achieve this in his predication algorithm by modeling how the meaning of a predicate (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 124
                            }
                        ],
                        "text": "Vector addition or averaging (which are equivalent under the cosine measure) is the most common form of vector combination (Foltz et al., 1998; Landauer & Dumais, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 137
                            }
                        ],
                        "text": "While vector averaging has been effective in some applications such as essay grading (Landauer & Dumais, 1997) and coherence assessment (Foltz et al., 1998), it is unfortunately insensitive to word order, and more generally syntactic structure, giving the same representation to any constructions\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 136
                            }
                        ],
                        "text": "While vector averaging has been effective in some applications such as essay grading (Landauer & Dumais, 1997) and coherence assessment (Foltz et al., 1998), it is unfortunately insensitive to word order, and more generally syntactic structure, giving the same representation to any constructions that happen to share the same vocabulary."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62729021,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4732c036d0b00d435bcd41ae904a9e936e4f683",
            "isKey": true,
            "numCitedBy": 792,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Latent Semantic Analysis (LSA) is used as a technique for measuring the coherence of texts. By comparing the vectors for 2 adjoining segments of text in a high\u2010dimensional semantic space, the method provides a characterization of the degree of semantic relatedness between the segments. We illustrate the approach for predicting coherence through reanalyzing sets of texts from 2 studies that manipulated the coherence of texts and assessed readers\u2019 comprehension. The results indicate that the method is able to predict the effect of text coherence on comprehension and is more effective than simple term\u2010term overlap measures. In this manner, LSA can be applied as an automated method that produces coherence predictions similar to propositional modeling. We describe additional studies investigating the application of LSA to analyzing discourse structure and examine the potential of LSA as a psychological model of coherence effects in text comprehension."
            },
            "slug": "The-Measurement-of-Textual-Coherence-with-Latent-Foltz-Kintsch",
            "title": {
                "fragments": [],
                "text": "The Measurement of Textual Coherence with Latent Semantic Analysis."
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The approach for predicting coherence through reanalyzing sets of texts from 2 studies that manipulated the coherence of texts and assessed readers\u2019 comprehension indicates that the method is able to predict the effect of text coherence on comprehension and is more effective than simple term\u2010term overlap measures."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50672236"
                        ],
                        "name": "D. J. Foss",
                        "slug": "D.-J.-Foss",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Foss",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. J. Foss"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 106
                            }
                        ],
                        "text": "Changes in the temporal order of words in a sentence decrease the strength of the related priming effect (Foss, 1982; Masson, 1986; O\u2019Seaghdha, 1989; Simpson, Peterson, Casteel, & Brugges, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 42931448,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "3b2809a14caf764ebe1f86f9a38be90b1033e28e",
            "isKey": false,
            "numCitedBy": 196,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-discourse-on-semantic-priming-Foss",
            "title": {
                "fragments": [],
                "text": "A discourse on semantic priming"
            },
            "venue": {
                "fragments": [],
                "text": "Cognitive Psychology"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145562084"
                        ],
                        "name": "John Hale",
                        "slug": "John-Hale",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hale",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Hale"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Hale (2001) proposes, specifically, that the level of cognitive load is related to the proportion of probability mass which is disconfirmed by the new input."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Consolidating these factors into a single measure is preferable over a situation in which syntactic constraint is modelled in terms of surprisal (Hale, 2001) whereas semantic constraint is modelled in terms of cosine similarity (Pynte et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Levy (2008) shows that this reduces to the same surprisal measure proposed by Hale (2001). Hale (2001) illustrates the surprisal measure with a small PCFG, calculating the prefix probabilities used in Equation 7."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Our approach is based on the notion of surprisal (Hale, 2001), which assumes that input which conflicts with readers expectations is associated with increased cognitive load."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5490143,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "a959ae0f08228d5d04eb46d5dc1ec3f4c9ee91d7",
            "isKey": true,
            "numCitedBy": 939,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "In human sentence processing, cognitive load can be defined many ways. This report considers a definition of cognitive load in terms of the total probability of structural options that have been disconfirmed at some point in a sentence: the surprisal of word wi given its prefix wo...i-1 on a phrase-structural language model. These loads can be efficiently calculated using a probabilistic Earley parser (Stolcke, 1995) which is interpreted as generating predictions about reading time on a word-by-word basis. Under grammatical assumptions supported by corpus-frequency data, the operation of Stolcke's probabilistic Earley parser correctly predicts processing phenomena associated with garden path structural ambiguity and with the subject/object relative asymmetry."
            },
            "slug": "A-Probabilistic-Earley-Parser-as-a-Psycholinguistic-Hale",
            "title": {
                "fragments": [],
                "text": "A Probabilistic Earley Parser as a Psycholinguistic Model"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Under grammatical assumptions supported by corpus-frequency data, the operation of Stolcke's probabilistic Earley parser correctly predicts processing phenomena associated with garden path structural ambiguity and with the subject/object relative asymmetry."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708425"
                        ],
                        "name": "Jay J. Jiang",
                        "slug": "Jay-J.-Jiang",
                        "structuredName": {
                            "firstName": "Jay",
                            "lastName": "Jiang",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jay J. Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075147"
                        ],
                        "name": "D. Conrath",
                        "slug": "D.-Conrath",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Conrath",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Conrath"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1359050,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b64e068a8face2540fc436af40dbcd2b0912bbf",
            "isKey": false,
            "numCitedBy": 3339,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new approach for measuring semantic similarity/distance between words and concepts. It combines a lexical taxonomy structure with corpus statistical information so that the semantic distance between nodes in the semantic space constructed by the taxonomy can be better quantified with the computational evidence derived from a distributional analysis of corpus data. Specifically, the proposed measure is a combined approach that inherits the edge-based approach of the edge counting scheme, which is then enhanced by the node-based approach of the information content calculation. When tested on a common data set of word pair similarity ratings, the proposed approach outperforms other computational models. It gives the highest correlation value (r = 0.828) with a benchmark based on human similarity judgements, whereas an upper bound (r = 0.885) is observed when human subjects replicate the same task."
            },
            "slug": "Semantic-Similarity-Based-on-Corpus-Statistics-and-Jiang-Conrath",
            "title": {
                "fragments": [],
                "text": "Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "This paper presents a new approach for measuring semantic similarity/distance between words and concepts that combines a lexical taxonomy structure with corpus statistical information so that the semantic distance between nodes in the semantic space constructed by the taxonomy can be better quantified with the computational evidence derived from a distributional analysis of corpus data."
            },
            "venue": {
                "fragments": [],
                "text": "ROCLING/IJCLCLP"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143981449"
                        ],
                        "name": "Mark Andrews",
                        "slug": "Mark-Andrews",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Andrews",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Andrews"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2517462"
                        ],
                        "name": "G. Vigliocco",
                        "slug": "G.-Vigliocco",
                        "structuredName": {
                            "firstName": "Gabriella",
                            "lastName": "Vigliocco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Vigliocco"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15109128"
                        ],
                        "name": "D. Vinson",
                        "slug": "D.-Vinson",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Vinson",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Vinson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17542062,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b3616e10fc5f810243f20b84ffc72acbf773cc3",
            "isKey": false,
            "numCitedBy": 381,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors identify 2 major types of statistical data from which semantic representations can be learned. These are denoted as experiential data and distributional data. Experiential data are derived by way of experience with the physical world and comprise the sensory-motor data obtained through sense receptors. Distributional data, by contrast, describe the statistical distribution of words across spoken and written language. The authors claim that experiential and distributional data represent distinct data types and that each is a nontrivial source of semantic information. Their theoretical proposal is that human semantic representations are derived from an optimal statistical combination of these 2 data types. Using a Bayesian probabilistic model, they demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it. The semantic representations that are learned in this manner are measurably more realistic-as verified by comparison to a set of human-based measures of semantic representation-than those available from either data type individually or from both sources independently. This is not a result of merely using quantitatively more data, but rather it is because experiential and distributional data are qualitatively distinct, yet intercorrelated, types of data. The semantic representations that are learned are based on statistical structures that exist both within and between the experiential and distributional data types."
            },
            "slug": "Integrating-experiential-and-distributional-data-to-Andrews-Vigliocco",
            "title": {
                "fragments": [],
                "text": "Integrating experiential and distributional data to learn semantic representations."
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "Using a Bayesian probabilistic model, the authors demonstrate how word meanings can be learned by treating experiential and distributional data as a single joint distribution and learning the statistical structure that underlies it."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144096985"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Miller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34729490"
                        ],
                        "name": "W. Charles",
                        "slug": "W.-Charles",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Charles",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Charles"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Much of the material in Chapters 4 and 5 was previously published in Mitchell and Lapata (2008) and Mitchell and Lapata (2010), with the former covering Experiment 1 and the latter Experiment 2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 145580646,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "402627e4eb8c95e4aae3026fd921aa08cd792006",
            "isKey": false,
            "numCitedBy": 1678,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract The relationship between semantic and contextual similarity is investigated for pairs of nouns that vary from high to low semantic similarity. Semantic similarity is estimated by subjective ratings; contextual similarity is estimated by the method of sorting sentential contexts. The results show an inverse linear relationship between similarity of meaning and the discriminability of contexts. This relation, is obtained for two separate corpora of sentence contexts. It is concluded that, on average, for words in the same language drawn from the same syntactic and semantic categories, the more often two words can be substituted into the same contexts the more similar in meaning they are judged to be."
            },
            "slug": "Contextual-correlates-of-semantic-similarity-Miller-Charles",
            "title": {
                "fragments": [],
                "text": "Contextual correlates of semantic similarity"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689647"
                        ],
                        "name": "Peter D. Turney",
                        "slug": "Peter-D.-Turney",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Turney",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D. Turney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 251,
                                "start": 239
                            }
                        ],
                        "text": "We intend to assess the potential of our composition models on context-sensitive semantic priming (Till, Mross, & Kintsch, 1988), inductive inference (Heit & Rubinstein, 1994), and analogical learning (Mangalath, Quesada, & Kintsch, 2004; Turney, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2468783,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "212d2715aee9fbefe140685b088b789d6c8277b0",
            "isKey": false,
            "numCitedBy": 416,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": "There are at least two kinds of similarity. Relational similarity is correspondence between relations, in contrast with attributional similarity, which is correspondence between attributes. When two words have a high degree of attributional similarity, we call them synonyms. When two pairs of words have a high degree of relational similarity, we say that their relations are analogous. For example, the word pair mason:stone is analogous to the pair carpenter:wood. This article introduces Latent Relational Analysis (LRA), a method for measuring relational similarity. LRA has potential applications in many areas, including information extraction, word sense disambiguation, and information retrieval. Recently the Vector Space Model (VSM) of information retrieval has been adapted to measuring relational similarity, achieving a score of 47% on a collection of 374 college-level multiple-choice word analogy questions. In the VSM approach, the relation between a pair of words is characterized by a vector of frequencies of predefined patterns in a large corpus. LRA extends the VSM approach in three ways: (1) The patterns are derived automatically from the corpus, (2) the Singular Value Decomposition (SVD) is used to smooth the frequency data, and (3) automatically generated synonyms are used to explore variations of the word pairs. LRA achieves 56% on the 374 analogy questions, statistically equivalent to the average human score of 57%. On the related problem of classifying semantic relations, LRA achieves similar gains over the VSM."
            },
            "slug": "Similarity-of-Semantic-Relations-Turney",
            "title": {
                "fragments": [],
                "text": "Similarity of Semantic Relations"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "LRA extends the VSM approach in three ways: the patterns are derived automatically from the corpus, the Singular Value Decomposition (SVD) is used to smooth the frequency data, and automatically generated synonyms are used to explore variations of the word pairs."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403825212"
                        ],
                        "name": "P. O'Seaghdha",
                        "slug": "P.-O'Seaghdha",
                        "structuredName": {
                            "firstName": "Padraig",
                            "lastName": "O'Seaghdha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. O'Seaghdha"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 21097518,
            "fieldsOfStudy": [
                "Psychology",
                "Linguistics"
            ],
            "id": "96c988765c2f4c70d2bb6219a983cfbed13fde4a",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Context effects on lexical decision were analyzed by manipulating lexical relatedness and syntactic connectedness. Related and unrelated word pairs were embedded in syntactic (e.g., \"the author of this book/floor\") and in scrambled (e.g., \"the author the and book/floor\") phrases. The sequences were presented serially and subjects made lexical decisions to the terminal targets. In four experiments, relatedness effects were substantial in syntactic phrases but only marginal in scrambled sequences. This result was unaffected by presentation rate or by blocking manipulations. A fifth experiment showed that the relatedness effect in syntactic phrases involved both facilitation of responses to related words and inhibition of responses to unrelated words. These results argue against a role for intralexical priming in on-line reading. They highlight the role of syntactic connectedness and suggest that contextual facilitation depends on the ease of integration of new words with the current text-level representation."
            },
            "slug": "The-dependence-of-lexical-relatedness-effects-on-O'Seaghdha",
            "title": {
                "fragments": [],
                "text": "The dependence of lexical relatedness effects on syntactic connectedness."
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The results argue against a role for intralexical priming in on-line reading and highlight the role of syntactic connectedness and suggest that contextual facilitation depends on the ease of integration of new words with the current text-level representation."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of experimental psychology. Learning, memory, and cognition"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145465286"
                        ],
                        "name": "Timothy Baldwin",
                        "slug": "Timothy-Baldwin",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Baldwin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timothy Baldwin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47473549"
                        ],
                        "name": "Colin Bannard",
                        "slug": "Colin-Bannard",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Bannard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Colin Bannard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49125917"
                        ],
                        "name": "Takaaki Tanaka",
                        "slug": "Takaaki-Tanaka",
                        "structuredName": {
                            "firstName": "Takaaki",
                            "lastName": "Tanaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Takaaki Tanaka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802811"
                        ],
                        "name": "D. Widdows",
                        "slug": "D.-Widdows",
                        "structuredName": {
                            "firstName": "Dominic",
                            "lastName": "Widdows",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Widdows"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1989,
                                "start": 125
                            }
                        ],
                        "text": "Such an approach may be better suited to modeling noncompositional structures that are lexicalized and frequently occurring (Baldwin et al., 2003; Bannard et al., 2003). Despite this success, a significant weakness of many of the models considered here is their insensitivity to syntax. The multiplicative model, in particular, is symmetric, and thus makes no distinction between the constituents it combines. Yet, in spite of this, it is the strongest model for the simple semantic space. And although the weighted addition and dilation models differentiate between constituents, their dependence on syntax is rather limited, involving only a differential weighting of the contribution of each constituent. Perhaps more importantly, none of the representations could be said to have any internal structure. Thus, they cannot be broken down into parts which can be independently interpreted or operated upon. Symbolic representations, by contrast, build complex structures by, for example, binding predicates to arguments. In fact, it is often argued that however composition is implemented it must exhibit certain features characteristic of this symbolic binding (Fodor & Pylyshyn, 1988; Holyoak & Hummel, 2000). Our results do not indicate that models which mimic symbolic binding (i.e., tensor products and circular convolution) are better than those that do not (at least for the phrase similarity task and the syntactic structures we examined). In particular, circular convolution is, across the board, the worst performing model. One issue in the application of circular convolution is that it is designed for use with random vectors, as opposed to the structured semantic vectors we assume here. A more significant issue, however, concerns symbol binding in general, which is somewhat distinct from semantic composition. In modeling the composition of an adjective with a noun, it is not enough to simply bind the representation J. Mitchell, M. Lapata \u2044 Cognitive Science 34 (2010) 1417"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 125
                            }
                        ],
                        "text": "Such an approach may be better suited to modeling noncompositional structures that are lexicalized and frequently occurring (Baldwin et al., 2003; Bannard et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1695436,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0756f5cb5ae444d153734edf68d1ce9b95a96d1a",
            "isKey": false,
            "numCitedBy": 269,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a construction-inspecific model of multiword expression decomposability based on latent semantic analysis. We use latent semantic analysis to determine the similarity between a multiword expression and its constituent words, and claim that higher similarities indicate greater decomposability. We test the model over English noun-noun compounds and verb-particles, and evaluate its correlation with similarities and hyponymy values in WordNet. Based on mean hyponymy over partitions of data ranked on similarity, we furnish evidence for the calculated similarities being correlated with the semantic relational content of WordNet."
            },
            "slug": "An-Empirical-Model-of-Multiword-Expression-Baldwin-Bannard",
            "title": {
                "fragments": [],
                "text": "An Empirical Model of Multiword Expression Decomposability"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A construction-inspecific model of multiword expression decomposability based on latent semantic analysis is presented, and evidence is furnished for the calculated similarities being correlated with the semantic relational content of WordNet."
            },
            "venue": {
                "fragments": [],
                "text": "ACL 2003"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34902160"
                        ],
                        "name": "Jeff Mitchell",
                        "slug": "Jeff-Mitchell",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Mitchell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Mitchell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2869436"
                        ],
                        "name": "V. Demberg",
                        "slug": "V.-Demberg",
                        "structuredName": {
                            "firstName": "Vera",
                            "lastName": "Demberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Demberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143694777"
                        ],
                        "name": "Frank Keller",
                        "slug": "Frank-Keller",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Keller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frank Keller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9808585,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "9824df37db10479e1784bacafb3b34db760318a4",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "The analysis of reading times can provide insights into the processes that underlie language comprehension, with longer reading times indicating greater cognitive load. There is evidence that the language processor is highly predictive, such that prior context allows upcoming linguistic material to be anticipated. Previous work has investigated the contributions of semantic and syntactic contexts in isolation, essentially treating them as independent factors. In this paper we analyze reading times in terms of a single predictive measure which integrates a model of semantic composition with an incremental parser and a language model."
            },
            "slug": "Syntactic-and-Semantic-Factors-in-Processing-An-Mitchell-Lapata",
            "title": {
                "fragments": [],
                "text": "Syntactic and Semantic Factors in Processing Difficulty: An Integrated Measure"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This paper analyzes reading times in terms of a single predictive measure which integrates a model of semantic composition with an incremental parser and a language model."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47002621"
                        ],
                        "name": "Yuhua Li",
                        "slug": "Yuhua-Li",
                        "structuredName": {
                            "firstName": "Yuhua",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuhua Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144960578"
                        ],
                        "name": "D. Mclean",
                        "slug": "D.-Mclean",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mclean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mclean"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1829501"
                        ],
                        "name": "Z. Bandar",
                        "slug": "Z.-Bandar",
                        "structuredName": {
                            "firstName": "Zuhair",
                            "lastName": "Bandar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Bandar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1410239467"
                        ],
                        "name": "J. O'Shea",
                        "slug": "J.-O'Shea",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "O'Shea",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. O'Shea"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2627074"
                        ],
                        "name": "Keeley A. Crockett",
                        "slug": "Keeley-A.-Crockett",
                        "structuredName": {
                            "firstName": "Keeley",
                            "lastName": "Crockett",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Keeley A. Crockett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "(2003) present an experiment where participants rate whether adjective-noun combinations and their paraphrases have similar meanings, whereas other work (Li et al., 2006) elicits similarity judgments for sentence pairs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12007882,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "58331d76610488c64ce104fbc361e30d8f0f8704",
            "isKey": false,
            "numCitedBy": 841,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Sentence similarity measures play an increasingly important role in text-related research and applications in areas such as text mining, Web page retrieval, and dialogue systems. Existing methods for computing sentence similarity have been adopted from approaches used for long text documents. These methods process sentences in a very high-dimensional space and are consequently inefficient, require human input, and are not adaptable to some application domains. This paper focuses directly on computing the similarity between very short texts of sentence length. It presents an algorithm that takes account of semantic information and word order information implied in the sentences. The semantic similarity of two sentences is calculated using information from a structured lexical database and from corpus statistics. The use of a lexical database enables our method to model human common sense knowledge and the incorporation of corpus statistics allows our method to be adaptable to different domains. The proposed method can be used in a variety of applications that involve text knowledge representation and discovery. Experiments on two sets of selected sentence pairs demonstrate that the proposed method provides a similarity measure that shows a significant correlation to human intuition"
            },
            "slug": "Sentence-similarity-based-on-semantic-nets-and-Li-Mclean",
            "title": {
                "fragments": [],
                "text": "Sentence similarity based on semantic nets and corpus statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Experiments demonstrate that the proposed method provides a similarity measure that shows a significant correlation to human intuition and can be used in a variety of applications that involve text knowledge representation and discovery."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Knowledge and Data Engineering"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680292"
                        ],
                        "name": "P. Resnik",
                        "slug": "P.-Resnik",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Resnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Resnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700007"
                        ],
                        "name": "Mona T. Diab",
                        "slug": "Mona-T.-Diab",
                        "structuredName": {
                            "firstName": "Mona",
                            "lastName": "Diab",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mona T. Diab"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 183
                            }
                        ],
                        "text": "The technique is a special case of nfold cross-validation (Weiss and Kulikowski, 1991) and has been previously used for measuring how well humans agree on judging semantic similarity (Resnik and Diab, 2000; Resnik, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 197
                            }
                        ],
                        "text": "The technique is a special case of n-fold cross-validation (Weiss & Kulikowski, 1991) and has been previously used for measuring how well humans agree on judging semantic similarity (Resnik, 1999; Resnik & Diab, 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18542790,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d23485f88032bf5db0c0360f1f8c1060c00126af",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : The way we model semantic similarity is closely tied to our understanding of linguistic representations. We present several models of semantic similarity, based on differing rep- resentational assumptions, and investigate their properties via comparison with human ratings of verb similarity. The results offer insight into the bases for human similarity judgments and provide a testbed for further investigation of the interactions among syntactic properties, semantic structure, and semantic content."
            },
            "slug": "Measuring-Verb-Similarity-Resnik-Diab",
            "title": {
                "fragments": [],
                "text": "Measuring Verb Similarity"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "Several models of semantic similarity are presented, based on differing rep- resentational assumptions, and their properties are investigated via comparison with human ratings of verb similarity to offer insight into the bases for human similarity judgments."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802969"
                        ],
                        "name": "Ciprian Chelba",
                        "slug": "Ciprian-Chelba",
                        "structuredName": {
                            "firstName": "Ciprian",
                            "lastName": "Chelba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ciprian Chelba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2877845,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "673992da19d9209434615b12d55bdd36be706e9e",
            "isKey": false,
            "numCitedBy": 210,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper presents a language model that develops syntactic structure and uses it to extract meaningful information from the word history, thus enabling the use of long distance dependencies. The model assigns probability to every joint sequence of words-binary-parse-structure with headword annotation and operates in a left-to-right manner --- therefore usable for automatic speech recognition. The model, its probabilistic parameterization, and a set of experiments meant to evaluate its predictive power are presented; an improvement over standard trigram modeling is achieved."
            },
            "slug": "Exploiting-Syntactic-Structure-for-Language-Chelba-Jelinek",
            "title": {
                "fragments": [],
                "text": "Exploiting Syntactic Structure for Language Modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "A language model that develops syntactic structure and uses it to extract meaningful information from the word history, thus enabling the use of long distance dependencies and usable for automatic speech recognition is presented."
            },
            "venue": {
                "fragments": [],
                "text": "COLING-ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804885"
                        ],
                        "name": "M. Steyvers",
                        "slug": "M.-Steyvers",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Steyvers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Steyvers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 18
                            }
                        ],
                        "text": "More recent work (Steyvers & Tenenbaum, 2005) creates a semantic network from word association norms (Nelson, McEvoy, & Schreiber, 1999); however, these can only represent a small fraction of the vocabulary of an adult speaker."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6000627,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "498a304a452fa4c3fb7ab91da7485bf0a405feac",
            "isKey": false,
            "numCitedBy": 1171,
            "numCiting": 128,
            "paperAbstract": {
                "fragments": [],
                "text": "We present statistical analyses of the large-scale structure of 3 types of semantic networks: word associations, WordNet, and Roget's Thesaurus. We show that they have a small-world structure, characterized by sparse connectivity, short average path lengths between words, and strong local clustering. In addition, the distributions of the number of connections follow power laws that indicate a scale-free pattern of connectivity, with most nodes having relatively few connections joined together through a small number of hubs with many connections. These regularities have also been found in certain other complex natural networks, such as the World Wide Web, but they are not consistent with many conventional models of semantic organization, based on inheritance hierarchies, arbitrarily structured networks, or high-dimensional vector spaces. We propose that these structures reflect the mechanisms by which semantic networks grow. We describe a simple model for semantic growth, in which each new word or concept is connected to an existing network by differentiating the connectivity pattern of an existing node. This model generates appropriate small-world statistics and power-law connectivity distributions, and it also suggests one possible mechanistic basis for the effects of learning history variables (age of acquisition, usage frequency) on behavioral performance in semantic processing tasks."
            },
            "slug": "The-Large-Scale-Structure-of-Semantic-Networks:-and-Steyvers-Tenenbaum",
            "title": {
                "fragments": [],
                "text": "The Large-Scale Structure of Semantic Networks: Statistical Analyses and a Model of Semantic Growth"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A simple model for semantic growth is described, in which each new word or concept is connected to an existing network by differentiating the connectivity pattern of an existing node, which generates appropriate small-world statistics and power-law connectivity distributions."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794100"
                        ],
                        "name": "Brian Roark",
                        "slug": "Brian-Roark",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Roark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Roark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39754422"
                        ],
                        "name": "A. Bachrach",
                        "slug": "A.-Bachrach",
                        "structuredName": {
                            "firstName": "Asaf",
                            "lastName": "Bachrach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bachrach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064441335"
                        ],
                        "name": "Carlos Cardenas",
                        "slug": "Carlos-Cardenas",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Cardenas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlos Cardenas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7892142"
                        ],
                        "name": "Christophe Pallier",
                        "slug": "Christophe-Pallier",
                        "structuredName": {
                            "firstName": "Christophe",
                            "lastName": "Pallier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christophe Pallier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14609577,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "3b690e96a987a20679e55e601c18ca6b8caf5910",
            "isKey": false,
            "numCitedBy": 161,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "A number of recent publications have made use of the incremental output of stochastic parsers to derive measures of high utility for psycholinguistic modeling, following the work of Hale (2001; 2003; 2006). In this paper, we present novel methods for calculating separate lexical and syntactic surprisal measures from a single incremental parser using a lexicalized PCFG. We also present an approximation to entropy measures that would otherwise be intractable to calculate for a grammar of that size. Empirical results demonstrate the utility of our methods in predicting human reading times."
            },
            "slug": "Deriving-lexical-and-syntactic-expectation-based-Roark-Bachrach",
            "title": {
                "fragments": [],
                "text": "Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Novel methods for calculating separate lexical and syntactic surprisal measures from a single incremental parser using a lexicalized PCFG and an approximation to entropy measures that would otherwise be intractable to calculate for a grammar of that size are presented."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2653356"
                        ],
                        "name": "R. E. Till",
                        "slug": "R.-E.-Till",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Till",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. E. Till"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3831099"
                        ],
                        "name": "E. F. Mross",
                        "slug": "E.-F.-Mross",
                        "structuredName": {
                            "firstName": "Ernest",
                            "lastName": "Mross",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. F. Mross"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3025325"
                        ],
                        "name": "W. Kintsch",
                        "slug": "W.-Kintsch",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Kintsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Kintsch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 42653521,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "c7e8f3b6f78a770b3e2e51d58421b311452ab2fe",
            "isKey": false,
            "numCitedBy": 225,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "The construction of word meanings in a discourse context was conceptualized as a process of sense activation, sense selection, and sense elaboration. In three experiments, subjects read texts presented by a rapid serial visual procedure and performed a lexical decision on visually presented targets that followed ambiguous prime words. When the target was a word, it was either an associate of the prime word, a probable inference suggested by the discourse, or an unrelated word. For associates, lexical decisions that related to either the appropriate or the inappropriate sense of the ambiguous word were generally facilitated at short (200-400 msec) prime-target stimulus onset asynchronies (SOAs). At longer SOAs, responses were faster to appropriate than to inappropriate associates. For the thematic inferences, there was no difference between these (appropriate) inferences and (inappropriate) control words at short SOAs. At long SOAs (1,000 and 1,500 msec), however, inference words were facilitated. The results are interpreted as consistent with a model of lexical processing in which sense activation functions independently of context. Discourse context effects, whether on sense selection (suppression of inappropriate associates) or on sense elaboration (creation of inferences), are seen as postlexical."
            },
            "slug": "Time-course-of-priming-for-associate-and-inference-Till-Mross",
            "title": {
                "fragments": [],
                "text": "Time course of priming for associate and inference words in a discourse context"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "The construction of word meanings in a discourse context was conceptualized as a process of sense activation, sense selection, and sense elaboration and the results are interpreted as consistent with a model of lexical processing in which sense activation functions independently of context."
            },
            "venue": {
                "fragments": [],
                "text": "Memory & cognition"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47277215"
                        ],
                        "name": "S. Duffy",
                        "slug": "S.-Duffy",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Duffy",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Duffy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144897958"
                        ],
                        "name": "J. Henderson",
                        "slug": "J.-Henderson",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Henderson",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Henderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33070393"
                        ],
                        "name": "R. Morris",
                        "slug": "R.-Morris",
                        "structuredName": {
                            "firstName": "Robin",
                            "lastName": "Morris",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Morris"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 13
                            }
                        ],
                        "text": "For example, Duffy, Henderson, and Morris (1989) showed that priming of sentence terminal words was dependent not simply on individual preceding words but on their combination, and Morris (1994) later demonstrated that this priming also showed dependencies on the syntactic relations in the\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 261
                            }
                        ],
                        "text": "For example, a model of semantic priming in sentential contexts ought to take account of the fact that the effect is not simply based on a relationship between individual words, but is dependent on the combination of word meanings and their syntactic structure (Duffy et al., 1989; Morris, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1199849,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "d55b04a65608f549c9637e78bc6e194f362f3c5c",
            "isKey": false,
            "numCitedBy": 140,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "In three experiments we investigated the effect of a sentence context on naming time for a target word. Contexts were presented by using a rapid serial visual presentation; subjects named the last word of the sentence. In the first two experiments, facilitation was observed for a fully congruent context containing a subject and verb that were weakly related to the target word. No facilitation was observed when either the subject or verb was replaced with a more neutral word. In the third experiment, the fully congruent contexts were modified either to preserve or to disrupt the original relation between the subject and verb. Facilitation was observed in both conditions. The full pattern of results suggests that a combination of lexical items can prime a target word in the absence of priming by any of the lexical items individually. This combination priming is not dependent upon the overall meaning of the sentence."
            },
            "slug": "Semantic-facilitation-of-lexical-access-during-Duffy-Henderson",
            "title": {
                "fragments": [],
                "text": "Semantic facilitation of lexical access during sentence processing."
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The full pattern of results suggests that a combination of lexical items can prime a target word in the absence of priming by any of the lexicalItems individually."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of experimental psychology. Learning, memory, and cognition"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144967725"
                        ],
                        "name": "J. Quesada",
                        "slug": "J.-Quesada",
                        "structuredName": {
                            "firstName": "Jose",
                            "lastName": "Quesada",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Quesada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103940467"
                        ],
                        "name": "Walter Kinstich",
                        "slug": "Walter-Kinstich",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Kinstich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Walter Kinstich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2978506"
                        ],
                        "name": "Praful Mangalath",
                        "slug": "Praful-Mangalath",
                        "structuredName": {
                            "firstName": "Praful",
                            "lastName": "Mangalath",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Praful Mangalath"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18322403,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2fda5af92b78650490680e2a34e4b73fbcc3ec0",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Analogy-making as Predication Using Relational Information and LSA Vectors Jose Quesada, Walter Kintsch, Praful Mangalath ([quesadaj, wkintsch, praful]@psych.colorado.edu) Institute of Cognitive Science, University of Colorado, Boulder Boulder, CO 80309-0344 USA Current models of analogy comprehension use hand - coded representations. As Hummel and Holyoak (2003) put it, \u201cthe problem pf hand-coded representations is among the most serious problems facing computational modeling as a scientific enterprise: All models are sensitive to their representations, so the choice of representation is among the most powerful wild cards at the modeler\u2019s disposal\u201d (p. 247). French (2002) reviews different computational models of analogy-making, and points out one of the most fundamental problems of the field: case representations are authored (hand-coded) to make the model work. Between the challenges and future directions he presents \u201cthe systematic exploration of experimenter-independent representation-building and learning mechanisms\u201d (p. 204). In this poster, we propose LSA as a method to generate the much-wanted non-hand-coded representations. However, LSA has severe limitations to represent structure. Turney and Littman (2003) pointed out that the similarity of semantic relations between words is not directly reducible to the semantic similarity of individual words. This is also the leitmotiv of some analogy models like Gentner\u2019s (1983; 1989).Thus, LSA alone would fail to explain analogy, where relations (structure) between words are fundamental. We use a predication (Kintsch, 2001) to represent structure comparisons in the LSA semantic space. Predication is able to select the features (neighbors) of one component of the analogy (the source) that are relevant to the other (the target). Table 1(a): a sample SAT question. Ostrich : bird Table 1(b): predication using analogy domains Number Percent T&L (2003) (a) Lion : Cat (b) Goose : Flock (c) Ewe : Sheep Skipped (d) Cub : Bear Total (e) Primate: Monkey Correct Incorrect Precision 157/307 0.51% Recall F We calculated the predication vectors for all the targets and alternatives of 374 items from the Scholastic aptitude test (SAT). This dataset of analogies was collected by Turney and Littman (2003). An example of a SAT item can be seen in Table 1(a). To calculate the correct alternative, we computed the cosine between the target vector and each alternative, and selected the alternative with the highest cosine. However, this method had poor results: using LSA this way leaves out most of the relational information. For example, relations such as is-a, part-of, causal-agent-of, etc. are all substituted by a very basic semantic distance measure when we compute the cosine between the target and the alternatives. To include this relational information in the comparison, we constructed a set of ten possible relations between the components in the 374 SAT analogies (table 2). Then we computed the cosine between the list of words that define the analogy domain and each analogy predication vector in the dataset. That is, for each analogy we created a vector of ten features, where each feature indicates how similar the analogy is to each of the analogy domains. For example, Ostrich::bird would load primarily in the taxonomy and Hyponymy domain components, but also in endonymy, synonymy, and degree. Then, we correlated these loading vectors for the target and each alternative, and selected the alternative that best correlated with the target to solve the SAT question. Table 2: Ten analogy domains and their characteristic words Hyponymy X is a type of Y (for example - Maple:Tree) [Subordinate of, superordinate to, rank, class, category, family, genus, variety, type of, kind of, hyponym] Degree X means Y at a certain degree (Pour:Drip) [level, stage, point, magnitude, extent, greater, lesser, intensity, severity, extreme, degree] Meronymy The parts of X include the Ys (Body:Arm) [part, whole, component, made up of, portion, contains, constituent, segment, piece of, composite, meronym] Taxonomy X is an item in the category Y (Milk:Beverage)[classification, containing, structure, relationship, hierarchy, system, framework, taxonym] Synonymy is the same as Y (Work:Labor) [equivalent, equal, likeness, match, interchangeable, alike, same as, similar, close to, like, synonym] Antonymy is the opposite of Y (Find:Hide) [opposite, unlike, different, antithesis, opposed, contradiction, contrast, reverse, anti, not the same as, antonym] Characteristic X is a characteristic of Y (Dishonesty:Liar) [indicative, representative of, typical of, feature, attribute, trait, property, mannerism, facet, quality, characteristic] Plurality X is many Ys (Throng:People) [mass, bulk, several, many, lots of, numerous, crowd, group, more, number, plural] Endonymy X entails Y (Coop:Poultry) [entails, require, evoke, involve, suggest, imply, presuppose, mean] Use X is used to Y (Scissors:Cut) [do with, manipulate, operate, function, purpose, role, action, utilize, employ, use] The results are displayed in Table 1(b). The performance of our model is very close to the state of the art in automatic analogy making when considering correct answers (42% vs. 47%, Turney & Littman, 2003), and precision, recall and F measures. Furthermore, our model is psychologically plausible."
            },
            "slug": "Analogy-making-as-Predication-Using-Relational-and-Quesada-Kinstich",
            "title": {
                "fragments": [],
                "text": "Analogy-making as Predication Using Relational Information and LSA Vectors"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This poster proposes LSA as a method to generate the much-wanted non-hand-coded representations and uses a predication to represent structure comparisons in the LSA semantic space."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2037240"
                        ],
                        "name": "U. Essen",
                        "slug": "U.-Essen",
                        "structuredName": {
                            "firstName": "Ute",
                            "lastName": "Essen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Essen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795942"
                        ],
                        "name": "Reinhard Kneser",
                        "slug": "Reinhard-Kneser",
                        "structuredName": {
                            "firstName": "Reinhard",
                            "lastName": "Kneser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Reinhard Kneser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 34
                            }
                        ],
                        "text": "For example, absolute discounting (Ney et al., 1994) provides a very simple means of deciding how much probability mass to re-assign, by simply subtracting the same amount from all non-zero counts."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206560877,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3e53b9c0e3a7a60e7a5295e9b08af74d6fb3dbf",
            "isKey": false,
            "numCitedBy": 599,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract In this paper, we study the problem of stochastic language modelling from the viewpoint of introducing suitable structures into the conditional probability distributions. The task of these distributions is to predict the probability of a new word by looking at M or even all predecessor words. The conventional approach is to limit M to 1 or 2 and to interpolate the resulting bigram and trigram models with a unigram model in a linear fashion. However, there are many other structures that can be used to model the probabilistic dependences between the predecessor word and the word to be predicted. The structures considered in this paper are: nonlinear interpolation as an alternative to linear interpolation; equivalence classes for word histories and single words; cache memory and word associations. For the optimal estimation of nonlinear and linear interpolation parameters, the leaving-one-out method is systematically used. For the determination of word equivalence classes in a bigram model, an automatic clustering procedure has been adapted. To capture long-distance dependences, we consider various models for word-by-word dependences; the cache model may be viewed as a special type of self-association. Experimental results are presented for two text databases, a Germany database and an English database."
            },
            "slug": "On-structuring-probabilistic-dependences-in-Ney-Essen",
            "title": {
                "fragments": [],
                "text": "On structuring probabilistic dependences in stochastic language modelling"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "The problem of stochastic language modelling is studied from the viewpoint of introducing suitable structures into the conditional probability distributions, and nonlinear interpolation as an alternative to linear interpolation; equivalence classes for word histories and single words; cache memory and word associations are considered."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Speech Lang."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388342831"
                        ],
                        "name": "Stefan L. Frank",
                        "slug": "Stefan-L.-Frank",
                        "structuredName": {
                            "firstName": "Stefan L.",
                            "lastName": "Frank",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefan L. Frank"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34647275"
                        ],
                        "name": "M. Koppen",
                        "slug": "M.-Koppen",
                        "structuredName": {
                            "firstName": "Mathieu",
                            "lastName": "Koppen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Koppen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145395855"
                        ],
                        "name": "Leo G. M. Noordman",
                        "slug": "Leo-G.-M.-Noordman",
                        "structuredName": {
                            "firstName": "Leo",
                            "lastName": "Noordman",
                            "middleNames": [
                                "G.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Leo G. M. Noordman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145699990"
                        ],
                        "name": "W. Vonk",
                        "slug": "W.-Vonk",
                        "structuredName": {
                            "firstName": "Wietske",
                            "lastName": "Vonk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Vonk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 54753366,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6797b320c62999eb6fac3ed2874e9e7ef44d6cb7",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Because higher level cognitive processes generally involve the use of world knowledge, computational models of these processes require the implementation of a knowledge base. This article identifies and discusses 4 strategies for dealing with world knowledge in computational models: disregarding world knowledge, ad hoc selection, extraction from text corpora, and implementation of all knowledge about a simplified microworld. Each of these strategies is illustrated by a detailed discussion of a model of discourse comprehension. It is argued that seemingly successful modeling results are uninformative if knowledge is implemented ad hoc or not at all, that knowledge extracted from large text corpora is not appropriate for discourse comprehension, and that a suitable implementation can be obtained by applying the microworld strategy."
            },
            "slug": "World-Knowledge-in-Computational-Models-of-Frank-Koppen",
            "title": {
                "fragments": [],
                "text": "World Knowledge in Computational Models of Discourse Comprehension"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is argued that seemingly successful modeling results are uninformative if knowledge is implemented ad hoc or not at all, that knowledge extracted from large text corpora is not appropriate for discourse comprehension, and that a suitable implementation can be obtained by applying the microworld strategy."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47709773"
                        ],
                        "name": "H. Rubenstein",
                        "slug": "H.-Rubenstein",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Rubenstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Rubenstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1898344"
                        ],
                        "name": "J. Goodenough",
                        "slug": "J.-Goodenough",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Goodenough",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Goodenough"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 49
                            }
                        ],
                        "text": "For example, the benchmark data set collected by Rubenstein and Goodenough (1965) consists of 65 noun pairs ranging from\nhighly synonymous (gem-jewel) to semantically unrelated (noon-string)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 74
                            }
                        ],
                        "text": "Similarly, we could study the extent of semantic similarity between words (Rubenstein and Goodenough, 1965), or analyse the conditions under which the meaning of one sentence is contained in or implied by the other (Dagan et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18309234,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "7ef3ac14cdb484aaa2b039850093febd5cf73a21",
            "isKey": false,
            "numCitedBy": 1460,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Experimentol corroboration was obtained for the hypothesis that the proportion of words common to the contexts of word A and to the contexts of word B is a function of the degree to which A and B are similar in meaning. The tests were carried out for variously defined contexts. The shapes of the functions, however, indicate that similarity of context is reliable as criterion only for detecting pairs of words that are very similar in meaning."
            },
            "slug": "Contextual-correlates-of-synonymy-Rubenstein-Goodenough",
            "title": {
                "fragments": [],
                "text": "Contextual correlates of synonymy"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The shapes of the functions indicate that similarity of context is reliable as criterion only for detecting pairs of words that are very similar in meaning."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36855520"
                        ],
                        "name": "R. F. West",
                        "slug": "R.-F.-West",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "West",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. F. West"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4248890"
                        ],
                        "name": "K. Stanovich",
                        "slug": "K.-Stanovich",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Stanovich",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Stanovich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 197
                            }
                        ],
                        "text": "However, there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville, Nichol, Barss, Forster, & Garrett, 1991; West & Stanovich, 1986)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 150
                            }
                        ],
                        "text": "In contrast, there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al., 1991; West and Stanovich, 1986)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 38513808,
            "fieldsOfStudy": [
                "Psychology",
                "Linguistics"
            ],
            "id": "60bd96f3aa44485f1fe5dbe65d83d16e56334711",
            "isKey": false,
            "numCitedBy": 99,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "A series of experiments explored the effect of the syntactic structure of a sentence fragment on the processing of a subsequent target word. In both a naming and a lexical decision task, modal verb contexts followed by main verb targets and preposition contexts followed by noun targets produced faster response times than did the opposite pairings (i.e., modal/noun and preposition/ verb). This syntactic context effect occurred across several different variations in the method of context presentation. Also, unlike some previous findings on syntactic priming, the present effects did not disappear when a naming task was employed. The magnitude of the syntactic priming effect was similar in the naming and lexical decision tasks when the response times were slow, but was larger in the lexical decision task when the response times were faster. The implications of these results for recent discussions of the relationship between task structure and the locus of observed contextual effects are discussed."
            },
            "slug": "Robust-effects-of-syntactic-structure-on-visual-West-Stanovich",
            "title": {
                "fragments": [],
                "text": "Robust effects of syntactic structure on visual word processing"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The magnitude of the syntactic priming effect was similar in the naming and lexical decision tasks when the response times were slow, but was larger in the Lexical decision task when theresponse times were faster."
            },
            "venue": {
                "fragments": [],
                "text": "Memory & cognition"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144514944"
                        ],
                        "name": "W. Lowe",
                        "slug": "W.-Lowe",
                        "structuredName": {
                            "firstName": "Will",
                            "lastName": "Lowe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Lowe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 81
                            }
                        ],
                        "text": "Our first model is a simple and popular semantic space (Bullinaria & Levy, 2007; Lowe, 2000; McDonald, 2000) that associates each vector component with a particular context word, and assigns it a value based on the strength of its co-occurrence with the target (i.e., the word for which a semantic\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 264,
                                "start": 218
                            }
                        ],
                        "text": "Within this approach, the cognitive structures that embody meanings could take many different forms, and both logical and distributional approaches have been proposed as accurate models of these mental representations (Stenning and van Lambalgen, 2008; Lowe, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2260968,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e43f8a9a1ff712bdb3192d6e66b04effaf56597f",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "McDonald and Lowe [15] showed that cosines in a semantic space of several hundred dimensions reflect human priming results for a wide range of semantic and associatively related words [16, Exp.2]. Previously, Lowe [II, 10] argued that the intrinsic dimensionality of semantic space is much lower, and that high-dimensional structure can be effectively captured in just two dimensions as the surface of a neural map. This paper provides a replication of McDonald and Lowe\u2019s results in two dimensions using the Generative Topographic Mapping [2], a statistically motivated neural network architecture for topographic maps."
            },
            "slug": "What-is-the-Dimensionality-of-Human-Semantic-Space-Lowe",
            "title": {
                "fragments": [],
                "text": "What is the Dimensionality of Human Semantic Space?"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper provides a replication of McDonald and Lowe\u2019s results in two dimensions using the Generative Topographic Mapping [2], a statistically motivated neural network architecture for topographic maps."
            },
            "venue": {
                "fragments": [],
                "text": "NCPW"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990190"
                        ],
                        "name": "P. Pantel",
                        "slug": "P.-Pantel",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Pantel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pantel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 50
                            }
                        ],
                        "text": "A different type of semantic space is proposed in Lin and Pantel (2001) (see also Turney and Pantel, 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9548219,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c384d9ee4fd8657b26a8165244eb4ad73df4f492",
            "isKey": false,
            "numCitedBy": 506,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose an unsupervised method for discovering inference rules from text, such as \u201cX is author of Y \u2248 X wrote Y\u201d, \u201cX solved Y \u2248 X found a solution to Y\u201d, and \u201cX caused Y \u2248 Y is triggered by X\u201d. Inference rules are extremely important in many fields such as natural language processing, information retrieval, and artificial intelligence in general. Our algorithm is based on an extended version of Harris\u2019 Distributional Hypothesis, which states that words that occurred in the same contexts tend to be similar. Instead of using this hypothesis on words, we apply it to paths in the dependency trees of a parsed corpus."
            },
            "slug": "DIRT-\u2013-Discovery-of-Inference-Rules-from-Text-Lin-Pantel",
            "title": {
                "fragments": [],
                "text": "DIRT \u2013 Discovery of Inference Rules from Text"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This paper proposes an unsupervised method for discovering inference rules from text, based on an extended version of Harris\u2019 Distributional Hypothesis, which states that words that occurred in the same contexts tend to be similar."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699177"
                        ],
                        "name": "Yonggang Deng",
                        "slug": "Yonggang-Deng",
                        "structuredName": {
                            "firstName": "Yonggang",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yonggang Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2803071"
                        ],
                        "name": "S. Khudanpur",
                        "slug": "S.-Khudanpur",
                        "structuredName": {
                            "firstName": "Sanjeev",
                            "lastName": "Khudanpur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Khudanpur"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 2004); and treating the LSA similarity as a feature in a maximum entropy language model (Deng and Khundanpur, 2003). Instead of constructing separate semantic and n-gram models which then need to be combined in some way, Wallach (2006) integrates a topic model with a bigram model directly."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": ", 2004); and treating the LSA similarity as a feature in a maximum entropy language model (Deng and Khundanpur, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1471881,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d06c859eeabc096bdf3fca622cc819b2a3c7eab8",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Latent semantic analysis (LSA), first exploited in indexing documents for information retrieval, has since been used by several researchers to demonstrate impressive reductions in the perplexity of statistical language models on text corpora such as the Wall Street Journal. In this paper we present an investigation into the use of LSA in language modeling for conversational speech recognition. We find that previously proposed methods of combining an LSA-based unigram model with an N-gram model yield much smaller reductions in perplexity on speech transcriptions than has been reported on written text. We next present a family of exponential models in which LSA similarity is a feature of a word-history pair. The maximum entropy model in this family yields a greater reduction in perplexity, and statistically significant improvements in recognition accuracy over a trigram model on the Switchboard corpus. We conclude with a comparison of this LSA-featured model with a previously proposed topic-dependent maximum entropy model."
            },
            "slug": "Latent-Semantic-Information-in-Maximum-Entropy-for-Deng-Khudanpur",
            "title": {
                "fragments": [],
                "text": "Latent Semantic Information in Maximum Entropy Language Models for Conversational Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An investigation into the use of LSA in language modeling for conversational speech recognition finds that previously proposed methods of combining an LSA-based unigram model with an N-gram model yield much smaller reductions in perplexity on speech transcriptions than has been reported on written text."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143937779"
                        ],
                        "name": "R. Kuhn",
                        "slug": "R.-Kuhn",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Kuhn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kuhn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714393"
                        ],
                        "name": "R. Mori",
                        "slug": "R.-Mori",
                        "structuredName": {
                            "firstName": "Renato",
                            "lastName": "Mori",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mori"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 31924166,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be1fed9544830df1137e72b1d2396c40d3e18365",
            "isKey": false,
            "numCitedBy": 580,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Speech-recognition systems must often decide between competing ways of breaking up the acoustic input into strings of words. Since the possible strings may be acoustically similar, a language model is required; given a word string, the model returns its linguistic probability. Several Markov language models are discussed. A novel kind of language model which reflects short-term patterns of word use by means of a cache component (analogous to cache memory in hardware terminology) is presented. The model also contains a 3g-gram component of the traditional type. The combined model and a pure 3g-gram model were tested on samples drawn from the Lancaster-Oslo/Bergen (LOB) corpus of English text. The relative performance of the two models is examined, and suggestions for the future improvements are made. >"
            },
            "slug": "A-Cache-Based-Natural-Language-Model-for-Speech-Kuhn-Mori",
            "title": {
                "fragments": [],
                "text": "A Cache-Based Natural Language Model for Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A novel kind of language model which reflects short-term patterns of word use by means of a cache component (analogous to cache memory in hardware terminology) is presented and contains a 3g-gram component of the traditional type."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714004"
                        ],
                        "name": "A. Mnih",
                        "slug": "A.-Mnih",
                        "structuredName": {
                            "firstName": "Andriy",
                            "lastName": "Mnih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mnih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 577005,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd7d93193aad6c4b71cc8942e808753019e87706",
            "isKey": false,
            "numCitedBy": 606,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The supremacy of n-gram models in statistical language modelling has recently been challenged by parametric models that use distributed representations to counteract the difficulties caused by data sparsity. We propose three new probabilistic language models that define the distribution of the next word in a sequence given several preceding words by using distributed representations of those words. We show how real-valued distributed representations for words can be learned at the same time as learning a large set of stochastic binary hidden features that are used to predict the distributed representation of the next word from previous distributed representations. Adding connections from the previous states of the binary hidden features improves performance as does adding direct connections between the real-valued distributed representations. One of our models significantly outperforms the very best n-gram models."
            },
            "slug": "Three-new-graphical-models-for-statistical-language-Mnih-Hinton",
            "title": {
                "fragments": [],
                "text": "Three new graphical models for statistical language modelling"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown how real-valued distributed representations for words can be learned at the same time as learning a large set of stochastic binary hidden features that are used to predict the distributed representation of the next word from previous distributed representations."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29874772"
                        ],
                        "name": "E. Kako",
                        "slug": "E.-Kako",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Kako",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Kako"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 104
                            }
                        ],
                        "text": "To give a concrete example, a brown cow does not identify a concept intermediate between brown and cow (Kako, 1999, p. 2)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 144448562,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "e5b09259b4047481c277de3225753696e5ab495f",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "Which properties of syntax are uniquely human, and which can be acquired by other animals? Relevant evidence is provided by work with three language-trained animals: the African gray parrot Alex, who can produce and comprehend a small fragment of English; the bottle-nosed dolphins Ake and Phoenix, who can comprehend a gestural and an acoustic language, respectively; and the bonobo Kanzi, who can produce combinations of lexigrams and comprehend a significant fragment of English. The systems of these animals are examined for evidence of four core properties of syntax: discrete combinatorics, category-based rules, argument structure, and closed-class items. Additional studies that explore further what these animals can learn about these core properties are suggested."
            },
            "slug": "Elements-of-syntax-in-the-systems-of-three-animals-Kako",
            "title": {
                "fragments": [],
                "text": "Elements of syntax in the systems of three language-trained animals"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3298207"
                        ],
                        "name": "P. Kanerva",
                        "slug": "P.-Kanerva",
                        "structuredName": {
                            "firstName": "Pentti",
                            "lastName": "Kanerva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kanerva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2532711"
                        ],
                        "name": "Jan Kristoferson",
                        "slug": "Jan-Kristoferson",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Kristoferson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan Kristoferson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078819344"
                        ],
                        "name": "Anders Holst",
                        "slug": "Anders-Holst",
                        "structuredName": {
                            "firstName": "Anders",
                            "lastName": "Holst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anders Holst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 0
                            }
                        ],
                        "text": "Kanerva, Kristoferson, and Holst (2000) propose the use of random indexing as an alternative to the computationally costly singular value decomposition employed in LSA."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60571601,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "932c8b99ef910bedd0f49d889230aba308004e0a",
            "isKey": false,
            "numCitedBy": 492,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Random Indexing of Text Samples for Latent Semantic Analysis Pentti Kanerva Jan Kristoferson Anders Holst kanerva@sics.se, janke@sics.se, aho@sics.se RWCP Theoretical Foundation SICS Laboratory Swedish Institute of Computer Science, Box 1263, SE-16429 Kista, Sweden Latent Semantic Analysis is a method of computing vectors|and it has several randomly placed ; 1s and high-dimensional semantic vectors, or context vectors, 1s, with the rest 0s (e.g., four each of ; 1 and 1, or for words from their co-occurrence statistics. An exper- eight non-0s in 1,800, instead of one non-0 in 30,000 iment by Landauer & Dumais (1997) covers a vocabu- as above). Thus, we would accumulate the same data lary of 60,000 words (unique letter strings delimited by into a 60,000 1,800 words-by-contexts matrix instead word-space characters) in 30,000 contexts (text samples of 60,000 30,000. or \\documents of about 150 words each). The data are Our method has been veried with dierent data, a rst collected into a 60,000 30,000 words-by-contexts ten-million-word \\TASA corpus consisting of a 79,000- co-occurrence matrix, with each row representing a word word vocabulary (when words are truncated after the 8th and each column representing a text sample so that each character) in 37,600 text samples. The data were accu- entry gives the frequency of a given word in a given mulated into a 79,000 1,800 words-by-contexts matrix, text sample. The frequencies are normalized, and the which was normalized by thresholding into a matrix of normalized matrix is transformed with Singular-Value ; 1s, 0s, and 1s. The unnormalized 1,800-dimensional Decomposition (SVD) reducing its original 30,000 doc- context vectors gave 35{44% correct in the TOEFL test ument dimensions into a much smaller number of latent and the normalized ones gave 48{51% correct, which cor- dimensions, 300 proving to be optimal. Thus words are respond to Landauer & Dumais' 36% for their normal- represented by 300-dimensional semantic vectors. ized 30,000-dimensional vectors before SVD, for a dier- The point in all of this is that the vectors capture ent corpus (see above). Our words-by-contexts matrix meaning. Landauer and Dumais demonstrate it with a can be transformed further, for example with SVD as in synonym test called TOEFL (for \\Test Of English as a LSA, except that the matrix is much smaller. Mathematically, the 30,000- or 37,600-dimensional in- Foreign Language ). For each test word, four alterna- dex vectors are orthogonal, whereas the 1,800-dimen- tives are given, and the \\contestant is asked to nd the one that's the most synonymous. Choosing at random sional ones are only nearly orthogonal. They seem to would yield 25% correct. However, when the seman- work just as well, in addition to which they are more tic vector for the test word is compared to the seman- \\brainlike and less aected by the number of text sam- tic vectors for the four alternatives, it correlates most ples (1,800-dimensional index vectors can cover a wide- highly with the correct alternative in 64% of the cases. ranging number of text samples). We have used such However, when the same test is based on the 30,000- vectors also to index words in narrow context windows, dimensional vectors before SVD, the result is not nearly getting 62{70% correct, and conclude that random in- as good: only 36% correct. The authors conclude that dexing deserves to be studied and understood more fully. Acknowledgments. This research is supported by the reorganization of information by SVD somehow cor- Japan's Ministry of International Trade and Industry responds to human psychology. under the Real World Computing Partnership We have studied high-dimensional random distributed (MITI) (RWCP) The TASA corpus and 80 TOEFL representations, as models of brainlike representation of test items program. were made available to us by courtesy of Pro- information (Kanerva, 1994; Kanerva & Sj\u007fodin, 1999). fessor Thomas Landauer, University of Colorado. In this poster we report on the use of such a repre- sentation to reduce the dimensionality of the original words-by-contexts matrix. The method can be explained Kanerva, P. (1994). References The Spatter Code for encoding by looking at the 60,000 30,000 matrix of frequencies concepts at many levels. In M. Marinaro and P. G. above. Assume that each text sample is represented by a Morasso (eds.), ICANN '94, Proc. Int'l Conference 30,000-bit vector with a single 1 marking the place of the on Articial Neural Networks (Sorrento, Italy), vol. 1, sample in a list of all samples, and call it the sample's pp. 226{229. London: Springer-Verlag. index vector (i.e., the n th bit of the index vector for the Kanerva, P., and Sj\u007fodin, G. (1999). Stochastic Pattern n th text sample is 1|the representation is unitary or lo- Computing. Proc. 2000 Real World Computing Sym- cal). Then the words-by-contexts matrix of frequencies bosium (Report TR-99-002, pp. 271{276). Tsukuba- can be gotten by the following procedure: every time city, Japan: Real World Computing Partnership. that the word w occurs in the n th text sample, the n th index vector is added to the row for the word w . Landauer, T. K., and Dumais, S. T. (1997). A solution We use the same procedure for accumulating a words- to Plato's problem: The Latent Semantic Analysis by-contexts matrix, except that the index vectors are theory of the acquisition, induction, and representa- not unitary. A text-sample's index vector is \\small tion of knowledge. Psychological Review 104 (2):211{ by comparison|we have used 1,800-dimensional index"
            },
            "slug": "Random-indexing-of-text-samples-for-latent-semantic-Kanerva-Kristoferson",
            "title": {
                "fragments": [],
                "text": "Random indexing of text samples for latent semantic analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "Random Indexing of Text Samples for Latent Semantic Analysis Pentti Kanerva Jan Kristoferson Anders Holst kanerva@sics.se, aho@sic.se RWCP Theoretical Foundation SICS Laboratory Swedish Institute of Computer Science, Box 1263, SE-16429 Kista, Sweden LatentSemantic Analysis is a method of computing vectors that captures ent corpus and the vectors capture words-by-contexts matrix meaning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747648"
                        ],
                        "name": "William Schuler",
                        "slug": "William-Schuler",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Schuler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William Schuler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144227123"
                        ],
                        "name": "S. Abdelrahman",
                        "slug": "S.-Abdelrahman",
                        "structuredName": {
                            "firstName": "Samir",
                            "lastName": "Abdelrahman",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Abdelrahman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144950569"
                        ],
                        "name": "Timothy Miller",
                        "slug": "Timothy-Miller",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Miller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timothy Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38879761"
                        ],
                        "name": "Lane Schwartz",
                        "slug": "Lane-Schwartz",
                        "structuredName": {
                            "firstName": "Lane",
                            "lastName": "Schwartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lane Schwartz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1249886,
            "fieldsOfStudy": [
                "Psychology",
                "Linguistics"
            ],
            "id": "07bd5c0d83f5227634f45538eb2fbb07a1973f00",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Psycholinguistic studies suggest a model of human language processing that 1) performs incremental interpretation of spoken utterances or written text, 2) preserves ambiguity by maintaining competing analyses in parallel, and 3) operates within a severely constrained short-term memory store --- possibly constrained to as few as four distinct elements. This paper describes a relatively simple model of language as a factored statistical time-series process that meets all three of the above desiderata; and presents corpus evidence that this model is sufficient to parse naturally occurring sentences using human-like bounds on memory."
            },
            "slug": "Toward-a-Psycholinguistically-Motivated-Model-of-Schuler-Abdelrahman",
            "title": {
                "fragments": [],
                "text": "Toward a Psycholinguistically-Motivated Model of Language Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A relatively simple model of language as a factored statistical time-series process that meets all three of the above desiderata is described; and corpus evidence that this model is sufficient to parse naturally occurring sentences using human-like bounds on memory is presented."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760695"
                        ],
                        "name": "Ichiro Yamada",
                        "slug": "Ichiro-Yamada",
                        "structuredName": {
                            "firstName": "Ichiro",
                            "lastName": "Yamada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ichiro Yamada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768754"
                        ],
                        "name": "Kentaro Torisawa",
                        "slug": "Kentaro-Torisawa",
                        "structuredName": {
                            "firstName": "Kentaro",
                            "lastName": "Torisawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kentaro Torisawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761994"
                        ],
                        "name": "Jun'ichi Kazama",
                        "slug": "Jun'ichi-Kazama",
                        "structuredName": {
                            "firstName": "Jun'ichi",
                            "lastName": "Kazama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun'ichi Kazama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2649737"
                        ],
                        "name": "Kow Kuroda",
                        "slug": "Kow-Kuroda",
                        "structuredName": {
                            "firstName": "Kow",
                            "lastName": "Kuroda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kow Kuroda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697381"
                        ],
                        "name": "M. Murata",
                        "slug": "M.-Murata",
                        "structuredName": {
                            "firstName": "Masaki",
                            "lastName": "Murata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Murata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806095"
                        ],
                        "name": "S. D. Saeger",
                        "slug": "S.-D.-Saeger",
                        "structuredName": {
                            "firstName": "Stijn",
                            "lastName": "Saeger",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D. Saeger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145440721"
                        ],
                        "name": "Francis Bond",
                        "slug": "Francis-Bond",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Bond",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Francis Bond"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40321278"
                        ],
                        "name": "A. Sumida",
                        "slug": "A.-Sumida",
                        "structuredName": {
                            "firstName": "Asuka",
                            "lastName": "Sumida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sumida"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 72
                            }
                        ],
                        "text": ", 1997), word sense discrimination (Sch\u00fctze, 1998), ontology extraction (Yamada et al., 2009) and modelling semantic priming (Lund and Burgess, 1996; Landuaer and Dumais, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8072713,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f2f5bd87d67e1ced430a4684ebda7665bd619804",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new method of developing a large-scale hyponymy relation database by combining Wikipedia and other Web documents. We attach new words to the hyponymy database extracted from Wikipedia by using distributional similarity calculated from documents on the Web. For a given target word, our algorithm first finds k similar words from the Wikipedia database. Then, the hypernyms of these k similar words are assigned scores by considering the distributional similarities and hierarchical distances in the Wikipedia database. Finally, new hyponymy relations are output according to the scores. In this paper, we tested two distributional similarities. One is based on raw verb-noun dependencies (which we call \"RVD\"), and the other is based on a large-scale clustering of verb-noun dependencies (called \"CVD\"). Our method achieved an attachment accuracy of 91.0% for the top 10,000 relations, and an attachment accuracy of 74.5% for the top 100,000 relations when using CVD. This was a far better outcome compared to the other baseline approaches. Excluding the region that had very high scores, CVD was found to be more effective than RVD. We also confirmed that most relations extracted by our method cannot be extracted merely by applying the well-known lexico-syntactic patterns to Web documents."
            },
            "slug": "Hypernym-Discovery-Based-on-Distributional-and-Yamada-Torisawa",
            "title": {
                "fragments": [],
                "text": "Hypernym Discovery Based on Distributional Similarity and Hierarchical Structures"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This paper presents a new method of developing a large-scale hyponymy relation database by combining Wikipedia and other Web documents, and confirms that most relations extracted by the method cannot be extracted merely by applying the well-known lexico-syntactic patterns to Web documents."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1990190"
                        ],
                        "name": "P. Pantel",
                        "slug": "P.-Pantel",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Pantel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pantel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 50
                            }
                        ],
                        "text": "A different type of semantic space is proposed in Lin and Pantel (2001) (see also Turney and Pantel, 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2971806,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f3f9a848dca0a80ef64987a9dd511ee6b7e19cd1",
            "isKey": false,
            "numCitedBy": 565,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose an unsupervised method for discovering inference rules from text, such as \"X is author of Y \u2248 X wrote Y\", \"X solved Y \u2248 X found a solution to Y\", and \"X caused Y \u2248 Y is triggered by X\". Inference rules are extremely important in many fields such as natural language processing, information retrieval, and artificial intelligence in general. Our algorithm is based on an extended version of Harris' Distributional Hypothesis, which states that words that occurred in the same contexts tend to be similar. Instead of using this hypothesis on words, we apply it to paths in the dependency trees of a parsed corpus."
            },
            "slug": "DIRT-@SBT@discovery-of-inference-rules-from-text-Lin-Pantel",
            "title": {
                "fragments": [],
                "text": "DIRT @SBT@discovery of inference rules from text"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This paper proposes an unsupervised method for discovering inference rules from text, based on an extended version of Harris' Distributional Hypothesis, which states that words that occurred in the same contexts tend to be similar."
            },
            "venue": {
                "fragments": [],
                "text": "KDD '01"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1969651"
                        ],
                        "name": "L. Rips",
                        "slug": "L.-Rips",
                        "structuredName": {
                            "firstName": "Lance",
                            "lastName": "Rips",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rips"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 224
                            }
                        ],
                        "text": "Examples include memory retrieval (Deese, 1959; Raaijmakers & Shiffrin, 1981), categorization (Estes, 1994; Nosofsky, 1984, 1986), problem solving (Holyoak & Koh, 1987; Ross, 1987, 1989), reasoning (Heit & Rubinstein, 1994; Rips, 1975), and learning (Gentner, 1989; Ross, 1984)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1733,
                                "start": 100
                            }
                        ],
                        "text": "However, a number of difficulties arise when working with such data (Murphy & Medin, 1985; Sloman & Rips, 1998). For example, the number and types of attributes generated can vary substantially as a function of the amount of time devoted to each word. There are many degrees of freedom in the way that responses are coded and analyzed. And multiple subjects are required to create a representation for each word, which in practice limits elicitation studies to a small-size lexicon. A third popular tradition of studying semantic representation has been driven by the assumption that word meaning can be learned from the linguistic environment. Words that are similar in meaning, for example, boat and ship tend to occur in contexts of similar words, such as sail, sea, sailor, and so on. Semantic space models capture meaning quantitatively in terms of simple co-occurrence statistics. Words are represented as vectors in a high-dimensional space, where each component corresponds to some co-occurring contextual element. The latter can be words themselves (Lund & Burgess, 1996), larger linguistic units such as paragraphs or documents (Landauer & Dumais, 1997), or even more complex linguistic representations such as n-grams (Jones & Mewhort, 2007) and the argument slots of predicates (Grefenstette, 1994; Lin, 1998; Pad\u00f3 & Lapata, 2007). The advantage of taking such a geometric approach is that the similarity of word meanings can be easily quantified by measuring their distance in the vector space, or the cosine of the angle between them. A simplified example of a two-dimensional semantic space is shown in Fig. 1 (semantic spaces usually have hundreds of dimensions). J. Mitchell, M. Lapata \u2044 Cognitive Science 34 (2010) 1389"
                    },
                    "intents": []
                }
            ],
            "corpusId": 143831277,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "ed6780677cc5d7128f0619327aa995a305cf839a",
            "isKey": false,
            "numCitedBy": 448,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Inductive-judgments-about-natural-categories.-Rips",
            "title": {
                "fragments": [],
                "text": "Inductive judgments about natural categories."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144748776"
                        ],
                        "name": "E. Gibson",
                        "slug": "E.-Gibson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Gibson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Gibson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 124
                            }
                        ],
                        "text": "It is also hypothesised that an integration cost is associated with incorporating each word into the structure built so far (Gibson, 2000), for example integrating a verb with its arguments."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 296321,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "53558f89cdf5027c29d2b81cd1f8465d01c2b3a2",
            "isKey": false,
            "numCitedBy": 885,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "A major issue in understanding how language is implemented in the brain involves understanding the use of language in language comprehension and production. However, before we look to the brain to see what areas are associated with language processing phenomena, it is necessary to have good psychological theories of the relevant behavioral phenomena. Recent results have suggested that constructing an interpretation for a sentence involves the moment-by-moment integration of a variety of different information sources, constrained by the available computational resources (see, e.g., Ford, Bresnan, and Kaplan 1982; MacDonald, Pearlmutter, and Seidenberg 1994; Trueswell, Tanenhaus, and Garnsey 1994; Trueswell1996; Tyler and MarslenWilson 1977; McClelland, St. John, and Taraban 1989; Pearlmutter and MacDonald 1992; Crain and Steedman 1985; Altmann and Steedman, 1988; Ni , Crain, and Shankweiler 1996; see Gibson and Pearlmutter 1998 and Tanenhaus and Trueswell 1995 for summaries). This chapter presents evidence for one theory of resource use in sentence co\"mprehension: the dependency locality theory (DLT ). If the evidence for a theory such as this one accumulates, it will then make sense to look for neural correlates of the theory (see Kaan et ale 1998; Harris 1998, for some initial attempts to find event-related potential measurements of brain activity corresponding to the components of the DLT ). An important part of a theory of sentence comprehension is a theory of how sentence structures are assembledsentence parsingas words are input one at a time. Two important components of sentence parsing consume computational resources:"
            },
            "slug": "The-dependency-locality-theory:-A-distance-based-of-Gibson",
            "title": {
                "fragments": [],
                "text": "The dependency locality theory: A distance-based theory of linguistic complexity."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "83415753"
                        ],
                        "name": "W. Dolan",
                        "slug": "W.-Dolan",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Dolan",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Dolan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2596310"
                        ],
                        "name": "Chris Quirk",
                        "slug": "Chris-Quirk",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Quirk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Quirk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3125776"
                        ],
                        "name": "Chris Brockett",
                        "slug": "Chris-Brockett",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Brockett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Brockett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 2006) and paraphrasing tasks (Dolan et al., 2004) are likely to provide appropriate applications for a model of semantics designed to quantify the similarity between pairs of phrases or sentences."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10181753,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7acfdc905f734abf966aed58abb983bc015ff7fe",
            "isKey": false,
            "numCitedBy": 794,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate unsupervised techniques for acquiring monolingual sentence-level paraphrases from a corpus of temporally and topically clustered news articles collected from thousands of web-based news sources. Two techniques are employed: (1) simple string edit distance, and (2) a heuristic strategy that pairs initial (presumably summary) sentences from different news stories in the same cluster. We evaluate both datasets using a word alignment algorithm and a metric borrowed from machine translation. Results show that edit distance data is cleaner and more easily-aligned than the heuristic data, with an overall alignment error rate (AER) of 11.58% on a similarly-extracted test set. On test data extracted by the heuristic strategy, however, performance of the two training sets is similar, with AERs of 13.2% and 14.7% respectively. Analysis of 100 pairs of sentences from each set reveals that the edit distance data lacks many of the complex lexical and syntactic alternations that characterize monolingual paraphrase. The summary sentences, while less readily alignable, retain more of the non-trivial alternations that are of greatest interest learning paraphrase relationships."
            },
            "slug": "Unsupervised-Construction-of-Large-Paraphrase-News-Dolan-Quirk",
            "title": {
                "fragments": [],
                "text": "Unsupervised Construction of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "Investigation of unsupervised techniques for acquiring monolingual sentence-level paraphrases from a corpus of temporally and topically clustered news articles collected from thousands of web-based news sources shows that edit distance data is cleaner and more easily-aligned than the heuristic data."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33696979"
                        ],
                        "name": "G. Murphy",
                        "slug": "G.-Murphy",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Murphy",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3023259"
                        ],
                        "name": "D. Medin",
                        "slug": "D.-Medin",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Medin",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Medin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 69
                            }
                        ],
                        "text": "However, a number of difficulties arise when working with such data (Murphy & Medin, 1985; Sloman & Rips, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14525617,
            "fieldsOfStudy": [
                "Philosophy",
                "Psychology"
            ],
            "id": "4336c33f0ab03e8722712792d25e46922eed73ed",
            "isKey": false,
            "numCitedBy": 2555,
            "numCiting": 147,
            "paperAbstract": {
                "fragments": [],
                "text": "The question of what makes a concept coherent (what makes its members form a comprehensible class) has received a variety of answers. In this article we review accounts based on similarity, feature correlations, and various theories of categorization. We find that each theory provides an inadequate account of conceptual coherence (or no account at all) because none provides enough constraints on possible concepts. We propose that concepts are coherent to the extent that they fit people's background knowledge or naive theories about the world. These theories help to relate the concepts in a domain and to structure the attributes that are internal to a concept. Evidence of the influence of theories on various conceptual tasks is presented, and the possible importance of theories in cognitive development is discussed."
            },
            "slug": "The-role-of-theories-in-conceptual-coherence.-Murphy-Medin",
            "title": {
                "fragments": [],
                "text": "The role of theories in conceptual coherence."
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is proposed that concepts are coherent to the extent that they fit people's background knowledge or naive theories about the world and to structure the attributes that are internal to a concept."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33070393"
                        ],
                        "name": "R. Morris",
                        "slug": "R.-Morris",
                        "structuredName": {
                            "firstName": "Robin",
                            "lastName": "Morris",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Morris"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 169
                            }
                        ],
                        "text": "\u2026Duffy, Henderson, and Morris (1989) showed that priming of sentence terminal words was dependent not simply on individual preceding words but on their combination, and Morris (1994) later demonstrated that this priming also showed dependencies on the syntactic relations in the preceding context."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 261
                            }
                        ],
                        "text": "For example, a model of semantic priming in sentential contexts ought to take account of the fact that the effect is not simply based on a relationship between individual words, but is dependent on the combination of word meanings and their syntactic structure (Duffy et al., 1989; Morris, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 23331798,
            "fieldsOfStudy": [
                "Psychology",
                "Linguistics"
            ],
            "id": "c8a3f203278da35f444923a5e2c79612c266b471",
            "isKey": false,
            "numCitedBy": 212,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Readers' eye movements were recorded as they read an unambiguous noun in a sentence context. In Experiment 1, fixation durations on a target noun were shorter when it was embedded in context containing a subject noun and a verb that were weakly related to the target than when either content word was replaced with a more neutral word. These results were not affected by changes in the syntactic relations between the content words. In Experiment 2, the semantic relations between the message-level representation of the sentence and the target word were altered whereas the lexical content was held constant. Fixation time on the target word was shorter when the context was semantically related to the target word than when it was unrelated. Intralexical priming effects between the subject noun and the verb were also observed. The results suggest that both lexical and message-level representations can influence the access of an individual lexical item in a sentence context."
            },
            "slug": "Lexical-and-message-level-sentence-context-effects-Morris",
            "title": {
                "fragments": [],
                "text": "Lexical and message-level sentence context effects on fixation times in reading."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The results suggest that both lexical and message-level representations can influence the access of an individual lexical item in a sentence context."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of experimental psychology. Learning, memory, and cognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2009767"
                        ],
                        "name": "K. Holyoak",
                        "slug": "K.-Holyoak",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Holyoak",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Holyoak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725887"
                        ],
                        "name": "J. Hummel",
                        "slug": "J.-Hummel",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hummel",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hummel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 592,
                                "start": 23
                            }
                        ],
                        "text": "In a related proposal, Holyoak and Hummel (2000) claim that in combining parts to form a whole, the parts remain independent and maintain their identities. This entails that John has the same independent meaning in both John loves the girl and The boy hates John. Aside from the philosophical difficulties in precisely determining what systematicity means in practice (Doumas & Hummel, 2005; Pullum & Scholz, 2007; Spenader & Blutner, 2007), it is worth noting that semantic transparency, the idea that words have meanings which remain unaffected by their context, contradicts Frege\u2019s (1884) claim that words only have definite meanings in context."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 23
                            }
                        ],
                        "text": "In a related proposal, Holyoak and Hummel (2000) claim that in combining parts to form a whole, the parts remain independent and maintain their identities."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 166
                            }
                        ],
                        "text": "In fact, it is often argued that however composition is implemented it must exhibit certain features characteristic of this symbolic binding (Fodor & Pylyshyn, 1988; Holyoak & Hummel, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 9
                            }
                        ],
                        "text": "However, Holyoak and Hummel (2000) claim that this form of binding violates role\u2013filler independence."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1098,
                                "start": 23
                            }
                        ],
                        "text": "In a related proposal, Holyoak and Hummel (2000) claim that in combining parts to form a whole, the parts remain independent and maintain their identities. This entails that John has the same independent meaning in both John loves the girl and The boy hates John. Aside from the philosophical difficulties in precisely determining what systematicity means in practice (Doumas & Hummel, 2005; Pullum & Scholz, 2007; Spenader & Blutner, 2007), it is worth noting that semantic transparency, the idea that words have meanings which remain unaffected by their context, contradicts Frege\u2019s (1884) claim that words only have definite meanings in context. Consider, for example, the adjective good whose meaning is modified by the context in which it occurs. The sentences John is a good neighbor and John is a lawyer do not imply John is a good lawyer. In fact, we might expect that some of the attributes of a good lawyer are incompatible with being a good neighbor, such as nit-picking over details, or not giving an inch unless required by law. More generally, the claims of Fodor and Pylyshyn (1988) and Holyoak and Hummel (2000) arise from a preconception of cognition as being essentially symbolic in character."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 60
                            }
                        ],
                        "text": "More generally, the claims of Fodor and Pylyshyn (1988) and Holyoak and Hummel (2000) arise from a preconception of cognition as being essentially symbolic in character."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15033838,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "8e975728daddfc0e6cfc2cb9495cb60559dbaba1",
            "isKey": true,
            "numCitedBy": 90,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "A foundational principle of modern cognitive science is the Physical Symbol System hypothesis, which states simply that human cognition is the product of a physical symbol system (PSS). A symbol is a pattern that denotes something else; a symbol system is a set of symbols that can be composed into more complex structures by a set of relations. The term \" physical \" conveys that a symbol system can and must be realized in some physical way in order to create intelligence. The physical basis may be the circuits of an electronic computer, the neural substrate of a thinking biological organism, or in principle anything else that could implement a Turing machine-like computing device. The PSS hypothesis, which implies that structured mental representations are central to human intelligence, was for some time uncontroversial, accepted by most cognitive scientists as an axiom of the field scarcely in need of either theoretical analysis or direct empirical support. In the mid-1980s, however, the hypothesis came under sharp attack from some proponents of connectionist models of cognition, particularly the advocates of models in the style of \" parallel many others; see Marcus, 1997, for a review). The representations used in such models are often described as \" subsymbolic \" because the elementary units correspond to (relatively) low-level features, over which meaningful concepts are represented in a distributed fashion. In so far as models based on \"subsymbolic\" representations are actually non-symbolic, yet adequate as accounts of human intelligence, the need for symbol systems would be eliminated; hence models of this general class constitute \"eliminative\" connectionism (Pinker & Prince, 1988). Eliminative connectionism offers a direct challenge to the PSS hypothesis, thereby transforming the latter from an axiom of cognitive science into a controversial theoretical position, which has been vigorously Regardless of whether models based on distributed representations provide genuine alternatives to physical symbol systems, it is apparent that they have attractive properties as possible algorithmic accounts of cognition. Discrete symbols represent entities in an \"all-or-none\" fashion, thereby violating the principle of least commitment (e.g., using the presence or absence of the symbol \"dog\" to represent the presence or absence of a dog affords no direct basis for expressing inconclusive evidence that there may be a dog). Discrete symbols also fail to express the semantic content of the represented entities (e.g., the symbols \"dog\" and \"cat\" do not signify what dogs and"
            },
            "slug": "The-Proper-Treatment-of-Symbols-in-a-Connectionist-Holyoak-Hummel",
            "title": {
                "fragments": [],
                "text": "The Proper Treatment of Symbols in a Connectionist Architecture"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Ezinative connectionism offers a direct challenge to the PSS hypothesis, thereby transforming the latter from an axiom of cognitive science into a controversial theoretical position, which has been vigorously Regardless of whether models based on distributed representations provide genuine alternatives to physical symbol systems, it is apparent that they have attractive properties as possible algorithmic accounts of cognition."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2869436"
                        ],
                        "name": "V. Demberg",
                        "slug": "V.-Demberg",
                        "structuredName": {
                            "firstName": "Vera",
                            "lastName": "Demberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Demberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143694777"
                        ],
                        "name": "Frank Keller",
                        "slug": "Frank-Keller",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Keller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frank Keller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 176
                            }
                        ],
                        "text": "Another interesting application concerns sentence processing and the extent to which the compositional models discussed here can explain reading times in eye-tracking corpora (Demberg & Keller, 2008; Pynte, New, & Kennedy, 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1444973,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "41215599674d8a81e7420f2a40320868f3dbc2f1",
            "isKey": false,
            "numCitedBy": 462,
            "numCiting": 69,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Data-from-eye-tracking-corpora-as-evidence-for-of-Demberg-Keller",
            "title": {
                "fragments": [],
                "text": "Data from eye-tracking corpora as evidence for theories of syntactic processing complexity"
            },
            "venue": {
                "fragments": [],
                "text": "Cognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721801"
                        ],
                        "name": "C. Fellbaum",
                        "slug": "C.-Fellbaum",
                        "structuredName": {
                            "firstName": "Christiane",
                            "lastName": "Fellbaum",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Fellbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5958691,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d87ceda3042f781c341ac17109d1e94a717f5f60",
            "isKey": false,
            "numCitedBy": 13574,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Part 1 The lexical database: nouns in WordNet, George A. Miller modifiers in WordNet, Katherine J. Miller a semantic network of English verbs, Christiane Fellbaum design and implementation of the WordNet lexical database and searching software, Randee I. Tengi. Part 2: automated discovery of WordNet relations, Marti A. Hearst representing verb alterations in WordNet, Karen T. Kohl et al the formalization of WordNet by methods of relational concept analysis, Uta E. Priss. Part 3 Applications of WordNet: building semantic concordances, Shari Landes et al performance and confidence in a semantic annotation task, Christiane Fellbaum et al WordNet and class-based probabilities, Philip Resnik combining local context and WordNet similarity for word sense identification, Claudia Leacock and Martin Chodorow using WordNet for text retrieval, Ellen M. Voorhees lexical chains as representations of context for the detection and correction of malapropisms, Graeme Hirst and David St-Onge temporal indexing through lexical chaining, Reem Al-Halimi and Rick Kazman COLOR-X - using knowledge from WordNet for conceptual modelling, J.F.M. Burg and R.P. van de Riet knowledge processing on an extended WordNet, Sanda M. Harabagiu and Dan I Moldovan appendix - obtaining and using WordNet."
            },
            "slug": "WordNet-:-an-electronic-lexical-database-Fellbaum",
            "title": {
                "fragments": [],
                "text": "WordNet : an electronic lexical database"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "The lexical database: nouns in WordNet, Katherine J. Miller a semantic network of English verbs, and applications of WordNet: building semantic concordances are presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38663378"
                        ],
                        "name": "J. Fodor",
                        "slug": "J.-Fodor",
                        "structuredName": {
                            "firstName": "Jerry",
                            "lastName": "Fodor",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Fodor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194015"
                        ],
                        "name": "Z. Pylyshyn",
                        "slug": "Z.-Pylyshyn",
                        "structuredName": {
                            "firstName": "Zenon",
                            "lastName": "Pylyshyn",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Pylyshyn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 0
                            }
                        ],
                        "text": "Fodor and Pylyshyn\n(1988) attempt to characterize this function by appealing to the notion of systematicity."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 226
                            }
                        ],
                        "text": "Connectionist representations, like their distributional counterparts, are essentially vectors, and there is a substantial literature concerned with their representational capacities in comparison to the symbolic alternatives (Fodor and Pylyshyn, 1988; Smolensky, 1990; Pollack, 1990; Plate, 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 142
                            }
                        ],
                        "text": "In fact, it is often argued that however composition is implemented it must exhibit certain features characteristic of this symbolic binding (Fodor & Pylyshyn, 1988; Holyoak & Hummel, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 163
                            }
                        ],
                        "text": "As a consequence, the contrasts between their structures and capabilities have generated substantial controversy over their relative merits as models of cognition (Fodor and Pylyshyn, 1988; Fodor and Lepore, 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 121
                            }
                        ],
                        "text": "Indeed, the issue of how to represent compositional structure in neural networks has been a matter of great controversy (Fodor & Pylyshyn, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 30
                            }
                        ],
                        "text": "More generally, the claims of Fodor and Pylyshyn (1988) and Holyoak and Hummel (2000) arise from a preconception of cognition as being essentially symbolic in character."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 230
                            }
                        ],
                        "text": "Rather than being concerned with optimising a probabilistic model of word sequences for application to some practical task, early connectionist work on language was often motivated by theoretical criticisms from the symbolic camp (Fodor and Pylyshyn, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29043627,
            "fieldsOfStudy": [
                "Philosophy",
                "Psychology"
            ],
            "id": "56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7",
            "isKey": true,
            "numCitedBy": 3540,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Connectionism-and-cognitive-architecture:-A-Fodor-Pylyshyn",
            "title": {
                "fragments": [],
                "text": "Connectionism and cognitive architecture: A critical analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Cognition"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4248890"
                        ],
                        "name": "K. Stanovich",
                        "slug": "K.-Stanovich",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Stanovich",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Stanovich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36855520"
                        ],
                        "name": "R. F. West",
                        "slug": "R.-F.-West",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "West",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. F. West"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 127
                            }
                        ],
                        "text": "It is well known that input which is semantically coherent with the previous context is read more quickly than anomalous input (Stanovich and West, 1981)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 222329537,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "8b91160b3135c357bea66d4824a65fd88158eb49",
            "isKey": false,
            "numCitedBy": 245,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Examined M. I. Posner and C. R. R. Snyder's (1975) 2-process theory of expectancy that provides an explanation of the effect of sentence context on ongoing word recognition and that accounts for some recent results on the effect of sentence context on the word recognition times of children differing in reading fluency. Three studies with 98 undergraduates were conducted to test the applicability of the theory to the performance of fluent adult readers. Exp I tested and falsified an alternative explanation of previous results. Exps II and III established and replicated the finding that difficult (but less predictable) words displayed larger context effects than did easy words that were more predictable from the preceding sentence context. It is concluded that while this finding cannot be explained by many recent models of contextual effects on ongoing word recognition, it can be reasonably well accounted for by the Posner-Snyder theory. (43 ref) (PsycINFO Database Record (c) 2006 APA, all rights reserved)"
            },
            "slug": "The-effect-of-sentence-context-on-ongoing-word-of-a-Stanovich-West",
            "title": {
                "fragments": [],
                "text": "The effect of sentence context on ongoing word recognition: Tests of a two-process theory."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143662525"
                        ],
                        "name": "Yuki Kamide",
                        "slug": "Yuki-Kamide",
                        "structuredName": {
                            "firstName": "Yuki",
                            "lastName": "Kamide",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuki Kamide"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781995"
                        ],
                        "name": "G. Altmann",
                        "slug": "G.-Altmann",
                        "structuredName": {
                            "firstName": "Gerry",
                            "lastName": "Altmann",
                            "middleNames": [
                                "T.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Altmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "25973828"
                        ],
                        "name": "S. Haywood",
                        "slug": "S.-Haywood",
                        "structuredName": {
                            "firstName": "Sarah",
                            "lastName": "Haywood",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Haywood"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4663616,
            "fieldsOfStudy": [
                "Linguistics",
                "Psychology"
            ],
            "id": "47759c5d28e38d61cd288d6a52227bee7c30fdb1",
            "isKey": false,
            "numCitedBy": 801,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-time-course-of-prediction-in-incremental-from-Kamide-Altmann",
            "title": {
                "fragments": [],
                "text": "The time-course of prediction in incremental sentence processing: Evidence from anticipatory eye-movements"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1831395"
                        ],
                        "name": "H. Wallach",
                        "slug": "H.-Wallach",
                        "structuredName": {
                            "firstName": "Hanna",
                            "lastName": "Wallach",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wallach"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1174898,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56702c8ddc72ba45eaad4e5b6d44afe86b8a4a9d",
            "isKey": false,
            "numCitedBy": 1083,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Some models of textual corpora employ text generation methods involving n-gram statistics, while others use latent topic variables inferred using the \"bag-of-words\" assumption, in which word order is ignored. Previously, these methods have not been combined. In this work, I explore a hierarchical generative probabilistic model that incorporates both n-gram statistics and latent topic variables by extending a unigram topic model to include properties of a hierarchical Dirichlet bigram language model. The model hyperparameters are inferred using a Gibbs EM algorithm. On two data sets, each of 150 documents, the new model exhibits better predictive accuracy than either a hierarchical Dirichlet bigram language model or a unigram topic model. Additionally, the inferred topics are less dominated by function words than are topics discovered using unigram statistics, potentially making them more meaningful."
            },
            "slug": "Topic-modeling:-beyond-bag-of-words-Wallach",
            "title": {
                "fragments": [],
                "text": "Topic modeling: beyond bag-of-words"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A hierarchical generative probabilistic model that incorporates both n-gram statistics and latent topic variables by extending a unigram topic model to include properties of a hierarchical Dirichlet bigram language model is explored."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143730547"
                        ],
                        "name": "S. Rudolph",
                        "slug": "S.-Rudolph",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Rudolph",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Rudolph"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48394537"
                        ],
                        "name": "Eugenie Giesbrecht",
                        "slug": "Eugenie-Giesbrecht",
                        "structuredName": {
                            "firstName": "Eugenie",
                            "lastName": "Giesbrecht",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugenie Giesbrecht"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2678583,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d9269fd1efeb87000d346cc8514dec3fc6ec01d6",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose CMSMs, a novel type of generic compositional models for syntactic and semantic aspects of natural language, based on matrix multiplication. We argue for the structural and cognitive plausibility of this model and show that it is able to cover and combine various common compositional NLP approaches ranging from statistical word space models to symbolic grammar formalisms."
            },
            "slug": "Compositional-Matrix-Space-Models-of-Language-Rudolph-Giesbrecht",
            "title": {
                "fragments": [],
                "text": "Compositional Matrix-Space Models of Language"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "CMSMs, a novel type of generic compositional models for syntactic and semantic aspects of natural language, based on matrix multiplication is proposed and it is shown that it is able to cover and combine various common compositional NLP approaches."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144553330"
                        ],
                        "name": "H. Neville",
                        "slug": "H.-Neville",
                        "structuredName": {
                            "firstName": "Helen",
                            "lastName": "Neville",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Neville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39151594"
                        ],
                        "name": "J. Nicol",
                        "slug": "J.-Nicol",
                        "structuredName": {
                            "firstName": "Janet",
                            "lastName": "Nicol",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nicol"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6060298"
                        ],
                        "name": "A. Barss",
                        "slug": "A.-Barss",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4750592"
                        ],
                        "name": "K. Forster",
                        "slug": "K.-Forster",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Forster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Forster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39997409"
                        ],
                        "name": "M. Garrett",
                        "slug": "M.-Garrett",
                        "structuredName": {
                            "firstName": "Merrill",
                            "lastName": "Garrett",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Garrett"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 150
                            }
                        ],
                        "text": "In contrast, there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al., 1991; West and Stanovich, 1986)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5090452,
            "fieldsOfStudy": [
                "Psychology",
                "Linguistics"
            ],
            "id": "21ace5a76dd71cefee73863d724eca1d0856bce9",
            "isKey": false,
            "numCitedBy": 839,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Theoretical considerations and diverse empirical data from clinical, psycholinguistic, and developmental studies suggest that language comprehension processes are decomposable into separate subsystems, including distinct systems for semantic and grammatical processing. Here we report that event-related potentials (ERPs) to syntactically well-formed but semantically anomalous sentences produced a pattern of brain activity that is distinct in timing and distribution from the patterns elicited by syntactically deviant sentences, and further, that different types of syntactic deviance produced distinct ERP patterns. Forty right-handed young adults read sentences presented at 2 words/sec while ERPs were recorded from over several positions between and within the hemispheres. Half of the sentences were semantically and grammatically acceptable and were controls for the remainder, which contained sentence medial words that violated (1) semantic expectations, (2) phrase structure rules, or (3) WH-movement constraints on Specificity and (4) Subjacency. As in prior research, the semantic anomalies produced a negative potential, N400, that was bilaterally distributed and was largest over posterior regions. The phrase structure violations enhanced the N125 response over anterior regions of the left hemisphere, and elicited a negative response (300-500 msec) over temporal and parietal regions of the left hemisphere. Violations of Specificity constraints produced a slow negative potential, evident by 125 msec, that was also largest over anterior regions of the left hemisphere. Violations of Subjacency constraints elicited a broadly and symmetrically distributed positivity that onset around 200 msec. The distinct timing and distribution of these effects provide biological support for theories that distinguish between these types of grammatical rules and constraints and more generally for the proposal that semantic and grammatical processes are distinct subsystems within the language faculty."
            },
            "slug": "Syntactically-Based-Sentence-Processing-Classes:-Neville-Nicol",
            "title": {
                "fragments": [],
                "text": "Syntactically Based Sentence Processing Classes: Evidence from Event-Related Brain Potentials"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The distinct timing and distribution of these effects provide biological support for theories that distinguish between these types of grammatical rules and constraints and more generally for the proposal that semantic and grammatical processes are distinct subsystems within the language faculty."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Cognitive Neuroscience"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31533192"
                        ],
                        "name": "A. Collins",
                        "slug": "A.-Collins",
                        "structuredName": {
                            "firstName": "Allan",
                            "lastName": "Collins",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69960817"
                        ],
                        "name": "M. R. Quillian",
                        "slug": "M.-R.-Quillian",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Quillian",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. R. Quillian"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 19
                            }
                        ],
                        "text": "Semantic networks (Collins & Quillian, 1969) represent concepts as nodes in a graph."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 32
                            }
                        ],
                        "text": "These include semantic networks (Collins and Quillian, 1969), featural models (Smith et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60922154,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "06cb835bda3420186e2c6f6fa2dbc1613a9b2d75",
            "isKey": false,
            "numCitedBy": 2946,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Retrieval-time-from-semantic-memory-Collins-Quillian",
            "title": {
                "fragments": [],
                "text": "Retrieval time from semantic memory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708339"
                        ],
                        "name": "J. V. Berkum",
                        "slug": "J.-V.-Berkum",
                        "structuredName": {
                            "firstName": "Jos",
                            "lastName": "Berkum",
                            "middleNames": [
                                "J.",
                                "A.",
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. V. Berkum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113880928"
                        ],
                        "name": "C. Brown",
                        "slug": "C.-Brown",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Brown",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2608476"
                        ],
                        "name": "P. Hagoort",
                        "slug": "P.-Hagoort",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hagoort",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hagoort"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 30461893,
            "fieldsOfStudy": [
                "Linguistics",
                "Psychology"
            ],
            "id": "f78684bdd818610f91f4265e9e275ec26e65fb9a",
            "isKey": false,
            "numCitedBy": 311,
            "numCiting": 86,
            "paperAbstract": {
                "fragments": [],
                "text": "An event-related brain potentials experiment was carried out to examine the interplay of referential and structural factors during sentence processing in discourse. Subjects read (Dutch) sentences beginning like \u201cDavid told the girl that \u2026 \u201d in short story contexts that had introduced either one or two referents for a critical singular noun phrase (\u201cthe girl\u201d). The waveforms showed that within 280 ms after onset of the critical noun the reader had already determined whether the noun phrase had a unique referent in earlier discourse. Furthermore, this referential information was immediately used in parsing the rest of the sentence, which was briefly ambiguous between a complement clause (\u201c \u2026 that there would be some visitors\u201d) and a relative clause (\u201c \u2026 that had been on the phone to hang up\u201d). A consistent pattern of P600/SPS effects elicited by various subsequent disambiguations revealed that a two-referent discourse context had led the parser to initially pursue the relative-clause alternative to a larger extent than a one-referent context. Together, the results suggest that during the processing of sentences in discourse, structural and referential sources of information interact on a word-by-word basis."
            },
            "slug": "Early-referential-context-effects-in-sentence-from-Berkum-Brown",
            "title": {
                "fragments": [],
                "text": "Early referential context effects in sentence processing: Evidence from event-related brain potentials"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "118213642"
                        ],
                        "name": "J. Sachs",
                        "slug": "J.-Sachs",
                        "structuredName": {
                            "firstName": "Jacqueline",
                            "lastName": "Sachs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sachs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 143791557,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "d6190336549fff6d971aecc373c195464788a52a",
            "isKey": false,
            "numCitedBy": 1016,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "This study investigates the pattern of retention of syntactic and semantic information shortly after comprehension of connected discourse. Ninety-six Ss listened to 24 taped passages and, after each passage, heard one recognition test sentence which was either identical to a sentence that had occurred in the passage, or was changed in some slight way. The Ss responded \u201cidentical\u201d or \u201cchanged,\u201d rated their confidence, and classified changes as \u201cmeaning\u201d or \u201cform.\u201d Two independent variables were manipulated: (1) The relationship between the original sentence in the passage and the test sentence. The test sentence was (a) semantically changed, (b) changed from active to passive voice or vice versa, (c) formally changed in other ways that did not affect the meaning, or (d) unchanged. Each sentence appeared in all change types. (2) The amount of interpolated material between the original and test sentences was zero, 80, or 160 syllables of connected discourse which was a continuation of the passage. Each S heard passages representing all levels of each variable. All combinations of particular passages, relationship of original and test sentence, and amount of interpolated material were tested.When the test sentence was heard immediately after the original, retention was high for all test types. But after 80\u00bd160 syllables, recognition for syntactic changes had dropped to near chance levels while remaining high for semantic changes. Even when the meaning of a sentence was remembered, formal properties that were not necessary for that meaning were forgotten very quickly. The results suggest that the original form of the sentence is stored only for the short time necessary for comprehension to occur. When a semantic interpretation has been made, the meaning is stored. Thus the memory of the meaning is not dependent on memory of the original form of the sentence."
            },
            "slug": "Recognition-memory-for-syntactic-and-semantic-of-Sachs",
            "title": {
                "fragments": [],
                "text": "Recognition memory for syntactic and semantic aspects of connected discourse"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 140
                            }
                        ],
                        "text": "Dimensionality reduction techniques can be also used to project high-dimensional vectors onto a lower dimensional space (Blei et al., 2003; Hofmann, 2001; Landauer & Dumais, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7605995,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e6dd83b2aa34c806596fc619ff3fbccf5f9830ab",
            "isKey": false,
            "numCitedBy": 2499,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel statistical method for factor analysis of binary and count data which is closely related to a technique known as Latent Semantic Analysis. In contrast to the latter method which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed technique uses a generative latent class model to perform a probabilistic mixture decomposition. This results in a more principled approach with a solid foundation in statistical inference. More precisely, we propose to make use of a temperature controlled version of the Expectation Maximization algorithm for model fitting, which has shown excellent performance in practice. Probabilistic Latent Semantic Analysis has many applications, most prominently in information retrieval, natural language processing, machine learning from text, and in related areas. The paper presents perplexity results for different types of text and linguistic data collections and discusses an application in automated document indexing. The experiments indicate substantial and consistent improvements of the probabilistic method over standard Latent Semantic Analysis."
            },
            "slug": "Unsupervised-Learning-by-Probabilistic-Latent-Hofmann",
            "title": {
                "fragments": [],
                "text": "Unsupervised Learning by Probabilistic Latent Semantic Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes to make use of a temperature controlled version of the Expectation Maximization algorithm for model fitting, which has shown excellent performance in practice, and results in a more principled approach with a solid foundation in statistical inference."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749837"
                        ],
                        "name": "Eugene Charniak",
                        "slug": "Eugene-Charniak",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Charniak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Charniak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Following previous work on syntactic language modeling (Roark, 2001; Charniak, 2001; Chelba and Jelinek, 1998), we trained the parser on sections 2\u201321 of the Penn Treebank containing 936,017 words. Note that Roark\u2019s (2001) parser produces prefix probabilities for each word of a sentence which we converted to conditional probabilities by dividing each current probability by the previous one."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Following previous work on syntactic language modeling (Roark, 2001; Charniak, 2001; Chelba and Jelinek, 1998), we trained the parser on sections 2\u201321 of the Penn Treebank containing 936,017 words."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, Charniak (2001) implements a language model based on immediate head parsing."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 457176,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "436772d9a916f0382800cf18581cfdfd4f83c457",
            "isKey": false,
            "numCitedBy": 280,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We present two language models based upon an \"immediate-head\" parser --- our name for a parser that conditions all events below a constituent c upon the head of c. While all of the most accurate statistical parsers are of the immediate-head variety, no previous grammatical language model uses this technology. The perplexity for both of these models significantly improve upon the trigram model base-line as well as the best previous grammar-based language model. For the better of our two models these improvements are 24% and 14% respectively. We also suggest that improvement of the underlying parser should significantly improve the model's perplexity and that even in the near term there is a lot of potential for improvement in immediate-head language models."
            },
            "slug": "Immediate-Head-Parsing-for-Language-Models-Charniak",
            "title": {
                "fragments": [],
                "text": "Immediate-Head Parsing for Language Models"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is suggested that improvement of the underlying parser should significantly improve the model's perplexity and that even in the near term there is a lot of potential for improvement in immediate-head language models."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690396"
                        ],
                        "name": "C. Eliasmith",
                        "slug": "C.-Eliasmith",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Eliasmith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Eliasmith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756123"
                        ],
                        "name": "P. Thagard",
                        "slug": "P.-Thagard",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Thagard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Thagard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 178
                            }
                        ],
                        "text": "The construction of higher level structures from low-level ones is fundamental not only to language but many aspects of human cognition such as analogy retrieval and processing (Eliasmith & Thagard, 2001; Plate, 2000), memory (Kanerva, 1988), and problem solving (Ross, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123203339,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "90cfc9fa6c26b65b661f0329c80aae212af42c2f",
            "isKey": false,
            "numCitedBy": 117,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Integrating-structure-and-meaning:-a-distributed-of-Eliasmith-Thagard",
            "title": {
                "fragments": [],
                "text": "Integrating structure and meaning: a distributed model of analogical mapping"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2896203"
                        ],
                        "name": "L. Doumas",
                        "slug": "L.-Doumas",
                        "structuredName": {
                            "firstName": "Leonidas",
                            "lastName": "Doumas",
                            "middleNames": [
                                "A.",
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Doumas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725887"
                        ],
                        "name": "J. Hummel",
                        "slug": "J.-Hummel",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hummel",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hummel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2570618"
                        ],
                        "name": "C. Sandhofer",
                        "slug": "C.-Sandhofer",
                        "structuredName": {
                            "firstName": "Catherine",
                            "lastName": "Sandhofer",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sandhofer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1989372,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "d4b9807973730f1af4b3e6af171d283328af99d8",
            "isKey": false,
            "numCitedBy": 327,
            "numCiting": 175,
            "paperAbstract": {
                "fragments": [],
                "text": "Relational thinking plays a central role in human cognition. However, it is not known how children and adults acquire relational concepts and come to represent them in a form that is useful for the purposes of relational thinking (i.e., as structures that can be dynamically bound to arguments). The authors present a theory of how a psychologically and neurally plausible cognitive architecture can discover relational concepts from examples and represent them as explicit structures (predicates) that can take arguments (i.e., predicate them). The theory is instantiated as a computer program called DORA (Discovery Of Relations by Analogy). DORA is used to simulate the discovery of novel properties and relations, as well as a body of empirical phenomena from the domain of relational learning and the development of relational representations in children and adults."
            },
            "slug": "A-theory-of-the-discovery-and-predication-of-Doumas-Hummel",
            "title": {
                "fragments": [],
                "text": "A theory of the discovery and predication of relational concepts."
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The authors present a theory of how a psychologically and neurally plausible cognitive architecture can discover relational concepts from examples and represent them as explicit structures (predicates) that can take arguments (i.e., predicate them)."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2859277"
                        ],
                        "name": "T. Plate",
                        "slug": "T.-Plate",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Plate",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Plate"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 205
                            }
                        ],
                        "text": "The construction of higher level structures from low-level ones is fundamental not only to language but many aspects of human cognition such as analogy retrieval and processing (Eliasmith & Thagard, 2001; Plate, 2000), memory (Kanerva, 1988), and problem solving (Ross, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 110015,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9091aa773f1b31b4ff6d15a676321fc781b7e8c9",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Holographic reduced representations (HRRs) are a method for encoding nested relational structures in fixed\u2010width vector representations. HRRs encode relational structures as vector representations in such a way that the superficial similarity of the vectors reflects both superficial and structural similarity of the relational structures. HRRs also support a number of operations that could be very useful in psychological models of human analogy processing: fast estimation of superficial and structural similarity via a vector dot\u2010product; finding corresponding objects in two structures; and chunking of vector representations. Although similarity assessment and discovery of corresponding objects both theoretically take exponential time to perform fully and accurately, with HRRs one can obtain approximate solutions in constant time. The accuracy of these operations with HRRs mirrors patterns of human performance on analog retrieval and processing tasks."
            },
            "slug": "Analogy-retrieval-and-processing-with-distributed-Plate",
            "title": {
                "fragments": [],
                "text": "Analogy retrieval and processing with distributed vector representations"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "Holographic reduced representations are a method for encoding nested relational structures in fixed\u2010width vector representations that support a number of operations that could be very useful in psychological models of human analogy processing."
            },
            "venue": {
                "fragments": [],
                "text": "Expert Syst. J. Knowl. Eng."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114531657"
                        ],
                        "name": "Noam Chomsky",
                        "slug": "Noam-Chomsky",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Chomsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noam Chomsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145868645"
                        ],
                        "name": "N. Smith",
                        "slug": "N.-Smith",
                        "structuredName": {
                            "firstName": "Neil",
                            "lastName": "Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 142683777,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "42a623f2660056242e88ea0488bb10058575737c",
            "isKey": false,
            "numCitedBy": 529,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "The terms of the title can be understood in various ways, along with the frameworks in which they are embedded. I would like to outline interpretations that I think are useful and proper, and to suggest a more general thesis, which would require much more comprehensive argument: that there is no coherent alternative to proceeding in this way for the range of issues addressed, and that other endeavors in roughly the same realm are clarified and facilitated if understood as extensions of the approach outlined. Deflating the terms Putting \u201clanguage\u201d aside for the moment, let's begin by taking the other terms of the title in ways that are innocent of far-reaching implications, specifically, divorced from any metaphysical connotations. Take the term \u201cmind\u201d or, as a preliminary, \u201cmental.\u201d Consider how we use such terms as \u201cchemical,\u201d \u201coptical,\u201d or \u201celectrical.\u201d Certain phenomena, events, processes, and states are called \u201cchemical\u201d (etc.), but no meta-physical divide is suggested by that usage. These are just various aspects of the world that we select as a focus of attention for the purposes of inquiry and exposition. I will understand the term \u201cmental\u201d in much the same way, with something like its traditional coverage, but without metaphysical import and with no suggestion that it would make any sense to try to identify the true criterion or mark of the mental."
            },
            "slug": "New-Horizons-in-the-Study-of-Language-and-Mind:-and-Chomsky-Smith",
            "title": {
                "fragments": [],
                "text": "New Horizons in the Study of Language and Mind: Naturalism and dualism in the study of language and mind"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2404363"
                        ],
                        "name": "S. Sloman",
                        "slug": "S.-Sloman",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Sloman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sloman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1969651"
                        ],
                        "name": "L. Rips",
                        "slug": "L.-Rips",
                        "structuredName": {
                            "firstName": "Lance",
                            "lastName": "Rips",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rips"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 91
                            }
                        ],
                        "text": "However, a number of difficulties arise when working with such data (Murphy & Medin, 1985; Sloman & Rips, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 39852555,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "1e6b9a32b5f57e23b8f2f61df8d1e802db5d9c68",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Similarity-as-an-explanatory-construct-Sloman-Rips",
            "title": {
                "fragments": [],
                "text": "Similarity as an explanatory construct"
            },
            "venue": {
                "fragments": [],
                "text": "Cognition"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5277263"
                        ],
                        "name": "J. Pynte",
                        "slug": "J.-Pynte",
                        "structuredName": {
                            "firstName": "Jo\u00ebl",
                            "lastName": "Pynte",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pynte"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145235075"
                        ],
                        "name": "Boris New",
                        "slug": "Boris-New",
                        "structuredName": {
                            "firstName": "Boris",
                            "lastName": "New",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Boris New"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46526669"
                        ],
                        "name": "A. Kennedy",
                        "slug": "A.-Kennedy",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Kennedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kennedy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 148177835,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "f16b0e0a7bd562d4734a0190a779f813fb0bc5ac",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic and syntactic influences during reading normal text were examined in a series of multiple regression analyses conducted on a large-scale corpus of eyemovement data. Two measures of contextual constraints, based on the syntactic descriptions provided by Abeille, Clement et Toussenel (2003) and one measure on semantic constraint, based on Latent Semantic Analysis, were included in the regression equation, together with a set of properties (length, frequency, etc.), known to affect inspection times. Both syntactic and semantic constraints were found to exert a significant influence, with less time spent inspecting highly constrained target words, relative to weakly constrained ones. Semantic and syntactic properties apparently exerted their influence independently from each other, as suggested by the lack of interaction."
            },
            "slug": "A-multiple-regression-analysis-of-syntactic-and-in-Pynte-New",
            "title": {
                "fragments": [],
                "text": "A multiple regression analysis of syntactic and semantic influences in reading normal text"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Both syntactic and semantic constraints were found to exert a significant influence, with less time spent inspecting highly constrained target words, relative to weakly constrained ones."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49312343"
                        ],
                        "name": "I. Begg",
                        "slug": "I.-Begg",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Begg",
                            "middleNames": [
                                "Maynard"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Begg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "3 that experiments on sentence recall (Sachs, 1967, 1988; Begg, 1971) demonstrate that semantic composition is not simply equivalent to memory for constituents and their stucture."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "However, semantic composition is distinct from the memorisation of constituents and their structure, as the experiments on sentence recall (Sachs, 1967, 1988; Begg, 1971) discussed in Section 2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Experiments on sentence recall (Sachs, 1967, 1988; Begg, 1971) also give us some indication of what semantic composition is not."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 144006772,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "c1da8ec91096d9c531de3a984fc8142f9aecbb13",
            "isKey": true,
            "numCitedBy": 74,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Recognition-memory-for-sentence-meaning-and-wording-Begg",
            "title": {
                "fragments": [],
                "text": "Recognition memory for sentence meaning and wording"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120600938"
                        ],
                        "name": "G. Simpson",
                        "slug": "G.-Simpson",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Simpson",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Simpson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152782175"
                        ],
                        "name": "R. R. Peterson",
                        "slug": "R.-R.-Peterson",
                        "structuredName": {
                            "firstName": "R",
                            "lastName": "Peterson",
                            "middleNames": [
                                "R"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. R. Peterson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5645275"
                        ],
                        "name": "M. A. Casteel",
                        "slug": "M.-A.-Casteel",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Casteel",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. A. Casteel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50611682"
                        ],
                        "name": "C. Burgess",
                        "slug": "C.-Burgess",
                        "structuredName": {
                            "firstName": "Curt",
                            "lastName": "Burgess",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burgess"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Such experiments can be based on word recognition times (Simpson et al., 1989), eye-movements during reading (Pynte et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 13
                            }
                        ],
                        "text": "For example, Simpson et al. (1989) found relatedness priming effects for words embedded in grammatical sentences (The auto accident drew a large crowd of people) but not for words in scrambled stimuli (Accident of large the drew auto crowd a people)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 144044038,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "44656a2e344549fffc3203580fe6e31085688716",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Three experiments examined lexical and sentence-level contributions to contextual facilitation effects in word recognition. Subjects named target words preceded by normal or scrambled sentence contexts that contained lexical associates of the target. In Experiment 1, normal sentences showed facilitation for related targets and inhibition for unrelated targets. Experiment 2 eliminated syntactically anomalous targets among unrelated items and showed only facilitation for related targets. In neither experiment was there any effect of relatedness for scrambled stimuli. Experiment 3 included syntactically normal but semantically anomalous sentences to test whether the failure of scrambled sentences to show priming was due to their syntactic incoherence. Normal sentences again showed contextual facilitation, but neither scrambled nor anomalous sentences showed such effects. The results indicate that there are sentence-context effects that do not arise solely from intralexical spreading activation and suggest that context facilitates the identification of a lexical candidate."
            },
            "slug": "Lexical-and-Sentence-Context-Effects-in-Word-Simpson-Peterson",
            "title": {
                "fragments": [],
                "text": "Lexical and Sentence Context Effects in Word Recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115953085"
                        ],
                        "name": "Cynthia A. Thompson and Raymond J. Mooney and Lappoon R. Tang",
                        "slug": "Cynthia-A.-Thompson-and-Raymond-J.-Mooney-and-R.",
                        "structuredName": {
                            "firstName": "Cynthia",
                            "lastName": "Tang",
                            "middleNames": [
                                "A.",
                                "Thompson",
                                "and",
                                "Raymond",
                                "J.",
                                "Mooney",
                                "and",
                                "Lappoon",
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cynthia A. Thompson and Raymond J. Mooney and Lappoon R. Tang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 108
                            }
                        ],
                        "text": "For example, logic based representations have been successful in natural language database querying systems (Thompson et al., 1997), question answering (Furbach et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6559541,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9277028170918023039bb88f31e625162be038bc",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "For most natural language processing tasks, a parser that maps sentences into a semantic representation is signi cantly more useful than a grammar or automata that simply recognizes syntactically wellformed strings. This paper reviews our work on using inductive logic programming methods to learn deterministic shift-reduce parsers that translate natural language into a semantic representation. We focus on the task of mapping database queries directly into executable logical form. An overview of the system is presented followed by recent experimental results on corpora of Spanish geography queries and English jobsearch queries."
            },
            "slug": "Learning-to-Parse-Natural-Language-Database-Queries-Tang",
            "title": {
                "fragments": [],
                "text": "Learning to Parse Natural Language Database Queries into Logical Form"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper reviews the work on using inductive logic programming methods to learn deterministic shift-reduce parsers that translate natural language into a semantic representation and focuses on the task of mapping database queries directly into executable logical form."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794100"
                        ],
                        "name": "Brian Roark",
                        "slug": "Brian-Roark",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Roark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Roark"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Constructing distributional representations from this data requires that the idea of context be given a concrete definition which allows us to compile counts for target words 1The development and test sets were drawn from the year 1987 to avoid overlap with the Penn Treebank, which we used to train Roark\u2019s (2001) parser."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Linear interpolation is guaranteed to produce valid probabilities, and has been used, for example, to integrate syntactic language models with n-gram models (Roark, 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Following previous work on syntactic language modeling (Roark, 2001; Charniak, 2001; Chelba and Jelinek, 1998), we trained the parser on sections 2\u201321 of the Penn Treebank containing 936,017 words."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We also integrate our models with a syntactic language model (Roark, 2001), as this allows us to compare the benefits derived from incorporating long-range syntactic dependencies to those derived from our semantic model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6237722,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e30d29fdf23e14623a2024d4fe0f7f3d5dc889d3",
            "isKey": true,
            "numCitedBy": 368,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition. The paper first introduces key notions in language modeling and probabilistic parsing, and briefly reviews some previous approaches to using syntactic structure for language modeling. A lexicalized probabilistic top-down parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers. A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity. Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models, demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model. A small recognition experiment also demonstrates the utility of the model."
            },
            "slug": "Probabilistic-Top-Down-Parsing-and-Language-Roark",
            "title": {
                "fragments": [],
                "text": "Probabilistic Top-Down Parsing and Language Modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A lexicalized probabilistic top-down parser is presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389036863"
                        ],
                        "name": "Jordan L. Boyd-Graber",
                        "slug": "Jordan-L.-Boyd-Graber",
                        "structuredName": {
                            "firstName": "Jordan",
                            "lastName": "Boyd-Graber",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jordan L. Boyd-Graber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Boyd-Graber and Blei (2008) integrate semantic and syntactic structure in a single model based on LDA."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 215824810,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "457628a1c232bb48acc2db8440571e289cc80e15",
            "isKey": false,
            "numCitedBy": 207,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop the syntactic topic model (STM), a nonparametric Bayesian model of parsed documents. The STM generates words that are both thematically and syntactically constrained, which combines the semantic insights of topic models with the syntactic information available from parse trees. Each word of a sentence is generated by a distribution that combines document-specific topic weights and parse-tree-specific syntactic transitions. Words are assumed to be generated in an order that respects the parse tree. We derive an approximate posterior inference method based on variational methods for hierarchical Dirichlet processes, and we report qualitative and quantitative results on both synthetic data and hand-parsed documents."
            },
            "slug": "Syntactic-Topic-Models-Boyd-Graber-Blei",
            "title": {
                "fragments": [],
                "text": "Syntactic Topic Models"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "The syntactic topic model (STM) is developed, a nonparametric Bayesian model of parsed documents that generates words that are both thematically and syntactically constrained, which combines the semantic insights of topic models with the syntactic information available from parse trees."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47473549"
                        ],
                        "name": "Colin Bannard",
                        "slug": "Colin-Bannard",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Bannard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Colin Bannard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145465286"
                        ],
                        "name": "Timothy Baldwin",
                        "slug": "Timothy-Baldwin",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Baldwin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timothy Baldwin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1876168"
                        ],
                        "name": "A. Lascarides",
                        "slug": "A.-Lascarides",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Lascarides",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lascarides"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 27
                            }
                        ],
                        "text": "Taking a similar approach, Bannard, Baldwin, and Lascarides (2003) develop a vector space model for representing the meaning of verb\u2013particle constructions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 147
                            }
                        ],
                        "text": "Such an approach may be better suited to modeling noncompositional structures that are lexicalized and frequently occurring (Baldwin et al., 2003; Bannard et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2356182,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "7feb6ba5666a5b106c5c141c4356587164d15614",
            "isKey": false,
            "numCitedBy": 138,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a distributional approach to the semantics of verb-particle constructions (e.g. put up, make off). We report first on a framework for implementing and evaluating such models. We then go on to report on the implementation of some techniques for using statistical models acquired from corpus data to infer the meaning of verb-particle constructions."
            },
            "slug": "A-Statistical-Approach-to-the-Semantics-of-Bannard-Baldwin",
            "title": {
                "fragments": [],
                "text": "A Statistical Approach to the Semantics of Verb-Particles"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A distributional approach to the semantics of verb-particle constructions (e.g. put up, make off) is described and some techniques for using statistical models acquired from corpus data to infer the meaning of verbs are reported on."
            },
            "venue": {
                "fragments": [],
                "text": "ACL 2003"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145586618"
                        ],
                        "name": "Diana McCarthy",
                        "slug": "Diana-McCarthy",
                        "structuredName": {
                            "firstName": "Diana",
                            "lastName": "McCarthy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diana McCarthy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733928"
                        ],
                        "name": "R. Navigli",
                        "slug": "R.-Navigli",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Navigli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Navigli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "and Pado (2008) apply this method to a word sense disambiguation task which involves finding synonyms which may be substituted for a given word in context (McCarthy and Navigli, 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 126584,
            "fieldsOfStudy": [
                "Linguistics",
                "Psychology"
            ],
            "id": "00162f43964fd457a9158408c1ac0e8990489782",
            "isKey": false,
            "numCitedBy": 220,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe the English Lexical Substitution task for SemEval. In the task, annotators and systems find an alternative substitute word or phrase for a target word in context. The task involves both finding the synonyms and disambiguating the context. Participating systems are free to use any lexical resource. There is a subtask which requires identifying cases where the word is functioning as part of a multiword in the sentence and detecting what that multiword is."
            },
            "slug": "SemEval-2007-Task-10:-English-Lexical-Substitution-McCarthy-Navigli",
            "title": {
                "fragments": [],
                "text": "SemEval-2007 Task 10: English Lexical Substitution Task"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "The English Lexical Substitution task for SemEval is described, in the task, annotators and systems find an alternative substitute word or phrase for a target word in context that involves both finding the synonyms and disambiguating the context."
            },
            "venue": {
                "fragments": [],
                "text": "Fourth International Workshop on Semantic Evaluations (SemEval-2007)"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680292"
                        ],
                        "name": "P. Resnik",
                        "slug": "P.-Resnik",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Resnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Resnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 183
                            }
                        ],
                        "text": "The technique is a special case of nfold cross-validation (Weiss and Kulikowski, 1991) and has been previously used for measuring how well humans agree on judging semantic similarity (Resnik and Diab, 2000; Resnik, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 183
                            }
                        ],
                        "text": "The technique is a special case of n-fold cross-validation (Weiss & Kulikowski, 1991) and has been previously used for measuring how well humans agree on judging semantic similarity (Resnik, 1999; Resnik & Diab, 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7872315,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4e89ac6de1ed1c63f26168b1afea9b64e0c766f4",
            "isKey": false,
            "numCitedBy": 2273,
            "numCiting": 99,
            "paperAbstract": {
                "fragments": [],
                "text": "This article presents a measure of semantic similarity in an IS-A taxonomy based on the notion of shared information content. Experimental evaluation against a benchmark set of human similarity judgments demonstrates that the measure performs better than the traditional edge-counting approach. The article presents algorithms that take advantage of taxonomic similarity in resolving syntactic and semantic ambiguity, along with experimental results demonstrating their effectiveness."
            },
            "slug": "Semantic-Similarity-in-a-Taxonomy:-An-Measure-and-Resnik",
            "title": {
                "fragments": [],
                "text": "Semantic Similarity in a Taxonomy: An Information-Based Measure and its Application to Problems of Ambiguity in Natural Language"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "This article presents a measure of semantic similarity in an IS-A taxonomy based on the notion of shared information content that performs better than the traditional edge-counting approach."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690956"
                        ],
                        "name": "Dekang Lin",
                        "slug": "Dekang-Lin",
                        "structuredName": {
                            "firstName": "Dekang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekang Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 123
                            }
                        ],
                        "text": "One alternative approach is to incorporate the syntactic structure of contexts into the procedure for constructing vectors (Grefenstette, 1994; Lin, 1998; Pad\u00f3 and Lapata, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 127
                            }
                        ],
                        "text": "One approach is to use word co-occurence counts that are further broken down by the syntactic dependencies between those words (Pad\u00f3 and Lapata, 2007; Lin, 1998; Grefenstette, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 100
                            }
                        ],
                        "text": "For example, they do so by defining context in terms of syntactic dependencies (Grefenstette, 1994; Lin, 1998; Pado\u0301 & Lapata, 2007) or by taking into account relational information about how roles and fillers combine to create specific factual knowledge (Dennis, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 17
                            }
                        ],
                        "text": "Syntactic models (Grefenstette, 1994; Lin, 1998; Pad\u00f3 and Lapata, 2007) take into account the syntactic relations between words in defining their contexts, and this can help improve their performance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 265
                            }
                        ],
                        "text": "\u2026themselves (Lund & Burgess, 1996), larger linguistic units such as paragraphs or documents (Landauer & Dumais, 1997), or even more complex linguistic representations such as n-grams (Jones & Mewhort, 2007) and the argument slots of predicates (Grefenstette, 1994; Lin, 1998; Pado\u0301 & Lapata, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15698938,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd1901f34cc3673072264104885d70555b1a4cdc",
            "isKey": true,
            "numCitedBy": 1928,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is."
            },
            "slug": "Automatic-Retrieval-and-Clustering-of-Similar-Words-Lin",
            "title": {
                "fragments": [],
                "text": "Automatic Retrieval and Clustering of Similar Words"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A word similarity measure based on the distributional pattern of words allows the automatically constructed thesaurus to be significantly closer to WordNet than Roget Thesaurus is."
            },
            "venue": {
                "fragments": [],
                "text": "COLING-ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748407"
                        ],
                        "name": "J. Bellegarda",
                        "slug": "J.-Bellegarda",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Bellegarda",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bellegarda"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15643429,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "98aa7cfc9b1efd016fb6da4e684f027d9fef6f15",
            "isKey": false,
            "numCitedBy": 323,
            "numCiting": 86,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Statistical-language-model-adaptation:-review-and-Bellegarda",
            "title": {
                "fragments": [],
                "text": "Statistical language model adaptation: review and perspectives"
            },
            "venue": {
                "fragments": [],
                "text": "Speech Commun."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107836"
                        ],
                        "name": "Patrick Sturt",
                        "slug": "Patrick-Sturt",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Sturt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Patrick Sturt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152862473"
                        ],
                        "name": "V. Lombardo",
                        "slug": "V.-Lombardo",
                        "structuredName": {
                            "firstName": "Vincenzo",
                            "lastName": "Lombardo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lombardo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 1975), which has been argued to provide a closer fit to the psycholinguistic evidence concerning incrementality (Sturt and Lombardo, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14252218,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "938912e7e39445f17c0d93a870c62be36cc2a19a",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "We recorded participants' eye movements while they read sentences containing verb-phrase coordination. Results showed evidence of immediate processing disruption when a reflexive pronoun embedded in the conjoined verb phrase mismatched the sentence subject. We argue that this result is incompatible with models of human parsing that employ only bottom-up parsing procedures, even when flexible constituency is employed. Models need to incorporate a mechanism similar to the adjoining operation in Tree-Adjoining Grammar, in which one structure is inserted into another."
            },
            "slug": "Processing-Coordinated-Structures:-Incrementality-Sturt-Lombardo",
            "title": {
                "fragments": [],
                "text": "Processing Coordinated Structures: Incrementality and Connectedness"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is argued that this result is incompatible with models of human parsing that employ only bottom-up parsing procedures, even when flexible constituency is employed, and models need to incorporate a mechanism similar to the adjoining operation in Tree-Adjoining Grammar."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695308"
                        ],
                        "name": "L. Konieczny",
                        "slug": "L.-Konieczny",
                        "structuredName": {
                            "firstName": "Lars",
                            "lastName": "Konieczny",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Konieczny"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This incrementality can be useful practically, in integrating with the other components of a speech recognition system, and also cognitively consistent with the incrementality of the human language processor (Tanenhaus et al., 1995; Marslen-Wilson, 1973; Konieczny, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "There is certainly evidence that interpretation proceeds word by word, with syntactic and semantic information being integrated into an incremental representation as it becomes available (Tanenhaus et al., 1995; Marslen-Wilson, 1973; Konieczny, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7546144,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "b4fd3bad83d0e64c5679dfa00b037db98342eec3",
            "isKey": false,
            "numCitedBy": 185,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Two experiments, an off-line acceptability judgment study and an on-line self-paced reading experiment, were conducted to tackle the question of locality-based preferences in sentence processing. The material consisted of German verb-final sentences containing a relative clause that was either host adjacent or extraposed. While the off-line data seem to reflect locality based integration cost, the on-line data revealed a highly different picture. It is argued that, in the present constructions at least, locality is relevant for production rather than perception. In perception, heads can be anticipated through incremental integration of their arguments."
            },
            "slug": "Locality-and-Parsing-Complexity-Konieczny",
            "title": {
                "fragments": [],
                "text": "Locality and Parsing Complexity"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is argued that, in the present constructions at least, locality is relevant for production rather than perception; in perception, heads can be anticipated through incremental integration of their arguments."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of psycholinguistic research"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781995"
                        ],
                        "name": "G. Altmann",
                        "slug": "G.-Altmann",
                        "structuredName": {
                            "firstName": "Gerry",
                            "lastName": "Altmann",
                            "middleNames": [
                                "T.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Altmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143662525"
                        ],
                        "name": "Yuki Kamide",
                        "slug": "Yuki-Kamide",
                        "structuredName": {
                            "firstName": "Yuki",
                            "lastName": "Kamide",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuki Kamide"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 1995; Altmann and Kamide, 1999). In this approach, linguistic input is paired with a visual context in which potential referents are located, with an eye-tracking setup allowing the identification of the subjects attention during the experiment. For example, Altmann and Kamide (1999) presented participants with the image in Figure 7."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "To investigate this issue more deeply, in particular to identify whether the language processor is capable of making the necessary predictions required for the former explanation, a number of reasearchers have turned to the visual world paradigm (Cooper, 1974; Tanenhaus et al., 1995; Altmann and Kamide, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "1: Visual display from Altmann and Kamide (1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1660615,
            "fieldsOfStudy": [
                "Linguistics",
                "Psychology"
            ],
            "id": "3ee6a5c813000fa4da8de9e8f3e314e9598089c7",
            "isKey": false,
            "numCitedBy": 1345,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Incremental-interpretation-at-verbs:-restricting-of-Altmann-Kamide",
            "title": {
                "fragments": [],
                "text": "Incremental interpretation at verbs: restricting the domain of subsequent reference"
            },
            "venue": {
                "fragments": [],
                "text": "Cognition"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1958983"
                        ],
                        "name": "D. Eakin",
                        "slug": "D.-Eakin",
                        "structuredName": {
                            "firstName": "Deborah",
                            "lastName": "Eakin",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Eakin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8065473,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "cf7c27cb86c6112a9df8aee1ebc44a1a64c47ed6",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Various areas of research (e.g., memory, metamemory, visual word recognition, associative priming) rely on the careful construction of reliable word lists. ListChecker Pro 1.2 is a computer program that accesses the University of South Florida word association norms (Nelson, McEvoy, & Schreiber, 1998, 2004) to report characteristics of words (e.g., frequency, concreteness), as well as direct and indirect associative relationships (e.g., shared associates, mediators). The present article presents the input requirements, menu options, and output obtained by ListChecker Pro 1.2. In addition, a randomly selected list of words from the associative versus semantic priming literature was submitted to ListChecker Pro 1.2 to demonstrate how seemingly unrelated words can be associated. The zipped file containing the program and database can be downloaded from www .eakinmemorylab.psychology.msstate.edu."
            },
            "slug": "ListChecker-Pro-1.2:-A-program-designed-to-creating-Eakin",
            "title": {
                "fragments": [],
                "text": "ListChecker Pro 1.2: A program designed to facilitate creating word lists using the University of South Florida word association norms"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The present article presents the input requirements, menu options, and output obtained by ListChecker Pro 1.2 and presents a randomly selected list of words from the associative versus semantic priming literature to demonstrate how seemingly unrelated words can be associated."
            },
            "venue": {
                "fragments": [],
                "text": "Behavior research methods"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145539951"
                        ],
                        "name": "J. Pollack",
                        "slug": "J.-Pollack",
                        "structuredName": {
                            "firstName": "Jordan",
                            "lastName": "Pollack",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pollack"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 226
                            }
                        ],
                        "text": "Connectionist representations, like their distributional counterparts, are essentially vectors, and there is a substantial literature concerned with their representational capacities in comparison to the symbolic alternatives (Fodor and Pylyshyn, 1988; Smolensky, 1990; Pollack, 1990; Plate, 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 95
                            }
                        ],
                        "text": "These include tensor products (Smolensky, 1990), recursive distributed representations (RAAMS, Pollack, 1990), spatter codes (Kanerva, 1988), holographic reduced representations (Plate, 1995), and convolution (Metcalfe, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 116
                            }
                        ],
                        "text": "This allows hierarchical structures to be represented and processed, for example in learning grammatical structures (Pollack, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 770011,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a835df43fdc2f79126319f6fa033bb42147c6f6",
            "isKey": false,
            "numCitedBy": 948,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Recursive-Distributed-Representations-Pollack",
            "title": {
                "fragments": [],
                "text": "Recursive Distributed Representations"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70856663"
                        ],
                        "name": "S. D. Lima",
                        "slug": "S.-D.-Lima",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Lima",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D. Lima"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114501517"
                        ],
                        "name": "A. Inhoff",
                        "slug": "A.-Inhoff",
                        "structuredName": {
                            "firstName": "Albrecht W.",
                            "lastName": "Inhoff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Inhoff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "However, information gained about words to the right of that being fixated consists mainly of its first 3 letters and total word length (Rayner et al., 1982; Lima and Inhoff, 1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 27992345,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "cb5b86b6d2026c76d09c65e4f64890afe99d7008",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Two experiments tested the hypothesis that lexical access in reading is initiated on the basis of word-initial letter information obtainable in the parafoveal region. Eye movements were monitored while college students read sentences containing target words whose initial trigram (Experiment 1) or bigram (Experiment 2) imposed either a high or a low degree of constraint in the lexicon. In contradiction to our hypothesis, high-constraint words (e.g., DWARF) received longer fixations than did low-constraint words (e.g., CLOWN), despite the fact that high-constraint words have an initial letter sequence shared by few other words in the lexicon. Moreover, a comparison of fixation times in viewing conditions with and without parafoveal letter information showed that the amount of decrease in target fixation time due to prior parafoveal availability was the same for high-constraint and low-constraint targets. We concluded that increased familiarity of word-initial letter sequence is beneficial to lexical access and that familiarity affects the efficiency of foveal but not parafoveal processing."
            },
            "slug": "Lexical-access-during-eye-fixations-in-reading:-of-Lima-Inhoff",
            "title": {
                "fragments": [],
                "text": "Lexical access during eye fixations in reading: effects of word-initial letter sequence."
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is concluded that increased familiarity of word-initial letter sequence is beneficial to lexical access and that familiarity affects the efficiency of foveal but not parafoveal processing."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of experimental psychology. Human perception and performance"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148762"
                        ],
                        "name": "K. Rayner",
                        "slug": "K.-Rayner",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Rayner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Rayner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47277215"
                        ],
                        "name": "S. Duffy",
                        "slug": "S.-Duffy",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Duffy",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Duffy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "It is also worth noting that although the total time spent reading an entire text is indicative of the total processing cost for that text, the breakdown of this relationship to a word-by-word basis is less reliable, due to spillover effects (Rayner and Duffy, 1986)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 34091980,
            "fieldsOfStudy": [
                "Psychology",
                "Linguistics"
            ],
            "id": "3c565c81a1e3f5d6556eedc49fc130ce6f2c5688",
            "isKey": false,
            "numCitedBy": 1027,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "Two experiments investigated whether lexical complexity increases a word\u2019s processing time. Subjects read sentences, each containing a target word, while their eye movements were monitored. In experiment 1, mean fixation time on infrequent words was longer than on their more frequent controls, as was the first fixation after the Infrequent Target. Fixation Times on Causative, factive, and negative verbs and ambiguous nouns were no longer than on their controls. Further analyses on the ambiguous nouns, however, suggested that the likelihood of their various meanings affected fixation time. This factor was investigated in experiment 2. subjects spent a longer time fixating ambiguous words with two equally likely meanings than fixating ambiguous words with one highly likely meaning. The results suggest that verb complexity does not affect lexical access time, and that word frequency And the presence of two highly likely meanings may affect lexical access and/or postaccess integration."
            },
            "slug": "Lexical-complexity-and-fixation-times-in-reading:-Rayner-Duffy",
            "title": {
                "fragments": [],
                "text": "Lexical complexity and fixation times in reading: Effects of word frequency, verb complexity, and lexical ambiguity"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The results suggest that verb complexity does not affect lexical access time, and that word frequency and the presence of two highly likely meanings may affect lexicals access and/or postaccess integration."
            },
            "venue": {
                "fragments": [],
                "text": "Memory & cognition"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741283"
                        ],
                        "name": "R. Barzilay",
                        "slug": "R.-Barzilay",
                        "structuredName": {
                            "firstName": "Regina",
                            "lastName": "Barzilay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Barzilay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145810617"
                        ],
                        "name": "Lillian Lee",
                        "slug": "Lillian-Lee",
                        "structuredName": {
                            "firstName": "Lillian",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lillian Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 292,
                                "start": 272
                            }
                        ],
                        "text": "Specifically, paraphrase pairs are presented to judges who are asked to decide whether they are semantically equivalent, that is, whether they can be generally substituted for one another in the same context without great information loss (Bannard & Callison-Burch, 2005; Barzilay & Lee, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 62
                            }
                        ],
                        "text": "For example, we can ask whether two sentences are paraphrases (Barzilay and Lee, 2003), i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6387310,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10b8f21e57b3392ce623c374c2c039f811ce5f69",
            "isKey": false,
            "numCitedBy": 523,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the text-to-text generation problem of sentence-level paraphrasing --- a phenomenon distinct from and more difficult than word- or phrase-level paraphrasing. Our approach applies multiple-sequence alignment to sentences gathered from unannotated comparable corpora: it learns a set of paraphrasing patterns represented by word lattice pairs and automatically determines how to apply these patterns to rewrite new sentences. The results of our evaluation experiments show that the system derives accurate paraphrases, outperforming baseline systems."
            },
            "slug": "Learning-to-Paraphrase:-An-Unsupervised-Approach-Barzilay-Lee",
            "title": {
                "fragments": [],
                "text": "Learning to Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work applies multiple-sequence alignment to sentences gathered from unannotated comparable corpora: it learns a set of paraphrasing patterns represented by word lattice pairs and automatically determines how to apply these patterns to rewrite new sentences."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3142223"
                        ],
                        "name": "M. Lesk",
                        "slug": "M.-Lesk",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lesk",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lesk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 81
                            }
                        ],
                        "text": "We used a well-known dictionary-based similarity measure, originally proposed by Lesk (1986), to rank the candidate phrase pairs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11892605,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "76e4e034c20bea86edcc6e71bbaddb47fafeecbc",
            "isKey": false,
            "numCitedBy": 2124,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The meaning of an English word can vary widely depending on which sense is intended. Does a fireman feed fires or put them out? It depends on whether or not he is on a steam locomotive. I am trying to decide automatically which sense of a word is intended (in written English) by using machine readable dictionaries, and looking for words in the sense definitions that overlap words in the definition of nearby words. The problem of deciding which sense of a word was intended by the writer is an important problem in information retrieval systems. At present most retrieval systems rely on manual indexing; if this is to be replaced with automatic text processing, it would be very desirable to recognize the correct sense of each word as often as possible. Previous work has generally either suggested (a) detailed frames describing the particular word senses,t*\u2019 or (b) global statistics about the word occurrences.3 The first has not yet been made available in any real application, and the second may give the wrong answer in specific local instances. This procedure uses available dictionaries, so that it will process any text; and uses solely the immediate context. To consider the example in the title, look at the definition of pine in the Oxford Advanced Learner\u2019s Dictionary of Current English: there are, of course, two major senses. \u201ckind of evergreen tree with needle-shaped leaves.. .\u201d and \u201cwaste away through sorrow or illness...\u201d And cone has three separate definitions: \u201csolid body which narrows to a\u2019 point . . . . *\u2019 \u201csomething of this shape w-hether solid or hollow...,\u201d and \u201cfruit of certain evergreen trees...\u201d Note that both evergreen and tree are common to two of the sense definitions: thus a program could guess that if the two words pine cone appear together, the likely senses are those of the tree and its fruit"
            },
            "slug": "Automatic-sense-disambiguation-using-machine-how-to-Lesk",
            "title": {
                "fragments": [],
                "text": "Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This procedure uses available dictionaries, so that it will process any text; and uses solely the immediate context to decide which sense of a word is intended (in written English)."
            },
            "venue": {
                "fragments": [],
                "text": "SIGDOC '86"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793218"
                        ],
                        "name": "D. Gildea",
                        "slug": "D.-Gildea",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Gildea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gildea"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A common approach to capturing these sort of effects in a language model is to base the probability of an upcoming word on its similarity to the history of previous words (Bellegarda, 2000; Coccaro and Jurafsky, 1998; Wandmacher and Antoine, 2007; Gildea and Hofmann, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The basic idea, which has been implemented in a variety of ways (Bellegarda, 2000; Coccaro and Jurafsky, 1998; Gildea and Hofmann, 1999), is that each new word in a sentence should be semantically similar to the prior context."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Instead, we use a rescaling approach (Kneser et al., 1997; Gildea and Hofmann, 1999)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5703318,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7abf3da3432a32a8be30a6c0e202929e6473929e",
            "isKey": false,
            "numCitedBy": 254,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a novel statistical language model to capture topic-related long-range dependencies. Topics are modeled in a latent variable framework in which we also derive an EM algorithm to perform a topic factor decomposition based on a segmented training corpus. The topic model is combined with a standard language model to be used for on-line word prediction. Perplexity results indicate an improvement over previously proposed topic models, which unfortunately has not translated into lower word error."
            },
            "slug": "Topic-based-language-models-using-EM-Gildea-Hofmann",
            "title": {
                "fragments": [],
                "text": "Topic-based language models using EM"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A novel statistical language model to capture topic-related long-range dependencies and an EM algorithm to perform a topic factor decomposition based on a segmented training corpus is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "EUROSPEECH"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47473549"
                        ],
                        "name": "Colin Bannard",
                        "slug": "Colin-Bannard",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Bannard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Colin Bannard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763608"
                        ],
                        "name": "Chris Callison-Burch",
                        "slug": "Chris-Callison-Burch",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Callison-Burch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Callison-Burch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 240
                            }
                        ],
                        "text": "Specifically, paraphrase pairs are presented to judges who are asked to decide whether they are semantically equivalent, that is, whether they can be generally substituted for one another in the same context without great information loss (Bannard & Callison-Burch, 2005; Barzilay & Lee, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15728911,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "36e8b8b35e51e5b39fcafdb5c2bc763796d0672e",
            "isKey": false,
            "numCitedBy": 620,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous work has used monolingual parallel corpora to extract and generate paraphrases. We show that this task can be done using bilingual parallel corpora, a much more commonly available resource. Using alignment techniques from phrase-based statistical machine translation, we show how paraphrases in one language can be identified using a phrase in another language as a pivot. We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities, and show how it can be refined to take contextual information into account. We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments, and contrast the quality with paraphrases extracted from automatic alignments."
            },
            "slug": "Paraphrasing-with-Bilingual-Parallel-Corpora-Bannard-Callison-Burch",
            "title": {
                "fragments": [],
                "text": "Paraphrasing with Bilingual Parallel Corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work defines a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities, and shows how it can be refined to take contextual information into account."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3298207"
                        ],
                        "name": "P. Kanerva",
                        "slug": "P.-Kanerva",
                        "structuredName": {
                            "firstName": "Pentti",
                            "lastName": "Kanerva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kanerva"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 205
                            }
                        ],
                        "text": "The success of circular correlation crucially depends on the components of the n-dimensional vectors u and v being real numbers and randomly distributed with mean 0 and variance 1/n. Binary spatter codes (Kanerva, 1988, 2009) are a particularly simple form of holographic reduced representation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3352,
                                "start": 227
                            }
                        ],
                        "text": "The construction of higher level structures from low-level ones is fundamental not only to language but many aspects of human cognition such as analogy retrieval and processing (Eliasmith & Thagard, 2001; Plate, 2000), memory (Kanerva, 1988), and problem solving (Ross, 1989). Indeed, the issue of how to represent compositional structure in neural networks has been a matter of great controversy (Fodor & Pylyshyn, 1988). While neural networks can readily represent single distinct objects, in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects. For the hierarchical structure of natural language this binding problem becomes particularly acute. For example, simplistic approaches to handling sentences such as John loves Mary and Mary loves John typically fail to make valid representations in one of two ways. Either there is a failure to distinguish between these two structures because the network fails to keep track of the fact that John is subject in one and object in the other, or there is a failure to recognize that both structures involve the same participants because John as a subject has a distinct representation from John as an object. The literature is littered with solutions to the binding problem (for a detailed overview, see the following section). These include tensor products (Smolensky, 1990), recursive distributed representations (RAAMS, Pollack, 1990), spatter codes (Kanerva, 1988), holographic reduced representations (Plate, 1995), and convolution (Metcalfe, 1990). In this article, we attempt to bridge the gap in the literature by developing models of semantic composition that can represent the meaning of word combinations as opposed to individual words. Our models are narrower in scope compared with those developed in earlier connectionist work. Our vectors represent words; they are high-dimensional but relatively structured, and every component corresponds to a predefined context in which the words are found. We take it as a defining property of the vectors we consider that the values of their components are derived from event frequencies such as the number of times a given word appears in a given context (Turney & Pantel, 2010).(1) Having this in mind, we present a general framework for vector-based composition that allows us to consider different classes of models. Specifically, we formulate composition as a function of two vectors and introduce models based on addition and multiplication. We also investigate how the choice of the underlying semantic representation interacts with the choice of composition function by comparing a spatial model that represents words as vectors in a high-dimensional space against a probabilistic model that represents words as topic distributions. We assess the performance of these models directly on a similarity task. We elicit similarity ratings for pairs of adjective\u2013noun, noun\u2013noun, and verb\u2013object constructions and examine the strength of the relationship between similarity ratings and the predictions of our models. In the remainder, we review previous research on semantic composition and vector binding models. Next, we describe our modeling framework, present our elicitation experiments, and discuss our results. 1392 J. Mitchell, M. Lapata \u2044 Cognitive Science 34 (2010)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 733980,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "425931e434f6b370cc6cdd2db58873843def7d7f",
            "isKey": false,
            "numCitedBy": 515,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "The 1990s saw the emergence of cognitive models that depend on very high dimensionality and randomness. They include Holographic Reduced Representations, Spatter Code, Semantic Vectors, Latent Semantic Analysis, Context-Dependent Thinning, and Vector-Symbolic Architecture. They represent things in high-dimensional vectors that are manipulated by operations that produce new high-dimensional vectors in the style of traditional computing, in what is called here hyperdimensional computing on account of the very high dimensionality. The paper presents the main ideas behind these models, written as a tutorial essay in hopes of making the ideas accessible and even provocative. A sketch of how we have arrived at these models, with references and pointers to further reading, is given at the end. The thesis of the paper is that hyperdimensional representation has much to offer to students of cognitive science, theoretical neuroscience, computer science and engineering, and mathematics."
            },
            "slug": "Hyperdimensional-Computing:-An-Introduction-to-in-Kanerva",
            "title": {
                "fragments": [],
                "text": "Hyperdimensional Computing: An Introduction to Computing in Distributed Representation with High-Dimensional Random Vectors"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The thesis of the paper is that hyperdimensional representation has much to offer to students of cognitive science, theoretical neuroscience, computer science and engineering, and mathematics."
            },
            "venue": {
                "fragments": [],
                "text": "Cognitive Computation"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 131
                            }
                        ],
                        "text": "Specifically, the simple additive model tends to produce the best results for representations based on Latent Dirichlet Allocation (Blei et al., 2003), whereas our novel simple multiplicative model is more effective on the simple semantic space representations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 285,
                                "start": 266
                            }
                        ],
                        "text": "Practical implementations of this idea range from ad-hoc approaches for turning word co-occurrence statistics into vector based representations (Lund and Burgess, 1996) to sophisticated generative models of the distribution of words across the documents in a corpus (Blei et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 55
                            }
                        ],
                        "text": "As explained in Chapter 2, Latent Dirichlet Allocation (Blei et al., 2003) models the relationship between words and documents in terms of topics, with each document being a mixture of topics and each topic being a unigram distribution over words."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 86
                            }
                        ],
                        "text": "Within two broad approaches, a simple semantic space and a Latent Dirchlet Allocation (Blei et al., 2003) model, we consider various parameter settings and evaluate the resulting representations on two tasks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 70
                            }
                        ],
                        "text": "Although several variants have been proposed in the literature (e.g., Blei et al., 2003; Griffiths et al., 2007), they are all based on the same fundamental idea: Documents are mixtures of topics where a topic is a probability distribution over words."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 121
                            }
                        ],
                        "text": "Dimensionality reduction techniques can be also used to project high-dimensional vectors onto a lower dimensional space (Blei et al., 2003; Hofmann, 2001; Landauer & Dumais, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 67
                            }
                        ],
                        "text": "Our experiments are based on the Latent Dirichlet Allocation (LDA, Blei et al., 2003) topic model where the generative process for a document d is as follows."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3177797,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f198043a866e9187925a8d8db9a55e3bfdd47f2c",
            "isKey": true,
            "numCitedBy": 30947,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Latent-Dirichlet-Allocation-Blei-Ng",
            "title": {
                "fragments": [],
                "text": "Latent Dirichlet Allocation"
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145903504"
                        ],
                        "name": "R. Rosenfeld",
                        "slug": "R.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rosenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13110923,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "076fa8d095c37c657f2aff39cf90bc2ea883b7cb",
            "isKey": false,
            "numCitedBy": 608,
            "numCiting": 94,
            "paperAbstract": {
                "fragments": [],
                "text": "An adaptive statistical language model is described, which successfully integrates long distance linguistic information with other knowledge sources. Most existing statistical language models exploit only the immediate history of a text. To extract information from further back in the document's history, we propose and usetrigger pairsas the basic information bearing elements. This allows the model to adapt its expectations to the topic of discourse. Next, statistical evidence from multiple sources must be combined. Traditionally, linear interpolation and its variants have been used, but these are shown here to be seriously deficient. Instead, we apply the principle of Maximum Entropy (ME). Each information source gives rise to a set of constraints, to be imposed on the combined estimate. The intersection of these constraints is the set of probability functions which are consistent with all the information sources. The function with the highest entropy within that set is the ME solution. Given consistent statistical evidence, a unique ME solution is guaranteed to exist, and an iterative algorithm exists which is guaranteed to converge to it. The ME framework is extremely general: any phenomenon that can be described in terms of statistics of the text can be readily incorporated. An adaptive language model based on the ME approach was trained on theWall Street Journalcorpus, and showed a 32\u201339% perplexity reduction over the baseline. When interfaced to SPHINX-II, Carnegie Mellon's speech recognizer, it reduced its error rate by 10\u201314%. This thus illustrates the feasibility of incorporating many diverse knowledge sources in a single, unified statistical framework."
            },
            "slug": "A-maximum-entropy-approach-to-adaptive-statistical-Rosenfeld",
            "title": {
                "fragments": [],
                "text": "A maximum entropy approach to adaptive statistical language modelling"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "An adaptive statistical language model is described, which successfully integrates long distance linguistic information with other knowledge sources, and shows the feasibility of incorporating many diverse knowledge sources in a single, unified statistical framework."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Speech Lang."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153592298"
                        ],
                        "name": "S. McDonald",
                        "slug": "S.-McDonald",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "McDonald",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. McDonald"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2662086"
                        ],
                        "name": "R. Shillcock",
                        "slug": "R.-Shillcock",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Shillcock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Shillcock"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 7622980,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "2024bc57379c4a34609135fb6fc987550a082149",
            "isKey": false,
            "numCitedBy": 161,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Low-level-predictive-inference-in-reading:-the-of-McDonald-Shillcock",
            "title": {
                "fragments": [],
                "text": "Low-level predictive inference in reading: the influence of transitional probabilities on eye movements"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "1 using an Earley parser (Stolcke, 1995)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11595344,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79fbfc1dc8846379074aaf4deb7fb0a96722eeed",
            "isKey": false,
            "numCitedBy": 401,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an extension of Earley's parser for stochastic context-free grammars that computes the following quantities given a stochastic context-free grammar and an input string: a) probabilities of successive prefixes being generated by the grammar; b) probabilities of substrings being generated by the nonterminals, including the entire string being generated by the grammar; c) most likely (Viterbi) parse of the string; d) posterior expected number of applications of each grammar production, as required for reestimating rule probabilities. Probabilities (a) and (b) are computed incrementally in a single left-to-right pass over the input. Our algorithm compares favorably to standard bottom-up parsing methods for SCFGs in that it works efficiently on sparse grammars by making use of Earley's top-down control structure. It can process any context-free rule format without conversion to some normal form, and combines computations for (a) through (d) in a single algorithm. Finally, the algorithm has simple extensions for processing partially bracketed inputs, and for finding partial parses and their likelihoods on ungrammatical inputs."
            },
            "slug": "An-Efficient-Probabilistic-Context-Free-Parsing-Stolcke",
            "title": {
                "fragments": [],
                "text": "An Efficient Probabilistic Context-Free Parsing Algorithm that Computes Prefix Probabilities"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "An extension of Earley's parser for stochastic context-free grammars that computes probabilities of successive prefixes being generated by the grammar and an input string and posterior expected number of applications of each grammar production, as required for reestimating rule probabilities."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2350399"
                        ],
                        "name": "K. Ehrlich",
                        "slug": "K.-Ehrlich",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Ehrlich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Ehrlich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148762"
                        ],
                        "name": "K. Rayner",
                        "slug": "K.-Rayner",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Rayner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Rayner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 203
                            }
                        ],
                        "text": "Similarly, the introduction of new discourse referents incurs a processing cost (Haviland and Clark, 1974; Garrod and Sanford, 1994), and the resolution of pronouns to these referents also incurs a cost (Ehrlich and Rayner, 1983)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 144057015,
            "fieldsOfStudy": [
                "Linguistics",
                "Business"
            ],
            "id": "78f84c08ffb7c052662f0049196111c449f8ea9d",
            "isKey": false,
            "numCitedBy": 262,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pronoun-assignment-and-semantic-integration-during-Ehrlich-Rayner",
            "title": {
                "fragments": [],
                "text": "Pronoun assignment and semantic integration during reading: eye movements and immediacy of processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36568337"
                        ],
                        "name": "B. Ross",
                        "slug": "B.-Ross",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Ross",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ross"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 169
                            }
                        ],
                        "text": "Examples include memory retrieval (Deese, 1959; Raaijmakers & Shiffrin, 1981), categorization (Estes, 1994; Nosofsky, 1984, 1986), problem solving (Holyoak & Koh, 1987; Ross, 1987, 1989), reasoning (Heit & Rubinstein, 1994; Rips, 1975), and learning (Gentner, 1989; Ross, 1984)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 144823696,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "2acac0a804c04b16c5940f978efaccfb4aa477e5",
            "isKey": false,
            "numCitedBy": 648,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Novices attempting to solve a problem often are reminded of an earlier problem that illustrated a principle. Two experiments examined how these earlier problems are used and how this use is related to these remindings. Subjects studied four probability principles with related word problems. Test problems varied in their similarity to the study problems on story lines, objects, and correspondence of objects (variable roles). Experiment 1 tested whether remindings cue the principle or serve as the sources of detailed analogies. When the appropriate formula was provided with each test, the similarity of story lines had no effect, but object correspondences had a large effect. These results support an analogical account in which mapping is affected by the similarity of objects between study and test problems. Experiment 2 began to separate the aspects of similarity affecting the access and use of earlier problems by showing that, with confusable principles, similar story lines increased the access, but did not affect the use. The access appears to be sensitive to the relative similarity of examples because with distinctive principles, similar story lines had little effect. Discussion focuses on the further specification of the processes of noticing and analogical use of earlier problems."
            },
            "slug": "This-is-like-that:-The-use-of-earlier-problems-and-Ross",
            "title": {
                "fragments": [],
                "text": "This is like that: The use of earlier problems and the separation of similarity effects."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8167983"
                        ],
                        "name": "J. Raaijmakers",
                        "slug": "J.-Raaijmakers",
                        "structuredName": {
                            "firstName": "Jeroen",
                            "lastName": "Raaijmakers",
                            "middleNames": [
                                "G.",
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Raaijmakers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2355175"
                        ],
                        "name": "R. Shiffrin",
                        "slug": "R.-Shiffrin",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Shiffrin",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Shiffrin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 48
                            }
                        ],
                        "text": "Examples include memory retrieval (Deese, 1959; Raaijmakers & Shiffrin, 1981), categorization (Estes, 1994; Nosofsky, 1984, 1986), problem solving (Holyoak & Koh, 1987; Ross, 1987, 1989), reasoning (Heit & Rubinstein, 1994; Rips, 1975), and learning (Gentner, 1989; Ross, 1984)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 28
                            }
                        ],
                        "text": ", 1974), associative models (Raaijmakers and Schiffrin, 1981) and cognitive architectures such as ACT-R (Anderson, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 199929697,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "062d5eecccf8cc93eb781b85da130ec675da018d",
            "isKey": false,
            "numCitedBy": 1562,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Describes search of associative memory (SAM), a general theory of retrieval from long-term memory that combines features of associative network models and random search models. It posits cue-dependent probabilistic sampling and recovery from an associative network, but the network is specified as a retrieval structure rather than a storage structure. A quantitative computer simulation of SAM was developed and applied to the part-list cuing paradigm. When free recall of a list of words was cued by a random subset of words from that list, the probability of recalling one of the remaining words was less than if no cues were provided at all. SAM predicted this effect in all its variations by making extensive use of interword associations in retrieval, a process that previous theorizing has dismissed. (55 ref)"
            },
            "slug": "Search-of-associative-memory.-Raaijmakers-Shiffrin",
            "title": {
                "fragments": [],
                "text": "Search of associative memory."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50288999"
                        ],
                        "name": "Randall Davis",
                        "slug": "Randall-Davis",
                        "structuredName": {
                            "firstName": "Randall",
                            "lastName": "Davis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Randall Davis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716356"
                        ],
                        "name": "H. Shrobe",
                        "slug": "H.-Shrobe",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Shrobe",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Shrobe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679873"
                        ],
                        "name": "Peter Szolovits",
                        "slug": "Peter-Szolovits",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Szolovits",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Szolovits"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 104
                            }
                        ],
                        "text": "(For a fuller account of the different approaches and issues involved we refer the interested reader to Markman, 1998)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1527228,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "8df1e4d7a7c2f288b7ca4645b444b128b076a572",
            "isKey": false,
            "numCitedBy": 1354,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Although knowledge representation is one of the central and, in some ways, most familiar concepts in AI, the most fundamental question about it -- What is it? -- has rarely been answered directly. Numerous papers have lobbied for one or another variety of representation, other papers have argued for various properties a representation should have, and still others have focused on properties that are important to the notion of representation in general. In this article, we go back to basics to address the question directly. We believe that the answer can best be understood in terms of five important and distinctly different roles that a representation plays, each of which places different and, at times, conflicting demands on the properties a representation should have. We argue that keeping in mind all five of these roles provides a usefully broad perspective that sheds light on some longstanding disputes and can invigorate both research and practice in the field."
            },
            "slug": "What-Is-a-Knowledge-Representation-Davis-Shrobe",
            "title": {
                "fragments": [],
                "text": "What Is a Knowledge Representation?"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is argued that keeping in mind all five of these roles that a representation plays provides a usefully broad perspective that sheds light on some longstanding disputes and can invigorate both research and practice in the field."
            },
            "venue": {
                "fragments": [],
                "text": "AI Mag."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40756494"
                        ],
                        "name": "J. Metcalfe",
                        "slug": "J.-Metcalfe",
                        "structuredName": {
                            "firstName": "Janet",
                            "lastName": "Metcalfe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Metcalfe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 210
                            }
                        ],
                        "text": "These include tensor products (Smolensky, 1990), recursive distributed representations (RAAMS, Pollack, 1990), spatter codes (Kanerva, 1988), holographic reduced representations (Plate, 1995), and convolution (Metcalfe, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 41426049,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "08703471946563df9b9682999b4a04d2d1ec4142",
            "isKey": false,
            "numCitedBy": 138,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "In this article, a highly interactive model of association formation, storage, and retrieval is described. Items, represented as sets of features, are associated by the operation of convolution. The associations are stored by being superimposed in a composite memory trace. Retrieval occurs when a cue item is correlated with the composite trace. The retrieved items are intrinsically noisy, may be ambiguous, and may under certain conditions be systematically distorted from their encoded form. A discrete response is selected by matching the retrieved item to all of the items in semantic memory. The model yields several new predictions about errors in single-trial cued recall that depend on similarity relations among the to-be-remembered items, and also about the efficacy of extralist cues. Experiments are presented that test these predictions against human recall. The model is then applied to several well-known results: prototype abstraction, the A-B A-D paradigm\u2014including the independence of the B and D responses\u2014 and the Osgood transfer surface."
            },
            "slug": "A-composite-holographic-associative-recall-model-Metcalfe",
            "title": {
                "fragments": [],
                "text": "A composite holographic associative recall model"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "A highly interactive model of association formation, storage, and retrieval that is applied to several well-known results, yielding several new predictions about errors in single-trial cued recall that depend on similarity relations among the to-be-remembered items, and also about the efficacy of extralist cues."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143902101"
                        ],
                        "name": "M. Masson",
                        "slug": "M.-Masson",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Masson",
                            "middleNames": [
                                "E",
                                "J"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Masson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 118
                            }
                        ],
                        "text": "Changes in the temporal order of words in a sentence decrease the strength of the related priming effect (Foss, 1982; Masson, 1986; O\u2019Seaghdha, 1989; Simpson, Peterson, Casteel, & Brugges, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 143939538,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "31a9416d2eb177b536aafa3eeca819dc2df455f7",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Comprehension-of-rapidly-presented-sentences:-The-Masson",
            "title": {
                "fragments": [],
                "text": "Comprehension of rapidly presented sentences: The mind is quicker than the eye"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34575455"
                        ],
                        "name": "S. Ehrlich",
                        "slug": "S.-Ehrlich",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Ehrlich",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ehrlich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148762"
                        ],
                        "name": "K. Rayner",
                        "slug": "K.-Rayner",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Rayner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Rayner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Evidence also indicates that the extent to which this partial structure constrains the subsequent input affects the processing load (Ehrlich and Rayner, 1981; Rayner and Well, 1996), with more constrained words having shorter fixations and being skipped more."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 143661385,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "4bf8a0cebedd2cdc9fb47edc4a5df601008686b4",
            "isKey": false,
            "numCitedBy": 610,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Contextual-effects-on-word-perception-and-eye-Ehrlich-Rayner",
            "title": {
                "fragments": [],
                "text": "Contextual effects on word perception and eye movements during reading"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39225310"
                        ],
                        "name": "S. Haviland",
                        "slug": "S.-Haviland",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Haviland",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Haviland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29224904"
                        ],
                        "name": "H. H. Clark",
                        "slug": "H.-H.-Clark",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Clark",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. H. Clark"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 80
                            }
                        ],
                        "text": "Similarly, the introduction of new discourse referents incurs a processing cost (Haviland and Clark, 1974; Garrod and Sanford, 1994), and the resolution of pronouns to these referents also incurs a cost (Ehrlich and Rayner, 1983)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 143967490,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "ec04db21ba46815280f3634ccde520d58b0f19bd",
            "isKey": false,
            "numCitedBy": 1054,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "What's-new-Acquiring-New-information-as-a-process-Haviland-Clark",
            "title": {
                "fragments": [],
                "text": "What's new? Acquiring New information as a process in comprehension"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2365155"
                        ],
                        "name": "S. Deerwester",
                        "slug": "S.-Deerwester",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Deerwester",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Deerwester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737579"
                        ],
                        "name": "G. Furnas",
                        "slug": "G.-Furnas",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Furnas",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Furnas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3154682"
                        ],
                        "name": "R. Harshman",
                        "slug": "R.-Harshman",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Harshman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Harshman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 83
                            }
                        ],
                        "text": "Similar ideas are also employed by the vector space model in information retrieval (Salton et al., 1975; Deerwester et al., 1990) as a practical solution to the engineering problem of how to match documents to queries."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3252915,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20a80a7356859daa4170fb4da6b87b84adbb547f",
            "isKey": false,
            "numCitedBy": 7019,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (\u201csemantic structure\u201d) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. initial tests find this completely automatic method for retrieval to be promising."
            },
            "slug": "Indexing-by-Latent-Semantic-Analysis-Deerwester-Dumais",
            "title": {
                "fragments": [],
                "text": "Indexing by Latent Semantic Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A new method for automatic indexing and retrieval to take advantage of implicit higher-order structure in the association of terms with documents (\u201csemantic structure\u201d) in order to improve the detection of relevant documents on the basis of terms found in queries."
            },
            "venue": {
                "fragments": [],
                "text": "J. Am. Soc. Inf. Sci."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2001885"
                        ],
                        "name": "Ted Pedersen",
                        "slug": "Ted-Pedersen",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Pedersen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Pedersen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145984521"
                        ],
                        "name": "Siddharth Patwardhan",
                        "slug": "Siddharth-Patwardhan",
                        "structuredName": {
                            "firstName": "Siddharth",
                            "lastName": "Patwardhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Siddharth Patwardhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3075310"
                        ],
                        "name": "Jason Michelizzi",
                        "slug": "Jason-Michelizzi",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Michelizzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason Michelizzi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1499545,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "495f3405da229b903797472c64d09d83659fdb34",
            "isKey": false,
            "numCitedBy": 1783,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "WordNet::Similarity is a freely available software package that makes it possible to measure the semantic similarity and relatedness between a pair of concepts (or synsets). It provides six measures of similarity, and three measures of relatedness, all of which are based on the lexical database WordNet. These measures are implemented as Perl modules which take as input two concepts, and return a numeric value that represents the degree to which they are similar or related."
            },
            "slug": "WordNet::Similarity-Measuring-the-Relatedness-of-Pedersen-Patwardhan",
            "title": {
                "fragments": [],
                "text": "WordNet::Similarity - Measuring the Relatedness of Concepts"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "WordNet::Similarity is a freely available software package that makes it possible to measure the semantic similarity and relatedness between a pair of concepts (or synsets)."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760616"
                        ],
                        "name": "F. J. Pelletier",
                        "slug": "F.-J.-Pelletier",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Pelletier",
                            "middleNames": [
                                "Jeffry"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. J. Pelletier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 58761668,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "975b9c246611a35a41474ddf7403828b437551e8",
            "isKey": false,
            "numCitedBy": 297,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This book introduces fundamental techniques for computing semantic representations for fragments of natural language and performing inference with the result. The prinary tools used are first-order logic and lambda calculus. All the techniques introduced are implemented in Prolog. The book also shown how to use theorem provers and model builders in parallel to deal with natrual language inference."
            },
            "slug": "Representation-and-Inference-for-Natural-Language:-Pelletier",
            "title": {
                "fragments": [],
                "text": "Representation and Inference for Natural Language: A First Course in Computational Semantics"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "This book introduces fundamental techniques for computing semantic representations for fragments of natural language and performing inference with the result by using first-order logic and lambda calculus."
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2693903"
                        ],
                        "name": "S. Pinker",
                        "slug": "S.-Pinker",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Pinker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Pinker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 73
                            }
                        ],
                        "text": "a process of combining and integrating the semantics of the constituents (Frege, 1884; Pinker, 1994; Partee, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 143780834,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "8a99ddac28e5695589789609b1258958c2a6c70a",
            "isKey": false,
            "numCitedBy": 1277,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In this classic, the world's expert on language and mind lucidly explains everything you always wanted to know about language: how it works, how children learn it, how it changes, how the brain computes it, and how it evolved. With deft use of examples of humor and wordplay, Steven Pinker weaves our vast knowledge of language into a compelling story: language is a human instinct, wired into our brains by evolution. The Language Instinct received the William James Book Prize from the American Psychological Association and the Public Interest Award from the Linguistics Society of America. This edition includes an update on advances in the science of language since The Language Instinct was first published."
            },
            "slug": "The-language-instinct-:-how-the-mind-creates-Pinker",
            "title": {
                "fragments": [],
                "text": "The language instinct : how the mind creates language"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704065"
                        ],
                        "name": "D. Gentner",
                        "slug": "D.-Gentner",
                        "structuredName": {
                            "firstName": "Dedre",
                            "lastName": "Gentner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gentner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 264,
                                "start": 251
                            }
                        ],
                        "text": "Examples include memory retrieval (Deese, 1959; Raaijmakers & Shiffrin, 1981), categorization (Estes, 1994; Nosofsky, 1984, 1986), problem solving (Holyoak & Koh, 1987; Ross, 1987, 1989), reasoning (Heit & Rubinstein, 1994; Rips, 1975), and learning (Gentner, 1989; Ross, 1984)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16400609,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41935319686770077dac8d53d44ddf5d1bdad26b",
            "isKey": false,
            "numCitedBy": 1136,
            "numCiting": 130,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : It is widely agreed that similarity and analogy are important in transfer of learning. Recent research suggests that different kinds of similarity enter into different parts of the transfer process. For example, access to long-term memory is more influenced by surface similarity than is analogical inference once an analogy is present. In this paper I decompose similarity-based transfer into separate subprocesses and compare how different kinds of similarity affect each of these processes. Keywords: Similarity, Transfer, Analogical mapping, Analogical soundness, Access."
            },
            "slug": "Mechanisms-of-Analogical-Learning.-Gentner",
            "title": {
                "fragments": [],
                "text": "Mechanisms of Analogical Learning."
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This paper decomposes similarity-based transfer into separate subprocesses and compares how different kinds of similarity affect each of these processes."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6318398"
                        ],
                        "name": "F. Vitu",
                        "slug": "F.-Vitu",
                        "structuredName": {
                            "firstName": "Fran\u00e7oise",
                            "lastName": "Vitu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Vitu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2994100"
                        ],
                        "name": "G. McConkie",
                        "slug": "G.-McConkie",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "McConkie",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. McConkie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4483460"
                        ],
                        "name": "D. Zola",
                        "slug": "D.-Zola",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zola"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Comprehension processes in reading can be studied in terms of whether words are skipped or fixated (Brysbaert and Vitu, 1998), when regressions occur (Vitu et al., 1998) and how long fixations last (Liversedge et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 63468609,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "5e7626c2680e5daad1896713608414a48c331ca4",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "About-Regressive-Saccades-in-Reading-and-Their-to-Vitu-McConkie",
            "title": {
                "fragments": [],
                "text": "About Regressive Saccades in Reading and Their Relation to Word Identification"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50064811"
                        ],
                        "name": "L. Finkelstein",
                        "slug": "L.-Finkelstein",
                        "structuredName": {
                            "firstName": "Lev",
                            "lastName": "Finkelstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Finkelstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718798"
                        ],
                        "name": "E. Gabrilovich",
                        "slug": "E.-Gabrilovich",
                        "structuredName": {
                            "firstName": "Evgeniy",
                            "lastName": "Gabrilovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Gabrilovich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745572"
                        ],
                        "name": "Y. Matias",
                        "slug": "Y.-Matias",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Matias",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Matias"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747801"
                        ],
                        "name": "E. Rivlin",
                        "slug": "E.-Rivlin",
                        "structuredName": {
                            "firstName": "Ehud",
                            "lastName": "Rivlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Rivlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3316511"
                        ],
                        "name": "Zach Solan",
                        "slug": "Zach-Solan",
                        "structuredName": {
                            "firstName": "Zach",
                            "lastName": "Solan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zach Solan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073936"
                        ],
                        "name": "G. Wolfman",
                        "slug": "G.-Wolfman",
                        "structuredName": {
                            "firstName": "Gadi",
                            "lastName": "Wolfman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wolfman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779370"
                        ],
                        "name": "E. Ruppin",
                        "slug": "E.-Ruppin",
                        "structuredName": {
                            "firstName": "Eytan",
                            "lastName": "Ruppin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ruppin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 162
                            }
                        ],
                        "text": "The ability of the semantic spaces described in the previous sections to predict human similarity judgements was evaluated against the WordSim353 test collection (Finkelstein et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 116
                            }
                        ],
                        "text": "The first of these involved predicting similarity judgements for the word pairs gathered in the WordSim353 database (Finkelstein et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 57
                            }
                        ],
                        "text": "This configuration gave the highest correlation with the WordSim353 similarity judgments using the cosine measure (Spearman\u2019s q \u00bc .42)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 42
                            }
                        ],
                        "text": "We used WordSim353, a benchmark data set (Finkelstein et al., 2002), consisting of relatedness judgments (on a scale of 0\u201310) for 353 word pairs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 88
                            }
                        ],
                        "text": "Chapter 3 evaluated semantic representations of individual words against the WordSim353 (Finkelstein et al., 2002) dataset of similarity ratings for word pairs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 137
                            }
                        ],
                        "text": "For our experiments, we trained an LDA model on the BNC corpus.10 We optimized the model\u2019s parameters in terms of correlation on the same WordSim353 data set used for the simpler semantic space model."
                    },
                    "intents": []
                }
            ],
            "corpusId": 12956853,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0c01df98a6b633b25c96c1a99b713ac96f1c5be",
            "isKey": true,
            "numCitedBy": 1725,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Keyword-based search engines are in widespread use today as a popular means for Web-based information retrieval. Although such systems seem deceptively simple, a considerable amount of skill is required in order to satisfy non-trivial information needs. This paper presents a new conceptual paradigm for performing search in context, that largely automates the search process, providing even non-professional users with highly relevant results. This paradigm is implemented in practice in the IntelliZap system, where search is initiated from a text query marked by the user in a document she views, and is guided by the text surrounding the marked query in that document (\"the context\"). The context-driven information retrieval process involves semantic keyword extraction and clustering to automatically generate new, augmented queries. The latter are submitted to a host of general and domain-specific search engines. Search results are then semantically reranked, using context. Experimental results testify that using context to guide search, effectively offers even inexperienced users an advanced search tool on the Web."
            },
            "slug": "Placing-search-in-context:-the-concept-revisited-Finkelstein-Gabrilovich",
            "title": {
                "fragments": [],
                "text": "Placing search in context: the concept revisited"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A new conceptual paradigm for performing search in context is presented, that largely automates the search process, providing even non-professional users with highly relevant results."
            },
            "venue": {
                "fragments": [],
                "text": "TOIS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "118213642"
                        ],
                        "name": "J. Sachs",
                        "slug": "J.-Sachs",
                        "structuredName": {
                            "firstName": "Jacqueline",
                            "lastName": "Sachs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sachs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 36148380,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "283311a2c4679b9c2291512644880148510776f4",
            "isKey": false,
            "numCitedBy": 202,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "After reading or listening to short passages, Ss attempted to recognize semantically changed sentences and paraphrases (syntactically and lexically changed sentences). The intervals between the original presentation and test ranged from 1 to 23 sec. In general, paraphrases were poorly detected after a brief time, supporting earlier findings that the exact wording of sentences is not stored in long-term memory. An exception was the high recognition of active-passive changes with the visual presentation. Recognition at the first test interval was significantly better after listening than after reading, although the eventual level of recognition memory was not different in the two modes. This result, consistent with other studies of modality effects in short-term memory, suggests that acoustic-phonetic memory played a role in the storage of the auditorally presented material."
            },
            "slug": "Memory-in-reading-and-listening-to-discourse-Sachs",
            "title": {
                "fragments": [],
                "text": "Memory in reading and listening to discourse"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Recognition at the first test interval was significantly better after listening than after reading, although the eventual level of recognition memory was not different in the two modes, suggesting that acoustic-phonetic memory played a role in the storage of the auditorally presented material."
            },
            "venue": {
                "fragments": [],
                "text": "Memory & cognition"
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2974669"
                        ],
                        "name": "R. Nosofsky",
                        "slug": "R.-Nosofsky",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Nosofsky",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Nosofsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 108
                            }
                        ],
                        "text": "Examples include memory retrieval (Deese, 1959; Raaijmakers & Shiffrin, 1981), categorization (Estes, 1994; Nosofsky, 1984, 1986), problem solving (Holyoak & Koh, 1987; Ross, 1987, 1989), reasoning (Heit & Rubinstein, 1994; Rips, 1975), and learning (Gentner, 1989; Ross, 1984)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8869524,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "527e1fcabce31b8cd2e720576a11f93622aae1ad",
            "isKey": false,
            "numCitedBy": 2581,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "A unified quantitative approach to modeling subjects' identification and categorization of multidimensional perceptual stimuli is proposed and tested. Two subjects identified and categorized the same set of perceptually confusable stimuli varying on separable dimensions. The identification data were modeled using Shepard's (1957) multidimensional scaling-choice framework. This framework was then extended to model the subjects' categorization performance. The categorization model, which generalizes the context theory of classification developed by Medin and Schaffer (1978), assumes that subjects store category exemplars in memory. Classification decisions are based on the similarity of stimuli to the stored exemplars. It is assumed that the same multidimensional perceptual representation underlies performance in both the identification and categorization paradigms. However, because of the influence of selective attention, similarity relationships change systematically across the two paradigms. Some support was gained for the hypothesis that subjects distribute attention among component dimensions so as to optimize categorization performance. Evidence was also obtained that subjects may have augmented their category representations with inferred exemplars. Implications of the results for theories of multidimensional scaling and categorization are discussed."
            },
            "slug": "Attention,-similarity,-and-the-relationship.-Nosofsky",
            "title": {
                "fragments": [],
                "text": "Attention, similarity, and the identification-categorization relationship."
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A unified quantitative approach to modeling subjects' identification and categorization of multidimensional perceptual stimuli is proposed and tested and some support was gained for the hypothesis that subjects distribute attention among component dimensions so as to optimize categorization performance."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of experimental psychology. General"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Hinton (1990) makes clear that one of the strengths of symbolic representations is the handling of structures of unbounded complexity, and discusses how this might be implemented in terms of fixed dimensionality connectionist representations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7544770,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71dd4d477ca17b4db3b270d25225822ff3a41fac",
            "isKey": false,
            "numCitedBy": 364,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Mapping-Part-Whole-Hierarchies-into-Connectionist-Hinton",
            "title": {
                "fragments": [],
                "text": "Mapping Part-Whole Hierarchies into Connectionist Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2859277"
                        ],
                        "name": "T. Plate",
                        "slug": "T.-Plate",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Plate",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Plate"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 103
                            }
                        ],
                        "text": "The n-grams are encoded with the aid of a place-holder environmental vector \u03a6 and circular convolution (Plate, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 179
                            }
                        ],
                        "text": "These include tensor products (Smolensky, 1990), recursive distributed representations (RAAMS, Pollack, 1990), spatter codes (Kanerva, 1988), holographic reduced representations (Plate, 1995), and convolution (Metcalfe, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 104
                            }
                        ],
                        "text": "The n-grams are encoded with the aid of a place-holder environmental vector U and circular convolution (Plate, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 204
                            }
                        ],
                        "text": "Circular convolution has been applied to memorising pen trajectories for handwritten digits (Plate, 1993) and complex semantic structures, consisting of agents playing particular roles in various actions (Plate, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2352281,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "564427596799f7967c91934966cd3c6bd31cb06d",
            "isKey": true,
            "numCitedBy": 542,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Associative memories are conventionally used to represent data with very simple structure: sets of pairs of vectors. This paper describes a method for representing more complex compositional structure in distributed representations. The method uses circular convolution to associate items, which are represented by vectors. Arbitrary variable bindings, short sequences of various lengths, simple frame-like structures, and reduced representations can be represented in a fixed width vector. These representations are items in their own right and can be used in constructing compositional structures. The noisy reconstructions extracted from convolution memories can be cleaned up by using a separate associative memory that has good reconstructive properties."
            },
            "slug": "Holographic-reduced-representations-Plate",
            "title": {
                "fragments": [],
                "text": "Holographic reduced representations"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper describes a method for representing more complex compositional structure in distributed representations that uses circular convolution to associate items, which are represented by vectors."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2496651"
                        ],
                        "name": "G. Denhi\u00e8re",
                        "slug": "G.-Denhi\u00e8re",
                        "structuredName": {
                            "firstName": "Guy",
                            "lastName": "Denhi\u00e8re",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Denhi\u00e8re"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144996667"
                        ],
                        "name": "Beno\u00eet Lemaire",
                        "slug": "Beno\u00eet-Lemaire",
                        "structuredName": {
                            "firstName": "Beno\u00eet",
                            "lastName": "Lemaire",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beno\u00eet Lemaire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, they have been used to model judgments of semantic similarity (McDonald, 2000) and association (Denhire and Lemaire, 2004; Griffiths et al., 2007) and have been shown to achieve human level performance on synonymy tests (Landuaer and Dumais, 1997; Griffiths et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9548665,
            "fieldsOfStudy": [
                "Psychology",
                "Computer Science"
            ],
            "id": "aaf611a06273a3f6bb1183cc81381009f6b6910d",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "A computational model of children's semantic memory is built from the Latent Semantic Analysis (LSA) of a multisource child corpus. Three tests of the model are described, simulating a vocabulary test, an association test and a recall task. For each one, results from experiments with children are presented and compared to the model data. Adequacy is correct, which means that this simulation of children's semantic memory can be used to simulate a variety of children's cognitive processes."
            },
            "slug": "A-Computational-Model-of-Children's-Semantic-Memory-Denhi\u00e8re-Lemaire",
            "title": {
                "fragments": [],
                "text": "A Computational Model of Children's Semantic Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A computational model of children's semantic memory is built from the Latent Semantic Analysis (LSA) of a multisource child corpus, simulating a vocabulary test, an association test and a recall task."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144171250"
                        ],
                        "name": "M. Tanenhaus",
                        "slug": "M.-Tanenhaus",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Tanenhaus",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Tanenhaus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403814853"
                        ],
                        "name": "M. Spivey-Knowlton",
                        "slug": "M.-Spivey-Knowlton",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Spivey-Knowlton",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Spivey-Knowlton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2323416"
                        ],
                        "name": "K. M. Eberhard",
                        "slug": "K.-M.-Eberhard",
                        "structuredName": {
                            "firstName": "Kathleen",
                            "lastName": "Eberhard",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M. Eberhard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39290981"
                        ],
                        "name": "J. Sedivy",
                        "slug": "J.-Sedivy",
                        "structuredName": {
                            "firstName": "Julie",
                            "lastName": "Sedivy",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sedivy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This incrementality can be useful practically, in integrating with the other components of a speech recognition system, and also cognitively consistent with the incrementality of the human language processor (Tanenhaus et al., 1995; Marslen-Wilson, 1973; Konieczny, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "There is certainly evidence that interpretation proceeds word by word, with syntactic and semantic information being integrated into an incremental representation as it becomes available (Tanenhaus et al., 1995; Marslen-Wilson, 1973; Konieczny, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "To investigate this issue more deeply, in particular to identify whether the language processor is capable of making the necessary predictions required for the former explanation, a number of reasearchers have turned to the visual world paradigm (Cooper, 1974; Tanenhaus et al., 1995; Altmann and Kamide, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3073956,
            "fieldsOfStudy": [
                "Linguistics",
                "Psychology"
            ],
            "id": "765491c9cfc746275000297fc0ae919b5c4440e0",
            "isKey": false,
            "numCitedBy": 2598,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Psycholinguists have commonly assumed that as a spoken linguistic message unfolds over time, it is initially structured by a syntactic processing module that is encapsulated from information provided by other perceptual and cognitive systems. To test the effects of relevant visual context on the rapid mental processes that accompany spoken language comprehension, eye movements were recorded with a head-mounted eye-tracking system while subjects followed instructions to manipulate real objects. Visual context influenced spoken word recognition and mediated syntactic processing, even during the earliest moments of language processing."
            },
            "slug": "Integration-of-visual-and-linguistic-information-in-Tanenhaus-Spivey-Knowlton",
            "title": {
                "fragments": [],
                "text": "Integration of visual and linguistic information in spoken language comprehension."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "To test the effects of relevant visual context on the rapid mental processes that accompany spoken language comprehension, eye movements were recorded with a head-mounted eye-tracking system while subjects followed instructions to manipulate real objects."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2326474"
                        ],
                        "name": "E. Heit",
                        "slug": "E.-Heit",
                        "structuredName": {
                            "firstName": "Evan",
                            "lastName": "Heit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Heit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069432298"
                        ],
                        "name": "J. Rubinstein",
                        "slug": "J.-Rubinstein",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Rubinstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rubinstein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 199
                            }
                        ],
                        "text": "Examples include memory retrieval (Deese, 1959; Raaijmakers & Shiffrin, 1981), categorization (Estes, 1994; Nosofsky, 1984, 1986), problem solving (Holyoak & Koh, 1987; Ross, 1987, 1989), reasoning (Heit & Rubinstein, 1994; Rips, 1975), and learning (Gentner, 1989; Ross, 1984)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 102
                            }
                        ],
                        "text": "The process by which subjects make such generalisations has often been linked to similarity relations (Heit and Rubinstein, 1994), and so may be more amenable to distributional representations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 151
                            }
                        ],
                        "text": "We intend to assess the potential of our composition models on context-sensitive semantic priming (Till, Mross, & Kintsch, 1988), inductive inference (Heit & Rubinstein, 1994), and analogical learning (Mangalath, Quesada, & Kintsch, 2004; Turney, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9806284,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "fa4153addc25d8a1319066c628d3d750f36ca2fa",
            "isKey": false,
            "numCitedBy": 249,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Three experiments investigated the proposal that inductive inferences about different properties depend on different measures of similarity. In Experiments 1 and 2, Ss were given the premise that a category of animals has some property and judged the probability that another category of animals also has that property. Ss made the strongest inferences when the kind of property (anatomical or behavioral) corresponded to the kind of similarity between the animal categories (anatomical or behavioral). These results cannot be explained in terms of a single measure of similarity underlying induction. In Experiment 3, Ss rated the similarity of animal pairs with respect to anatomy or behavior. Regression analyses showed that both behavioral and anatomical similarity influenced behavioral inferences, but only anatomical similarity influenced anatomical inferences."
            },
            "slug": "Similarity-and-property-effects-in-inductive-Heit-Rubinstein",
            "title": {
                "fragments": [],
                "text": "Similarity and property effects in inductive reasoning."
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "Three experiments investigated the proposal that inductive inferences about different properties depend on different measures of similarity, and found that Ss made the strongest inferences when the kind of property corresponded to theKind of similarity between the animal categories (anatomical or behavioral)."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of experimental psychology. Learning, memory, and cognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714374"
                        ],
                        "name": "A. Joshi",
                        "slug": "A.-Joshi",
                        "structuredName": {
                            "firstName": "Aravind",
                            "lastName": "Joshi",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Joshi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "28347374"
                        ],
                        "name": "L. Levy",
                        "slug": "L.-Levy",
                        "structuredName": {
                            "firstName": "Leon",
                            "lastName": "Levy",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Levy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40224630"
                        ],
                        "name": "Masako Takahashi",
                        "slug": "Masako-Takahashi",
                        "structuredName": {
                            "firstName": "Masako",
                            "lastName": "Takahashi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Masako Takahashi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 2954113,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2350cbf16800cac895b58dd5091da595822acdd6",
            "isKey": false,
            "numCitedBy": 898,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Tree-Adjunct-Grammars-Joshi-Levy",
            "title": {
                "fragments": [],
                "text": "Tree Adjunct Grammars"
            },
            "venue": {
                "fragments": [],
                "text": "J. Comput. Syst. Sci."
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3029039"
                        ],
                        "name": "T. Booth",
                        "slug": "T.-Booth",
                        "structuredName": {
                            "firstName": "Taylor",
                            "lastName": "Booth",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Booth"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For example a Probabilistic Context Free Grammar (PCFG) associates a probability with each rule in a CFG (Booth, 1969)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 36970812,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f71ab539337bb1e496df363a3cc2a66849117314",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of assigning a probability to each string of a language L(G) generated by a grammar G is considered. Two methods are considered. One method assigns a probability to each production associated with G and the other assigns the probabilities on the basis of particular features of the language. Several necessary conditions that must be satisfied by these probability assignment techniques if they are to be consistant are presented. The problem of recognizing languages is also considered. It is shown that under some conditions it is possible to recognize a non finitestate language with a finite state acceptor if one is willing to accept a small probability of making an error."
            },
            "slug": "Probabilistic-Representation-of-Formal-Languages-Booth",
            "title": {
                "fragments": [],
                "text": "Probabilistic Representation of Formal Languages"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that under some conditions it is possible to recognize a non finitestate language with a finite state acceptor if one is willing to accept a small probability of making an error."
            },
            "venue": {
                "fragments": [],
                "text": "SWAT"
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2974669"
                        ],
                        "name": "R. Nosofsky",
                        "slug": "R.-Nosofsky",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Nosofsky",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Nosofsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 108
                            }
                        ],
                        "text": "Examples include memory retrieval (Deese, 1959; Raaijmakers & Shiffrin, 1981), categorization (Estes, 1994; Nosofsky, 1984, 1986), problem solving (Holyoak & Koh, 1987; Ross, 1987, 1989), reasoning (Heit & Rubinstein, 1994; Rips, 1975), and learning (Gentner, 1989; Ross, 1984)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10618271,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "1824dbb20a8593865b12fefe60bf25c860975863",
            "isKey": false,
            "numCitedBy": 716,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Medin and Schaffer's (1978) context theory of classification learning is interpreted in terms of Luce's (1963) choice theory and in terms of theoretical results obtained in multidimensional scaling theory. En route to this interpretation, quantitative relationships that may exist between identification and classification performance are investigated. It is suggested that the same basic choice processes may operate in the two paradigms but that the similarity parameters that determine performance change systematically according to the structure of the choice paradigm. In particular, when subjects are able to attend selectively to the component dimensions that compose the stimuli, the similarity parameters may tend toward what is optimal for maximizing performance."
            },
            "slug": "Choice,-similarity,-and-the-context-theory-of-Nosofsky",
            "title": {
                "fragments": [],
                "text": "Choice, similarity, and the context theory of classification."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is suggested that the same basic choice processes may operate in the two paradigms but that the similarity parameters that determine performance change systematically according to the structure of the choice paradigm."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of experimental psychology. Learning, memory, and cognition"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47934005"
                        ],
                        "name": "T. Richter",
                        "slug": "T.-Richter",
                        "structuredName": {
                            "firstName": "Tobias",
                            "lastName": "Richter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Richter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Thus, rather than applying ordinary regression methods inappropriately to this data, LME methods are recommended for the analysis of reading times (Richter, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Our experiments used the grammar-based language model of Roark (2001), which is suitable for integration with the semantic and n-gram components because of its incrementality."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 51758242,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "3dab9bb9dc355ab087a4da373131f80eaaab5195",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Most reading time studies using naturalistic texts yield data sets characterized by a multilevel structure: Sentences (sentence level) are nested within persons (person level). In contrast to analysis of variance and multiple regression techniques, hierarchical linear models take the multilevel structure of reading time data into account. They provide methods to estimate variance components and to model the influence of predictor variables on different levels as well as cross-level interactions between these predictors. This article gives a brief introduction to the method and proposes practical guidelines for its application to reading time data, including a discussion of power issues and the scaling of predictor variables. The basic principles of model building and hypothesis testing are illustrated with original data from a reading time study with naturalistic texts."
            },
            "slug": "What-Is-Wrong-With-ANOVA-and-Multiple-Regression-Richter",
            "title": {
                "fragments": [],
                "text": "What Is Wrong With ANOVA and Multiple Regression? Analyzing Sentence Reading Times With Hierarchical Linear Models"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A brief introduction to the hierarchical linear models method is given and practical guidelines for its application to reading time data are proposed, including a discussion of power issues and the scaling of predictor variables."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2009767"
                        ],
                        "name": "K. Holyoak",
                        "slug": "K.-Holyoak",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Holyoak",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Holyoak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46371477"
                        ],
                        "name": "K. Koh",
                        "slug": "K.-Koh",
                        "structuredName": {
                            "firstName": "Kyunghee",
                            "lastName": "Koh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Koh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 0
                            }
                        ],
                        "text": "Kanerva, Kristoferson, and Holst (2000) propose the use of random indexing as an alternative to the computationally costly singular value decomposition employed in LSA."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 148
                            }
                        ],
                        "text": "Examples include memory retrieval (Deese, 1959; Raaijmakers & Shiffrin, 1981), categorization (Estes, 1994; Nosofsky, 1984, 1986), problem solving (Holyoak & Koh, 1987; Ross, 1987, 1989), reasoning (Heit & Rubinstein, 1994; Rips, 1975), and learning (Gentner, 1989; Ross, 1984)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 0
                            }
                        ],
                        "text": "Jones and Mewhort (2007) propose a model that makes use of the linear order of words in a context."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1371,
                                "start": 0
                            }
                        ],
                        "text": "Jones and Mewhort (2007) propose a model that makes use of the linear order of words in a context. Their model represents words by high-dimensional holographic vectors. Each word is assigned a random(4) environmental vector. Contextual information is stored in a lexical vector, which is computed with the aid of the environmental vectors. Specifically, a word\u2019s lexical vector is the superposition of the environmental vectors corresponding to its co-occurring words in a sentence. Order information is the sum of all n-grams that include the target word. The n-grams are encoded with the aid of a place-holder environmental vector U and circular convolution (Plate, 1995). The order vector is finally added to the lexical vector to jointly represent structural and contextual information. Despite the fact that these vectors contain information about multiword structures in the contexts of target words, they are, nonetheless, still fundamentally representations of individual isolated target words. Circular convolution is only used to bind environmental vectors, which being random contain no semantic information. To make a useful semantic representation of a target word, the vectors representing its contexts are summed over, producing a vector which is no longer random and for which circular convolution is no longer optimal. Sahlgren, Host, and Kanerva (2008) provide an alternative to convolution by showing that order information can also be captured by permuting the vector coordinates."
                    },
                    "intents": []
                }
            ],
            "corpusId": 13553955,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "d44a35794785eba1164bc1bd2117f909112e4398",
            "isKey": true,
            "numCitedBy": 866,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Two experiments investigated factors that influence the retrieval and use of analogies in problem solving, Experiment 1 demonstrated substantial spontaneous analogical transfer with a delay of several days between presentation of the source and target analogues. Experiment 2 examined the influence of different types of similarity between the analogues. A mechanism for retrieval of source analogues is proposed, based on summation of activation from features shared with a target problem. The results of Experiment 2 indicated that both structural features, which play a causal role in determining possible problem solutions, and salient surface features, which do not have a causal role, influence spontaneous selection of an analogue. Structural features, however, have a greater impact than do surface features on a problem solver\u2019s ability to use an analogue once its relevance has been pointed out."
            },
            "slug": "Surface-and-structural-similarity-in-analogical-Holyoak-Koh",
            "title": {
                "fragments": [],
                "text": "Surface and structural similarity in analogical transfer"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The results of Experiment 2 indicated that both structural features and salient surface features influence spontaneous selection of an analogue, however, structural features have a greater impact than do surface features on a problem solver\u2019s ability to use an analogue once its relevance has been pointed out."
            },
            "venue": {
                "fragments": [],
                "text": "Memory & cognition"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795942"
                        ],
                        "name": "Reinhard Kneser",
                        "slug": "Reinhard-Kneser",
                        "structuredName": {
                            "firstName": "Reinhard",
                            "lastName": "Kneser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Reinhard Kneser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9685476,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9548ac30c113562a51e603dbbc8e9fa651cfd3ab",
            "isKey": false,
            "numCitedBy": 1792,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In stochastic language modeling, backing-off is a widely used method to cope with the sparse data problem. In case of unseen events this method backs off to a less specific distribution. In this paper we propose to use distributions which are especially optimized for the task of backing-off. Two different theoretical derivations lead to distributions which are quite different from the probability distributions that are usually used for backing-off. Experiments show an improvement of about 10% in terms of perplexity and 5% in terms of word error rate."
            },
            "slug": "Improved-backing-off-for-M-gram-language-modeling-Kneser-Ney",
            "title": {
                "fragments": [],
                "text": "Improved backing-off for M-gram language modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper proposes to use distributions which are especially optimized for the task of back-off, which are quite different from the probability distributions that are usually used for backing-off."
            },
            "venue": {
                "fragments": [],
                "text": "1995 International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065109"
                        ],
                        "name": "M. Just",
                        "slug": "M.-Just",
                        "structuredName": {
                            "firstName": "Marcel",
                            "lastName": "Just",
                            "middleNames": [
                                "Adam"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Just"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145761969"
                        ],
                        "name": "P. Carpenter",
                        "slug": "P.-Carpenter",
                        "structuredName": {
                            "firstName": "Patricia",
                            "lastName": "Carpenter",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Carpenter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Word length carries a processing cost, with longer words taking longer to read (Just and Carpenter, 1980). The length of a saccade also affects reading time, as does where the saccade lands in a word (Vitu et al., 2001). Vitu et al. (2001) found that landing positions near the center of a word result in greater reading times than those near the edge, and dubbed this the inverted optimal viewing position effect."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Word length carries a processing cost, with longer words taking longer to read (Just and Carpenter, 1980)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3793521,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "7219ac5424180d57c13da42a811a764bd9698903",
            "isKey": false,
            "numCitedBy": 3273,
            "numCiting": 80,
            "paperAbstract": {
                "fragments": [],
                "text": "This article presents a model of reading comprehension that accounts for the allocation of eye fixations of college students reading scientific passages. The model deals with processing at the level of words, clauses, and text units. Readers make longer pauses at points where processing loads are greater. Greater loads occur while readers are accessing infrequent words, integrating information from important clauses, and making inferences at the ends of sentences. The model accounts forthe gaze duration on each word of text as a function of the involvement of the various levels of processing. The model is embedded in a theoretical framework capable of accommodating the flexibility of reading."
            },
            "slug": "A-theory-of-reading:-from-eye-fixations-to-Just-Carpenter",
            "title": {
                "fragments": [],
                "text": "A theory of reading: from eye fixations to comprehension."
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A model of reading comprehension that accounts for the allocation of eye fixations of college students reading scientific passages is presented, embedded in a theoretical framework capable of accommodating the flexibility of reading."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683588"
                        ],
                        "name": "Hagen F\u00fcrstenau",
                        "slug": "Hagen-F\u00fcrstenau",
                        "structuredName": {
                            "firstName": "Hagen",
                            "lastName": "F\u00fcrstenau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hagen F\u00fcrstenau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 2004) and semantic role labelling (F\u00fcrstenau and Lapata, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8045155,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c73e0c7b718788414d7349cb6283d10adf344aaa",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Large scale annotated corpora are prerequisite to developing high-performance semantic role labeling systems. Unfortunately, such corpora are expensive to produce, limited in size, and may not be representative. Our work aims to reduce the annotation effort involved in creating resources for semantic role labeling via semi-supervised learning. Our algorithm augments a small number of manually labeled instances with unlabeled examples whose roles are inferred automatically via annotation projection. We formulate the projection task as a generalization of the linear assignment problem. We seek to find a role assignment in the unlabeled data such that the argument similarity between the labeled and unlabeled instances is maximized. Experimental results on semantic role labeling show that the automatic annotations produced by our method improve performance over using hand-labeled instances alone."
            },
            "slug": "Semi-Supervised-Semantic-Role-Labeling-F\u00fcrstenau-Lapata",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Semantic Role Labeling"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work aims to reduce the annotation effort involved in creating resources for semantic role labeling via semi-supervised learning by augments a small number of manually labeled instances with unlabeled examples whose roles are inferred automatically via annotation projection."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3298207"
                        ],
                        "name": "P. Kanerva",
                        "slug": "P.-Kanerva",
                        "structuredName": {
                            "firstName": "Pentti",
                            "lastName": "Kanerva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kanerva"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 205
                            }
                        ],
                        "text": "The success of circular correlation crucially depends on the components of the n-dimensional vectors u and v being real numbers and randomly distributed with mean 0 and variance 1/n. Binary spatter codes (Kanerva, 1988, 2009) are a particularly simple form of holographic reduced representation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 126
                            }
                        ],
                        "text": "These include tensor products (Smolensky, 1990), recursive distributed representations (RAAMS, Pollack, 1990), spatter codes (Kanerva, 1988), holographic reduced representations (Plate, 1995), and convolution (Metcalfe, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 227
                            }
                        ],
                        "text": "The construction of higher level structures from low-level ones is fundamental not only to language but many aspects of human cognition such as analogy retrieval and processing (Eliasmith & Thagard, 2001; Plate, 2000), memory (Kanerva, 1988), and problem solving (Ross, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 57931704,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dcdb9bd64e3d7885c10938291153257b94f3df91",
            "isKey": false,
            "numCitedBy": 1008,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nMotivated by the remarkable fluidity of memory the way in which items are pulled spontaneously and effortlessly from our memory by vague similarities to what is currently occupying our attention Sparse Distributed Memory presents a mathematically elegant theory of human long term memory. \nThe book, which is self contained, begins with background material from mathematics, computers, and neurophysiology; this is followed by a step by step development of the memory model. The concluding chapter describes an autonomous system that builds from experience an internal model of the world and bases its operation on that internal model. Close attention is paid to the engineering of the memory, including comparisons to ordinary computer memories. \nSparse Distributed Memory provides an overall perspective on neural systems. The model it describes can aid in understanding human memory and learning, and a system based on it sheds light on outstanding problems in philosophy and artificial intelligence. Applications of the memory are expected to be found in the creation of adaptive systems for signal processing, speech, vision, motor control, and (in general) robots. Perhaps the most exciting aspect of the memory, in its implications for research in neural networks, is that its realization with neuronlike components resembles the cortex of the cerebellum. \nPentti Kanerva is a scientist at the Research Institute for Advanced Computer Science at the NASA Ames Research Center and a visiting scholar at the Stanford Center for the Study of Language and Information. A Bradford Book."
            },
            "slug": "Sparse-Distributed-Memory-Kanerva",
            "title": {
                "fragments": [],
                "text": "Sparse Distributed Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Pentti Kanerva's Sparse Distributed Memory presents a mathematically elegant theory of human long term memory that resembles the cortex of the cerebellum, and provides an overall perspective on neural systems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153873834"
                        ],
                        "name": "A. Wong",
                        "slug": "A.-Wong",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Wong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40498308"
                        ],
                        "name": "Chung-Shu Yang",
                        "slug": "Chung-Shu-Yang",
                        "structuredName": {
                            "firstName": "Chung-Shu",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chung-Shu Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 83
                            }
                        ],
                        "text": "Similar ideas are also employed by the vector space model in information retrieval (Salton et al., 1975; Deerwester et al., 1990) as a practical solution to the engineering problem of how to match documents to queries."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6473756,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5f169880e30e1f76827d72f862555d00b01bed9",
            "isKey": false,
            "numCitedBy": 7617,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "In a document retrieval, or other pattern matching environment where stored entities (documents) are compared with each other or with incoming patterns (search requests), it appears that the best indexing (property) space is one where each entity lies as far away from the others as possible; in these circumstances the value of an indexing system may be expressible as a function of the density of the object space; in particular, retrieval performance may correlate inversely with space density. An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents. Typical evaluation results are shown, demonstating the usefulness of the model."
            },
            "slug": "A-vector-space-model-for-automatic-indexing-Salton-Wong",
            "title": {
                "fragments": [],
                "text": "A vector space model for automatic indexing"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents, demonstating the usefulness of the model."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5277263"
                        ],
                        "name": "J. Pynte",
                        "slug": "J.-Pynte",
                        "structuredName": {
                            "firstName": "Jo\u00ebl",
                            "lastName": "Pynte",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pynte"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145235075"
                        ],
                        "name": "Boris New",
                        "slug": "Boris-New",
                        "structuredName": {
                            "firstName": "Boris",
                            "lastName": "New",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Boris New"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46526669"
                        ],
                        "name": "A. Kennedy",
                        "slug": "A.-Kennedy",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Kennedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kennedy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 1989), eye-movements during reading (Pynte et al., 2008) or even event related potentials in the brain (van Berkum et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Consolidating these factors into a single measure is preferable over a situation in which syntactic constraint is modelled in terms of surprisal (Hale, 2001) whereas semantic constraint is modelled in terms of cosine similarity (Pynte et al., 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, they have been used to model human similarity judgements (McDonald, 2000), enhance n-gram language models with long range semantic information (Bellegarda, 2000; Coccaro and Jurafsky, 1998) and quantify the effect of semantic constraint on reading times (Pynte et al., 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2480748,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "2510e7833d94b07cec9ed95dfbfbacf5d4011a08",
            "isKey": true,
            "numCitedBy": 34,
            "numCiting": 69,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-line-contextual-influences-during-reading-normal-Pynte-New",
            "title": {
                "fragments": [],
                "text": "On-line contextual influences during reading normal text: A multiple-regression analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795942"
                        ],
                        "name": "Reinhard Kneser",
                        "slug": "Reinhard-Kneser",
                        "structuredName": {
                            "firstName": "Reinhard",
                            "lastName": "Kneser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Reinhard Kneser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40293552"
                        ],
                        "name": "J. Peters",
                        "slug": "J.-Peters",
                        "structuredName": {
                            "firstName": "Jochen",
                            "lastName": "Peters",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Peters"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2561225"
                        ],
                        "name": "D. Klakow",
                        "slug": "D.-Klakow",
                        "structuredName": {
                            "firstName": "Dietrich",
                            "lastName": "Klakow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Klakow"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Instead, we use a rescaling approach (Kneser et al., 1997; Gildea and Hofmann, 1999)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3027407,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2854e19e23a0987d0989cd8dfadebec3692e3a60",
            "isKey": false,
            "numCitedBy": 114,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method is presented to quickly adapt a given language model to local text characteristics. The basic approach is to choose the adaptive models as close as possible to the background estimates while constraining them to respect the locally estimated unigram probabilities. Several means are investigated to speed up the calculations. We measure both perplexity and word error rate to gauge the quality of our model."
            },
            "slug": "Language-model-adaptation-using-dynamic-marginals-Kneser-Peters",
            "title": {
                "fragments": [],
                "text": "Language model adaptation using dynamic marginals"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A new method is presented to quickly adapt a given language model to local text characteristics by choosing the adaptive models as close as possible to the background estimates while constraining them to respect the locally estimated unigram probabilities."
            },
            "venue": {
                "fragments": [],
                "text": "EUROSPEECH"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46526669"
                        ],
                        "name": "A. Kennedy",
                        "slug": "A.-Kennedy",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Kennedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kennedy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5277263"
                        ],
                        "name": "J. Pynte",
                        "slug": "J.-Pynte",
                        "structuredName": {
                            "firstName": "Jo\u00ebl",
                            "lastName": "Pynte",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pynte"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "They analysed reading times in the Dundee corpus (Kennedy and Pynte, 2005) in terms of both surprisal and DLT costs, finding that only surprisal was an effective predictor across the whole corpus."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "To do this, we will use the eye-movement data contained in the Dundee corpus (Kennedy and Pynte, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "9, respectively), Kintsch\u2019s (Kintsch (2001)) model (Equation 4."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We then evaluated these as predictors of reading times, which were derived from the English portion of the Dundee Corpus (Kennedy and Pynte, 2005), containing eye-tracking data for 10 English native speakers on 20 texts."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "(2008) measure semantic coherence in terms of LSA similarities and find this to be a significant predictor of reading times on the French part of the Dundee corpus (Kennedy and Pynte, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13291832,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "0fef86468f95ed1bffd4644a06b18a4fa17b4e7d",
            "isKey": true,
            "numCitedBy": 273,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Parafoveal-on-foveal-effects-in-normal-reading-Kennedy-Pynte",
            "title": {
                "fragments": [],
                "text": "Parafoveal-on-foveal effects in normal reading"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46831169"
                        ],
                        "name": "G. Hinton",
                        "slug": "G.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144323951"
                        ],
                        "name": "T. Shallice",
                        "slug": "T.-Shallice",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Shallice",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Shallice"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 63
                            }
                        ],
                        "text": "In many cases these are created manually by the modeler (e.g., Hinton & Shallice, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 9
                            }
                        ],
                        "text": "However, Holyoak and Hummel (2000) claim that this form of binding violates role\u2013filler independence."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14715071,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "7b62ab1607ff003300b8819e1b8a76035406e7b6",
            "isKey": false,
            "numCitedBy": 254,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "A recurrent connectionist network was trained to output semantic feature vectors when presented with letter strings. When damaged, the network exhibited characteristics that resembled several of the phenomena found in deep dyslexia and semantic-access dyslexia. Damaged networks sometimes settled to the semantic vectors for semantically similar but visually dissimilar words. With severe damage, a forced-choice decision between categories was possible even when the choice of the particular semantic vector within the category was not possible. The damaged networks typically exhibited many mixed visual and semantic errors in which the output corresponded to a word that was both visually and semantically similar. Surprisingly, damage near the output sometimes caused pure visual errors. Indeed, the characteristic error pattern of deep dyslexia occurred with damage to virtually any part of the network."
            },
            "slug": "Lesioning-an-attractor-network:-investigations-of-Hinton-Shallice",
            "title": {
                "fragments": [],
                "text": "Lesioning an attractor network: investigations of acquired dyslexia."
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A recurrent connectionist network was trained to output semantic feature vectors when presented with letter strings and typically exhibited many mixed visual and semantic errors in which the output corresponded to a word that was both visually and semantically similar."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 44
                            }
                        ],
                        "text": "The model was built using the SRILM toolkit (Stolcke, 2002) with backoff and modified Kneser-Ney smoothing (Chen and Goodman, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1988103,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "399da68d3b97218b6c80262df7963baa89dcc71b",
            "isKey": false,
            "numCitedBy": 4997,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "SRILM is a collection of C++ libraries, executable programs, and helper scripts designed to allow both production of and experimentation with statistical language models for speech recognition and other applications. SRILM is freely available for noncommercial purposes. The toolkit supports creation and evaluation of a variety of language model types based on N-gram statistics, as well as several related tasks, such as statistical tagging and manipulation of N-best lists and word lattices. This paper summarizes the functionality of the toolkit and discusses its design and implementation, highlighting ease of rapid prototyping, reusability, and combinability of tools."
            },
            "slug": "SRILM-an-extensible-language-modeling-toolkit-Stolcke",
            "title": {
                "fragments": [],
                "text": "SRILM - an extensible language modeling toolkit"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The functionality of the SRILM toolkit is summarized and its design and implementation is discussed, highlighting ease of rapid prototyping, reusability, and combinability of tools."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36568337"
                        ],
                        "name": "B. Ross",
                        "slug": "B.-Ross",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Ross",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ross"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 276,
                                "start": 266
                            }
                        ],
                        "text": "Examples include memory retrieval (Deese, 1959; Raaijmakers & Shiffrin, 1981), categorization (Estes, 1994; Nosofsky, 1984, 1986), problem solving (Holyoak & Koh, 1987; Ross, 1987, 1989), reasoning (Heit & Rubinstein, 1994; Rips, 1975), and learning (Gentner, 1989; Ross, 1984)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 45244660,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "ea126c28da4aa7f9635ebb79dba11eb4f68c6625",
            "isKey": false,
            "numCitedBy": 434,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Remindings-and-their-effects-in-learning-a-skill-Ross",
            "title": {
                "fragments": [],
                "text": "Remindings and their effects in learning a cognitive skill"
            },
            "venue": {
                "fragments": [],
                "text": "Cognitive Psychology"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6478921"
                        ],
                        "name": "A. Inhoff",
                        "slug": "A.-Inhoff",
                        "structuredName": {
                            "firstName": "Albrecht",
                            "lastName": "Inhoff",
                            "middleNames": [
                                "Werner"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Inhoff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148762"
                        ],
                        "name": "K. Rayner",
                        "slug": "K.-Rayner",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Rayner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Rayner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "As discussed, reading times are influenced by unigram word frequencies (Inhoff and Rayner, 1986; Rayner and Raney, 1996), with less frequent words taking longer to process."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Infrequent or unfamiliar words incur greater processing costs, in terms of processing them both as isolated words (Cattell, 1886) and also in context (Inhoff and Rayner, 1986; Rayner and Raney, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 31963995,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "d6769a6cb2951bfae0df2b31738e90c5e2833c24",
            "isKey": false,
            "numCitedBy": 555,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "The present experiment measured eye fixations in reading to determine whether word frequency affects the processing of the fixated word and the processing of the word to the right of the fixated word (the parafoveal word). In the experiment, subjects read sentences that contained either a critical high- or low-frequency target word. High- and low-frequency targets were matched on word length and a number of other variables. In one condition, parafoveal visual information to the right of the fixated word was denied or distorted; in other conditions, information about the parafoveal word to the right of the fixated word was available. The main results showed shorter fixations on high-frequency than on low-frequency target words. Furthermore, readers gained more effective previews from high-frequency parafoveal target words than from low-frequency parafoveal target words."
            },
            "slug": "Parafoveal-word-processing-during-eye-fixations-in-Inhoff-Rayner",
            "title": {
                "fragments": [],
                "text": "Parafoveal word processing during eye fixations in reading: Effects of word frequency"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Eye fixations in reading showed shorter fixations on high-frequency than on low-frequency target words, and readers gained more effective previews from high- frequencies parafoveal target words than from low- frequencies."
            },
            "venue": {
                "fragments": [],
                "text": "Perception & psychophysics"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145693410"
                        ],
                        "name": "Ted Briscoe",
                        "slug": "Ted-Briscoe",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Briscoe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Briscoe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144708726"
                        ],
                        "name": "John A. Carroll",
                        "slug": "John-A.-Carroll",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Carroll",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John A. Carroll"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 122
                            }
                        ],
                        "text": "Initially, we extracted all adjective-noun, noun-noun, and verb-object combinations attested in the BNC, parsed with RASP (Briscoe and Carroll, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 33
                            }
                        ],
                        "text": "The latter was parsed with RASP (Briscoe & Carroll, 2002), a broad coverage syntactic analyzer."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 90
                            }
                        ],
                        "text": "All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll, 2002) version of the BNC."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5823614,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8fe99b6dc76dc342fe9fb47740fee40381fa13d",
            "isKey": true,
            "numCitedBy": 326,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a robust accurate domain-independent approach to statistical parsing incorporated into the new release of the ANLT toolkit, and publicly available as a research tool. The system has been used to parse many well known corpora in order to produce data for lexical acquisition efforts; it has also been used as a component in an open-domain question answering project. The performance of the system is competitive with that of statistical parsers using highly lexicalised parse selection models. However, we plan to extend the system to improve parse coverage, depth and accuracy."
            },
            "slug": "Robust-Accurate-Statistical-Annotation-of-General-Briscoe-Carroll",
            "title": {
                "fragments": [],
                "text": "Robust Accurate Statistical Annotation of General Text"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A robust accurate domain-independent approach to statistical parsing incorporated into the new release of the ANLT toolkit, and publicly available as a research tool."
            },
            "venue": {
                "fragments": [],
                "text": "LREC"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32914745"
                        ],
                        "name": "Daoud Clarke",
                        "slug": "Daoud-Clarke",
                        "structuredName": {
                            "firstName": "Daoud",
                            "lastName": "Clarke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daoud Clarke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153160640"
                        ],
                        "name": "R. Lutz",
                        "slug": "R.-Lutz",
                        "structuredName": {
                            "firstName": "Rudi",
                            "lastName": "Lutz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lutz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2106279802"
                        ],
                        "name": "David Weir",
                        "slug": "David-Weir",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Weir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Weir"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2865790,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "05d6bb7e190f465f54e7b866838c8f4a7bf0a22f",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an algebraic approach for computing with vector based semantics. The tensor product has been proposed as a method of composition, but has the undesirable property that strings of different length are incomparable. We consider how a quotient algebra of the tensor algebra can allow such comparisons to be made, offering the possibility of data-driven models of semantic composition."
            },
            "slug": "Semantic-Composition-with-Quotient-Algebras-Clarke-Lutz",
            "title": {
                "fragments": [],
                "text": "Semantic Composition with Quotient Algebras"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is considered how a quotient algebra of the tensor algebra can allow comparisons to be made, offering the possibility of data-driven models of semantic composition."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787006"
                        ],
                        "name": "Ruifang Ge",
                        "slug": "Ruifang-Ge",
                        "structuredName": {
                            "firstName": "Ruifang",
                            "lastName": "Ge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ruifang Ge"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 2003) and natural language interfaces for robot control (Ge and Mooney, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14703535,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d920bde096e2a9d9a6c689a9fbbaddaba1aa452",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new approach to learning a semantic parser (a system that maps natural language sentences into logical form). Unlike previous methods, it exploits an existing syntactic parser to produce disambiguated parse trees that drive the compositional semantic interpretation. The resulting system produces improved results on standard corpora on natural language interfaces for database querying and simulated robot control."
            },
            "slug": "Learning-a-Compositional-Semantic-Parser-using-an-Ge-Mooney",
            "title": {
                "fragments": [],
                "text": "Learning a Compositional Semantic Parser using an Existing Syntactic Parser"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "A new approach to learning a semantic parser that exploits an existing syntactic parser to produce disambiguated parse trees that drive the compositional semantic interpretation."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2693903"
                        ],
                        "name": "S. Pinker",
                        "slug": "S.-Pinker",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Pinker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Pinker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 143765897,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "d6ac23dab945332ac0cffada2bfb3400331dc8f0",
            "isKey": false,
            "numCitedBy": 5098,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In this \"extremely valuable book, very informative, and very well written\" (Noam Chomsky), one of the greatest thinkers in the field of linguistics explains how language works--how people, ny making noises with their mouths, can cause ideas to arise in other people's minds."
            },
            "slug": "The-Language-Instinct-Pinker",
            "title": {
                "fragments": [],
                "text": "The Language Instinct"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2859277"
                        ],
                        "name": "T. Plate",
                        "slug": "T.-Plate",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Plate",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Plate"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 88
                            }
                        ],
                        "text": "We relate a number of existing proposals, such as vector addition, circular convolution (Widdows, 2008; Plate, 1991), and Kintsch\u2019s (2001) model, to our framework and also develop a number of novel functions, such as the simple multiplicative and dilation models."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 226
                            }
                        ],
                        "text": "Connectionist representations, like their distributional counterparts, are essentially vectors, and there is a substantial literature concerned with their representational capacities in comparison to the symbolic alternatives (Fodor and Pylyshyn, 1988; Smolensky, 1990; Pollack, 1990; Plate, 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 37
                            }
                        ],
                        "text": "Holographic reduced representations (Plate, 1991) are one implementation of this idea where the tensor product is projected onto the space of the original vectors, thus avoiding any dimensionality increase."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 177
                            }
                        ],
                        "text": "As an alternative to simple vector addition, one set of approaches (Aerts and Czachor, 2004; Clark and Pulman, 2007; Widdows, 2008) has proposed using vector binding operations (Smolensky, 1990; Plate, 1991)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 36
                            }
                        ],
                        "text": "In particular, circular convolution (Widdows, 2008; Plate, 1991), a vector binding function, gives very weak results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1197280,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a7a9eeb64ec4511ed1415baa4716da15e2897641",
            "isKey": true,
            "numCitedBy": 127,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "A solution to the problem of representing compo-sitional structure using distributed representations is described. The method uses circular convolution to associate items, which are represented by v ec-tors. Arbitrary variable bindings, short sequences of various lengths, frames, and reduced representations can be compressed into a xed width vector. These representations are items in their own right, and can be used in constructing compositional structures. The noisy reconstructions given by convolution memories can be cleaned up by using a separate associative memory that has good recon-structive properties."
            },
            "slug": "Holographic-Reduced-Representations:-Convolution-Plate",
            "title": {
                "fragments": [],
                "text": "Holographic Reduced Representations: Convolution Algebra for Compositional Distributed Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "A solution to the problem of representing compo-sitional structure using distributed representations is described, which uses circular convolution to associate items, which are represented by v ec-tors."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2993988"
                        ],
                        "name": "M. Brysbaert",
                        "slug": "M.-Brysbaert",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Brysbaert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Brysbaert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6318398"
                        ],
                        "name": "F. Vitu",
                        "slug": "F.-Vitu",
                        "structuredName": {
                            "firstName": "Fran\u00e7oise",
                            "lastName": "Vitu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Vitu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Comprehension processes in reading can be studied in terms of whether words are skipped or fixated (Brysbaert and Vitu, 1998), when regressions occur (Vitu et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1321900,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "b0e6731ab08a9cd35cfaa1d1e0b57783c31bf4bd",
            "isKey": false,
            "numCitedBy": 270,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Word-skipping:-Implications-for-theories-of-eye-in-Brysbaert-Vitu",
            "title": {
                "fragments": [],
                "text": "Word skipping: Implications for theories of eye movement control in reading"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684407"
                        ],
                        "name": "U. Furbach",
                        "slug": "U.-Furbach",
                        "structuredName": {
                            "firstName": "Ulrich",
                            "lastName": "Furbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Furbach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716883"
                        ],
                        "name": "Ingo Gl\u00f6ckner",
                        "slug": "Ingo-Gl\u00f6ckner",
                        "structuredName": {
                            "firstName": "Ingo",
                            "lastName": "Gl\u00f6ckner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ingo Gl\u00f6ckner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39612865"
                        ],
                        "name": "H. Helbig",
                        "slug": "H.-Helbig",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Helbig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Helbig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3062822"
                        ],
                        "name": "Bj\u00f6rn Pelzer",
                        "slug": "Bj\u00f6rn-Pelzer",
                        "structuredName": {
                            "firstName": "Bj\u00f6rn",
                            "lastName": "Pelzer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bj\u00f6rn Pelzer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 1997), question answering (Furbach et al., 2010; Moldovan et al., 2003) and natural language interfaces for robot control (Ge and Mooney, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2359492,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e945603ea636e57849e68c51c223215d5153262d",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Question answering systems aim to provide concise and correct responses to arbitrary questions, communicating with the user in a natural language. This way they help making the knowledge of large textual sources accessible in an intuitive manner which goes beyond the capabilities of conventional search engines. In the LogAnswer project the universities of Hagen and Koblenz cooperate to build a German language question answering system which combines computational linguistics and automated reasoning to deduce answers from a knowledge base derived from Wikipedia."
            },
            "slug": "Logic-Based-Question-Answering-Furbach-Gl\u00f6ckner",
            "title": {
                "fragments": [],
                "text": "Logic-Based Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "In the LogAnswer project the universities of Hagen and Koblenz cooperate to build a German language question answering system which combines computational linguistics and automated reasoning to deduce answers from a knowledge base derived from Wikipedia."
            },
            "venue": {
                "fragments": [],
                "text": "KI - K\u00fcnstliche Intelligenz"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148762"
                        ],
                        "name": "K. Rayner",
                        "slug": "K.-Rayner",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Rayner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Rayner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3627238"
                        ],
                        "name": "A. Well",
                        "slug": "A.-Well",
                        "structuredName": {
                            "firstName": "Arnold",
                            "lastName": "Well",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Well"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10265572"
                        ],
                        "name": "A. Pollatsek",
                        "slug": "A.-Pollatsek",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Pollatsek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pollatsek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6174639"
                        ],
                        "name": "J. H. Bertera",
                        "slug": "J.-H.-Bertera",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bertera",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H. Bertera"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "However, information gained about words to the right of that being fixated consists mainly of its first 3 letters and total word length (Rayner et al., 1982; Lima and Inhoff, 1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 39395183,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "524a4e47375eeafdfc5da62b4e944f4e3d82d4eb",
            "isKey": false,
            "numCitedBy": 323,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "A series of experiments that examined the characteristics of useful information to the right of fixation during reading is reported. In Experiments 1 and 2, reading performance when the information available to the right of fixation was determined by a fixed number of letters was compared with reading performance when the information to the right of fixation was determined by a fixed number of words. Beyond making more letters visible, both experiments showed that preserving all of the letters of a word was of no special benefit to reading. By explicitly presenting parts of the word to the right of fixation as well as the fixated word, Experiments 3 and 4 followed up on the implication that readers utilize partial letter information from words. Both experiments showed that reading was improved by this partial information and that preserving three letters of the word to the right of fixation improved reading almost as much as presenting the entire word. The implications the results have for models of reading are discussed."
            },
            "slug": "The-availability-of-useful-information-to-the-right-Rayner-Well",
            "title": {
                "fragments": [],
                "text": "The availability of useful information to the right of fixation in reading"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A series of experiments that examined the characteristics of useful information to the right of fixation during reading showed that reading was improved by this partial information and that preserving three letters of the word to theright of fixation improved reading almost as much as presenting the entire word."
            },
            "venue": {
                "fragments": [],
                "text": "Perception & psychophysics"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35229948"
                        ],
                        "name": "S. Katz",
                        "slug": "S.-Katz",
                        "structuredName": {
                            "firstName": "Slava",
                            "lastName": "Katz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Katz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "As indicated, Experiment 1 is based on a sentence similarity task initially proposed by Kintsch (2001). In his study, Kintsch investigates how the meaning of an ambiguous verb, for example run, changes in the context of specific noun subjects, for example horse or colour."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A common method for distributing the discounted probability mass over unseen events is in terms of back off probabilities (Katz, 1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6555412,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0130277677e5b915d5cd86b3afafd77fd08eb2e",
            "isKey": false,
            "numCitedBy": 1908,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The description of a novel type of m-gram language model is given. The model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data. This solution compares favorably to other proposed methods. While the method has been developed for and successfully implemented in the IBM Real Time Speech Recognizers, its generality makes it applicable in other areas where the problem of estimating probabilities from sparse data arises."
            },
            "slug": "Estimation-of-probabilities-from-sparse-data-for-of-Katz",
            "title": {
                "fragments": [],
                "text": "Estimation of probabilities from sparse data for the language model component of a speech recognizer"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data, and compares favorably to other proposed methods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40497400"
                        ],
                        "name": "D. Moldovan",
                        "slug": "D.-Moldovan",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Moldovan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Moldovan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113262829"
                        ],
                        "name": "Christine Clark",
                        "slug": "Christine-Clark",
                        "structuredName": {
                            "firstName": "Christine",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christine Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713428"
                        ],
                        "name": "S. Harabagiu",
                        "slug": "S.-Harabagiu",
                        "structuredName": {
                            "firstName": "Sanda",
                            "lastName": "Harabagiu",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Harabagiu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2248349"
                        ],
                        "name": "Steven J. Maiorano",
                        "slug": "Steven-J.-Maiorano",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Maiorano",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven J. Maiorano"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 1997), question answering (Furbach et al., 2010; Moldovan et al., 2003) and natural language interfaces for robot control (Ge and Mooney, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 34491971,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aee98350f29a407596d782b09bb37e0f94a7a76f",
            "isKey": false,
            "numCitedBy": 198,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent TREC results have demonstrated the need for deeper text understanding methods. This paper introduces the idea of automated reasoning applied to question answering and shows the feasibility of integrating a logic prover into a Question Answering system. The approach is to transform questions and answer passages into logic representations. World knowledge axioms as well as linguistic axioms are supplied to the prover which renders a deep understanding of the relationship between question text and answer text. Moreover, the trace of the proofs provide answer justifications. The results show that the prover boosts the performance of the QA system on TREC questions by 30%."
            },
            "slug": "COGEX:-A-Logic-Prover-for-Question-Answering-Moldovan-Clark",
            "title": {
                "fragments": [],
                "text": "COGEX: A Logic Prover for Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The idea of automated reasoning applied to question answering is introduced and the feasibility of integrating a logic prover into a Question Answering system is shown."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762910"
                        ],
                        "name": "X. Phan",
                        "slug": "X.-Phan",
                        "structuredName": {
                            "firstName": "Xuan",
                            "lastName": "Phan",
                            "middleNames": [
                                "Hieu"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Phan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789308"
                        ],
                        "name": "Minh Le Nguyen",
                        "slug": "Minh-Le-Nguyen",
                        "structuredName": {
                            "firstName": "Minh",
                            "lastName": "Nguyen",
                            "middleNames": [
                                "Le"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minh Le Nguyen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060737928"
                        ],
                        "name": "S. Horiguchi",
                        "slug": "S.-Horiguchi",
                        "structuredName": {
                            "firstName": "Susumu",
                            "lastName": "Horiguchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Horiguchi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In fact, this is explicit in the work of Plate (1991) who refers to his architecture as a convolution memory and also in the name recursive auto-associative memory chosen by Pollack (1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16198890,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8907ed38d6f9c56aa406704c7354afd2472d364e",
            "isKey": false,
            "numCitedBy": 768,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a general framework for building classifiers that deal with short and sparse text & Web segments by making the most of hidden topics discovered from large-scale data collections. The main motivation of this work is that many classification tasks working with short segments of text & Web, such as search snippets, forum & chat messages, blog & news feeds, product reviews, and book & movie summaries, fail to achieve high accuracy due to the data sparseness. We, therefore, come up with an idea of gaining external knowledge to make the data more related as well as expand the coverage of classifiers to handle future data better. The underlying idea of the framework is that for each classification task, we collect a large-scale external data collection called \"universal dataset\", and then build a classifier on both a (small) set of labeled training data and a rich set of hidden topics discovered from that data collection. The framework is general enough to be applied to different data domains and genres ranging from Web search results to medical text. We did a careful evaluation on several hundred megabytes of Wikipedia (30M words) and MEDLINE (18M words) with two tasks: \"Web search domain disambiguation\" and \"disease categorization for medical text\", and achieved significant quality enhancement."
            },
            "slug": "Learning-to-classify-short-and-sparse-text-&-web-Phan-Nguyen",
            "title": {
                "fragments": [],
                "text": "Learning to classify short and sparse text & web with hidden topics from large-scale data collections"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A general framework for building classifiers that deal with short and sparse text & Web segments by making the most of hidden topics discovered from large-scale data collections that is general enough to be applied to different data domains and genres ranging from Web search results to medical text."
            },
            "venue": {
                "fragments": [],
                "text": "WWW"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2125286015"
                        ],
                        "name": "David E. Rumelhari",
                        "slug": "David-E.-Rumelhari",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhari",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David E. Rumelhari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2125288353"
                        ],
                        "name": "Geoffrey E. Hintont",
                        "slug": "Geoffrey-E.-Hintont",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hintont",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hintont"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "82961593"
                        ],
                        "name": "Ronald",
                        "slug": "Ronald",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Ronald",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070444145"
                        ],
                        "name": "J.",
                        "slug": "J.",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "J.",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J."
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058683052"
                        ],
                        "name": "Williams",
                        "slug": "Williams",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Williams",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 88
                            }
                        ],
                        "text": "Various approaches to solving such a problem exist, including back-propagation networks (Rumelhart et al., 1986) and support vector machines (Vapnik, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 237368852,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "ae3fe34be9230c98b04d68b4621c89b7dbc2d717",
            "isKey": false,
            "numCitedBy": 1037,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "delineating the absolute indigeneity of amino acids in fossils. As AMS iechniques are refined to handle smaller samples, it may also become possible to date individual amino acid enantiomers by the \u00b0C method. If one enantiomer is entirely derived from the other by racemization during diagenesis, the individual Dp. and L-enantiomers for a given amino acid should have identical \u201cC ages. Older, more poorly preserved fossils may not always prove amenable to the determination of amino acid indigeneity by the stable isotope method, as the prospects for complete replacement of indigenous amino acids with non-indigenous amino acids increases with time. As non-indigenous amino acids undergo racemization, the enantiomers may have identical isotopic compositions and still not be related to the original organisms. Such a circumstance may, however, become easier to recognize as more information becomes available concerning the distribution and stable isotopic composition of the amino acid constituents of modern representatives of fossil organisms. Also, AMS dates on individual amino acid enantiomers may, in some cases, help to clarify indigeneity problems, in particular when stratigraphic controls can be used to estimate a general age range for the fossil in question. Finally, the development of techniques for determining the stable isotopic compasition of amino acid enantiomers may enable us to establish whether non-racemic amino acids in some carbonaceous meteorites\u201d are indigenous, or result in part from terrestrial contamination. M.H.E. thanks the NSF, Division of Earth Sciences (grant | EAR-8352085) and the folowing contributors to his Presidential Young Investigator Award for partial support of this research: LETTERSTONATURE 533"
            },
            "slug": "Learning-representations-by-backpropagating-errors-Rumelhari-Hintont",
            "title": {
                "fragments": [],
                "text": "Learning representations by backpropagating errors"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158597261"
                        ],
                        "name": "John R. Anderson",
                        "slug": "John-R.-Anderson",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Anderson",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John R. Anderson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 1974), associative models (Raaijmakers and Schiffrin, 1981) and cognitive architectures such as ACT-R (Anderson, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61326110,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ccb5eb4c222e396e738e772f93f97b0c2147af49",
            "isKey": false,
            "numCitedBy": 2507,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Contents: Production Systems and the ACT-R Theory. Knowledge Representation. Performance. Learning. N. Kushmerick, C. Lebiere, Navigation and Conflict Resolution. N. Kushmerick, C. Lebiere, The Tower of Hanoi and Goal Structures. F.G. Conrad, A.T. Corbett, The LISP Tutor and Skill Acquisition. F.S. Bellezza, C.F. Boyle, The Geometry Tutor and Skill Acquisition. M.K. Singley, The Identical Elements Theory of Transfer. F.G. Conrad, A.T. Corbett, J.M. Fincham, D. Hoffman, Q. Wu, Computer Programming and Transfer. A.T. Corbett, Tutoring of Cognitive Skill. Creating Production-Rule Models. Reflections on the Theory."
            },
            "slug": "Rules-of-the-Mind-Anderson",
            "title": {
                "fragments": [],
                "text": "Rules of the Mind"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "Production Systems and the ACT-R Theory and the Identical Elements Theory of Transfer."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1826924"
                        ],
                        "name": "Brian Falkenhainer",
                        "slug": "Brian-Falkenhainer",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Falkenhainer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Falkenhainer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713121"
                        ],
                        "name": "Kenneth D. Forbus",
                        "slug": "Kenneth-D.-Forbus",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Forbus",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth D. Forbus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704065"
                        ],
                        "name": "D. Gentner",
                        "slug": "D.-Gentner",
                        "structuredName": {
                            "firstName": "Dedre",
                            "lastName": "Gentner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gentner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "On the other hand, sophisticated measures of similarity have been investigated for more complex symbolic structures, such as methods based on structural alignment (Falkenhainer et al., 1989) or representation distortion (Hahn et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8751960,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15d7aa9ce648e8d5ed950d082f10e743fe2401b4",
            "isKey": false,
            "numCitedBy": 1638,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Structure-Mapping-Engine:-Algorithm-and-Falkenhainer-Forbus",
            "title": {
                "fragments": [],
                "text": "The Structure-Mapping Engine: Algorithm and Examples"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143662525"
                        ],
                        "name": "Yuki Kamide",
                        "slug": "Yuki-Kamide",
                        "structuredName": {
                            "firstName": "Yuki",
                            "lastName": "Kamide",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuki Kamide"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In further experiments, Kamide et al. (2003) showed that such argument prediction could occur even before the relevant head had been encountered, in Japanese, a head-final language."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Two competing explanations can be used to explain these constraint effects (Kamide, 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14135200,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "57005c0581dbf676c2f63700ea6d92b0b442267b",
            "isKey": false,
            "numCitedBy": 139,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "Anticipation is an essential ability for the human cognitive system to survive in its surrounding environment. The present article will review previous research on anticipatory processes in sentence processing (comprehension). I start by pointing out past research carried out with inadequate methods, then move on to reviewing recent research with relatively new, more appropriate methods, specifically, the so-called \u2018visual-world\u2019 eye-tracking paradigm, and neuropsychological techniques. I then discuss remaining unresolved issues, both methodological and theoretical."
            },
            "slug": "Anticipatory-Processes-in-Sentence-Processing-Kamide",
            "title": {
                "fragments": [],
                "text": "Anticipatory Processes in Sentence Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "Previous research on anticipatory processes in sentence processing (comprehension) is reviewed, pointing out past research carried out with inadequate methods, then reviewing recent research with relatively new, more appropriate methods, specifically, the so-called \u2018visual-world\u2019 eye-tracking paradigm, and neuropsychological techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Lang. Linguistics Compass"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7465342"
                        ],
                        "name": "Ido Dagan",
                        "slug": "Ido-Dagan",
                        "structuredName": {
                            "firstName": "Ido",
                            "lastName": "Dagan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ido Dagan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2807469"
                        ],
                        "name": "Oren Glickman",
                        "slug": "Oren-Glickman",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Glickman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Glickman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712352"
                        ],
                        "name": "B. Magnini",
                        "slug": "B.-Magnini",
                        "structuredName": {
                            "firstName": "Bernardo",
                            "lastName": "Magnini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Magnini"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "More generally, entailment (Dagan et al., 2006) and paraphrasing tasks (Dolan et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Similarly, we could study the extent of semantic similarity between words (Rubenstein and Goodenough, 1965), or analyse the conditions under which the meaning of one sentence is contained in or implied by the other (Dagan et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8587959,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de794d50713ea5f91a7c9da3d72041e2f5ef8452",
            "isKey": false,
            "numCitedBy": 1762,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents the Third PASCAL Recognising Textual Entailment Challenge (RTE-3), providing an overview of the dataset creating methodology and the submitted systems. In creating this year's dataset, a number of longer texts were introduced to make the challenge more oriented to realistic scenarios. Additionally, a pool of resources was offered so that the participants could share common tools. A pilot task was also set up, aimed at differentiating unknown entailments from identified contradictions and providing justifications for overall system decisions. 26 participants submitted 44 runs, using different approaches and generally presenting new entailment models and achieving higher scores than in the previous challenges."
            },
            "slug": "The-PASCAL-Recognising-Textual-Entailment-Challenge-Dagan-Glickman",
            "title": {
                "fragments": [],
                "text": "The PASCAL Recognising Textual Entailment Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper presents the Third PASCAL Recognising Textual Entailment Challenge (RTE-3), providing an overview of the dataset creating methodology and the submitted systems."
            },
            "venue": {
                "fragments": [],
                "text": "MLCW"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2189054"
                        ],
                        "name": "S. Liversedge",
                        "slug": "S.-Liversedge",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Liversedge",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Liversedge"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13444290"
                        ],
                        "name": "K. Paterson",
                        "slug": "K.-Paterson",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Paterson",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Paterson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2425749"
                        ],
                        "name": "M. Pickering",
                        "slug": "M.-Pickering",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Pickering",
                            "middleNames": [
                                "John"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pickering"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 36
                            }
                        ],
                        "text": ", 1998) and how long fixations last (Liversedge et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60866576,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "eaee95c1d3c2c60a6289196eb0970cb0b0507318",
            "isKey": false,
            "numCitedBy": 167,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Chapter-3-\u2013-Eye-Movements-and-Measures-of-Reading-Liversedge-Paterson",
            "title": {
                "fragments": [],
                "text": "Chapter 3 \u2013 Eye Movements and Measures of Reading Time"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767844"
                        ],
                        "name": "Diederik Aerts",
                        "slug": "Diederik-Aerts",
                        "structuredName": {
                            "firstName": "Diederik",
                            "lastName": "Aerts",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diederik Aerts"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3278816"
                        ],
                        "name": "M. Czachor",
                        "slug": "M.-Czachor",
                        "structuredName": {
                            "firstName": "Marek",
                            "lastName": "Czachor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Czachor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 82
                            }
                        ],
                        "text": "Tensor products have been recently proposed as an alternative to vector addition (Aerts & Czachor, 2004; Clark & Pulman, 2007; Widdows, 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 196
                            }
                        ],
                        "text": "us to differentiate between additive and multiplicative composition models and encompassed several existing proposals, including simple vector addition, Kintsch\u2019s (2001) model, the tensor product (Aerts and Czachor, 2004; Clark and Pulman, 2007; Widdows, 2008), and circular convolution (Widdows, 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 12
                            }
                        ],
                        "text": "Recent work (Aerts and Czachor, 2004; Clark and Pulman, 2007; Widdows, 2008), has examined this possibility in more depth, and in Chapter 5 we will evaluate the tensor product and circular convolution as models of semantic composition on their ability to predict similarity judgements for short phrases."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 17
                            }
                        ],
                        "text": "Some recent work (Aerts and Czachor, 2004; Clark and Pulman, 2007; Widdows, 2008) has drawn on this literature in addressing the problem of semantic composition within distributional models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 67
                            }
                        ],
                        "text": "As an alternative to simple vector addition, one set of approaches (Aerts and Czachor, 2004; Clark and Pulman, 2007; Widdows, 2008) has proposed using vector binding operations (Smolensky, 1990; Plate, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16701954,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "ec013e75fb6486994455db8c31d2feef71bb63a6",
            "isKey": true,
            "numCitedBy": 119,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "A zipper tooth has a body portion with an opening at one end for connection to a tape to form a zipper stringer of a slide fastener. At the other end of the body portion is an engaging head. Just behind the engaging head are a pair of grooves, one on each side of the body portion. The grooves are contoured to accommodate portions of the engaging heads of other similar zipper teeth. At each end of each of the grooves are outwardly extending tapered projections. The projections are rolled over to provide endwalls for the grooves so that a cavity is formed for locking accepting portions of engaging heads of other zipper teeth."
            },
            "slug": "LETTER-TO-THE-EDITOR:-Quantum-aspects-of-semantic-Aerts-Czachor",
            "title": {
                "fragments": [],
                "text": "LETTER TO THE EDITOR: Quantum aspects of semantic analysis and symbolic artificial intelligence"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "A zipper tooth has a body portion with an opening at one end for connection to a tape to form a zipper stringer of a slide fastener to accommodate portions of the engaging heads of other similar zipper teeth."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 205001834,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "052b1d8ce63b07fec3de9dbb583772d860b7c769",
            "isKey": false,
            "numCitedBy": 20332,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1."
            },
            "slug": "Learning-representations-by-back-propagating-errors-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning representations by back-propagating errors"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "Back-propagation repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector, which helps to represent important features of the task domain."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "104552353"
                        ],
                        "name": "J. Locke",
                        "slug": "J.-Locke",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Locke",
                            "middleNames": [
                                "Courtenay"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Locke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4121929,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "35fa04e14704eee0f089c506caab3a5cca6207ba",
            "isKey": false,
            "numCitedBy": 4087,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "IT is possible to appreciate the reason for this abridged edition of Locke's great Essay and at the same time to regret its appearance. If it had to be done, no living philosopher is so qualified to do it well as Prof. Pringle-Pattison. It is issued for academic reasons and appeals to academic purposes. Every teacher of philosophy knows that it is useless and undesirable to expect the student to read the whole work intensively as he must read, say, Spinoza's \u201cEthics\u201d or Kant's \u201cCritique of Pure Reason\u201d if he would understand those philosophers. Locke is prolix and also a large part of the Essay has lost the interest and influence it had. On the other hand, nothing is easier than to direct the student as to what is important and what comparatively negligible. Abridgments are unwise. However well-informed, it is certain, for it is human nature, that the reader will suspect that something of importance has escaped or been suppressed.An Essay concerning Human Understanding.JohnLockeBy. Abridged and edited by A. S. Pringle-Pattison. Pp. xlviii + 380. (Oxford: At the Clarendon Press; London: Oxford University Press, 1924.) 8s. 6d. net."
            },
            "slug": "An-Essay-concerning-Human-Understanding-Locke",
            "title": {
                "fragments": [],
                "text": "An Essay concerning Human Understanding"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1924
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35400286"
                        ],
                        "name": "Z. Harris",
                        "slug": "Z.-Harris",
                        "structuredName": {
                            "firstName": "Zellig",
                            "lastName": "Harris",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Harris"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 133
                            }
                        ],
                        "text": "Despite their differences, they are all based on the same premise: Words occurring within similar contexts are semantically similar (Harris, 1968)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 32164517,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "99eadd5e29a85f30cafef7f2c915f384715e3b89",
            "isKey": false,
            "numCitedBy": 986,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We may not be able to make you love reading, but mathematical structures of language will lead you to love reading starting from now. Book is the window to open the new world. The world that you want is in the better stage and level. World will always guide you to even the prestige stage of the life. You know, this is some of how reading will give you the kindness. In this case, more books you read more knowledge you know, but it can mean also the bore is full."
            },
            "slug": "Mathematical-structures-of-language-Harris",
            "title": {
                "fragments": [],
                "text": "Mathematical structures of language"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The authors may not be able to make you love reading, but mathematical structures of language will lead you to love reading starting from now."
            },
            "venue": {
                "fragments": [],
                "text": "Interscience tracts in pure and applied mathematics"
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15291527,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff2c2e3e83d1e8828695484728393c76ee07a101",
            "isKey": false,
            "numCitedBy": 15710,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The fundamental principles, basic mechanisms, and formal analyses involved in the development of parallel distributed processing (PDP) systems are presented in individual chapters contributed by leading experts. Topics examined include distributed representations, PDP models and general issues in cognitive science, feature discovery by competitive learning, the foundations of harmony theory, learning and relearning in Boltzmann machines, and learning internal representations by error propagation. Consideration is given to linear algebra in PDP, the logic of additive functions, resource requirements of standard and programmable nets, and the P3 parallel-network simulating system."
            },
            "slug": "Parallel-distributed-processing:-explorations-in-of-Rumelhart-McClelland",
            "title": {
                "fragments": [],
                "text": "Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148762"
                        ],
                        "name": "K. Rayner",
                        "slug": "K.-Rayner",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Rayner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Rayner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3627238"
                        ],
                        "name": "A. Well",
                        "slug": "A.-Well",
                        "structuredName": {
                            "firstName": "Arnold",
                            "lastName": "Well",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Well"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Evidence also indicates that the extent to which this partial structure constrains the subsequent input affects the processing load (Ehrlich and Rayner, 1981; Rayner and Well, 1996), with more constrained words having shorter fixations and being skipped more."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 45444783,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "831d12d117f8313d36eac6f46670268b8bbc2331",
            "isKey": false,
            "numCitedBy": 416,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "The effect of contextual constraint on eye movements in reading was examined by asking subjects to read sentences that contained a target word that varied in contextual constraint; high-, medium-, or low-constraint target words were used. Subjects fixated low-constraint target words longer than they did either high- or medium-constraint target words. In addition, they skipped high-constraint words more than they did either medium- or low-constraint target words. The results further confirm that contextual constraint has a strong influence on eye movements during reading."
            },
            "slug": "Effects-of-contextual-constraint-on-eye-movements-A-Rayner-Well",
            "title": {
                "fragments": [],
                "text": "Effects of contextual constraint on eye movements in reading: A further examination"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "It is confirmed that contextual constraint has a strong influence on eye movements during reading by asking subjects to read sentences that contained a target word that varied in contextual constraint."
            },
            "venue": {
                "fragments": [],
                "text": "Psychonomic bulletin & review"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37557257"
                        ],
                        "name": "U. Hahn",
                        "slug": "U.-Hahn",
                        "structuredName": {
                            "firstName": "Ulrike",
                            "lastName": "Hahn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Hahn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803359"
                        ],
                        "name": "N. Chater",
                        "slug": "N.-Chater",
                        "structuredName": {
                            "firstName": "Nick",
                            "lastName": "Chater",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Chater"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38814489"
                        ],
                        "name": "L. B. Richardson",
                        "slug": "L.-B.-Richardson",
                        "structuredName": {
                            "firstName": "Lucy",
                            "lastName": "Richardson",
                            "middleNames": [
                                "Brenda",
                                "Clare"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. B. Richardson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 1989) or representation distortion (Hahn et al., 2003)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5743682,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "068d75d081aebeef31be1ebfeed9ddd809a9a8ee",
            "isKey": false,
            "numCitedBy": 205,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Similarity-as-transformation-Hahn-Chater",
            "title": {
                "fragments": [],
                "text": "Similarity as transformation"
            },
            "venue": {
                "fragments": [],
                "text": "Cognition"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706504"
                        ],
                        "name": "J. Hopcroft",
                        "slug": "J.-Hopcroft",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopcroft",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopcroft"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742391"
                        ],
                        "name": "J. Ullman",
                        "slug": "J.-Ullman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Ullman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ullman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "If we assume that the parse trees are derived from a grammar in Chomsky Normal Form (Hopcroft and Ullman, 1979), then each binary node involves composing a pair of constituents."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 31901407,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41a88a490d7ba9e383ecb16c4290083413a08258",
            "isKey": false,
            "numCitedBy": 13820,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Introduction-to-Automata-Theory,-Languages-and-Hopcroft-Ullman",
            "title": {
                "fragments": [],
                "text": "Introduction to Automata Theory, Languages and Computation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6318398"
                        ],
                        "name": "F. Vitu",
                        "slug": "F.-Vitu",
                        "structuredName": {
                            "firstName": "Fran\u00e7oise",
                            "lastName": "Vitu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Vitu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2994100"
                        ],
                        "name": "G. McConkie",
                        "slug": "G.-McConkie",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "McConkie",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. McConkie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35779873"
                        ],
                        "name": "P. Kerr",
                        "slug": "P.-Kerr",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Kerr",
                            "middleNames": [
                                "William"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kerr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398984373"
                        ],
                        "name": "J. O'Regan",
                        "slug": "J.-O'Regan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "O'Regan",
                            "middleNames": [
                                "Kevin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. O'Regan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In addition, we included a quadratic landing position term in our set of candidate predictors, to allow the model to capture the inverted optimal viewing position effect (Vitu et al., 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The length of a saccade also affects reading time, as does where the saccade lands in a word (Vitu et al., 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16932869,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "ceff4908577b98eece69a1e06eb992274d07b6b8",
            "isKey": false,
            "numCitedBy": 194,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Fixation-location-effects-on-fixation-durations-an-Vitu-McConkie",
            "title": {
                "fragments": [],
                "text": "Fixation location effects on fixation durations during reading: an inverted optimal viewing position effect"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144999249"
                        ],
                        "name": "G. Kane",
                        "slug": "G.-Kane",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Kane",
                            "middleNames": [
                                "Stanley"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kane"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 71267845,
            "fieldsOfStudy": [
                "Computer Science",
                "History"
            ],
            "id": "78f6f0ac3d501cb0073a7d94edde5267044a59ae",
            "isKey": false,
            "numCitedBy": 2758,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural Computing: Theory and Practice , by Philip Wasserman, 230 pp, $41.95, with illus, ISBN 0-442-20743-3, New York, NY, Van Nostrand Reinhold, 1989. Neural Networks: A Tutorial , by Michael Chester, 182 pp, $38, with illus, ISBN 0-13-368903-4, Englewood Cliffs, NJ, Prentice Hall, 1993. Neural Networks: Algorithms, Applications, and Programming Techniques , by James Freeman and David Skapura, 401 pp, $50.50, with illus, ISBN 0-201-51376-5, Reading, Mass, Addison-Wesley, 1991. Understanding Neural Networks: Computer Explorations , vol 1: Basic Networks , vol 2: Advanced Networks , 309, 367 pp, by Maureen Caudill and Charles Butler, paper, with illus, spiral-bound, with 1 diskette/vol, $39.95/vol, vol 1: ISBN0-262-53102-X (Macintosh), 0-262-53099-6 (IBM), vol 2: ISBN 0-262-53103-8 (Macintosh), 0-262-53100-3 (IBM), Cambridge, Mass, The MIT Press, 1992. Artificial neural network research began in the early 1940s, advancing in fits and starts, until the late 1960s when Minsky and Papert published Perceptrons , in which they proved that neural networks, as then conceived, can"
            },
            "slug": "Parallel-Distributed-Processing:-Explorations-in-of-Kane",
            "title": {
                "fragments": [],
                "text": "Parallel Distributed Processing: Explorations in the Microstructure of Cognition, vol 1: Foundations, vol 2: Psychological and Biological Models"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Artificial neural network research began in the early 1940s, advancing in fits and starts, until the late 1960s when Minsky and Papert published Perceptrons, in which they proved that neural networks, as then conceived, can be proved."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2994100"
                        ],
                        "name": "G. McConkie",
                        "slug": "G.-McConkie",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "McConkie",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. McConkie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148762"
                        ],
                        "name": "K. Rayner",
                        "slug": "K.-Rayner",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Rayner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Rayner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 145566872,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "a1dd086f99d646334dccf6332cc1862c45094780",
            "isKey": false,
            "numCitedBy": 1119,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A computer-based eye-movement controlled, display system was developed for the study of perceptual processes in reading. A study was conducted to identify the region from which skilled readers pick up various types of visual information during a fixation while reading. This study involved making display changes, based on eye position, in the text pattern as the subject was in the act of reading from it, and then examining the effects these changes produced on eye behavior. The results indicated that the subjects acquired word-length pattern information at least 12 to 15 character positions to the right of the fixation point, and that this information primarily influenced saccade lengths. Specific letter- and word-shape information were acquired no further than 10 character positions to the right of the fixation point."
            },
            "slug": "The-span-of-the-effective-stimulus-during-a-in-McConkie-Rayner",
            "title": {
                "fragments": [],
                "text": "The span of the effective stimulus during a fixation in reading"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746017"
                        ],
                        "name": "G. Grefenstette",
                        "slug": "G.-Grefenstette",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Grefenstette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Grefenstette"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 123
                            }
                        ],
                        "text": "One alternative approach is to incorporate the syntactic structure of contexts into the procedure for constructing vectors (Grefenstette, 1994; Lin, 1998; Pad\u00f3 and Lapata, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 235
                            }
                        ],
                        "text": "These semantic similarities can then be used in variety of tasks, including modelling semantic priming (Landuaer and Dumais, 1997; Lund and Burgess, 1996) and human similarity judgments (McDonald, 2000), automatic thesaurus extraction (Grefenstette, 1994) and word sense discrimination (Sch\u00fctze, 1998) and disambiguation (McCarthy et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 127
                            }
                        ],
                        "text": "One approach is to use word co-occurence counts that are further broken down by the syntactic dependencies between those words (Pad\u00f3 and Lapata, 2007; Lin, 1998; Grefenstette, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 80
                            }
                        ],
                        "text": "For example, they do so by defining context in terms of syntactic dependencies (Grefenstette, 1994; Lin, 1998; Pado\u0301 & Lapata, 2007) or by taking into account relational information about how roles and fillers combine to create specific factual knowledge (Dennis, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 17
                            }
                        ],
                        "text": "Syntactic models (Grefenstette, 1994; Lin, 1998; Pad\u00f3 and Lapata, 2007) take into account the syntactic relations between words in defining their contexts, and this can help improve their performance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 77
                            }
                        ],
                        "text": "This ability has been put to practical use in automatic thesaurus extraction (Grefenstette, 1994)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 263,
                                "start": 245
                            }
                        ],
                        "text": "\u2026themselves (Lund & Burgess, 1996), larger linguistic units such as paragraphs or documents (Landauer & Dumais, 1997), or even more complex linguistic representations such as n-grams (Jones & Mewhort, 2007) and the argument slots of predicates (Grefenstette, 1994; Lin, 1998; Pado\u0301 & Lapata, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59167516,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4471e3117cdac2fae74d305d54b237bb3addd749",
            "isKey": true,
            "numCitedBy": 873,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface. 1. Introduction. 2. Semantic Extraction. 3. Sextant. 4. Evaluation. 5. Applications. 6. Conclusion. 1: Preprocesors. 2. Webster Stopword List. 3: Similarity List. 4: Semantic Clustering. 5: Automatic Thesaurus Generation. 6. Corpora Treated. Index."
            },
            "slug": "Explorations-in-automatic-thesaurus-discovery-Grefenstette",
            "title": {
                "fragments": [],
                "text": "Explorations in automatic thesaurus discovery"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The aim of this monograph is to provide a catalog of words and phrases used in ThesaurusGeneration, as well as some examples of other writers' work, which have been used in similar contexts."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38663378"
                        ],
                        "name": "J. Fodor",
                        "slug": "J.-Fodor",
                        "structuredName": {
                            "firstName": "Jerry",
                            "lastName": "Fodor",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Fodor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113061150"
                        ],
                        "name": "E. Lepore",
                        "slug": "E.-Lepore",
                        "structuredName": {
                            "firstName": "Ernie",
                            "lastName": "Lepore",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Lepore"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "As a consequence, the contrasts between their structures and capabilities have generated substantial controversy over their relative merits as models of cognition (Fodor and Pylyshyn, 1988; Fodor and Lepore, 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61077014,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "6843890926bf0e5c887ffc78dcb1203135981bf1",
            "isKey": false,
            "numCitedBy": 227,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. WHY MEANING PROBABLY ISN'T CONCEPTUAL ROLE (1991) 5. THE EMPTINESS OF THE LEXICON (1998) 7. BRANDOM'S BURDENS: CRITICAL STUDY OF BRANDOM'S ARTICULATING REASONS"
            },
            "slug": "The-compositionality-papers-Fodor-Lepore",
            "title": {
                "fragments": [],
                "text": "The compositionality papers"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The author examines the role that language plays in the development of identity and the search for meaning in the stories of dominant culture."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148762"
                        ],
                        "name": "K. Rayner",
                        "slug": "K.-Rayner",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Rayner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Rayner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A wealth of evidence demonstrates the strong relation between eye-movements and cognitive activity during reading (Rayner, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3015502,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "87c8a7be8d5e2e2209e766c3e28a3e8ee5babb64",
            "isKey": false,
            "numCitedBy": 6474,
            "numCiting": 1101,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent studies of eye movements in reading and other information processing tasks, such as music reading, typing, visual search, and scene perception, are reviewed. The major emphasis of the review is on reading as a specific example of cognitive processing. Basic topics discussed with respect to reading are (a) the characteristics of eye movements, (b) the perceptual span, (c) integration of information across saccades, (d) eye movement control, and (e) individual differences (including dyslexia). Similar topics are discussed with respect to the other tasks examined. The basic theme of the review is that eye movement data reflect moment-to-moment cognitive processes in the various tasks examined. Theoretical and practical considerations concerning the use of eye movement data are also discussed."
            },
            "slug": "Eye-movements-in-reading-and-information-20-years-Rayner",
            "title": {
                "fragments": [],
                "text": "Eye movements in reading and information processing: 20 years of research."
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The basic theme of the review is that eye movement data reflect moment-to-moment cognitive processes in the various tasks examined."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological bulletin"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1908277"
                        ],
                        "name": "G. Underwood",
                        "slug": "G.-Underwood",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Underwood",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Underwood"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 142459780,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "9baf0719b11e44d746c28dedb3950230a74b427a",
            "isKey": false,
            "numCitedBy": 408,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Eye-guidance-in-reading-and-scene-perception-Underwood",
            "title": {
                "fragments": [],
                "text": "Eye guidance in reading and scene perception"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148762"
                        ],
                        "name": "K. Rayner",
                        "slug": "K.-Rayner",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Rayner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Rayner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4610239"
                        ],
                        "name": "G. Raney",
                        "slug": "G.-Raney",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Raney",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Raney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "As discussed, reading times are influenced by unigram word frequencies (Inhoff and Rayner, 1986; Rayner and Raney, 1996), with less frequent words taking longer to process."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Infrequent or unfamiliar words incur greater processing costs, in terms of processing them both as isolated words (Cattell, 1886) and also in context (Inhoff and Rayner, 1986; Rayner and Raney, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 33379414,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "44d9a258c4326eba9772a7a75dc79661217839d3",
            "isKey": false,
            "numCitedBy": 158,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Eye movements were recorded as subjects either read text or searched through texts for a target word. In the reading task, there was a robust word frequency effect wherein readers looked longer at low-frequency words than at high-frequency words. However, there was no frequency effect in the search task. The results suggest that decisions to move the eyes during reading are made on a different basis than they are during visual search. Implications for current models of eye movement control in reading are discussed."
            },
            "slug": "Eye-movement-control-in-reading-and-visual-search:-Rayner-Raney",
            "title": {
                "fragments": [],
                "text": "Eye movement control in reading and visual search: Effects of word frequency"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The results suggest that decisions to move the eyes during reading are made on a different basis than they are during visual search, and implications for current models of eye movement control in reading are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Psychonomic bulletin & review"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48603538"
                        ],
                        "name": "K. Ackermann",
                        "slug": "K.-Ackermann",
                        "structuredName": {
                            "firstName": "Karin",
                            "lastName": "Ackermann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Ackermann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 110
                            }
                        ],
                        "text": "An alternative to semantic networks is the idea that word meaning can be described in terms of feature lists (Smith & Medin, 1981)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 63086488,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "8b920b84b54425b3334f7971573c455703715fc3",
            "isKey": false,
            "numCitedBy": 399,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Thank you for downloading categories and concepts. Maybe you have knowledge that, people have look hundreds times for their chosen novels like this categories and concepts, but end up in malicious downloads. Rather than enjoying a good book with a cup of tea in the afternoon, instead they are facing with some infectious virus inside their laptop. categories and concepts is available in our book collection an online access to it is set as public so you can get it instantly. Our book servers spans in multiple locations, allowing you to get the most less latency time to download any of our books like this one. Merely said, the categories and concepts is universally compatible with any devices to read."
            },
            "slug": "Categories-and-Concepts-Ackermann",
            "title": {
                "fragments": [],
                "text": "Categories and Concepts"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The categories and concepts is universally compatible with any devices to read and is available in the book collection an online access to it is set as public so you can get it instantly."
            },
            "venue": {
                "fragments": [],
                "text": "Job 28. Cognition in Context"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400881722"
                        ],
                        "name": "W. Marslen-Wilson",
                        "slug": "W.-Marslen-Wilson",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Marslen-Wilson",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Marslen-Wilson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This incrementality can be useful practically, in integrating with the other components of a speech recognition system, and also cognitively consistent with the incrementality of the human language processor (Tanenhaus et al., 1995; Marslen-Wilson, 1973; Konieczny, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "There is certainly evidence that interpretation proceeds word by word, with syntactic and semantic information being integrated into an incremental representation as it becomes available (Tanenhaus et al., 1995; Marslen-Wilson, 1973; Konieczny, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4220775,
            "fieldsOfStudy": [
                "Physics",
                "Psychology"
            ],
            "id": "1a6b2d237b873703b3f0af644f7953e3f2c7116e",
            "isKey": false,
            "numCitedBy": 433,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "SPEECH shadowing is an experimental task in which the subject is required to repeat (shadow) speech as he hears it. When the shadower is presented with a sentence, he will start to repeat it before he has heard all of it. The response latency to each word of a sentence can therefore be measured."
            },
            "slug": "Linguistic-Structure-and-Speech-Shadowing-at-Very-Marslen-Wilson",
            "title": {
                "fragments": [],
                "text": "Linguistic Structure and Speech Shadowing at Very Short Latencies"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This paper presents an experimental task in which the subject is required to repeat (shadow) speech as he hears it, and the response latency to each word of a sentence is measured."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143694777"
                        ],
                        "name": "Frank Keller",
                        "slug": "Frank-Keller",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Keller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frank Keller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3613636"
                        ],
                        "name": "S. Gunasekharan",
                        "slug": "S.-Gunasekharan",
                        "structuredName": {
                            "firstName": "Subahshini",
                            "lastName": "Gunasekharan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gunasekharan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32498452"
                        ],
                        "name": "N. Mayo",
                        "slug": "N.-Mayo",
                        "structuredName": {
                            "firstName": "Neil",
                            "lastName": "Mayo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Mayo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1996123"
                        ],
                        "name": "M. Corley",
                        "slug": "M.-Corley",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Corley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Corley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17047232,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "8db3dc5390a0fc7bd4e605aa493eaf5e42abaea6",
            "isKey": false,
            "numCitedBy": 119,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Although Internet-based experiments are gaining in popularity, most studies rely on directly evaluating participants\u2019 responses rather than response times. In the present article, we present two experiments that demonstrate the feasibility of collecting response latency data over the World-Wide Web using WebExp\u2014a software package designed to run psychological experiments over the Internet. Experiment 1 uses WebExp to collect measurements for known time intervals (generated using keyboard repetition). The resulting measurements are found to be accurate across platforms and load conditions. In Experiment 2, we use WebExp to replicate a lab-based self-paced reading study from the psycholinguistic literature. The data of the Web-based replication correlate significantly with those of the original study and show the same main effects and interactions. We conclude that WebExp can be used to obtain reliable response time data, at least for the self-paced reading paradigm."
            },
            "slug": "Timing-accuracy-of-Web-experiments:-A-case-study-Keller-Gunasekharan",
            "title": {
                "fragments": [],
                "text": "Timing accuracy of Web experiments: A case study using the WebExp software package"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Two experiments demonstrate the feasibility of collecting response latency data over the World-Wide Web using WebExp\u2014a software package designed to run psychological experiments over the Internet and conclude that WebExp can be used to obtain reliable response time data, at least for the self-paced reading paradigm."
            },
            "venue": {
                "fragments": [],
                "text": "Behavior research methods"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2994100"
                        ],
                        "name": "G. McConkie",
                        "slug": "G.-McConkie",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "McConkie",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. McConkie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148762"
                        ],
                        "name": "K. Rayner",
                        "slug": "K.-Rayner",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Rayner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Rayner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 145740740,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "2c366cd212b3b4abdab00ea7f44ca95d7a972bad",
            "isKey": false,
            "numCitedBy": 303,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "An on-line computer technique was used to determine whether three skilled readers acquired visual information equally far to the left and right of central vision during fixations in reading. None of the subjects appeared to use visual information more than four character positions to the left of the fixation point (smaller distances were not tested), though all of them acquired visual information substantially further than that to the right. Thus, the region of useful visual information in reading is asymmetric around the fixation point."
            },
            "slug": "Asymmetry-of-the-perceptual-span-in-reading-McConkie-Rayner",
            "title": {
                "fragments": [],
                "text": "Asymmetry of the perceptual span in reading"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4800025"
                        ],
                        "name": "F. Campbell",
                        "slug": "F.-Campbell",
                        "structuredName": {
                            "firstName": "Fergus",
                            "lastName": "Campbell",
                            "middleNames": [
                                "William"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Campbell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143670798"
                        ],
                        "name": "R. Wurtz",
                        "slug": "R.-Wurtz",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Wurtz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Wurtz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "During all saccades, input from the eyes is suppressed (Campbell and Wurtz, 1978; Riggs et al., 1974), and so visual information is gathered by readers entirely during fixations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8049517,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "802b34afc5d3a63d580a787918479f41de41d5ec",
            "isKey": false,
            "numCitedBy": 332,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Saccadic-omission:-Why-we-do-not-see-a-grey-out-a-Campbell-Wurtz",
            "title": {
                "fragments": [],
                "text": "Saccadic omission: Why we do not see a grey-out during a saccadic eye movement"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51966268"
                        ],
                        "name": "W. Estes",
                        "slug": "W.-Estes",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Estes",
                            "middleNames": [
                                "Kaye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Estes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 95
                            }
                        ],
                        "text": "Examples include memory retrieval (Deese, 1959; Raaijmakers & Shiffrin, 1981), categorization (Estes, 1994; Nosofsky, 1984, 1986), problem solving (Holyoak & Koh, 1987; Ross, 1987, 1989), reasoning (Heit & Rubinstein, 1994; Rips, 1975), and learning (Gentner, 1989; Ross, 1984)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 61051459,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "61e79a9d4865dfd80a73eaaaa24386c594a819c1",
            "isKey": false,
            "numCitedBy": 516,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Introduction and Basic Concepts 2. Category Structures and Categorization 3. Models for Category Learning 4. Categorization and Memory Processing 5. On the Storage and Retrieval of Categorical Information 6. Extensions and New Applications of the Exemplar-Similarity Model"
            },
            "slug": "Classification-and-cognition-Estes",
            "title": {
                "fragments": [],
                "text": "Classification and cognition"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper presents a meta-modelling framework for category learning called the Exemplar-Similarity Model, which automates the very labor-intensive and therefore time-heavy and expensive process of category classification and classification-based learning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 226
                            }
                        ],
                        "text": "Connectionist representations, like their distributional counterparts, are essentially vectors, and there is a substantial literature concerned with their representational capacities in comparison to the symbolic alternatives (Fodor and Pylyshyn, 1988; Smolensky, 1990; Pollack, 1990; Plate, 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 177
                            }
                        ],
                        "text": "As an alternative to simple vector addition, one set of approaches (Aerts and Czachor, 2004; Clark and Pulman, 2007; Widdows, 2008) has proposed using vector binding operations (Smolensky, 1990; Plate, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 31
                            }
                        ],
                        "text": "These include tensor products (Smolensky, 1990), recursive distributed representations (RAAMS, Pollack, 1990), spatter codes (Kanerva, 1988), holographic reduced representations (Plate, 1995), and convolution (Metcalfe, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 0
                            }
                        ],
                        "text": "Smolensky (1990), for example, proposed the use of tensor products as a means of binding one vector to another to produce structured representations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 125580247,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "9438172bfbb74a6a4ea4242b180d4335bb1f18b7",
            "isKey": true,
            "numCitedBy": 642,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: 1. Introduction, 2. Connectionist Representation and Tensor Product Binding: Definition and Examples, 3. Tensor Product Representation: Properties, 4. Conclusion"
            },
            "slug": "Tensor-Product-Variable-Binding-and-the-of-Symbolic-Hinton",
            "title": {
                "fragments": [],
                "text": "Tensor Product Variable Binding and the Representation of Symbolic Structures in Connectionist Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "This chapter contains sections titled connectionist Representation and Tensor Product Binding: Definition and Examples, and tensor Product Representation: Properties."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1947963"
                        ],
                        "name": "L. Riggs",
                        "slug": "L.-Riggs",
                        "structuredName": {
                            "firstName": "Lorrin",
                            "lastName": "Riggs",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Riggs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4217584"
                        ],
                        "name": "P. Merton",
                        "slug": "P.-Merton",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Merton",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Merton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2514582"
                        ],
                        "name": "H. Morton",
                        "slug": "H.-Morton",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Morton",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Morton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Following Roark (2001), we combined the probabilistic parser with a trigram model using linear interpolation (the weights were optimized on the development set)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "During all saccades, input from the eyes is suppressed (Campbell and Wurtz, 1978; Riggs et al., 1974), and so visual information is gathered by readers entirely during fixations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26281527,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "b31e03a11be0b8002a7afc6ab3c90b13e4bbed1a",
            "isKey": false,
            "numCitedBy": 148,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Suppression-of-visual-phosphenes-during-saccadic-Riggs-Merton",
            "title": {
                "fragments": [],
                "text": "Suppression of visual phosphenes during saccadic eye movements."
            },
            "venue": {
                "fragments": [],
                "text": "Vision research"
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148762"
                        ],
                        "name": "K. Rayner",
                        "slug": "K.-Rayner",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Rayner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Rayner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 146444040,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "114bc5d43e30150da1d25523991c9afb4396186d",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Models of eye movement control in reading and their impact on the field are discussed. Differences between the E-Z Reader model and the SWIFT model are reviewed, as are benchmark data that need to be accounted for by any model of eye movement control. Predictions made by the models and how models can sometimes account for counterintuitive findings are also discussed. Finally, the role of models and data in further understanding the reading process is considered."
            },
            "slug": "Eye-Movements-in-Reading:-Models-and-Data.-Rayner",
            "title": {
                "fragments": [],
                "text": "Eye Movements in Reading: Models and Data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148762"
                        ],
                        "name": "K. Rayner",
                        "slug": "K.-Rayner",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Rayner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Rayner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6478921"
                        ],
                        "name": "A. Inhoff",
                        "slug": "A.-Inhoff",
                        "structuredName": {
                            "firstName": "Albrecht",
                            "lastName": "Inhoff",
                            "middleNames": [
                                "Werner"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Inhoff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057120495"
                        ],
                        "name": "R. E. Morrison",
                        "slug": "R.-E.-Morrison",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Morrison",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. E. Morrison"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6177964"
                        ],
                        "name": "M. Slowiaczek",
                        "slug": "M.-Slowiaczek",
                        "structuredName": {
                            "firstName": "Maria",
                            "lastName": "Slowiaczek",
                            "middleNames": [
                                "Louisa"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Slowiaczek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6174639"
                        ],
                        "name": "J. H. Bertera",
                        "slug": "J.-H.-Bertera",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bertera",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H. Bertera"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "It has been shown that the required information for normal reading can be acquired within around 50ms (Rayner et al., 1981), so the remainder of fixation time is available for further processes: e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1386356,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "f20d90e2e9091cf8a5a65ff5294ccdb694d76117",
            "isKey": false,
            "numCitedBy": 418,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "A window or visual mask as moved across text in synchrony with the reader's eye movements. The size of the window or mask was varied so that either information in foveal or parafoveal vision was masked on each fixation. In another experiment, the onset of the mask was delayed for a certain amount of time following the end of the saccade. The results of the experiments point out the relative importance of foveal and parafoveal vision for reading and further indicate that most of the visual information necessary for reading can be acquired during the first 50 msec that information is available during an eye fixation."
            },
            "slug": "Masking-of-foveal-and-parafoveal-vision-during-eye-Rayner-Inhoff",
            "title": {
                "fragments": [],
                "text": "Masking of foveal and parafoveal vision during eye fixations in reading."
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The results of the experiments point out the relative importance of foveal and parafoveal vision for reading and indicate that most of the visual information necessary for reading can be acquired during the first 50 msec that information is available during an eye fixation."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of experimental psychology. Human perception and performance"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145700185"
                        ],
                        "name": "S. Weiss",
                        "slug": "S.-Weiss",
                        "structuredName": {
                            "firstName": "Sholom",
                            "lastName": "Weiss",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Weiss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3285789"
                        ],
                        "name": "C. Kulikowski",
                        "slug": "C.-Kulikowski",
                        "structuredName": {
                            "firstName": "Casimir",
                            "lastName": "Kulikowski",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Kulikowski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 58
                            }
                        ],
                        "text": "The technique is a special case of nfold cross-validation (Weiss and Kulikowski, 1991) and has been previously used for measuring how well humans agree on judging semantic similarity (Resnik and Diab, 2000; Resnik, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 60
                            }
                        ],
                        "text": "The technique is a special case of n-fold cross-validation (Weiss & Kulikowski, 1991) and has been previously used for measuring how well humans agree on judging semantic similarity (Resnik, 1999; Resnik & Diab, 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12484204,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "664b701a39371c5356754dc72cea1349233c8506",
            "isKey": false,
            "numCitedBy": 1046,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface 1 Overview of Learning Systems 1.1 What is a Learning System? 1.2 Motivation for Building Learning Systems 1.3 Types of Practical Empirical Learning Systems 1.3.1 Common Theme: The Classification Model 1.3.2 Let the Data Speak 1.4 What's New in Learning Methods 1.4.1 The Impact of New Technology 1.5 Outline of the Book 1.6 Bibliographical and Historical Remarks 2 How to Estimate the True Performance of a Learning System 2.1 The Importance of Unbiased Error Rate Estimation 2.2. What is an Error? 2.2.1 Costs and Risks 2.3 Apparent Error Rate Estimates 2.4 Too Good to Be True: Overspecialization 2.5 True Error Rate Estimation 2.5.1 The Idealized Model for Unlimited Samples 2.5.2 Train-and Test Error Rate Estimation 2.5.3 Resampling Techniques 2.5.4 Finding the Right Complexity Fit 2.6 Getting the Most Out of the Data 2.7 Classifier Complexity and Feature Dimensionality 2.7.1 Expected Patterns of Classifier Behavior 2.8 What Can Go Wrong? 2.8.1 Poor Features, Data Errors, and Mislabeled Classes 2.8.2 Unrepresentative Samples 2.9 How Close to the Truth? 2.10 Common Mistakes in Performance Analysis 2.11 Bibliographical and Historical Remarks 3 Statistical Pattern Recognition 3.1 Introduction and Overview 3.2 A Few Sample Applications 3.3 Bayesian Classifiers 3.3.1 Direct Application of the Bayes Rule 3.4 Linear Discriminants 3.4.1 The Normality Assumption and Discriminant Functions 3.4.2 Logistic Regression 3.5 Nearest Neighbor Methods 3.6 Feature Selection 3.7 Error Rate Analysis 3.8 Bibliographical and Historical Remarks 4 Neural Nets 4.1 Introduction and Overview 4.2 Perceptrons 4.2.1 Least Mean Square Learning Systems 4.2.2 How Good Is a Linear Separation Network? 4.3 Multilayer Neural Networks 4.3.1 Back-Propagation 4.3.2 The Practical Application of Back-Propagation 4.4 Error Rate and Complexity Fit Estimation 4.5 Improving on Standard Back-Propagation 4.6 Bibliographical and Historical Remarks 5 Machine Learning: Easily Understood Decision Rules 5.1 Introduction and Overview 5.2 Decision Trees 5.2.1 Finding the Perfect Tree 5.2.2 The Incredible Shrinking Tree 5.2.3 Limitations of Tree Induction Methods 5.3 Rule Induction 5.3.1 Predictive Value Maximization 5.4 Bibliographical and Historical Remarks 6 Which Technique is Best? 6.1 What's Important in Choosing a Classifier? 6.1.1 Prediction Accuracy 6.1.2 Speed of Learning and Classification 6.1.3 Explanation and Insight 6.2 So, How Do I Choose a Learning System? 6.3 Variations on the Standard Problem 6.3.1 Missing Data 6.3.2 Incremental Learning 6.4 Future Prospects for Improved Learning Methods 6.5 Bibliographical and Historical Remarks 7 Expert Systems 7.1 Introduction and Overview 7.1.1 Why Build Expert Systems? New vs. Old Knowledge 7.2 Estimating Error Rates for Expert Systems 7.3 Complexity of Knowledge Bases 7.3.1 How Many Rules Are Too Many? 7.4 Knowledge Base Example 7.5 Empirical Analysis of Knowledge Bases 7.6 Future: Combined Learning and Expert Systems 7.7 Bibliographical and Historical Remarks References Author Index Subject Index"
            },
            "slug": "Computer-Systems-That-Learn:-Classification-and-and-Weiss-Kulikowski",
            "title": {
                "fragments": [],
                "text": "Computer Systems That Learn: Classification and Prediction Methods from Statistics, Neural Nets, Machine Learning and Expert Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This book discusses how to Estimate the True Performance of a Learning System, and the Importance of Unbiased Error Rate Estimation, and Machine Learning: Easily Understood Decision Rules."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2859277"
                        ],
                        "name": "T. Plate",
                        "slug": "T.-Plate",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Plate",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Plate"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Circular convolution has been applied to memorising pen trajectories for handwritten digits (Plate, 1993) and complex semantic structures, consisting of agents playing particular roles in various actions (Plate, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13629215,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ebf62950d733a4a8f9ecd8d3752dee8d13fc8e6d",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Holographic Recurrent Networks (HRNs) are recurrent networks which incorporate associative memory techniques for storing sequential structure. HRNs can be easily and quickly trained using gradient descent techniques to generate sequences of discrete outputs and trajectories through continuous space. The performance of HRNs is found to be superior to that of ordinary recurrent networks on these sequence generation tasks."
            },
            "slug": "Holographic-Recurrent-Networks-Plate",
            "title": {
                "fragments": [],
                "text": "Holographic Recurrent Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "Holographic Recurrent Networks are recurrent networks which incorporate associative memory techniques for storing sequential structure and the performance of HRNs is found to be superior to that of ordinary recurrent networks on sequence generation tasks."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726379"
                        ],
                        "name": "K. Stenning",
                        "slug": "K.-Stenning",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Stenning",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Stenning"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698106"
                        ],
                        "name": "M. V. Lambalgen",
                        "slug": "M.-V.-Lambalgen",
                        "structuredName": {
                            "firstName": "Michiel",
                            "lastName": "Lambalgen",
                            "middleNames": [
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. V. Lambalgen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14439847,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "1ce03bdf54afc8b04f9aa71f92f020877c6902fe",
            "isKey": false,
            "numCitedBy": 430,
            "numCiting": 353,
            "paperAbstract": {
                "fragments": [],
                "text": "A new proposal for integrating the employment of formal and empirical methods in the study of human reasoning."
            },
            "slug": "Human-Reasoning-and-Cognitive-Science-Stenning-Lambalgen",
            "title": {
                "fragments": [],
                "text": "Human Reasoning and Cognitive Science"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A new proposal for integrating the employment of formal and empirical methods in the study of human reasoning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50640067"
                        ],
                        "name": "J. M. Cattell",
                        "slug": "J.-M.-Cattell",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Cattell",
                            "middleNames": [
                                "Mckeen"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. M. Cattell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 170442200,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "201343cc7f7ce7a784063b5f0990d83308ed3ec5",
            "isKey": false,
            "numCitedBy": 249,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Mental states correspond to physical changes in the brain. The object of this paper is to inquire into the time needed to bring about changes in the brain, and thus to determine the rapidity of thought. When waves in the luminiferous ether of a particular length strike the retina a red light is seen, but a certain time passes after the waves have struck the retina before the light is seen: (1) It takes time for the light waves to work on the retina, and generate in the cells a nervous impulse corresponding to the nature of the light; (2) it takes time for the nervous impulse to be conveyed along the optic nerve to the brain; (3) it takes time for the nervous impulse to be conveyed through the brain to the visual centre; and (4) it takes time for the nervous impulse to bring about changes in the visual centre corresponding to its own nature, and consequently to the nature of the external stimulus. When these changes are brought about a red light is seen. It does not take any time for a sensation or perception to arise after the proper changes in the brain have been brought about. The sensation of a red light is a state of consciousness corresponding to a certain condition of the brain. The chemical changes in a galvanic battery take time, but after they are brought about, no additional time is needed to produce the electric current. The current is the product of chemical changes in the battery, but at the same time the immediate representative of these changes; and the relation is so far analogous between states of consciousness and changes in the brain. Again, as it takes time to see a light, so it takes time to make a motion. Changes in the brain, the origin and nature of which we do not understand (physiologically they are part of the continuous life of the brain, mentally they are often given in continuous life of the brain, mentally they are often given in consciousness as a will-impulse), excite the centre for the coordination of motions. The impulse there developed is conveyed through the brain (and it may be spinal cord) to a motor nerve, and along the nerve to the muscle, which is contracted in accordance with the will-impulse. We have here in the reverse direction the same four periods as in the case of a stimulus giving rise to a sensation. In each case there is the latent period in the sense-organ or muscle, the centripetal or centrifugal time in the nerve, the centripetal or centrifugal time in the brain, and the time of growing energy in the sensory or motor centre. Besides these [p. 221] two classes of processes, the one centripetal, the other centrifugal, there are centrimanent cerebral operations, some of which are given in consciousness, and make up the mental life of thought and feeling. These cerebral changes all take time, and, as I shall show, the times can in many cases be determined."
            },
            "slug": "THE-TIME-TAKEN-UP-BY-CEREBRAL-OPERATIONS-Cattell",
            "title": {
                "fragments": [],
                "text": "THE TIME TAKEN UP BY CEREBRAL OPERATIONS"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The object of this paper is to inquire into the time needed to bring about changes in the brain, and thus to determine the rapidity of thought, and to show that the times can in many cases be determined."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1886
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054297221"
                        ],
                        "name": "Josae C. Pinheiro",
                        "slug": "Josae-C.-Pinheiro",
                        "structuredName": {
                            "firstName": "Josae",
                            "lastName": "Pinheiro",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josae C. Pinheiro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2437910"
                        ],
                        "name": "D. Bates",
                        "slug": "D.-Bates",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Bates",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bates"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117882390,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "35fa23c9f5577761ff79fd14356736570c43b68a",
            "isKey": false,
            "numCitedBy": 8715,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Linear Mixed-Effects * Theory and Computational Methods for LME Models * Structure of Grouped Data * Fitting LME Models * Extending the Basic LME Model * Nonlinear Mixed-Effects * Theory and Computational Methods for NLME Models * Fitting NLME Models"
            },
            "slug": "Mixed-Effects-Models-in-S-and-S-PLUS-Pinheiro-Bates",
            "title": {
                "fragments": [],
                "text": "Mixed-Effects Models in S and S-PLUS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670758"
                        ],
                        "name": "Jacob Cohen",
                        "slug": "Jacob-Cohen",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Cohen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacob Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054136717"
                        ],
                        "name": "P. Cohen",
                        "slug": "P.-Cohen",
                        "structuredName": {
                            "firstName": "Patricia",
                            "lastName": "Cohen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31291341"
                        ],
                        "name": "S. West",
                        "slug": "S.-West",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "West",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. West"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4246971"
                        ],
                        "name": "L. Aiken",
                        "slug": "L.-Aiken",
                        "structuredName": {
                            "firstName": "Leona",
                            "lastName": "Aiken",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Aiken"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121953269,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e2f4f64a17a05379e45f713d10d7c546bda7734a",
            "isKey": false,
            "numCitedBy": 30479,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Contents: Preface. Introduction. Bivariate Correlation and Regression. Multiple Regression/Correlation With Two or More Independent Variables. Data Visualization, Exploration, and Assumption Checking: Diagnosing and Solving Regression Problems I. Data-Analytic Strategies Using Multiple Regression/Correlation. Quantitative Scales, Curvilinear Relationships, and Transformations. Interactions Among Continuous Variables. Categorical or Nominal Independent Variables. Interactions With Categorical Variables. Outliers and Multicollinearity: Diagnosing and Solving Regression Problems II. Missing Data. Multiple Regression/Correlation and Causal Models. Alternative Regression Models: Logistic, Poisson Regression, and the Generalized Linear Model. Random Coefficient Regression and Multilevel Models. Longitudinal Regression Methods. Multiple Dependent Variables: Set Correlation. Appendices: The Mathematical Basis for Multiple Regression/Correlation and Identification of the Inverse Matrix Elements. Determination of the Inverse Matrix and Applications Thereof."
            },
            "slug": "Applied-multiple-regression/correlation-analysis-Cohen-Cohen",
            "title": {
                "fragments": [],
                "text": "Applied multiple regression/correlation analysis for the behavioral sciences"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712211"
                        ],
                        "name": "G. Golub",
                        "slug": "G.-Golub",
                        "structuredName": {
                            "firstName": "Gene",
                            "lastName": "Golub",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Golub"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1930930"
                        ],
                        "name": "F. Luk",
                        "slug": "F.-Luk",
                        "structuredName": {
                            "firstName": "Franklin",
                            "lastName": "Luk",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Luk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723232"
                        ],
                        "name": "M. Overton",
                        "slug": "M.-Overton",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Overton",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Overton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 29
                            }
                        ],
                        "text": "Singular Value Decomposition (Golub et al., 1981), a dimensionality reduction technique, is then applied to this matrix, producing lower dimensional vectors representing words and documents."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15955214,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "d4f3776437fa3a05ca7c5da701690f1f1c8291d7",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a block Lanczos method to compute the largest singular values and corresponding left and right singular vectors of a large sparse matrix. Our algorithm does not transform the matrix A but accesses it only through a user-supplied routine which computes AX or $A^t$X for a given matrix X. This paper also includes a thorough discussion of the various ways to compute the singular value decomposition of a banded upper triangular matrix; this problem arises as a subproblem to be solved during the block Lanczos procedure."
            },
            "slug": "A-Block-Lanczos-Method-for-Computing-the-Singular-a-Golub-Luk",
            "title": {
                "fragments": [],
                "text": "A Block Lanczos Method for Computing the Singular Values and Corresponding Singular Vectors of a Matrix"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A block Lanczos method is presented to compute the largest singular values and corresponding left and right singular vectors of a large sparse matrix to solve the singular value decomposition of a banded upper triangular matrix."
            },
            "venue": {
                "fragments": [],
                "text": "TOMS"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", 1986) and support vector machines (Vapnik, 1995)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7138354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8213dbed4db44e113af3ed17d6dad57471a0c048",
            "isKey": false,
            "numCitedBy": 38755,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?."
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 108
                            }
                        ],
                        "text": "While background knowledge undoubtedly contributes to the compositional process, and resources like WordNet (Fellbaum, 1998) may be used to provide this information, from a methodological perspective it is preferable to understand the fundamental processes of how representations are composed before trying to understand the interaction between existing representations and those under construction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 65
                            }
                        ],
                        "text": "A pair of landmarks were then chosen for each verb using WordNet (Fellbaum, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 240
                            }
                        ],
                        "text": "Each item consisted of two simple sentences: both having a subject-verb structure, with one based on such a construction identified in the BNC, and the other having the same noun but with the verb replaced with a synonym taken from WordNet (Fellbaum, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 109
                            }
                        ],
                        "text": "While background knowledge undoubtedly contributes to the compositional process, and resources like WordNet (Fellbaum, 1998) may be used to provide this information, from a methodological perspective it is preferable to understand the fundamental processes of how representations are composed before\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 123
                            }
                        ],
                        "text": "From these we identified a subset manifesting limited ambiguity, defined as having between two and eight senses in WordNet (Fellbaum, 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 171
                            }
                        ],
                        "text": "In order to reduce the set of items to a more manageable size and more importantly to guarantee that the phrases were indeed semantically similar, we resorted to WordNet (Fellbaum, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "WordNet: An Electronic Lexical Database (Language, Speech, and Communication)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48959839"
                        ],
                        "name": "R. Montague",
                        "slug": "R.-Montague",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Montague",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Montague"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 94
                            }
                        ],
                        "text": "Composition operations can be naturally accounted for within logic-based semantic frameworks (Montague, 1974)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 174
                            }
                        ],
                        "text": "Within symbolic logic, compositionality is accounted for elegantly by assuming a tight correspondence between syntactic expressions and semantic form (Blackburn & Bos, 2005;\nMontague, 1974)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 61
                            }
                        ],
                        "text": "This idea is fundamental in logic-based semantic frameworks (Montague, 1974), where different syntactic structures are given different function types."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60562957,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "1b5ad278a01a2c91f35c4c86fbbffc09d6fe2d72",
            "isKey": true,
            "numCitedBy": 699,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "ENGLISH-AS-A-FORMAL-LANGUAGE-Montague",
            "title": {
                "fragments": [],
                "text": "ENGLISH AS A FORMAL LANGUAGE"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145179124"
                        ],
                        "name": "I. Good",
                        "slug": "I.-Good",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Good",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Good"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Whereas this approach discounts all counts by the same amount, the Good-Turing (Good, 1953) method takes into account the number of different n-grams having a given count in determining the discounts."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "produce more reliable results than using the more restricted sets of Rubenstein and Goodenough (1965) or Miller and Charles (1991)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, the idea of representing word meaning in a geometrical space can be traced back to Osgood et al. (1957), who used elicited similarity judgments to construct semantic spaces."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11945361,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b2986b25f50babd536dd0ecf2237d9eabf5843c2",
            "isKey": true,
            "numCitedBy": 3274,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "THE-POPULATION-FREQUENCIES-OF-SPECIES-AND-THE-OF-Good",
            "title": {
                "fragments": [],
                "text": "THE POPULATION FREQUENCIES OF SPECIES AND THE ESTIMATION OF POPULATION PARAMETERS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1953
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39340971"
                        ],
                        "name": "B. Partee",
                        "slug": "B.-Partee",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Partee",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Partee"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 73
                            }
                        ],
                        "text": "a process of combining and integrating the semantics of the constituents (Frege, 1884; Pinker, 1994; Partee, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 126
                            }
                        ],
                        "text": "This property is often captured in the following principle: The meaning of a whole is a function of the meaning of the parts (Partee, 1995, p. 313)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 116976482,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "07d2993d7b5cce0058c29010d5f85d4f2dc02067",
            "isKey": false,
            "numCitedBy": 210,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Lexical-semantics-and-compositionality.-Partee",
            "title": {
                "fragments": [],
                "text": "Lexical semantics and compositionality."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144191879"
                        ],
                        "name": "J. Firth",
                        "slug": "J.-Firth",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Firth",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Firth"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The basic idea is that words with similar meanings will be found in similar contexts, and that therefore the way in which a word\u2019s occurrences are distributed across a set of contexts can be used to infer its meaning (Firth, 1957; Harris, 1954)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 208093066,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "88b3959b6f5333e5358eac43970a5fa29b54642c",
            "isKey": false,
            "numCitedBy": 1923,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Synopsis-of-Linguistic-Theory,-1930-1955-Firth",
            "title": {
                "fragments": [],
                "text": "A Synopsis of Linguistic Theory, 1930-1955"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1957
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102157754"
                        ],
                        "name": "G. Frege",
                        "slug": "G.-Frege",
                        "structuredName": {
                            "firstName": "Gottlob",
                            "lastName": "Frege",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Frege"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 73
                            }
                        ],
                        "text": "a process of combining and integrating the semantics of the constituents (Frege, 1884; Pinker, 1994; Partee, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 169330149,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "c746ff3fe16f15a5d7503f0554d2784a733b886e",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Die-Grundlagen-der-Arithmetik-Frege",
            "title": {
                "fragments": [],
                "text": "Die Grundlagen der Arithmetik"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40642524"
                        ],
                        "name": "A. Doumas",
                        "slug": "A.-Doumas",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Doumas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Doumas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725887"
                        ],
                        "name": "J. Hummel",
                        "slug": "J.-Hummel",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hummel",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hummel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Doumas and Hummel (2005) propose a model of role\u2013filler binding based on synchrony of neural firing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 105
                            }
                        ],
                        "text": "Aside from the philosophical difficulties in precisely determining what systematicity means in practice (Doumas & Hummel, 2005; Pullum & Scholz, 2007; Spenader & Blutner, 2007), it is worth noting that semantic transparency, the idea that words have meanings which remain unaffected by their\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 195
                            }
                        ],
                        "text": "In a truly compositional system, complex structures gain meaning from the simpler parts from which they are formed and the simpler components remain independent, that is, preserve their meaning (Doumas & Hummel, 2005, Doumas, Hummel, & Sandhofer, 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 164972488,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "3d45455b8c06ff7598f590bb1b98acda70d4b567",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Modeling-Human-Mental-Representations:-What-Works,-Doumas-Hummel",
            "title": {
                "fragments": [],
                "text": "Modeling Human Mental Representations: What Works, What doesn't and Why?"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117488877"
                        ],
                        "name": "Nolan Miller",
                        "slug": "Nolan-Miller",
                        "structuredName": {
                            "firstName": "Nolan",
                            "lastName": "Miller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nolan Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "89117352"
                        ],
                        "name": "G. Oldham",
                        "slug": "G.-Oldham",
                        "structuredName": {
                            "firstName": "Gerda",
                            "lastName": "Oldham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Oldham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2693903"
                        ],
                        "name": "S. Pinker",
                        "slug": "S.-Pinker",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Pinker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Pinker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 164152624,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "63458be2006a89c4b1469e49f1bed09e45929a94",
            "isKey": false,
            "numCitedBy": 1161,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Language-Instinct:-How-the-Mind-Creates-Miller-Oldham",
            "title": {
                "fragments": [],
                "text": "The Language Instinct: How the Mind Creates Language"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144268003"
                        ],
                        "name": "S. Garrod",
                        "slug": "S.-Garrod",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Garrod",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Garrod"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2213526"
                        ],
                        "name": "A. Sanford",
                        "slug": "A.-Sanford",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Sanford",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sanford"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 80
                            }
                        ],
                        "text": "Similarly, the introduction of new discourse referents incurs a processing cost (Haviland and Clark, 1974; Garrod and Sanford, 1994), and the resolution of pronouns to these referents also incurs a cost (Ehrlich and Rayner, 1983)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 158049503,
            "fieldsOfStudy": [
                "Sociology"
            ],
            "id": "01a7832c93f5dbc5eb4ea034d8e90102f2ac38e7",
            "isKey": false,
            "numCitedBy": 141,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Resolving-sentences-in-a-discourse-context:-How-Garrod-Sanford",
            "title": {
                "fragments": [],
                "text": "Resolving sentences in a discourse context: How discourse representation affects language understanding."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The basic idea is that words with similar meanings will be found in similar contexts, and that therefore the way in which a word\u2019s occurrences are distributed across a set of contexts can be used to infer its meaning (Firth, 1957; Harris, 1954)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 150037525,
            "fieldsOfStudy": [],
            "id": "5cd6b00917bb25e3565c501ddc4898fcce944e40",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Distributional Structure"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1954
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144931292"
                        ],
                        "name": "R. Cooper",
                        "slug": "R.-Cooper",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Cooper",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cooper"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "To investigate this issue more deeply, in particular to identify whether the language processor is capable of making the necessary predictions required for the former explanation, a number of reasearchers have turned to the visual world paradigm (Cooper, 1974; Tanenhaus et al., 1995; Altmann and Kamide, 1999)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 144653324,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "96723873b92a28782056d8ff6095bee238401df2",
            "isKey": false,
            "numCitedBy": 703,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-control-of-eye-fixation-by-the-meaning-of-A-new-Cooper",
            "title": {
                "fragments": [],
                "text": "The control of eye fixation by the meaning of spoken language: A new methodology for the real-time investigation of speech perception, memory, and language processing."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36568337"
                        ],
                        "name": "B. Ross",
                        "slug": "B.-Ross",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Ross",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ross"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 169
                            }
                        ],
                        "text": "Examples include memory retrieval (Deese, 1959; Raaijmakers & Shiffrin, 1981), categorization (Estes, 1994; Nosofsky, 1984, 1986), problem solving (Holyoak & Koh, 1987; Ross, 1987, 1989), reasoning (Heit & Rubinstein, 1994; Rips, 1975), and learning (Gentner, 1989; Ross, 1984)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 264
                            }
                        ],
                        "text": "The construction of higher level structures from low-level ones is fundamental not only to language but many aspects of human cognition such as analogy retrieval and processing (Eliasmith & Thagard, 2001; Plate, 2000), memory (Kanerva, 1988), and problem solving (Ross, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 144801745,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "85d9466e4dce5f59330396905c563b4829298a02",
            "isKey": false,
            "numCitedBy": 470,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Distinguishing-Types-of-Superficial-Similarities:-Ross",
            "title": {
                "fragments": [],
                "text": "Distinguishing Types of Superficial Similarities: Different Effects on the Access and Use of Earlier Problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2151306382"
                        ],
                        "name": "J. Hayes",
                        "slug": "J.-Hayes",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Hayes",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hayes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 143570129,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "d4f5ea61c1c13aa29a87afb072336113f513df5e",
            "isKey": false,
            "numCitedBy": 1046,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Cognition-and-the-development-of-language-Hayes",
            "title": {
                "fragments": [],
                "text": "Cognition and the development of language"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39340971"
                        ],
                        "name": "B. Partee",
                        "slug": "B.-Partee",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Partee",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Partee"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 158
                            }
                        ],
                        "text": "Most significantly, there is the fundamental difficulty of specifying what sort of \u2018\u2018function of the meanings of the parts\u2019\u2019 is involved in semantic composition (Partee, 2004, p. 153)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 124676946,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cfe99696be15b4edc86650f19b478a32ba48d344",
            "isKey": false,
            "numCitedBy": 119,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Compositionality-in-Formal-Semantics-Partee",
            "title": {
                "fragments": [],
                "text": "Compositionality in Formal Semantics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "More formally, we might define the meaning of a sentence to be its truth conditions, the conditions that must exist in the world to make it true (Davidson, 1967)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121167517,
            "fieldsOfStudy": [],
            "id": "93b9810b60baa1a7692d110583f1370223c96534",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Truth and Meaning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3679969"
                        ],
                        "name": "O. Spies",
                        "slug": "O.-Spies",
                        "structuredName": {
                            "firstName": "Otto",
                            "lastName": "Spies",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Spies"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119008519,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e4c25e5db186835944660b11196dfe66bd6608a0",
            "isKey": false,
            "numCitedBy": 311,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Die-grundlagen-der-arithmetik:-by-G.-Frege.-English-Spies",
            "title": {
                "fragments": [],
                "text": "Die grundlagen der arithmetik: by G. Frege. English translation by J. L. Austin. 119 pages, 14 \u00d7 22 cm. Breslau, Verlag von Wilhelm Koebner, 1884, and New York, Philosophical Library, 1950. Price, $4.75"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1950
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The model was built using the SRILM toolkit (Stolcke, 2002) with backoff and modified Kneser-Ney smoothing (Chen and Goodman, 1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 221583858,
            "fieldsOfStudy": [],
            "id": "769dbbe88801b57a9b44f89c5516264f16cbed60",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An empirical study of smoothing techniques for language modeling"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144541931"
                        ],
                        "name": "Edward E. Smith",
                        "slug": "Edward-E.-Smith",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Smith",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward E. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6973925"
                        ],
                        "name": "E. Shoben",
                        "slug": "E.-Shoben",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Shoben",
                            "middleNames": [
                                "Joseph",
                                "Jr."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Shoben"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1969651"
                        ],
                        "name": "L. Rips",
                        "slug": "L.-Rips",
                        "structuredName": {
                            "firstName": "Lance",
                            "lastName": "Rips",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rips"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "These include semantic networks (Collins and Quillian, 1969), featural models (Smith et al., 1974), associative models (Raaijmakers and Schiffrin, 1981) and cognitive architectures such as ACT-R (Anderson, 1993)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 62523339,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "697092975148f18bcacc79631c3fd9a3b557139c",
            "isKey": false,
            "numCitedBy": 1323,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Structure-and-process-in-semantic-memory:-A-model-Smith-Shoben",
            "title": {
                "fragments": [],
                "text": "Structure and process in semantic memory: A featural model for semantic decisions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69334380"
                        ],
                        "name": "J. M. Kittross",
                        "slug": "J.-M.-Kittross",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Kittross",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. M. Kittross"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 75
                            }
                        ],
                        "text": "The idea of representing word meaning in a geometrical space dates back to Osgood, Suci, and Tannenbaum (1957), who used elicited similarity judgments to construct semantic spaces."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59962008,
            "fieldsOfStudy": [
                "Sociology"
            ],
            "id": "30a3177646528a00f19d716a1a03946e3502fc9d",
            "isKey": false,
            "numCitedBy": 4074,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-measurement-of-meaning-Kittross",
            "title": {
                "fragments": [],
                "text": "The measurement of meaning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1959
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The second task was based on a TOEFL task, commonly used to evaluate distributional models (Landauer and Dumais, 1996; Bullinaria and Levy, 2007; Pad\u00f3 and Lapata, 2007), and involved identifying synonyms from amongst a set of alternatives."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We next examined the performance of these models on a TOEFL task (Landauer and Dumais, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The fundamental semantic information which can be extracted from distributional models is the similarity between pairs of representations and their evaluation is typically made in terms of tasks which rely on this property, for example priming (Lund and Burgess, 1996; Landuaer and Dumais, 1997) or synonymy identification (Bullinaria and Levy, 2007; Landauer and Dumais, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59680915,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "5e13c603f201672af5fd1116a6ca4a492bfab206",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "How-come-you-know-so-much-From-practical-problem-to-Landauer-Dumais",
            "title": {
                "fragments": [],
                "text": "How come you know so much? From practical problem to theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144263847"
                        ],
                        "name": "J. Deese",
                        "slug": "J.-Deese",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Deese",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Deese"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 35
                            }
                        ],
                        "text": "Examples include memory retrieval (Deese, 1959; Raaijmakers & Shiffrin, 1981), categorization (Estes, 1994; Nosofsky, 1984, 1986), problem solving (Holyoak & Koh, 1987; Ross, 1987, 1989), reasoning (Heit & Rubinstein, 1994; Rips, 1975), and learning (Gentner, 1989; Ross, 1984)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 30259120,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "id": "03894f9f549972bd77963172ccecc4e8144a72f4",
            "isKey": false,
            "numCitedBy": 1882,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-prediction-of-occurrence-of-particular-in-Deese",
            "title": {
                "fragments": [],
                "text": "On the prediction of occurrence of particular verbal intrusions in immediate recall."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of experimental psychology"
            },
            "year": 1959
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2500077"
                        ],
                        "name": "Julie Weeds",
                        "slug": "Julie-Weeds",
                        "structuredName": {
                            "firstName": "Julie",
                            "lastName": "Weeds",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julie Weeds"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 30
                            }
                        ],
                        "text": "Baldwin, Bannard, Tanaka, and Widdows (2003) apply this method to model the decomposability of multiword expressions such as noun compounds and phrasal verbs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 30
                            }
                        ],
                        "text": "Baldwin, Bannard, Tanaka, and Widdows (2003) apply this method to model the decomposability of multiword expressions such as noun compounds and phrasal verbs. Taking a similar approach, Bannard, Baldwin, and Lascarides (2003) develop a vector space model for representing the meaning of verb\u2013particle constructions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 116
                            }
                        ],
                        "text": "A large number of such measures have been proposed in the literature (for an overview, see Bullinaria & Levy, 2007; Weeds, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 22521075,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb8b0c56bad2d3952ad1a71ed479b1679e0963eb",
            "isKey": false,
            "numCitedBy": 130,
            "numCiting": 217,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Measures-and-applications-of-lexical-distributional-Weeds",
            "title": {
                "fragments": [],
                "text": "Measures and applications of lexical distributional similarity"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 327,
                                "start": 255
                            }
                        ],
                        "text": "For example, they do so by defining context in terms of syntactic dependencies (Grefenstette, 1994; Lin, 1998; Pad\u00f3 & Lapata, 2007) or by taking into account relational information about how roles and fillers combine to create specific factual knowledge (Dennis, 2007). 1398 J. Mitchell, M. Lapata \u2044 Cognitive Science 34 (2010)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 267,
                                "start": 255
                            }
                        ],
                        "text": "For example, they do so by defining context in terms of syntactic dependencies (Grefenstette, 1994; Lin, 1998; Pado\u0301 & Lapata, 2007) or by taking into account relational information about how roles and fillers combine to create specific factual knowledge (Dennis, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 254
                            }
                        ],
                        "text": "For example, they do so by defining context in terms of syntactic dependencies (Grefenstette, 1994; Lin, 1998; Pad\u00f3 & Lapata, 2007) or by taking into account relational information about how roles and fillers combine to create specific factual knowledge (Dennis, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Introducing word order in an LSA framework"
            },
            "venue": {
                "fragments": [],
                "text": "Handbook of latent semantic analysis (pp. 449\u2013464)"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 344,
                                "start": 321
                            }
                        ],
                        "text": "These semantic similarities can then be used in variety of tasks, including modelling semantic priming (Landuaer and Dumais, 1997; Lund and Burgess, 1996) and human similarity judgments (McDonald, 2000), automatic thesaurus extraction (Grefenstette, 1994) and word sense discrimination (Sch\u00fctze, 1998) and disambiguation (McCarthy et al., 2004) The underlying motivations and assumptions of these models have their origins in a variety of disparate sources."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 10
                            }
                        ],
                        "text": "biguation (McCarthy et al., 2004) and semantic role labelling (F\u00fcrstenau and Lapata, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Finding predominant senses in untagged text"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 280\u2013287, Barcelona, Spain."
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 52
                            }
                        ],
                        "text": "Connectionist models of cognition (see among others Elman et al., 1996; Rumelhart, McClelland, & the PDP Research Group, 1986) can be seen as a response to the limitations of traditional symbolic models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1484,
                                "start": 52
                            }
                        ],
                        "text": "Connectionist models of cognition (see among others Elman et al., 1996; Rumelhart, McClelland, & the PDP Research Group, 1986) can be seen as a response to the limitations of traditional symbolic models. The key premise here is that knowledge is represented not as discrete symbols that enter into symbolic expressions, but as patterns of activation distributed over many processing elements. These representations are distributed in the sense that any single concept is represented as a pattern, that is, vector, of activation over many elements (nodes or units) that are typically assumed to correspond to neurons or small collections of neurons. Much effort in the literature has been invested in enhancing the representational capabilities of connectionist models with the means to combine a finite number of symbols into a much larger, possibly infinite, number of specific structures. The key property of symbolic representations that connectionist models attempt to emulate is their ability to bind one representation to another. The fundamental operation underlying binding in symbolic systems is the concatenation of symbols according to certain syntactic processes. And crucially the results of this operation can be broken down into their original constituents. Thus, connectionists have sought ways of constructing complex structures by binding one distributed representation to another in a manner that is reversible. J. Mitchell, M. Lapata \u2044 Cognitive Science 34 (2010) 1395"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Rethinking innateness: A connectionist perspective"
            },
            "venue": {
                "fragments": [],
                "text": "mapping. Cognitive Science,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "WordNet::Similarity\u2014Measuring the relatedness"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Composition in distributional models of semantics"
            },
            "venue": {
                "fragments": [],
                "text": "Cognitive Science, 34(8):1388\u20131429."
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the nature and scope of featural representations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 995,
                                "start": 85
                            }
                        ],
                        "text": "as those included in the Test of English as Foreign Language (TOEFL), reading times (Griffiths et al., 2007; McDonald, 2000), and judgments of semantic similarity (McDonald, 2000) and association (Denhire & Lemaire, 2004; Griffiths et al., 2007). Despite their widespread use, these models are typically directed at representing words in isolation, and methods for constructing representations for phrases or sentences have received little attention in the literature. However, it is well known that linguistic structures are compositional (simpler elements are combined to form more complex ones). For example, morphemes are combined into words, words into phrases, and phrases into sentences. It is also reasonable to assume that the meaning of sentences is composed of the meanings of individual words or phrases. Much experimental evidence also suggests that semantic similarity is more complex than simply a relation between isolated words. For example, Duffy, Henderson, and Morris (1989) showed that priming of sentence terminal words was dependent not simply on individual preceding words but on their combination, and Morris (1994) later demonstrated that this priming also showed dependencies on the syntactic relations in the preceding context."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1626,
                                "start": 85
                            }
                        ],
                        "text": "as those included in the Test of English as Foreign Language (TOEFL), reading times (Griffiths et al., 2007; McDonald, 2000), and judgments of semantic similarity (McDonald, 2000) and association (Denhire & Lemaire, 2004; Griffiths et al., 2007). Despite their widespread use, these models are typically directed at representing words in isolation, and methods for constructing representations for phrases or sentences have received little attention in the literature. However, it is well known that linguistic structures are compositional (simpler elements are combined to form more complex ones). For example, morphemes are combined into words, words into phrases, and phrases into sentences. It is also reasonable to assume that the meaning of sentences is composed of the meanings of individual words or phrases. Much experimental evidence also suggests that semantic similarity is more complex than simply a relation between isolated words. For example, Duffy, Henderson, and Morris (1989) showed that priming of sentence terminal words was dependent not simply on individual preceding words but on their combination, and Morris (1994) later demonstrated that this priming also showed dependencies on the syntactic relations in the preceding context. Additional evidence comes from experiments where target words in sentences are compared with target words in lists or scrambled sentences. Changes in the temporal order of words in a sentence decrease the strength of the related priming effect (Foss, 1982; Masson, 1986; O\u2019Seaghdha, 1989; Simpson, Peterson, Casteel, & Brugges, 1989). For example, Simpson et al. (1989) found relatedness priming effects for words embedded in grammatical sentences (The auto accident drew a large crowd of people) but not for words in scrambled stimuli (Accident of large the drew auto crowd a people)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "World knowledge in computational models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 13
                            }
                        ],
                        "text": "For example, Simpson et al. (1989) found relatedness priming effects for words embedded in grammatical sentences (The auto accident drew a large crowd of people) but not for words in scrambled stimuli (Accident of large the drew auto crowd a people)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Lexical and context effects in word"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "122 7.1.1 Eye-movements"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 18
                            }
                        ],
                        "text": "More recent work (Steyvers & Tenenbaum, 2005) creates a semantic network from word association norms (Nelson, McEvoy, & Schreiber, 1999); however, these can only represent a small fraction of the vocabulary of an adult speaker."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The large-scale structure of semantic networks: Statistical analyses"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 95
                            }
                        ],
                        "text": "Examples include memory retrieval (Deese, 1959; Raaijmakers & Shiffrin, 1981), categorization (Estes, 1994; Nosofsky, 1984, 1986), problem solving (Holyoak & Koh, 1987; Ross, 1987, 1989), reasoning (Heit & Rubinstein, 1994; Rips, 1975), and learning (Gentner, 1989; Ross, 1984)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "WordNet: An electronic database"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Doumas and Hummel (2005) propose a model of role\u2013filler binding based on synchrony of neural firing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 105
                            }
                        ],
                        "text": "Aside from the philosophical difficulties in precisely determining what systematicity means in practice (Doumas & Hummel, 2005; Pullum & Scholz, 2007; Spenader & Blutner, 2007), it is worth noting that semantic transparency, the idea that words have meanings which remain unaffected by their\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 195
                            }
                        ],
                        "text": "In a truly compositional system, complex structures gain meaning from the simpler parts from which they are formed and the simpler components remain independent, that is, preserve their meaning (Doumas & Hummel, 2005, Doumas, Hummel, & Sandhofer, 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Modeling human mental representations: What works and what doesn't and why In"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 13
                            }
                        ],
                        "text": "For example, Simpson et al. (1989) found relatedness priming effects for words embedded in grammatical sentences (The auto accident drew a large crowd of people) but not for words in scrambled stimuli (Accident of large the drew auto crowd a people)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Lexical and context effects in word recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Experimental Psychology : Learning , Memory and Cognition"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 146
                            }
                        ],
                        "text": "\u2026& Dumais, 1997; Lund & Burgess, 1996), discourse comprehension (Foltz, Kintsch, & Landauer, 1998; Landauer & Dumais, 1997), word categorization (Laham, 2000), judgments of essay quality (Landauer, Laham, Rehder, & Schreiner, 1997), synonymy tests (Griffiths et al., 2007; Landauer & Dumais,\u2026"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automated content assessment of text using latent semantic analysis to simulate human cognition. Unpublished doctoral dissertation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 202
                            }
                        ],
                        "text": "LDA derives semantic representations for individual words rather than word combinations (although extensions of the basic model have been proposed that take word order into account; for an example, see Wallach 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Structured topic models for language. Unpublished doctoral dissertation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 146
                            }
                        ],
                        "text": "\u2026& Dumais, 1997; Lund & Burgess, 1996), discourse comprehension (Foltz, Kintsch, & Landauer, 1998; Landauer & Dumais, 1997), word categorization (Laham, 2000), judgments of essay quality (Landauer, Laham, Rehder, & Schreiner, 1997), synonymy tests (Griffiths et al., 2007; Landauer & Dumais,\u2026"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automated content assessment of text using latent semantic analysis to simulate human cognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 157
                            }
                        ],
                        "text": "Furthermore, HAL has been applied to modelling the cerebral asymmetries in word recognition (Burgess and Lund, 1998) and also lexical emotional connotations (Burgess and Lund, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Representing abstract words and emotional connotation in high-dimensional memory space"
            },
            "venue": {
                "fragments": [],
                "text": "Shafto, M. G. and Langley, P., editors, Proceedings of the nineteenth annual conference of the cognitive science society, pages 61\u201366, Mahwah, NJ. Lawrence Erlbaum Associates."
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 126
                            }
                        ],
                        "text": "This property is often captured in the following principle: The meaning of a whole is a function of the meaning of the parts (Partee, 1995, p. 313)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Invitation to cognitive science part I: Language"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 121
                            }
                        ],
                        "text": "In context, the first occurrence of a word incurs the greatest processing cost, with subsequent reading times decreasing (Rayner et al., 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Eye movements and discourse processing"
            },
            "venue": {
                "fragments": [],
                "text": "Lorch, R. F. and O\u2019Brien, E. J., editors, Sources of coherence in reading, pages 9\u201336. Erlbaum, Hillsdale, NJ."
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 92
                            }
                        ],
                        "text": "Furthermore, HAL has been applied to modelling the cerebral asymmetries in word recognition (Burgess and Lund, 1998) and also lexical emotional connotations (Burgess and Lund, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Modeling cerebral asymmetries in high-dimensional space"
            },
            "venue": {
                "fragments": [],
                "text": "Beeman, M. and Chiarello, C., editors, Right hemisphere language comprehension: Perspectives from cognitive neuroscience, pages 215\u2013244. Lawrence Erlbaum Associates, Mahwah, NJ."
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 189
                            }
                        ],
                        "text": "Although all the required visual information is obtained during fixations, many words are skipped entirely, with only 35% of function words being fixated, compared to 85% for content words (Carpenter and Just, 1983)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "What your eyes do while your mind is reading"
            },
            "venue": {
                "fragments": [],
                "text": "Rayner, K., editor, Eye movements in reading: Perceptual and language processes, pages 275\u2013307. Academic Press, New York."
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Linguistic gestalts"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "106 6.2.1 Vector Composition from a Probabilistic Perspective"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A multiple regression analysis of syntactic and semantic influences"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 146
                            }
                        ],
                        "text": "\u2026& Dumais, 1997; Lund & Burgess, 1996), discourse comprehension (Foltz, Kintsch, & Landauer, 1998; Landauer & Dumais, 1997), word categorization (Laham, 2000), judgments of essay quality (Landauer, Laham, Rehder, & Schreiner, 1997), synonymy tests (Griffiths et al., 2007; Landauer & Dumais,\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 366,
                                "start": 353
                            }
                        ],
                        "text": "Semantic space models (and the related topic models) have been successful at simulating a wide range of psycholinguistic phenomena, including semantic priming (Griffiths, Steyvers, & Tenenbaum, 2007; Landauer & Dumais, 1997; Lund & Burgess, 1996), discourse comprehension (Foltz, Kintsch, & Landauer, 1998; Landauer & Dumais, 1997), word categorization (Laham, 2000), judgments of essay quality (Landauer, Laham, Rehder, & Schreiner, 1997), synonymy tests (Griffiths et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automated content assessment of text using latent semantic analysis to simulate human cognition. Unpublished doctoral dissertation, University of Colorado at Boulder"
            },
            "venue": {
                "fragments": [],
                "text": "Lakoff, G"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A maximum entropy approach to adaptive statistical language modeling"
            },
            "venue": {
                "fragments": [],
                "text": "Computer Speech and Language, 10:187\u2013228."
            },
            "year": 1996
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 154,
            "methodology": 55,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 256,
        "totalPages": 26
    },
    "page_url": "https://www.semanticscholar.org/paper/Composition-in-Distributional-Models-of-Semantics-Mitchell-Lapata/745d86adca56ec50761591733e157f84cfb19671?sort=total-citations"
}