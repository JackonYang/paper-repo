{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2365442"
                        ],
                        "name": "B. Alexe",
                        "slug": "B.-Alexe",
                        "structuredName": {
                            "firstName": "Bogdan",
                            "lastName": "Alexe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Alexe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1879646"
                        ],
                        "name": "Thomas Deselaers",
                        "slug": "Thomas-Deselaers",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Deselaers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Deselaers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143865718"
                        ],
                        "name": "V. Ferrari",
                        "slug": "V.-Ferrari",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Ferrari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Ferrari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 56
                            }
                        ],
                        "text": "Since the publication of the first version of this work [3], two closely related independent works appeared [9], [16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 59
                            }
                        ],
                        "text": "These are the images used in the first version of our work [3] and correspond to our software release."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 60
                            }
                        ],
                        "text": "Since the publication of a preliminary version of this work [3], objectness has already been used successfully to:"
                    },
                    "intents": []
                }
            ],
            "corpusId": 11515509,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2dd55b3bcaf50c1228569d0efe5620a910c1cd07",
            "isKey": false,
            "numCitedBy": 933,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a generic objectness measure, quantifying how likely it is for an image window to contain an object of any class. We explicitly train it to distinguish objects with a well-defined boundary in space, such as cows and telephones, from amorphous background elements, such as grass and road. The measure combines in a Bayesian framework several image cues measuring characteristics of objects, such as appearing different from their surroundings and having a closed boundary. This includes an innovative cue measuring the closed boundary characteristic. In experiments on the challenging PASCAL VOC 07 dataset, we show this new cue to outperform a state-of-the-art saliency measure [17], and the combined measure to perform better than any cue alone. Finally, we show how to sample windows from an image according to their objectness distribution and give an algorithm to employ them as location priors for modern class-specific object detectors. In experiments on PASCAL VOC 07 we show this greatly reduces the number of windows evaluated by class-specific object detectors."
            },
            "slug": "What-is-an-object-Alexe-Deselaers",
            "title": {
                "fragments": [],
                "text": "What is an object?"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A generic objectness measure, quantifying how likely it is for an image window to contain an object of any class, is presented, combining in a Bayesian framework several image cues measuring characteristics of objects, such as appearing different from their surroundings and having a closed boundary."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728179"
                        ],
                        "name": "G. Heitz",
                        "slug": "G.-Heitz",
                        "structuredName": {
                            "firstName": "Geremy",
                            "lastName": "Heitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Heitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 225
                            }
                        ],
                        "text": "Objects are stand-alone things with a well-defined boundary and center, such as cows, cars, and telephones, as opposed to amorphous background stuff, such as sky, grass, and road (as in the things versus stuff distinction of [26])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1899092,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a709dcc204af8bade3ed4040dc6bc738e80995a",
            "isKey": false,
            "numCitedBy": 447,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "The sliding window approach of detecting rigid objects (such as cars) is predicated on the belief that the object can be identified from the appearance in a small region around the object. Other types of objects of amorphous spatial extent (e.g., trees, sky), however, are more naturally classified based on texture or color. In this paper, we seek to combine recognition of these two types of objects into a system that leverages \"context\" toward improving detection. In particular, we cluster image regions based on their ability to serve as context for the detection of objects. Rather than providing an explicit training set with region labels, our method automatically groups regions based on both their appearance and their relationships to the detections in the image. We show that our things and stuff (TAS) context model produces meaningful clusters that are readily interpretable, and helps improve our detection ability over state-of-the-art detectors. We also present a method for learning the active set of relationships for a particular dataset. We present results on object detection in images from the PASCAL VOC 2005/2006 datasets and on the task of overhead car detection in satellite images, demonstrating significant improvements over state-of-the-art detectors."
            },
            "slug": "Learning-Spatial-Context:-Using-Stuff-to-Find-Heitz-Koller",
            "title": {
                "fragments": [],
                "text": "Learning Spatial Context: Using Stuff to Find Things"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper clusters image regions based on their ability to serve as context for the detection of objects and shows that the things and stuff (TAS) context model produces meaningful clusters that are readily interpretable, and helps improve detection ability over state-of-the-art detectors."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2096527"
                        ],
                        "name": "G. Wang",
                        "slug": "G.-Wang",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In Section 5 we evaluate objectness on the very challenging PASCAL VOC 2007 dataset, and then show applications to aiding class-specific object detectors [11], [18], [50] in Sections 6 and 7."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16140390,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb4362bd6f0bc5bb467fc8f169723243caa97d1d",
            "isKey": false,
            "numCitedBy": 174,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method to learn visual attributes (eg.\u201cred\u201d, \u201cmetal\u201d, \u201cspotted\u201d) and object classes (eg. \u201ccar\u201d, \u201cdress\u201d, \u201cumbrella\u201d) together. We assume images are labeled with category, but not location, of an instance. We estimate models with an iterative procedure: the current model is used to produce a saliency score, which, together with a homogeneity cue, identifies likely locations for the object (resp. attribute); then those locations are used to produce better models with multiple instance learning. Crucially, the object and attribute models must agree on the potential locations of an object. This means that the more accurate of the two models can guide the improvement of the less accurate model. Our method is evaluated on two data sets of images of real scenes, one in which the attribute is color and the other in which it is material. We show that our joint learning produces improved detectors. We demonstrate generalization by detecting attribute-object pairs which do not appear in our training data. The iteration gives significant improvement in performance."
            },
            "slug": "Joint-learning-of-visual-attributes,-object-classes-Wang-Forsyth",
            "title": {
                "fragments": [],
                "text": "Joint learning of visual attributes, object classes and visual saliency"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A method to learn visual attributes and object classes together and shows that the more accurate of the two models can guide the improvement of the less accurate model."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110264574"
                        ],
                        "name": "Tie Liu",
                        "slug": "Tie-Liu",
                        "structuredName": {
                            "firstName": "Tie",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tie Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33762094"
                        ],
                        "name": "Zejian Yuan",
                        "slug": "Zejian-Yuan",
                        "structuredName": {
                            "firstName": "Zejian",
                            "lastName": "Yuan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zejian Yuan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688516"
                        ],
                        "name": "Jingdong Wang",
                        "slug": "Jingdong-Wang",
                        "structuredName": {
                            "firstName": "Jingdong",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jingdong Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122737130"
                        ],
                        "name": "N. Zheng",
                        "slug": "N.-Zheng",
                        "structuredName": {
                            "firstName": "Nanning",
                            "lastName": "Zheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Zheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50295995"
                        ],
                        "name": "Xiaoou Tang",
                        "slug": "Xiaoou-Tang",
                        "structuredName": {
                            "firstName": "Xiaoou",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoou Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144154486"
                        ],
                        "name": "H. Shum",
                        "slug": "H.-Shum",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Shum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Shum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[36] detect a single dominant salient object in an image."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "the entire image or the surrounding area [36], [38]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "CC is related to the center-surround histogram cue of [36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "a different appearance from its surroundings [36], [38], ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "However, [36] computes a center-surround histogram centered at a pixel, whereas CC scores a whole window as whether it contains an entire object."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14833979,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4f03f0c435f8a2891b048d19d7a0b8e3e5263b4",
            "isKey": true,
            "numCitedBy": 2326,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we study the salient object detection problem for images. We formulate this problem as a binary labeling task where we separate the salient object from the background. We propose a set of novel features, including multiscale contrast, center-surround histogram, and color spatial distribution, to describe a salient object locally, regionally, and globally. A conditional random field is learned to effectively combine these features for salient object detection. Further, we extend the proposed approach to detect a salient object from sequential images by introducing the dynamic salient features. We collected a large image database containing tens of thousands of carefully labeled images by multiple users and a video segment database, and conducted a set of experiments over them to demonstrate the effectiveness of the proposed approach."
            },
            "slug": "Learning-to-Detect-a-Salient-Object-Liu-Yuan",
            "title": {
                "fragments": [],
                "text": "Learning to Detect a Salient Object"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A set of novel features, including multiscale contrast, center-surround histogram, and color spatial distribution, are proposed to describe a salient object locally, regionally, and globally."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9301018"
                        ],
                        "name": "Roberto Valenti",
                        "slug": "Roberto-Valenti",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Valenti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roberto Valenti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703601"
                        ],
                        "name": "N. Sebe",
                        "slug": "N.-Sebe",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Sebe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Sebe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695527"
                        ],
                        "name": "T. Gevers",
                        "slug": "T.-Gevers",
                        "structuredName": {
                            "firstName": "Theo",
                            "lastName": "Gevers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Gevers"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[49] measure the saliency of individual pixels, and then returns the region with the highest sum of pixelsaliency as the single dominant object."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2082071,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0cb898218e3c0db2fff8a6dcfadfa6d4d3517336",
            "isKey": false,
            "numCitedBy": 169,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we propose a novel computational method to infer visual saliency in images. The method is based on the idea that salient objects should have local characteristics that are different than the rest of the scene, being edges, color or shape. By using a novel operator, these characteristics are combined to infer global information. The obtained information is used as a weighting for the output of a segmentation algorithm so that the salient object in the scene can easily be distinguished from the background. The proposed approach is fast and it does not require any learning. The experimentation shows that the system can enhance interesting objects in images and it is able to correctly locate the same object annotated by humans with an F-measure of 85.61% when the object size is known, and 79.19% when the object size is unknown, improving the state of the art performance on a public dataset."
            },
            "slug": "Image-saliency-by-isocentric-curvedness-and-color-Valenti-Sebe",
            "title": {
                "fragments": [],
                "text": "Image saliency by isocentric curvedness and color"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A novel computational method to infer visual saliency in images based on the idea that salient objects should have local characteristics that are different than the rest of the scene, being edges, color or shape by using a novel operator."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1879646"
                        ],
                        "name": "Thomas Deselaers",
                        "slug": "Thomas-Deselaers",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Deselaers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Deselaers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2365442"
                        ],
                        "name": "B. Alexe",
                        "slug": "B.-Alexe",
                        "structuredName": {
                            "firstName": "Bogdan",
                            "lastName": "Alexe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Alexe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143865718"
                        ],
                        "name": "V. Ferrari",
                        "slug": "V.-Ferrari",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Ferrari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Ferrari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 162
                            }
                        ],
                        "text": "Several recent works are increasingly demonstrating the value of objectness in other applications, such as learning object classes in weakly supervised scenarios [13], [30], [47], pixelwise segmentation of objects [2], [52], unsupervised object discovery [34], and learning humans-object interactions [45]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 64
                            }
                        ],
                        "text": "Facilitate learning new classes in a weakly supervised scenario [13], [30], [47], where the location of object instances is unknown."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7664974,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42bb241681c4bec1fa36211a204fa0dc8158e5ff",
            "isKey": false,
            "numCitedBy": 204,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning a new object class from cluttered training images is very challenging when the location of object instances is unknown. Previous works generally require objects covering a large portion of the images. We present a novel approach that can cope with extensive clutter as well as large scale and appearance variations between object instances. To make this possible we propose a conditional random field that starts from generic knowledge and then progressively adapts to the new class. Our approach simultaneously localizes object instances while learning an appearance model specific for the class. We demonstrate this on the challenging PASCAL VOC 2007 dataset. Furthermore, our method enables to train any state-of-the-art object detector in a weakly supervised fashion, although it would normally require object location annotations."
            },
            "slug": "Localizing-Objects-While-Learning-Their-Appearance-Deselaers-Alexe",
            "title": {
                "fragments": [],
                "text": "Localizing Objects While Learning Their Appearance"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes a conditional random field that starts from generic knowledge and then progressively adapts to the new class to enable any state-of-the-art object detector in a weakly supervised fashion, although it would normally require object location annotations."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787591"
                        ],
                        "name": "Christoph H. Lampert",
                        "slug": "Christoph-H.-Lampert",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Lampert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christoph H. Lampert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758219"
                        ],
                        "name": "Matthew B. Blaschko",
                        "slug": "Matthew-B.-Blaschko",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Blaschko",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew B. Blaschko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 105
                            }
                        ],
                        "text": "For detection, we use the source code of [18] for both [18] and [11], and the source code of ESS(10) for [33]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "We compare the three object detectors [11], [18], [33] to our algorithm 1 on the entire PASCAL VOC 07 test set (20 object classes over 4,952 images) using the standard PASCAL VOC protocol (Fig."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "For [33], we obtained precomputed image features and SVM hyperplanes from the authors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "Differently from ESS [33], our method imposes no restriction on the class model used to score a window."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 93
                            }
                        ],
                        "text": "For detection, we use the source code of [18] for both [18] and [11], and the source code of ESS10 for [33]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "words classifier [33], which represents a window with a histogram of local features quantized over a precomputed"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 106
                            }
                        ],
                        "text": "Since for this type of window scoring function it is possible to apply the branch-and-bound technique ESS [33], we compare to ESS rather than to traditional sliding-windows on a grid [11], [18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "Interestingly, it also evaluates 60 fewer windows than ESS [33] (notice that the implicit search space of [33] is larger than that of [11], [18] as it contains all windows, not just a grid)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "Here, we follow the setup of [33] and use a linear SVM as c."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 49
                            }
                        ],
                        "text": "can be applied to any window classifier, whereas [33] is"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 146
                            }
                        ],
                        "text": "Although a variety of approaches exist [4], [11], [35], most state-of-the-art detectors follow the slidingwindow paradigm [11], [12], [18], [25], [33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "ESS instead is restricted to functions for which the user can provide a (tight) bound on the best score in a contiguous set of windows (for example, for the scoring functions of [11], [18] no bound is known and ESS is not applicable)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6131848,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54b224478a63e33441c651175c522f3702062fc4",
            "isKey": true,
            "numCitedBy": 799,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Most successful object recognition systems rely on binary classification, deciding only if an object is present or not, but not providing information on the actual object location. To perform localization, one can take a sliding window approach, but this strongly increases the computational cost, because the classifier function has to be evaluated over a large set of candidate subwindows. In this paper, we propose a simple yet powerful branch-and-bound scheme that allows efficient maximization of a large class of classifier functions over all possible subimages. It converges to a globally optimal solution typically in sublinear time. We show how our method is applicable to different object detection and retrieval scenarios. The achieved speedup allows the use of classifiers for localization that formerly were considered too slow for this task, such as SVMs with a spatial pyramid kernel or nearest neighbor classifiers based on the chi2-distance. We demonstrate state-of-the-art performance of the resulting systems on the UIUC Cars dataset, the PASCAL VOC 2006 dataset and in the PASCAL VOC 2007 competition."
            },
            "slug": "Beyond-sliding-windows:-Object-localization-by-Lampert-Blaschko",
            "title": {
                "fragments": [],
                "text": "Beyond sliding windows: Object localization by efficient subwindow search"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A simple yet powerful branch-and-bound scheme that allows efficient maximization of a large class of classifier functions over all possible subimages and converges to a globally optimal solution typically in sublinear time is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2918740"
                        ],
                        "name": "A. Vezhnevets",
                        "slug": "A.-Vezhnevets",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Vezhnevets",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vezhnevets"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143865718"
                        ],
                        "name": "V. Ferrari",
                        "slug": "V.-Ferrari",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Ferrari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Ferrari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682548"
                        ],
                        "name": "J. Buhmann",
                        "slug": "J.-Buhmann",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Buhmann",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Buhmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1203780,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c2932a9c686ede327f41069e17962d330a7a3ebf",
            "isKey": false,
            "numCitedBy": 170,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel method for weakly supervised semantic segmentation. Training images are labeled only by the classes they contain, not by their location in the image. On test images instead, the method predicts a class label for every pixel. Our main innovation is a multi-image model (MIM) - a graphical model for recovering the pixel labels of the training images. The model connects superpixels from all training images in a data-driven fashion, based on their appearance similarity. For generalizing to new test images we integrate them into MIM using a learned multiple kernel metric, instead of learning conventional classifiers on the recovered pixel labels. We also introduce an \u201cobjectness\u201d potential, that helps separating objects (e.g. car, dog, human) from background classes (e.g. grass, sky, road). In experiments on the MSRC 21 dataset and the LabelMe subset of [18], our technique outperforms previous weakly supervised methods and achieves accuracy comparable with fully supervised methods."
            },
            "slug": "Weakly-supervised-semantic-segmentation-with-a-Vezhnevets-Ferrari",
            "title": {
                "fragments": [],
                "text": "Weakly supervised semantic segmentation with a multi-image model"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "A novel method for weakly supervised semantic segmentation using a multi-image model (MIM) - a graphical model for recovering the pixel labels of the training images and introducing an \u201cobjectness\u201d potential, that helps separating objects from background classes."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35681810"
                        ],
                        "name": "Jo\u00e3o Carreira",
                        "slug": "Jo\u00e3o-Carreira",
                        "structuredName": {
                            "firstName": "Jo\u00e3o",
                            "lastName": "Carreira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jo\u00e3o Carreira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781120"
                        ],
                        "name": "C. Sminchisescu",
                        "slug": "C.-Sminchisescu",
                        "structuredName": {
                            "firstName": "Cristian",
                            "lastName": "Sminchisescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sminchisescu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 5
                            }
                        ],
                        "text": "both [9], [16] are very expensive (about 420 seconds per"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 17
                            }
                        ],
                        "text": "While both works [9], [16] produce rough segmentations as object candidates, they are computationally very expensive and take 2-7 minutes per image (compared to a few seconds for our method)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 14
                            }
                        ],
                        "text": "Note how both [9], [16] produce a more detailed"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[9], [16] that produce segmentations as object candidates."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 19
                            }
                        ],
                        "text": "As the plot shows, [9] and objectness exhibit the same performance when considering a rather low number of windows (up to 100)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 108
                            }
                        ],
                        "text": "Since the publication of the first version of this work [3], two closely related independent works appeared [9], [16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 39
                            }
                        ],
                        "text": "Object proposals based on segmentation [9], [16]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 30
                            }
                        ],
                        "text": "17 we plot the performance of [9], [16] against our objectness measure (MS\u00feCC\u00feSS)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6041168,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4131a2862d9f926c6727da6dc75c8fda25f4a9e5",
            "isKey": true,
            "numCitedBy": 476,
            "numCiting": 103,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel framework for generating and ranking plausible objects hypotheses in an image using bottom-up processes and mid-level cues. The object hypotheses are represented as figure-ground segmentations, and are extracted automatically, without prior knowledge about properties of individual object classes, by solving a sequence of constrained parametric min-cut problems (CPMC) on a regular image grid. We then learn to rank the object hypotheses by training a continuous model to predict how plausible the segments are, given their mid-level region properties. We show that this algorithm significantly outperforms the state of the art for low-level segmentation in the VOC09 segmentation dataset. It achieves the same average best segmentation covering as the best performing technique to date [2], 0.61 when using just the top 7 ranked segments, instead of the full hierarchy in [2]. Our method achieves 0.78 average best covering using 154 segments. In a companion paper [18], we also show that the algorithm achieves state-of-the art results when used in a segmentation-based recognition pipeline."
            },
            "slug": "Constrained-parametric-min-cuts-for-automatic-Carreira-Sminchisescu",
            "title": {
                "fragments": [],
                "text": "Constrained parametric min-cuts for automatic object segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that this algorithm significantly outperforms the state of the art for low-level segmentation in the VOC09 segmentation dataset and achieves the same average best segmentation covering as the best performing technique to date."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153432684"
                        ],
                        "name": "Devi Parikh",
                        "slug": "Devi-Parikh",
                        "structuredName": {
                            "firstName": "Devi",
                            "lastName": "Parikh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Devi Parikh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40894914"
                        ],
                        "name": "Tsuhan Chen",
                        "slug": "Tsuhan-Chen",
                        "structuredName": {
                            "firstName": "Tsuhan",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tsuhan Chen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 12
                            }
                        ],
                        "text": "A few works [40, 42, 44, 53] define as salient the visual characteristics that best distinguish a particular object class (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 729046,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15fb0072c3781c1a7cb7f0540bcd7f2d04421d1f",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "The increased use of context for high level reasoning has been popular in recent works to increase recognition accuracy. In this paper, we consider an orthogonal application of context. We explore the use of context to determine which low-level appearance cues in an image are salient or representative of an image's contents. Existing classes of low-level saliency measures for image patches include those based on interest points, as well as supervised discriminative measures. We propose a new class of unsupervised contextual saliency measures based on co-occurrence and spatial information between image patches. For recognition, image patches are sampled using a weighted random sampling based on saliency, or using a sequential approach based on maximizing the likelihoods of the image patches. We compare the different classes of saliency measures, along with a baseline uniform measure, for the task of scene and object recognition using the bag-of-features paradigm. In our results, the contextual saliency measures achieve improved accuracies over the previous methods. Moreover, our highest accuracy is achieved using a sparse sampling of the image, unlike previous approaches who's performance increases with the sampling density."
            },
            "slug": "Determining-Patch-Saliency-Using-Low-Level-Context-Parikh-Zitnick",
            "title": {
                "fragments": [],
                "text": "Determining Patch Saliency Using Low-Level Context"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes a new class of unsupervised contextual saliency measures based on co-occurrence and spatial information between image patches, and compares the different classes of saliency Measures for the task of scene and object recognition using the bag-of-features paradigm."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40277674"
                        ],
                        "name": "C. Desai",
                        "slug": "C.-Desai",
                        "structuredName": {
                            "firstName": "Chaitanya",
                            "lastName": "Desai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Desai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143800213"
                        ],
                        "name": "Charless C. Fowlkes",
                        "slug": "Charless-C.-Fowlkes",
                        "structuredName": {
                            "firstName": "Charless",
                            "lastName": "Fowlkes",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charless C. Fowlkes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "Many modern class-specific object detectors [12], [18], [25]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 128
                            }
                        ],
                        "text": "Although a variety of approaches exist [4], [11], [35], most state-of-the-art detectors follow the slidingwindow paradigm [11], [12], [18], [25], [33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1454551,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "78662a293888d7e982061d16f6a71d0223420fad",
            "isKey": false,
            "numCitedBy": 336,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Many state-of-the-art approaches for object recognition reduce the problem to a 0-1 classification task. This allows one to leverage sophisticated machine learning techniques for training classifiers from labeled examples. However, these models are typically trained independently for each class using positive and negative examples cropped from images. At test-time, various post-processing heuristics such as non-maxima suppression (NMS) are required to reconcile multiple detections within and between different classes for each image. Though crucial to good performance on benchmarks, this post-processing is usually defined heuristically.We introduce a unified model for multi-class object recognition that casts the problem as a structured prediction task. Rather than predicting a binary label for each image window independently, our model simultaneously predicts a structured labeling of the entire image (Fig.\u00a01). Our model learns statistics that capture the spatial arrangements of various object classes in real images, both in terms of which arrangements to suppress through NMS and which arrangements to favor through spatial co-occurrence statistics.We formulate parameter estimation in our model as a max-margin learning problem. Given training images with ground-truth object locations, we show how to formulate learning as a convex optimization problem. We employ the cutting plane algorithm of Joachims et al. (Mach. Learn.\u00a02009) to efficiently learn a model from thousands of training images. We show state-of-the-art results on the PASCAL VOC benchmark that indicate the benefits of learning a global model encapsulating the spatial layout of multiple object classes (a preliminary version of this work appeared in ICCV 2009, Desai et al., IEEE international conference on computer vision,\u00a02009)."
            },
            "slug": "Discriminative-Models-for-Multi-Class-Object-Layout-Desai-Ramanan",
            "title": {
                "fragments": [],
                "text": "Discriminative Models for Multi-Class Object Layout"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A unified model for multi-class object recognition is introduced that casts the problem as a structured prediction task and how to formulate learning as a convex optimization problem is shown."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143865718"
                        ],
                        "name": "V. Ferrari",
                        "slug": "V.-Ferrari",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Ferrari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Ferrari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145786090"
                        ],
                        "name": "L. Fevrier",
                        "slug": "L.-Fevrier",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Fevrier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Fevrier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "82117876"
                        ],
                        "name": "F. Jurie",
                        "slug": "F.-Jurie",
                        "structuredName": {
                            "firstName": "Fr\u00e9d\u00e9ric",
                            "lastName": "Jurie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jurie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In Section 5 we evaluate objectness on the very challenging PASCAL VOC 2007 dataset, and then show applications to aiding class-specific object detectors [11], [18], [50] in Sections 6 and 7."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 215513937,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "02bc39849a15c84c6ebea35c7923d1824981cb7c",
            "isKey": false,
            "numCitedBy": 606,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a family of scale-invariant local shape features formed by chains of k connected roughly straight contour segments (kAS), and their use for object class detection. kAS are able to cleanly encode pure fragments of an object boundary without including nearby clutter. Moreover, they offer an attractive compromise between information content and repeatability and encompass a wide variety of local shape structures. We also define a translation and scale invariant descriptor encoding the geometric configuration of the segments within a kAS, making kAS easy to reuse in other frameworks, for example, as a replacement or addition to interest points (IPs). Software for detecting and describing kAS is released at http://lear.inrialpes.fr/software. We demonstrate the high performance of kAS within a simple but powerful sliding-window object detection scheme. Through extensive evaluations, involving eight diverse object classes and more than 1,400 images, we (1) study the evolution of performance as the degree of feature complexity k varies and determine the best degree, (2) show that kAS substantially outperform IPs for detecting shape-based classes, and (3) compare our object detector to the recent state-of-the-art system by Dalal and Triggs (2005)."
            },
            "slug": "Groups-of-Adjacent-Contour-Segments-for-Object-Ferrari-Fevrier",
            "title": {
                "fragments": [],
                "text": "Groups of Adjacent Contour Segments for Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that kAS substantially outperform IPs for detecting shape-based classes, and the object detector is compared to the recent state-of-the-art system by Dalal and Triggs (2005)."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2365442"
                        ],
                        "name": "B. Alexe",
                        "slug": "B.-Alexe",
                        "structuredName": {
                            "firstName": "Bogdan",
                            "lastName": "Alexe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Alexe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1879646"
                        ],
                        "name": "Thomas Deselaers",
                        "slug": "Thomas-Deselaers",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Deselaers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Deselaers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143865718"
                        ],
                        "name": "V. Ferrari",
                        "slug": "V.-Ferrari",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Ferrari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Ferrari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 214
                            }
                        ],
                        "text": "Several recent works are increasingly demonstrating the value of objectness in other applications, such as learning object classes in weakly supervised scenarios [13], [30], [47], pixelwise segmentation of objects [2], [52], unsupervised object discovery [34], and learning humans-object interactions [45]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 83
                            }
                        ],
                        "text": "Analogously, to support weakly supervised pixelwise segmentation of object classes [2], [52] and unsupervised object discovery [34]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5611404,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d957ad316f7145c054d2dcbd47949869e46776b0",
            "isKey": false,
            "numCitedBy": 1008,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel method for unsupervised class segmentation on a set of images. It alternates between segmenting object instances and learning a class model. The method is based on a segmentation energy defined over all images at the same time, which can be optimized efficiently by techniques used before in interactive segmentation. Over iterations, our method progressively learns a class model by integrating observations over all images. In addition to appearance, this model captures the location and shape of the class with respect to an automatically determined coordinate frame common across images. This frame allows us to build stronger shape and location models, similar to those used in object class detection. Our method is inspired by interactive segmentation methods [1], but it is fully automatic and learns models characteristic for the object class rather than specific to one particular object/image. We experimentally demonstrate on the Caltech4, Caltech101, and Weizmann horses datasets that our method (a) transfers class knowledge across images and this improves results compared to segmenting every image independently; (b) outperforms Grabcut [1] for the task of unsupervised segmentation; (c) offers competitive performance compared to the state-of-the-art in unsupervised segmentation and in particular it outperforms the topic model [2]."
            },
            "slug": "ClassCut-for-Unsupervised-Class-Segmentation-Alexe-Deselaers",
            "title": {
                "fragments": [],
                "text": "ClassCut for Unsupervised Class Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A novel method for unsupervised class segmentation on a set of images that alternates between segmenting object instances and learning a class model based on a segmentation energy defined over all images at the same time, which can be optimized efficiently by techniques used before in interactive segmentation."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145273587"
                        ],
                        "name": "Stephen Gould",
                        "slug": "Stephen-Gould",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Gould",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen Gould"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "97911988"
                        ],
                        "name": "Richard Fulton",
                        "slug": "Richard-Fulton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Fulton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard Fulton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "In the second experiment, we insert [22] into our frame-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 288,
                                "start": 271
                            }
                        ],
                        "text": "On the task of detecting objects in the challenging PASCAL VOC 07 dataset [17], we demonstrate that the combined objectness measure performs better than any cue alone, and also outperforms traditional salient blob detectors [27], [28], interest point detectors [41], the Semantic Labeling technique of Gould et al. [22], and a HOG detector [11] trained to detect objects of arbitrary classes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 30
                            }
                        ],
                        "text": "The semantic pixel labeler of [22] divides the image into"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 41
                            }
                        ],
                        "text": "We compare our objectness measure to the Semantic Labeling technique of Gould et al. [22]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[22] is well suited as an objectness cue and that sampling a larger set of windows per image is important for reaching high recall."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "(a) MS versus [27], [28]; (b) baselines and semantic labeling [22]; (c) single cues."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 138
                            }
                        ],
                        "text": "The authors thank A. Zisserman for suggesting the basic intuition behind the LS cue and D. Koller and S. Gould for helping them use their Semantic Labeling software."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[22], and a HOG detector [11] trained to detect objects of arbitrary classes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 180
                            }
                        ],
                        "text": "On the task of\ndetecting objects of new classes unseen during training, we have demonstrated that objectness outperforms traditional saliency [27], [28], interest point detectors, Semantic Labeling [22], and the HOG detector [11]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 198
                            }
                        ],
                        "text": "On the task of detecting objects of new classes unseen during training, we have demonstrated that objectness outperforms traditional saliency [27], [28], interest point detectors, Semantic Labeling [22], and the HOG detector [11]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17448963,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc08847b65953ef2ae3542e47b08b57a46b5ba34",
            "isKey": true,
            "numCitedBy": 709,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "High-level, or holistic, scene understanding involves reasoning about objects, regions, and the 3D relationships between them. This requires a representation above the level of pixels that can be endowed with high-level attributes such as class of object/region, its orientation, and (rough 3D) location within the scene. Towards this goal, we propose a region-based model which combines appearance and scene geometry to automatically decompose a scene into semantically meaningful regions. Our model is defined in terms of a unified energy function over scene appearance and structure. We show how this energy function can be learned from data and present an efficient inference technique that makes use of multiple over-segmentations of the image to propose moves in the energy-space. We show, experimentally, that our method achieves state-of-the-art performance on the tasks of both multi-class image segmentation and geometric reasoning. Finally, by understanding region classes and geometry, we show how our model can be used as the basis for 3D reconstruction of the scene."
            },
            "slug": "Decomposing-a-scene-into-geometric-and-semantically-Gould-Fulton",
            "title": {
                "fragments": [],
                "text": "Decomposing a scene into geometric and semantically consistent regions"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A region-based model which combines appearance and scene geometry to automatically decompose a scene into semantically meaningful regions and which achieves state-of-the-art performance on the tasks of both multi-class image segmentation and geometric reasoning."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789756"
                        ],
                        "name": "B. Leibe",
                        "slug": "B.-Leibe",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Leibe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Leibe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "Although a variety of approaches exist [4], [11], [35], most state-of-the-art detectors follow the slidingwindow paradigm [11], [12], [18], [25], [33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18901556,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49a7ff6c753a79ed063fe2c4bf3eca3fa03c2f7e",
            "isKey": false,
            "numCitedBy": 179,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of our work is object categorization in real-world scenes. That is, given a novel image we want to recognize and localize unseen-before objects based on their similarity to a learned object category. For use in a real-world system, it is important that this includes the ability to recognize objects at multiple scales. In this paper, we present an approach to multi-scale object categorization using scale-invariant interest points and a scale-adaptive Mean-Shift search. The approach builds on the method from [12], which has been demonstrated to achieve excellent results for the single-scale case, and extends it to multiple scales. We present an experimental comparison of the influence of different interest point operators and quantitatively show the method's robustness to large scale changes."
            },
            "slug": "Scale-Invariant-Object-Categorization-Using-a-Leibe-Schiele",
            "title": {
                "fragments": [],
                "text": "Scale-Invariant Object Categorization Using a Scale-Adaptive Mean-Shift Search"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper presents an approach to multi-scale object categorization using scale-invariant interest points and a scale-adaptive Mean-Shift search, and presents an experimental comparison of the influence of different interest point operators and quantitatively shows the method's robustness to large scale changes."
            },
            "venue": {
                "fragments": [],
                "text": "DAGM-Symposium"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685089"
                        ],
                        "name": "Pedro F. Felzenszwalb",
                        "slug": "Pedro-F.-Felzenszwalb",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Felzenszwalb",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro F. Felzenszwalb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689002"
                        ],
                        "name": "David A. McAllester",
                        "slug": "David-A.-McAllester",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McAllester",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. McAllester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "We then run the nonmaxima suppression of [18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "evaluated by modern class-specific object detectors [11], [18], [50] (Section 6)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "Many modern class-specific object detectors [12], [18], [25]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "applications to aiding class-specific object detectors [11], [18],"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 189
                            }
                        ],
                        "text": "Since for this type of window scoring function it is possible to apply the branch-and-bound technique ESS [33], we compare to ESS rather than to traditional sliding-windows on a grid [11], [18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "The detector of [18] models a class with a mixtures of multiscale deformable part models."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 12
                            }
                        ],
                        "text": "tried using [18], which extends HOG with parts."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 134
                            }
                        ],
                        "text": "Although a variety of approaches exist [4], [11], [35], most state-of-the-art detectors follow the slidingwindow paradigm [11], [12], [18], [25], [33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 170
                            }
                        ],
                        "text": "We evaluate the performance of the objectness measure on the popular PASCAL VOC 07 dataset [17], which is commonly used to evaluate class-specific object detectors [11], [18], [50]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "window paradigm [11], [18], [50], which scores a large"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3198903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e79272fe3d65197100eae8be9fec6469107969ae",
            "isKey": true,
            "numCitedBy": 9371,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL data sets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin-sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI--SVM in terms of latent variables. A latent SVM is semiconvex, and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function."
            },
            "slug": "Object-Detection-with-Discriminatively-Trained-Part-Felzenszwalb-Girshick",
            "title": {
                "fragments": [],
                "text": "Object Detection with Discriminatively Trained Part Based Models"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "An object detection system based on mixtures of multiscale deformable part models that is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges is described."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145233481"
                        ],
                        "name": "S. Ravishankar",
                        "slug": "S.-Ravishankar",
                        "structuredName": {
                            "firstName": "Saiprasad",
                            "lastName": "Ravishankar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ravishankar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35561551"
                        ],
                        "name": "Arpit Jain",
                        "slug": "Arpit-Jain",
                        "structuredName": {
                            "firstName": "Arpit",
                            "lastName": "Jain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Arpit Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50853059"
                        ],
                        "name": "Anurag Mittal",
                        "slug": "Anurag-Mittal",
                        "structuredName": {
                            "firstName": "Anurag",
                            "lastName": "Mittal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anurag Mittal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18462124,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f7b09631653008812e52ed3182ce3aecae0298f6",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an efficient multi stage approach to detection of deformable objects in real, cluttered images given a single or few hand drawn examples as models. The method handles deformations of the object by first breaking the given model into segments at high curvature points. We allow bending at these points as it has been studied that deformation typically happens at high curvature points. The broken segments are then scaled, rotated, deformed and searched independently in the gradient image. Point maps are generated for each segment that represent the locations of the matches for that segment. We then group kpoints from the point maps of kadjacent segments using a cost function that takes into account local scale variations as well as inter-segment orientations. These matched groups yield plausible locations for the objects. In the fine matching stage, the entire object contour in the localized regions is built from the k-segment groups and given a comprehensive score in a method that uses dynamic programming. An evaluation of our algorithm on a standard dataset yielded results that are better than published work on the same dataset. At the same time, we also evaluate our algorithm on additional images with considerable object deformations to verify the robustness of our method."
            },
            "slug": "Multi-stage-Contour-Based-Detection-of-Deformable-Ravishankar-Jain",
            "title": {
                "fragments": [],
                "text": "Multi-stage Contour Based Detection of Deformable Objects"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "An efficient multi stage approach to detection of deformable objects in real, cluttered images given a single or few hand drawn examples as models and given a comprehensive score in a method that uses dynamic programming is presented."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114568379"
                        ],
                        "name": "W. Kienzle",
                        "slug": "W.-Kienzle",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Kienzle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Kienzle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1924112"
                        ],
                        "name": "Felix Wichmann",
                        "slug": "Felix-Wichmann",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Wichmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Felix Wichmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30541601"
                        ],
                        "name": "M. Franz",
                        "slug": "M.-Franz",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Franz",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Franz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "Since [28], numerous works [7], [21], [24], [27], [31] appeared to measure the saliency of pixels, as the degree of uniqueness of their neighborhood w."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11543059,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "40cd3cd0d1da27297ad84fb8750d906ade0b1f6f",
            "isKey": false,
            "numCitedBy": 216,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the bottom-up influence of local image information on human eye movements. Most existing computational models use a set of biologically plausible linear filters, e.g., Gabor or Difference-of-Gaussians filters as a front-end, the outputs of which are nonlinearly combined into a real number that indicates visual saliency. Unfortunately, this requires many design parameters such as the number, type, and size of the front-end filters, as well as the choice of nonlinearities, weighting and normalization schemes etc., for which biological plausibility cannot always be justified. As a result, these parameters have to be chosen in a more or less ad hoc way. Here, we propose to learn a visual saliency model directly from human eye movement data. The model is rather simplistic and essentially parameter-free, and therefore contrasts recent developments in the field that usually aim at higher prediction rates at the cost of additional parameters and increasing model complexity. Experimental results show that\u2014despite the lack of any biological prior knowledge\u2014our model performs comparably to existing approaches, and in fact learns image features that resemble findings from several previous studies. In particular, its maximally excitatory stimuli have center-surround structure, similar to receptive fields in the early human visual system."
            },
            "slug": "A-Nonparametric-Approach-to-Bottom-Up-Visual-Kienzle-Wichmann",
            "title": {
                "fragments": [],
                "text": "A Nonparametric Approach to Bottom-Up Visual Saliency"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The model is rather simplistic and essentially parameter-free, and contrasts recent developments in the field that usually aim at higher prediction rates at the cost of additional parameters and increasing model complexity, and in fact learns image features that resemble findings from several previous studies."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2787367"
                        ],
                        "name": "P. Siva",
                        "slug": "P.-Siva",
                        "structuredName": {
                            "firstName": "Parthipan",
                            "lastName": "Siva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Siva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145406421"
                        ],
                        "name": "T. Xiang",
                        "slug": "T.-Xiang",
                        "structuredName": {
                            "firstName": "Tao",
                            "lastName": "Xiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Xiang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 174
                            }
                        ],
                        "text": "Several recent works are increasingly demonstrating the value of objectness in other applications, such as learning object classes in weakly supervised scenarios [13], [30], [47], pixelwise segmentation of objects [2], [52], unsupervised object discovery [34], and learning humans-object interactions [45]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "Facilitate learning new classes in a weakly supervised scenario [13], [30], [47], where the location of object instances is unknown."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7659762,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43aeb06eb886e56414951550f34548939840308d",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "A conventional approach to learning object detectors uses fully supervised learning techniques which assumes that a training image set with manual annotation of object bounding boxes are provided. The manual annotation of objects in large image sets is tedious and unreliable. Therefore, a weakly supervised learning approach is desirable, where the training set needs only binary labels regarding whether an image contains the target object class. In the weakly supervised approach a detector is used to iteratively annotate the training set and learn the object model. We present a novel weakly supervised learning framework for learning an object detector. Our framework incorporates a new initial annotation model to start the iterative learning of a detector and a model drift detection method that is able to detect and stop the iterative learning when the detector starts to drift away from the objects of interest. We demonstrate the effectiveness of our approach on the challenging PASCAL 2007 dataset."
            },
            "slug": "Weakly-supervised-object-detector-learning-with-Siva-Xiang",
            "title": {
                "fragments": [],
                "text": "Weakly supervised object detector learning with model drift detection"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work presents a novel weakly supervised learning framework for learning an object detector that incorporates a new initial annotation model to start the iterative learning of a detector and a model drift detection method that is able to detect and stop the iteratives learning when the detector starts to drift away from the objects of interest."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118589"
                        ],
                        "name": "R. Achanta",
                        "slug": "R.-Achanta",
                        "structuredName": {
                            "firstName": "Radhakrishna",
                            "lastName": "Achanta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Achanta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720838"
                        ],
                        "name": "S. Hemami",
                        "slug": "S.-Hemami",
                        "structuredName": {
                            "firstName": "Sheila",
                            "lastName": "Hemami",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hemami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145337110"
                        ],
                        "name": "F. Estrada",
                        "slug": "F.-Estrada",
                        "structuredName": {
                            "firstName": "Francisco",
                            "lastName": "Estrada",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Estrada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735035"
                        ],
                        "name": "S. S\u00fcsstrunk",
                        "slug": "S.-S\u00fcsstrunk",
                        "structuredName": {
                            "firstName": "Sabine",
                            "lastName": "S\u00fcsstrunk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. S\u00fcsstrunk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1334960,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f52d46714a3fccbe9ea293763b03550ada7bd7d",
            "isKey": false,
            "numCitedBy": 1569,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Detection of visually salient image regions is useful for applications like object segmentation, adaptive compression, and object recognition. In this paper, we introduce a method for salient region detection that outputs full resolution saliency maps with well-defined boundaries of salient objects. These boundaries are preserved by retaining substantially more frequency content from the original image than other existing techniques. Our method exploits features of color and luminance, is simple to implement, and is computationally efficient. We compare our algorithm to five state-of-the-art salient region detection methods with a frequency domain analysis, ground truth, and a salient object segmentation application. Our method outperforms the five algorithms both on the ground-truth evaluation and on the segmentation task by achieving both higher precision and better recall."
            },
            "slug": "Frequency-tuned-salient-region-detection-Achanta-Hemami",
            "title": {
                "fragments": [],
                "text": "Frequency-tuned salient region detection"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This paper introduces a method for salient region detection that outputs full resolution saliency maps with well-defined boundaries of salient objects that outperforms the five algorithms both on the ground-truth evaluation and on the segmentation task by achieving both higher precision and better recall."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48950628"
                        ],
                        "name": "N. Dalal",
                        "slug": "N.-Dalal",
                        "structuredName": {
                            "firstName": "Navneet",
                            "lastName": "Dalal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dalal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206590483,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cec734d7097ab6b1e60d95228ffd64248eb89d66",
            "isKey": false,
            "numCitedBy": 29259,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds."
            },
            "slug": "Histograms-of-oriented-gradients-for-human-Dalal-Triggs",
            "title": {
                "fragments": [],
                "text": "Histograms of oriented gradients for human detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection, and the influence of each stage of the computation on performance is studied."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145160921"
                        ],
                        "name": "Bryan C. Russell",
                        "slug": "Bryan-C.-Russell",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Russell",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan C. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782755"
                        ],
                        "name": "Josef Sivic",
                        "slug": "Josef-Sivic",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Sivic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josef Sivic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2066830,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56766bab76cdcd541bf791730944a5e453006239",
            "isKey": false,
            "numCitedBy": 740,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a large dataset of images, we seek to automatically determine the visually similar object and scene classes together with their image segmentation. To achieve this we combine two ideas: (i) that a set of segmented objects can be partitioned into visual object classes using topic discovery models from statistical text analysis; and (ii) that visual object classes can be used to assess the accuracy of a segmentation. To tie these ideas together we compute multiple segmentations of each image and then: (i) learn the object classes; and (ii) choose the correct segmentations. We demonstrate that such an algorithm succeeds in automatically discovering many familiar objects in a variety of image datasets, including those from Caltech, MSRC and LabelMe."
            },
            "slug": "Using-Multiple-Segmentations-to-Discover-Objects-in-Russell-Freeman",
            "title": {
                "fragments": [],
                "text": "Using Multiple Segmentations to Discover Objects and their Extent in Image Collections"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work compute multiple segmentations of each image and then learns the object classes and chooses the correct segmentations, demonstrating that such an algorithm succeeds in automatically discovering many familiar objects in a variety of image datasets, including those from Caltech, MSRC and LabelMe."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685538"
                        ],
                        "name": "Tamara L. Berg",
                        "slug": "Tamara-L.-Berg",
                        "structuredName": {
                            "firstName": "Tamara",
                            "lastName": "Berg",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tamara L. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 14
                            }
                        ],
                        "text": "Berg and Berg [6] find iconic images representative for an object class."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15311570,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "453c90a8ad2d0b26a82d0478d05281736e411e6a",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We demonstrate that is it possible to automatically find representative example images of a specified object category. These canonical examples are perhaps the kind of images that one would show a child to teach them what, for example a horse is - images with a large object clearly separated from the background. Given a large collection of images returned by a web search for an object category, our approach proceeds without any user supplied training data for the category. First images are ranked according to a category independent composition model that predicts whether they contain a large clearly depicted object, and outputs an estimated location of that object. Then local features calculated on the proposed object regions are used to eliminate images not distinctive to the category and to cluster images by similarity of object appearance. We present results and a user evaluation on a variety of object categories, demonstrating the effectiveness of the approach."
            },
            "slug": "Finding-iconic-images-Berg-Berg",
            "title": {
                "fragments": [],
                "text": "Finding iconic images"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "It is demonstrated that is it possible to automatically find representative example images of a specified object category using a category independent composition model that predicts whether they contain a large clearly depicted object, and outputs an estimated location of that object."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66987745"
                        ],
                        "name": "Dattaguru V Kamat",
                        "slug": "Dattaguru-V-Kamat",
                        "structuredName": {
                            "firstName": "Dattaguru",
                            "lastName": "Kamat",
                            "middleNames": [
                                "V"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dattaguru V Kamat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6036783,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a3826c9a88d873331c13e3305cf16aa19144b12c",
            "isKey": false,
            "numCitedBy": 170,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel framework for visual saliency detection based on a simple principle: images sharing their global visual appearances are likely to share similar salience. Assuming that an annotated image database is available, we first retrieve the most similar images to the target image; secondly, we build a simple classifier and we use it to generate saliency maps. Finally, we refine the maps and we extract thumbnails. We show that in spite of its simplicity, our framework outperforms state-of-the art approaches. Another advantage is its ability to deal with visual pop-up and application/task-driven saliency, if appropriately annotated images are available."
            },
            "slug": "A-framework-for-visual-saliency-detection-with-to-Kamat",
            "title": {
                "fragments": [],
                "text": "A framework for visual saliency detection with applications to image thumbnailing"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A novel framework for visual saliency detection based on a simple principle: images sharing their global visual appearances are likely to share similar salience, which outperforms state-of-the art approaches."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2862000"
                        ],
                        "name": "A. Prest",
                        "slug": "A.-Prest",
                        "structuredName": {
                            "firstName": "Alessandro",
                            "lastName": "Prest",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Prest"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143865718"
                        ],
                        "name": "V. Ferrari",
                        "slug": "V.-Ferrari",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Ferrari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Ferrari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 305,
                                "start": 301
                            }
                        ],
                        "text": "Several recent works are increasingly demonstrating the value of objectness in other applications, such as learning object classes in weakly supervised scenarios [13], [30], [47], pixelwise segmentation of objects [2], [52], unsupervised object discovery [34], and learning humans-object interactions [45]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 64
                            }
                        ],
                        "text": "Learn spatial models of interactions between humans and objects [45]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1819788,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3a2427eeb32d59ccfc634b46eae350be14d10e88",
            "isKey": false,
            "numCitedBy": 218,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a weakly supervised approach for learning human actions modeled as interactions between humans and objects. Our approach is human-centric: We first localize a human in the image and then determine the object relevant for the action and its spatial relation with the human. The model is learned automatically from a set of still images annotated only with the action label. Our approach relies on a human detector to initialize the model learning. For robustness to various degrees of visibility, we build a detector that learns to combine a set of existing part detectors. Starting from humans detected in a set of images depicting the action, our approach determines the action object and its spatial relation to the human. Its final output is a probabilistic model of the human-object interaction, i.e., the spatial relation between the human and the object. We present an extensive experimental evaluation on the sports action data set from [1], the PASCAL Action 2010 data set [2], and a new human-object interaction data set."
            },
            "slug": "Weakly-Supervised-Learning-of-Interactions-between-Prest-Schmid",
            "title": {
                "fragments": [],
                "text": "Weakly Supervised Learning of Interactions between Humans and Objects"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An extensive experimental evaluation on the sports action data set from [1], the PASCAL Action 2010 data set [2], and a new human-object interaction data set are presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2831988"
                        ],
                        "name": "Ian Endres",
                        "slug": "Ian-Endres",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Endres",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian Endres"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2433269"
                        ],
                        "name": "Derek Hoiem",
                        "slug": "Derek-Hoiem",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Hoiem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Derek Hoiem"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "both [9], [16] are very expensive (about 420 seconds per"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "While both works [9], [16] produce rough segmentations as object candidates, they are computationally very expensive and take 2-7 minutes per image (compared to a few seconds for our method)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "Note how both [9], [16] produce a more detailed"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "The method of Endres and Hoiem [16] achieves moderately 2198 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 5
                            }
                        ],
                        "text": "[9], [16] that produce segmentations as object candidates."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 113
                            }
                        ],
                        "text": "Since the publication of the first version of this work [3], two closely related independent works appeared [9], [16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "Object proposals based on segmentation [9], [16]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "17 we plot the performance of [9], [16] against our objectness measure (MS\u00feCC\u00feSS)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 14
                            }
                        ],
                        "text": "The method of Endres and Hoiem [16] achieves moderately\n6. http://ai.stanford.edu/~sgould/svl/."
                    },
                    "intents": []
                }
            ],
            "corpusId": 697224,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f0640da2565bad7037d969e9f07276c10102083d",
            "isKey": true,
            "numCitedBy": 495,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a category-independent method to produce a bag of regions and rank them, such that top-ranked regions are likely to be good segmentations of different objects. Our key objectives are completeness and diversity: every object should have at least one good proposed region, and a diverse set should be top-ranked. Our approach is to generate a set of segmentations by performing graph cuts based on a seed region and a learned affinity function. Then, the regions are ranked using structured learning based on various cues. Our experiments on BSDS and PASCAL VOC 2008 demonstrate our ability to find most objects within a small bag of proposed regions."
            },
            "slug": "Category-Independent-Object-Proposals-Endres-Hoiem",
            "title": {
                "fragments": [],
                "text": "Category Independent Object Proposals"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A category-independent method to produce a bag of regions and rank them, such that top-ranked regions are likely to be good segmentations of different objects, and the ability to find most objects within a small bag of proposed regions is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145090451"
                        ],
                        "name": "G. Krieger",
                        "slug": "G.-Krieger",
                        "structuredName": {
                            "firstName": "Gerhard",
                            "lastName": "Krieger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Krieger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2580914"
                        ],
                        "name": "I. Rentschler",
                        "slug": "I.-Rentschler",
                        "structuredName": {
                            "firstName": "Ingo",
                            "lastName": "Rentschler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Rentschler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3267783"
                        ],
                        "name": "G. Hauske",
                        "slug": "G.-Hauske",
                        "structuredName": {
                            "firstName": "Gert",
                            "lastName": "Hauske",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Hauske"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755034"
                        ],
                        "name": "K. Schill",
                        "slug": "K.-Schill",
                        "structuredName": {
                            "firstName": "Kerstin",
                            "lastName": "Schill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Schill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683937"
                        ],
                        "name": "C. Zetzsche",
                        "slug": "C.-Zetzsche",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Zetzsche",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Zetzsche"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 147
                            }
                        ],
                        "text": "These works implement selective visual attention from a bottom-up perspective and are often inspired by studies of human eye movements [14], [15], [32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1176232,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "3bfeecf1123829293384d5342324bc50943cdd24",
            "isKey": false,
            "numCitedBy": 192,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Based on an information theoretical approach, we investigate feature selection processes in saccadic object and scene analysis. Saccadic eye movements of human observers are recorded for a variety of natural and artificial test images. These experimental data are used for a statistical evaluation of the fixated image regions. Analysis of second-order statistics indicates that regions with higher spatial variance have a higher probability to be fixated, but no significant differences beyond these variance effects could be found at the level of power spectra. By contrast, an investigation with higher-order statistics, as reflected in the bispectral density, yielded clear structural differences between the image regions selected by saccadic eye movements as opposed to regions selected by a random process. These results indicate that nonredundant, intrinsically two-dimensional image features like curved lines and edges, occlusions, isolated spots, etc. play an important role in the saccadic selection process which must be integrated with top-down knowledge to fully predict object and scene analysis by human observers."
            },
            "slug": "Object-and-scene-analysis-by-saccadic-an-with-Krieger-Rentschler",
            "title": {
                "fragments": [],
                "text": "Object and scene analysis by saccadic eye-movements: an investigation with higher-order statistics."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Results indicate that nonredundant, intrinsically two-dimensional image features like curved lines and edges, occlusions, isolated spots, etc. play an important role in the saccadic selection process which must be integrated with top-down knowledge to fully predict object and scene analysis by human observers."
            },
            "venue": {
                "fragments": [],
                "text": "Spatial vision"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2820887"
                        ],
                        "name": "Hedi Harzallah",
                        "slug": "Hedi-Harzallah",
                        "structuredName": {
                            "firstName": "Hedi",
                            "lastName": "Harzallah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hedi Harzallah"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "82117876"
                        ],
                        "name": "F. Jurie",
                        "slug": "F.-Jurie",
                        "structuredName": {
                            "firstName": "Fr\u00e9d\u00e9ric",
                            "lastName": "Jurie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jurie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "Many modern class-specific object detectors [12], [18], [25]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 140
                            }
                        ],
                        "text": "Although a variety of approaches exist [4], [11], [35], most state-of-the-art detectors follow the slidingwindow paradigm [11], [12], [18], [25], [33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8879271,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5fae850e7b85e91b11a2874252ec617c3cb064c6",
            "isKey": false,
            "numCitedBy": 338,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a combined approach for object localization and classification. Our contribution is twofold. (a) A contextual combination of localization and classification which shows that classification can improve detection and vice versa. (b) An efficient two stage sliding window object localization method that combines the efficiency of a linear classifier with the robustness of a sophisticated non-linear one. Experimental results evaluate the parameters of our two stage sliding window approach and show that our combined object localization and classification methods outperform the state-of-the-art on the PASCAL VOC 2007 and 2008 datasets."
            },
            "slug": "Combining-efficient-object-localization-and-image-Harzallah-Jurie",
            "title": {
                "fragments": [],
                "text": "Combining efficient object localization and image classification"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "An efficient two stage sliding window object localization method that combines the efficiency of a linear classifier with the robustness of a sophisticated non-linear one and shows that classification can improve detection and vice versa is presented."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2036170"
                        ],
                        "name": "D. Gao",
                        "slug": "D.-Gao",
                        "structuredName": {
                            "firstName": "Dashan",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699559"
                        ],
                        "name": "N. Vasconcelos",
                        "slug": "N.-Vasconcelos",
                        "structuredName": {
                            "firstName": "Nuno",
                            "lastName": "Vasconcelos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Vasconcelos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "sometimes it is unique within the image and stands out as salient [7], [21], [27], [28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "Since [28], numerous works [7], [21], [24], [27], [31] appeared to measure the saliency of pixels, as the degree of uniqueness of their neighborhood w."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15026004,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "2c3288b051e5555a62438af00e9497cf653b6d15",
            "isKey": false,
            "numCitedBy": 218,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "A bottom-up visual saliency detector is proposed, following a decision-theoretic formulation of saliency, previously developed for top-down processing (object recognition) [5]. The saliency of a given location of the visual field is defined as the power of a Gabor-like feature set to discriminate between the visual appearance of 1) a neighborhood centered at that location (the center) and 2) a neighborhood that surrounds it (the surround). Discrimination is defined in an information-theoretic sense and the optimal saliency detector derived for a class of stimuli that complies with known statistical properties of natural images, so as to achieve a computationally efficient solution. The resulting saliency detector is shown to replicate the fundamental properties of the psychophysics of pre-attentive vision, including stimulus pop-out, inability to detect feature conjunctions, asymmetries with respect to feature presence vs. absence, and compliance with Weber's law. It is also shown that the detector produces better predictions of human eye fixations than two previously proposed bottom-up saliency detectors."
            },
            "slug": "Bottom-up-saliency-is-a-discriminant-process-Gao-Vasconcelos",
            "title": {
                "fragments": [],
                "text": "Bottom-up saliency is a discriminant process"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The resulting saliency detector is shown to replicate the fundamental properties of the psychophysics of pre-attentive vision, including stimulus pop-out, inability to detect feature conjunctions, asymmetries with respect to feature presence vs. absence, and compliance with Weber's law."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3236386"
                        ],
                        "name": "Varun Gulshan",
                        "slug": "Varun-Gulshan",
                        "structuredName": {
                            "firstName": "Varun",
                            "lastName": "Gulshan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Varun Gulshan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145859952"
                        ],
                        "name": "M. Varma",
                        "slug": "M.-Varma",
                        "structuredName": {
                            "firstName": "Manik",
                            "lastName": "Varma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Varma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 176
                            }
                        ],
                        "text": "We evaluate the performance of the objectness measure on the popular PASCAL VOC 07 dataset [17], which is commonly used to evaluate class-specific object detectors [11], [18], [50]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 64
                            }
                        ],
                        "text": "evaluated by modern class-specific object detectors [11], [18], [50] (Section 6)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "window paradigm [11], [18], [50], which scores a large"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206769604,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c9c7ab03bdb5fee15174d910d7fea14a16b086b7",
            "isKey": false,
            "numCitedBy": 883,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Our objective is to obtain a state-of-the art object category detector by employing a state-of-the-art image classifier to search for the object in all possible image sub-windows. We use multiple kernel learning of Varma and Ray (ICCV 2007) to learn an optimal combination of exponential \u03c72 kernels, each of which captures a different feature channel. Our features include the distribution of edges, dense and sparse visual words, and feature descriptors at different levels of spatial organization."
            },
            "slug": "Multiple-kernels-for-object-detection-Vedaldi-Gulshan",
            "title": {
                "fragments": [],
                "text": "Multiple kernels for object detection"
            },
            "tldr": {
                "abstractSimilarityScore": 35,
                "text": "This work uses multiple kernel learning of Varma and Ray (ICCV 2007) to learn an optimal combination of exponential \u03c72 kernels, each of which captures a different feature channel."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35238678"
                        ],
                        "name": "D. Lowe",
                        "slug": "D.-Lowe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lowe",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lowe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5258236,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9f836d28f52ad260213d32224a6d227f8e8849a",
            "isKey": false,
            "numCitedBy": 16255,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds."
            },
            "slug": "Object-recognition-from-local-scale-invariant-Lowe",
            "title": {
                "fragments": [],
                "text": "Object recognition from local scale-invariant features"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh IEEE International Conference on Computer Vision"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729041"
                        ],
                        "name": "J. Canny",
                        "slug": "J.-Canny",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Canny",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Canny"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 79
                            }
                        ],
                        "text": "2 seconds, including the time to compute the edge map using the Canny detector [8]; (SS): around 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 72
                            }
                        ],
                        "text": "The binary edgemap IED\u00f0p\u00de 2 f0; 1g is obtained using the Canny detector [8], and Len\u00f0 \u00demeasures the perimeter of the inner ring."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13284142,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fcf9fc4e23b45345c2404ce7d6cb0fc9dea2c9ec",
            "isKey": false,
            "numCitedBy": 27658,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a computational approach to edge detection. The success of the approach depends on the definition of a comprehensive set of goals for the computation of edge points. These goals must be precise enough to delimit the desired behavior of the detector while making minimal assumptions about the form of the solution. We define detection and localization criteria for a class of edges, and present mathematical forms for these criteria as functionals on the operator impulse response. A third criterion is then added to ensure that the detector has only one response to a single edge. We use the criteria in numerical optimization to derive detectors for several common image features, including step edges. On specializing the analysis to step edges, we find that there is a natural uncertainty principle between detection and localization performance, which are the two main goals. With this principle we derive a single operator shape which is optimal at any scale. The optimal detector has a simple approximate implementation in which edges are marked at maxima in gradient magnitude of a Gaussian-smoothed image. We extend this simple detector using operators of several widths to cope with different signal-to-noise ratios in the image. We present a general method, called feature synthesis, for the fine-to-coarse integration of information from operators at different scales. Finally we show that step edge detector performance improves considerably as the operator point spread function is extended along the edge."
            },
            "slug": "A-Computational-Approach-to-Edge-Detection-Canny",
            "title": {
                "fragments": [],
                "text": "A Computational Approach to Edge Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "There is a natural uncertainty principle between detection and localization performance, which are the two main goals, and with this principle a single operator shape is derived which is optimal at any scale."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3084719"
                        ],
                        "name": "Xiaodi Hou",
                        "slug": "Xiaodi-Hou",
                        "structuredName": {
                            "firstName": "Xiaodi",
                            "lastName": "Hou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodi Hou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143998402"
                        ],
                        "name": "Liqing Zhang",
                        "slug": "Liqing-Zhang",
                        "structuredName": {
                            "firstName": "Liqing",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liqing Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "We incorporate a state-of-the-art saliency detector [27] as one cue into our objectness measure."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "Moreover, as [27] suggests, we process the color channels independently as separate images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 83
                            }
                        ],
                        "text": "where F is the FFT, R\u00f0f\u00de and P\u00f0f\u00de are the spectral residual and the phase spectrum [27] of the image f , and g is a Gaussian filter used for smoothing the output."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "Hou and Zhang [27] proposed a global saliency measure based on the spectral residual of the FFT which favors regions with an unique appearance within the entire image f ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "sometimes it is unique within the image and stands out as salient [7], [21], [27], [28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "Since [28], numerous works [7], [21], [24], [27], [31] appeared to measure the saliency of pixels, as the degree of uniqueness of their neighborhood w."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 130
                            }
                        ],
                        "text": "The saliency maps are obtained by combining several band-pass filters that retain a wider range of spatial frequencies than [24], [27], [38]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "For each scale s, we use [27] to obtain a saliency map I MS\u00f0p\u00de."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "4 we propose a new image cue and demonstrate it performs better than traditional saliency cues [27] at finding entire objects."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 224
                            }
                        ],
                        "text": "On the task of detecting objects in the challenging PASCAL VOC 07 dataset [17], we demonstrate that the combined objectness measure performs better than any cue alone, and also outperforms traditional salient blob detectors [27], [28], interest point detectors [41], the Semantic Labeling technique of Gould et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15611611,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9ce3c18eb4fa86fd19bce46227be39895de4e4ab",
            "isKey": true,
            "numCitedBy": 3202,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability of human visual system to detect visual saliency is extraordinarily fast and reliable. However, computational modeling of this basic intelligent behavior still remains a challenge. This paper presents a simple method for the visual saliency detection. Our model is independent of features, categories, or other forms of prior knowledge of the objects. By analyzing the log-spectrum of an input image, we extract the spectral residual of an image in spectral domain, and propose a fast method to construct the corresponding saliency map in spatial domain. We test this model on both natural pictures and artificial images such as psychological patterns. The result indicate fast and robust saliency detection of our method."
            },
            "slug": "Saliency-Detection:-A-Spectral-Residual-Approach-Hou-Zhang",
            "title": {
                "fragments": [],
                "text": "Saliency Detection: A Spectral Residual Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A simple method for the visual saliency detection is presented, independent of features, categories, or other forms of prior knowledge of the objects, and a fast method to construct the corresponding saliency map in spatial domain is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144756076"
                        ],
                        "name": "Yong Jae Lee",
                        "slug": "Yong-Jae-Lee",
                        "structuredName": {
                            "firstName": "Yong",
                            "lastName": "Lee",
                            "middleNames": [
                                "Jae"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yong Jae Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794409"
                        ],
                        "name": "K. Grauman",
                        "slug": "K.-Grauman",
                        "structuredName": {
                            "firstName": "Kristen",
                            "lastName": "Grauman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Grauman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 245
                            }
                        ],
                        "text": "Several recent works are increasingly demonstrating the value of objectness in other applications, such as learning object classes in weakly supervised scenarios [13, 30], pixelwise segmentation of objects [2, 52], unsupervised object discovery [34],"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 194
                            }
                        ],
                        "text": "avoiding to select only grass regions in a set of sheep images; (b) analoguously, to support weakly supervised pixelwise segmentation of object classes [2, 52] and unsupervised object discovery [34]; (c) learning spatial models of interactions between humans and objects [46]; (d) content-aware image resizing [5, 48]; (e) object tracking in video, as an additional likelihood term preventing the tracker from drifting to the background (ongoing work by Van Gool\u2019s group at ETH Zurich)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15615355,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8a1440354e299d5cac6f80f1f4102cc6c6546311",
            "isKey": false,
            "numCitedBy": 192,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Objects vary in their visual complexity, yet existing discovery methods perform \u201cbatch\u201d clustering, paying equal attention to all instances simultaneously \u2014 regardless of the strength of their appearance or context cues. We propose a self-paced approach that instead focuses on the easiest instances first, and progressively expands its repertoire to include more complex objects. Easier regions are defined as those with both high likelihood of generic objectness and high familiarity of surrounding objects. At each cycle of the discovery process, we re-estimate the easiness of each subwindow in the pool of unlabeled images, and then retrieve a single prominent cluster from among the easiest instances. Critically, as the system gradually accumulates models, each new (more difficult) discovery benefits from the context provided by earlier discoveries. Our experiments demonstrate the clear advantages of self-paced discovery relative to conventional batch approaches, including both more accurate summarization as well as stronger predictive models for novel data."
            },
            "slug": "Learning-the-easy-things-first:-Self-paced-visual-Lee-Grauman",
            "title": {
                "fragments": [],
                "text": "Learning the easy things first: Self-paced visual category discovery"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work proposes a self-paced approach that focuses on the easiest instances first, and progressively expands its repertoire to include more complex objects, including both more accurate summarization as well as stronger predictive models for novel data."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143733406"
                        ],
                        "name": "J. Sun",
                        "slug": "J.-Sun",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805398"
                        ],
                        "name": "Haibin Ling",
                        "slug": "Haibin-Ling",
                        "structuredName": {
                            "firstName": "Haibin",
                            "lastName": "Ling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haibin Ling"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "Content-aware image resizing [5], [48]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15177263,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "011dae9d808380a40f172c7b2e91d0f299f95173",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Many image retargeting algorithms, despite aesthetically carving images smaller, pay limited attention to image browsing tasks where tiny thumbnails are presented. When applying traditional retargeting methods for generating thumbnails, several important issues frequently arise, including thumbnail scales, object completeness and local structure smoothness. To address these issues, we propose a novel image retargeting algorithm, Scale and Object Aware Retargeting (SOAR), which has four components: (1) a scale dependent saliency map to integrate size information of thumbnails, (2) objectness (Alexe et al. 2010) for preserving object completeness, (3) a cyclic seam carving algorithm to guide continuous retarget warping, and (4) a thin-plate-spline (TPS) retarget warping algorithm that champions local structure smoothness. The effectiveness of the proposed algorithm is evaluated both quantitatively and qualitatively. The quantitative evaluation is conducted through an image browsing user study to measure the effectiveness of different thumbnail generating algorithms, followed by the ANOVA analysis. The qualitative study is performed on the RetargetMe benchmark dataset. In both studies, SOAR generates very promising performance, in comparison with state-of-the-art retargeting algorithms."
            },
            "slug": "Scale-and-object-aware-image-retargeting-for-Sun-Ling",
            "title": {
                "fragments": [],
                "text": "Scale and object aware image retargeting for thumbnail browsing"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A novel image retargeting algorithm, Scale and Object Aware Retargeting (SOAR), which has four components: a scale dependent saliency map to integrate size information of thumbnails, objectness, a cyclic seam carving algorithm to guide continuous retarget warping, and a thin-plate-spline retarget Warping algorithm that champions local structure smoothness."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1440386,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "9217484cc329dc0aa37614ebac60f530106706a4",
            "isKey": false,
            "numCitedBy": 572,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Maji and Berg [13] have recently introduced an explicit feature map approximating the intersection kernel. This enables efficient learning methods for linear kernels to be applied to the non-linear intersection kernel, expanding the applicability of this model to much larger problems. In this paper we generalize this idea, and analyse a large family of additive kernels, called homogeneous, in a unified framework. The family includes the intersection, Hellinger's, and \u03c72 kernels commonly employed in computer vision. Using the framework we are able to: (i) provide explicit feature maps for all homogeneous additive kernels along with closed form expression for all common kernels; (ii) derive corresponding approximate finite-dimensional feature maps based on the Fourier sampling theorem; and (iii) quantify the extent of the approximation. We demonstrate that the approximations have indistinguishable performance from the full kernel on a number of standard datasets, yet greatly reduce the train/test times of SVM implementations. We show that the \u03c72 kernel, which has been found to yield the best performance in most applications, also has the most compact feature representation. Given these train/test advantages we are able to obtain a significant performance improvement over current state of the art results based on the intersection kernel."
            },
            "slug": "Efficient-additive-kernels-via-explicit-feature-Vedaldi-Zisserman",
            "title": {
                "fragments": [],
                "text": "Efficient additive kernels via explicit feature maps"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that the \u03c72 kernel, which has been found to yield the best performance in most applications, also has the most compact feature representation, and is able to obtain a significant performance improvement over current state of the art results based on the intersection kernel."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109173569"
                        ],
                        "name": "Yu-Fei Ma",
                        "slug": "Yu-Fei-Ma",
                        "structuredName": {
                            "firstName": "Yu-Fei",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu-Fei Ma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15948355,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b183b5c2a58ae8e1dac196831f0725513153d93",
            "isKey": false,
            "numCitedBy": 1038,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual attention analysis provides an alternative methodology to semantic image understanding in many applications such as adaptive content delivery and region-based image retrieval. In this paper, we propose a feasible and fast approach to attention area detection in images based on contrast analysis. The main contributions are threefold: 1) a new saliency map generation method based on local contrast analysis is proposed; 2) by simulating human perception, a fuzzy growing method is used to extract attended areas or objects from the saliency map; and 3) a practicable framework for image attention analysis is presented, which provides three-level attention analysis, i.e., attended view, attended areas and attended points. This framework facilitates visual analysis tools or vision systems to automatically extract attentions from images in a manner like human perception. User study results indicate that the proposed approach is effective and practicable."
            },
            "slug": "Contrast-based-image-attention-analysis-by-using-Ma-Zhang",
            "title": {
                "fragments": [],
                "text": "Contrast-based image attention analysis by using fuzzy growing"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A feasible and fast approach to attention area detection in images based on contrast analysis is proposed and a practicable framework for image attention analysis is presented, which provides three-level attention analysis, i.e., attended view, attended areas and attended points."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '03"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39810944"
                        ],
                        "name": "Jonathan Harel",
                        "slug": "Jonathan-Harel",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Harel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Harel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145624227"
                        ],
                        "name": "C. Koch",
                        "slug": "C.-Koch",
                        "structuredName": {
                            "firstName": "Christof",
                            "lastName": "Koch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Koch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 124
                            }
                        ],
                        "text": "The saliency maps are obtained by combining several band-pass filters that retain a wider range of spatial frequencies than [24], [27], [38]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "Since [28], numerous works [7], [21], [24], [27], [31] appeared to measure the saliency of pixels, as the degree of uniqueness of their neighborhood w."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 629401,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f412bb31ec9ef8bbef70eefc7ffd04420c1365d9",
            "isKey": false,
            "numCitedBy": 3394,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "A new bottom-up visual saliency model, Graph-Based Visual Saliency (GBVS), is proposed. It consists of two steps: first forming activation maps on certain feature channels, and then normalizing them in a way which highlights conspicuity and admits combination with other maps. The model is simple, and biologically plausible insofar as it is naturally parallelized. This model powerfully predicts human fixations on 749 variations of 108 natural images, achieving 98% of the ROC area of a human-based control, whereas the classical algorithms of Itti & Koch ([2], [3], [4]) achieve only 84%."
            },
            "slug": "Graph-Based-Visual-Saliency-Harel-Koch",
            "title": {
                "fragments": [],
                "text": "Graph-Based Visual Saliency"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "A new bottom-up visual saliency model, Graph-Based Visual Saliency (GBVS), is proposed, which powerfully predicts human fixations on 749 variations of 108 natural images, achieving 98% of the ROC area of a human-based control, whereas the classical algorithms of Itti & Koch achieve only 84%."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712041"
                        ],
                        "name": "K. Mikolajczyk",
                        "slug": "K.-Mikolajczyk",
                        "structuredName": {
                            "firstName": "Krystian",
                            "lastName": "Mikolajczyk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Mikolajczyk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 37
                            }
                        ],
                        "text": "Interest point detectors (IPs) [29], [41] respond to local textured image neighborhoods and are widely used for finding image correspondences [41] and recognizing specific objects [37]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 261
                            }
                        ],
                        "text": "On the task of detecting objects in the challenging PASCAL VOC 07 dataset [17], we demonstrate that the combined objectness measure performs better than any cue alone, and also outperforms traditional salient blob detectors [27], [28], interest point detectors [41], the Semantic Labeling technique of Gould et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "For IP we extract Laplacian-based multiscale Harris interest points [41] and score every window w in a 4D regular grid by the density of IPs it contains:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1704741,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8b440596b28dc6683caa2b5f6fbca70963e5909e",
            "isKey": false,
            "numCitedBy": 4161,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we propose a novel approach for detecting interest points invariant to scale and affine transformations. Our scale and affine invariant detectors are based on the following recent results: (1) Interest points extracted with the Harris detector can be adapted to affine transformations and give repeatable results (geometrically stable). (2) The characteristic scale of a local structure is indicated by a local extremum over scale of normalized derivatives (the Laplacian). (3) The affine shape of a point neighborhood is estimated based on the second moment matrix.Our scale invariant detector computes a multi-scale representation for the Harris interest point detector and then selects points at which a local measure (the Laplacian) is maximal over scales. This provides a set of distinctive points which are invariant to scale, rotation and translation as well as robust to illumination changes and limited changes of viewpoint. The characteristic scale determines a scale invariant region for each point. We extend the scale invariant detector to affine invariance by estimating the affine shape of a point neighborhood. An iterative algorithm modifies location, scale and neighborhood of each point and converges to affine invariant points. This method can deal with significant affine transformations including large scale changes. The characteristic scale and the affine shape of neighborhood determine an affine invariant region for each point.We present a comparative evaluation of different detectors and show that our approach provides better results than existing methods. The performance of our detector is also confirmed by excellent matching results; the image is described by a set of scale/affine invariant descriptors computed on the regions associated with our points."
            },
            "slug": "Scale-&-Affine-Invariant-Interest-Point-Detectors-Mikolajczyk-Schmid",
            "title": {
                "fragments": [],
                "text": "Scale & Affine Invariant Interest Point Detectors"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A comparative evaluation of different detectors is presented and it is shown that the proposed approach for detecting interest points invariant to scale and affine transformations provides better results than existing methods."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2866780"
                        ],
                        "name": "Neil D. B. Bruce",
                        "slug": "Neil-D.-B.-Bruce",
                        "structuredName": {
                            "firstName": "Neil",
                            "lastName": "Bruce",
                            "middleNames": [
                                "D.",
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Neil D. B. Bruce"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727853"
                        ],
                        "name": "John K. Tsotsos",
                        "slug": "John-K.-Tsotsos",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Tsotsos",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John K. Tsotsos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 66
                            }
                        ],
                        "text": "sometimes it is unique within the image and stands out as salient [7], [21], [27], [28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 27
                            }
                        ],
                        "text": "Since [28], numerous works [7], [21], [24], [27], [31] appeared to measure the saliency of pixels, as the degree of uniqueness of their neighborhood w."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18236666,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4f847b4ddc105d73bc78f3e7220e6c1f71a7dfb6",
            "isKey": false,
            "numCitedBy": 1177,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A model of bottom-up overt attention is proposed based on the principle of maximizing information sampled from a scene. The proposed operation is based on Shannon's self-information measure and is achieved in a neural circuit, which is demonstrated as having close ties with the circuitry existent in die primate visual cortex. It is further shown that the proposed salicney measure may be extended to address issues that currently elude explanation in the domain of saliency based models. Results on natural images are compared with experimental eye tracking data revealing the efficacy of the model in predicting the deployment of overt attention as compared with existing efforts."
            },
            "slug": "Saliency-Based-on-Information-Maximization-Bruce-Tsotsos",
            "title": {
                "fragments": [],
                "text": "Saliency Based on Information Maximization"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A model of bottom-up overt attention is proposed based on the principle of maximizing information sampled from a scene and is achieved in a neural circuit, which is demonstrated as having close ties with the circuitry existent in die primate visual cortex."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144375727"
                        ],
                        "name": "R. Desimone",
                        "slug": "R.-Desimone",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Desimone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Desimone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145822452"
                        ],
                        "name": "J. Duncan",
                        "slug": "J.-Duncan",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Duncan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Duncan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 135
                            }
                        ],
                        "text": "These works implement selective visual attention from a bottom-up perspective and are often inspired by studies of human eye movements [14], [15], [32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14290580,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "5d2ef51c912df93d31314d6827a98bc474374105",
            "isKey": false,
            "numCitedBy": 7407,
            "numCiting": 257,
            "paperAbstract": {
                "fragments": [],
                "text": "The two basic phenomena that define the problem of visual attention can be illustrated in a simple example. Consider the arrays shown in each panel of Figure 1. In a typical experiment, before the arrays were presented, subjects would be asked to report letters appearing in one color (targets, here black letters), and to disregard letters in the other color (nontargets, here white letters). The array would then be briefly flashed, and the subjects, without any opportunity for eye movements, would give their report. The display mimics our. usual cluttered visual environment: It contains one or more objects that are relevant to current behavior, along with others that are irrelevant. The first basic phenomenon is limited capacity for processing information. At any given time, only a small amount of the information available on the retina can be processed and used in the control of behavior. Subjectively, giving attention to any one target leaves less available for others. In Figure 1, the probability of reporting the target letter N is much lower with two accompa\u00ad nying targets (Figure la) than with none (Figure Ib). The second basic phenomenon is selectivity-the ability to filter out un\u00ad wanted information. Subjectively, one is aware of attended stimuli and largely unaware of unattended ones. Correspondingly, accuracy in identifying an attended stimulus may be independent of the number of nontargets in a display (Figure la vs Ie) (see Bundesen 1990, Duncan 1980)."
            },
            "slug": "Neural-mechanisms-of-selective-visual-attention.-Desimone-Duncan",
            "title": {
                "fragments": [],
                "text": "Neural mechanisms of selective visual attention."
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "The two basic phenomena that define the problem of visual attention can be illustrated in a simple example and selectivity-the ability to filter out un\u00ad wanted information is illustrated."
            },
            "venue": {
                "fragments": [],
                "text": "Annual review of neuroscience"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710312"
                        ],
                        "name": "A. Neubeck",
                        "slug": "A.-Neubeck",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Neubeck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Neubeck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 102
                            }
                        ],
                        "text": "We run nonmaximum suppression on this 4D score space using the efficient technique of Neubeck and Van Gool [43], resulting in a set of local maxima windows Wsmax."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 135
                            }
                        ],
                        "text": "Object tracking in video, as an additional likelihood\nterm preventing the tracker from drifting to the background (ongoing work by Van Gool\u2019s group at ETH Zurich)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "Van Gool [43], resulting in a set of local maxima windows Wmax."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5057778,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "52ca4ed04d1d9dba3e6ae30717898276735e0b79",
            "isKey": true,
            "numCitedBy": 833,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we scrutinize a low level computer vision task - non-maximum suppression (NMS) - which is a crucial preprocessing step in many computer vision applications. Especially in real time scenarios, efficient algorithms for such preprocessing algorithms, which operate on the full image resolution, are important. In the case of NMS, it seems that merely the straightforward implementation or slight improvements are known. We show that these are far from being optimal, and derive several algorithms ranging from easy-to-implement to highly-efficient"
            },
            "slug": "Efficient-Non-Maximum-Suppression-Neubeck-Gool",
            "title": {
                "fragments": [],
                "text": "Efficient Non-Maximum Suppression"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This work scrutinize a low level computer vision task - non-maximum suppression (NMS) - and derive several algorithms ranging from easy-to-implement to highly-efficient."
            },
            "venue": {
                "fragments": [],
                "text": "18th International Conference on Pattern Recognition (ICPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35208858"
                        ],
                        "name": "Subhransu Maji",
                        "slug": "Subhransu-Maji",
                        "structuredName": {
                            "firstName": "Subhransu",
                            "lastName": "Maji",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Subhransu Maji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "Excellent approximations to the histogram intersection kernel [39] and the 2 kernel [51] have been"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2990061,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7086378e68dae59975cf749c101c53a0fa90eab",
            "isKey": false,
            "numCitedBy": 1082,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Straightforward classification using kernelized SVMs requires evaluating the kernel for a test vector and each of the support vectors. For a class of kernels we show that one can do this much more efficiently. In particular we show that one can build histogram intersection kernel SVMs (IKSVMs) with runtime complexity of the classifier logarithmic in the number of support vectors as opposed to linear for the standard approach. We further show that by precomputing auxiliary tables we can construct an approximate classifier with constant runtime and space requirements, independent of the number of support vectors, with negligible loss in classification accuracy on various tasks. This approximation also applies to 1 - chi2 and other kernels of similar form. We also introduce novel features based on a multi-level histograms of oriented edge energy and present experiments on various detection datasets. On the INRIA pedestrian dataset an approximate IKSVM classifier based on these features has the current best performance, with a miss rate 13% lower at 10-6 False Positive Per Window than the linear SVM detector of Dalal & Triggs. On the Daimler Chrysler pedestrian dataset IKSVM gives comparable accuracy to the best results (based on quadratic SVM), while being 15times faster. In these experiments our approximate IKSVM is up to 2000times faster than a standard implementation and requires 200times less memory. Finally we show that a 50times speedup is possible using approximate IKSVM based on spatial pyramid features on the Caltech 101 dataset with negligible loss of accuracy."
            },
            "slug": "Classification-using-intersection-kernel-support-is-Maji-Berg",
            "title": {
                "fragments": [],
                "text": "Classification using intersection kernel support vector machines is efficient"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that one can build histogram intersection kernel SVMs (IKSVMs) with runtime complexity of the classifier logarithmic in the number of support vectors as opposed to linear for the standard approach."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1917767"
                        ],
                        "name": "W. Einh\u00e4user",
                        "slug": "W.-Einh\u00e4user",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Einh\u00e4user",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Einh\u00e4user"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40089171"
                        ],
                        "name": "P. K\u00f6nig",
                        "slug": "P.-K\u00f6nig",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "K\u00f6nig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. K\u00f6nig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7763820,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "b3cbe851a041fb82a2caf73e3a3aa60713275fda",
            "isKey": false,
            "numCitedBy": 237,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "In natural environments, humans select a subset of visual stimuli by directing their gaze to locations attended. In previous studies it has been found that at fixation points luminance\u2010contrast is higher than average. This led to the hypothesis that luminance\u2010contrast makes a major contribution to a saliency map of visual overt attention, consistent with a computation of stimulus saliency in early visual cortical areas. We re\u2010evaluate this hypothesis by using natural and modified natural images to uncover the causal effects of luminance\u2010contrast to human overt visual attention and: (i) we confirm that when viewing natural images, contrasts are elevated at fixation points. This, however, only holds for low spatial frequencies and in a limited temporal window after stimulus onset; (ii) however, despite this correlation between overt attention and luminance\u2010contrast, moderate modifications of contrast in natural images do not measurably affect the selection of fixation points. Furthermore, strong local reductions of luminance\u2010contrast do not repel but attract fixation; (iii) neither contrast nor contrast modification is correlated to fixation duration; and (iv), even the moderate contrast modifications used fall into the physiologically relevant range, and subjects are well able to detect them in a forced choice paradigm. In summary, no causal contribution of luminance\u2010contrast to a saliency map of human overt attention is detectable. In conjunction with recent results on the relation of contrast sensitivity of neuronal activity to the level in the visual cortical hierarchy, the present study provides evidence that, for natural scenes, saliency is computed not early but late during processing."
            },
            "slug": "Does-luminance\u2010contrast-contribute-to-a-saliency-Einh\u00e4user-K\u00f6nig",
            "title": {
                "fragments": [],
                "text": "Does luminance\u2010contrast contribute to a saliency map for overt visual attention?"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "No causal contribution of luminance\u2010contrast to a saliency map of human overt attention is detectable, and evidence is provided that, for natural scenes, saliency is computed not early but late during processing."
            },
            "venue": {
                "fragments": [],
                "text": "The European journal of neuroscience"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "119995642"
                        ],
                        "name": "Franck Moosmann",
                        "slug": "Franck-Moosmann",
                        "structuredName": {
                            "firstName": "Franck",
                            "lastName": "Moosmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Franck Moosmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2295553"
                        ],
                        "name": "Diane Larlus",
                        "slug": "Diane-Larlus",
                        "structuredName": {
                            "firstName": "Diane",
                            "lastName": "Larlus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diane Larlus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "82117876"
                        ],
                        "name": "F. Jurie",
                        "slug": "F.-Jurie",
                        "structuredName": {
                            "firstName": "Fr\u00e9d\u00e9ric",
                            "lastName": "Jurie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jurie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "A few works [40], [42], [53] define as salient the visual characteristics that best distinguish a particular object class (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17603866,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "53220fd070c23c44dd9b0557ea019104ec13c93a",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel approach for object category recognition that can find objects in challenging conditions using visual attention technique. It combines saliency maps very closely with the extraction of random subwindows for classification purposes. The maps are built online by the classifier while being used by it to classify the image."
            },
            "slug": "Learning-Saliency-Maps-for-Object-Categorization-Moosmann-Larlus",
            "title": {
                "fragments": [],
                "text": "Learning Saliency Maps for Object Categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A novel approach for object category recognition that can find objects in challenging conditions using visual attention technique that combines saliency maps very closely with the extraction of random subwindows for classification purposes."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2264779"
                        ],
                        "name": "T. Kadir",
                        "slug": "T.-Kadir",
                        "structuredName": {
                            "firstName": "Timor",
                            "lastName": "Kadir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kadir"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144431498"
                        ],
                        "name": "M. Brady",
                        "slug": "M.-Brady",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Brady",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Brady"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8868228,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5a3ad8b87e865665d6879a63578990af1bde8055",
            "isKey": false,
            "numCitedBy": 558,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe a novel technique for detecting salient regions in an image. The detector is a generalization to affine invariance of the method introduced by Kadir and Brady [10]. The detector deems a region salient if it exhibits unpredictability in both its attributes and its spatial scale."
            },
            "slug": "An-Affine-Invariant-Salient-Region-Detector-Kadir-Zisserman",
            "title": {
                "fragments": [],
                "text": "An Affine Invariant Salient Region Detector"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A novel technique for detecting salient regions in an image is described, which is a generalization to affine invariance of the method introduced by Kadir and Brady."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107641"
                        ],
                        "name": "Xuan Bao",
                        "slug": "Xuan-Bao",
                        "structuredName": {
                            "firstName": "Xuan",
                            "lastName": "Bao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuan Bao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35668370"
                        ],
                        "name": "Trevor Narayan",
                        "slug": "Trevor-Narayan",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Narayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Narayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34651617"
                        ],
                        "name": "A. A. Sani",
                        "slug": "A.-A.-Sani",
                        "structuredName": {
                            "firstName": "Ardalan",
                            "lastName": "Sani",
                            "middleNames": [
                                "Amiri"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. A. Sani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145252638"
                        ],
                        "name": "Wolfgang Richter",
                        "slug": "Wolfgang-Richter",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Richter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wolfgang Richter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694368"
                        ],
                        "name": "Romit Roy Choudhury",
                        "slug": "Romit-Roy-Choudhury",
                        "structuredName": {
                            "firstName": "Romit",
                            "lastName": "Choudhury",
                            "middleNames": [
                                "Roy"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Romit Roy Choudhury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2171880383"
                        ],
                        "name": "Lin Zhong",
                        "slug": "Lin-Zhong",
                        "structuredName": {
                            "firstName": "Lin",
                            "lastName": "Zhong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lin Zhong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747303"
                        ],
                        "name": "M. Satyanarayanan",
                        "slug": "M.-Satyanarayanan",
                        "structuredName": {
                            "firstName": "Mahadev",
                            "lastName": "Satyanarayanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Satyanarayanan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 29
                            }
                        ],
                        "text": "Content-aware image resizing [5], [48]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6040084,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "635847ad3ebbbe421b328dcec3927badc671c03d",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The proliferation of pictures and videos in the Internet is imposing heavy demands on mobile data networks. This demand is expected to grow rapidly and a one-fit-all solution is unforeseeable. While researchers are approaching the problem from different directions, we identify a human-centric opportunity to reduce content size. Our intuition is that humans exhibit unequal interest towards different parts of a content, and parts that are less important may be traded off for price/performance benefits. For instance, a picture with the Statue of Liberty against a blue sky may be partitioned into two categories -- the semantically important statue, and the less important blue sky. When the need to minimize bandwidth/energy is acute, only the picture of the statue may be downloaded, along with a meta tag \"background: blue sky\". Once downloaded, an arbitrary \"blue sky\" may be suitably inserted behind the statue, reconstructing an approximation of the original picture. As long as the essence of the picture is retained from the human's perspective, such an approximation may be acceptable. This paper attempts to explore the scope and usefulness of this idea, and develop a broader research theme that we call context-aware compression."
            },
            "slug": "The-case-for-context-aware-compression-Bao-Narayan",
            "title": {
                "fragments": [],
                "text": "The case for context-aware compression"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper identifies a human-centric opportunity to reduce content size, and develops a broader research theme that is context-aware compression, and attempts to explore the scope and usefulness of this idea."
            },
            "venue": {
                "fragments": [],
                "text": "HotMobile '11"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7326223"
                        ],
                        "name": "L. Itti",
                        "slug": "L.-Itti",
                        "structuredName": {
                            "firstName": "Laurent",
                            "lastName": "Itti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Itti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145624227"
                        ],
                        "name": "C. Koch",
                        "slug": "C.-Koch",
                        "structuredName": {
                            "firstName": "Christof",
                            "lastName": "Koch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Koch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3271571"
                        ],
                        "name": "E. Niebur",
                        "slug": "E.-Niebur",
                        "structuredName": {
                            "firstName": "Ernst",
                            "lastName": "Niebur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Niebur"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "The Itti-Direct curve refers to [28] as originally proposed."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 83
                            }
                        ],
                        "text": "sometimes it is unique within the image and stands out as salient [7], [21], [27], [28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 103
                            }
                        ],
                        "text": "The saliency map is computed and the most salient objects are extracted from it using Itti\u2019s procedure [28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "Since [28], numerous works [7], [21], [24], [27], [31] appeared to measure the saliency of pixels, as the degree of uniqueness of their neighborhood w."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "(a) MS versus [27], [28]; (b) baselines and semantic labeling [22]; (c) single cues."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 230
                            }
                        ],
                        "text": "On the task of detecting objects in the challenging PASCAL VOC 07 dataset [17], we demonstrate that the combined objectness measure performs better than any cue alone, and also outperforms traditional salient blob detectors [27], [28], interest point detectors [41], the Semantic Labeling technique of Gould et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "We compare our MS cue to [27] and [28] (Fig."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 148
                            }
                        ],
                        "text": "On the task of detecting objects of new classes unseen during training, we have demonstrated that objectness outperforms traditional saliency [27], [28], interest point detectors, Semantic Labeling [22], and the HOG detector [11]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3108956,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4816f0b6f0d05da3901441bfa5cc7be044b4da8b",
            "isKey": true,
            "numCitedBy": 9757,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "A visual attention system, inspired by the behavior and the neuronal architecture of the early primate visual system, is presented. Multiscale image features are combined into a single topographical saliency map. A dynamical neural network then selects attended locations in order of decreasing saliency. The system breaks down the complex problem of scene understanding by rapidly selecting, in a computationally efficient manner, conspicuous locations to be analyzed in detail."
            },
            "slug": "A-Model-of-Saliency-Based-Visual-Attention-for-Itti-Koch",
            "title": {
                "fragments": [],
                "text": "A Model of Saliency-Based Visual Attention for Rapid Scene Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A visual attention system, inspired by the behavior and the neuronal architecture of the early primate visual system, is presented, which breaks down the complex problem of scene understanding by rapidly selecting conspicuous locations to be analyzed in detail."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1832462"
                        ],
                        "name": "F. Crow",
                        "slug": "F.-Crow",
                        "structuredName": {
                            "firstName": "Franklin",
                            "lastName": "Crow",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Crow"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 135
                            }
                        ],
                        "text": "For every scale s 2 f16; 24; 32; 48; 64g and channel c we rescale the image to s s and then compute MS\u00f0w; MS\u00de using one integral image [10] (indicating the sum over the saliency of pixels in a rectangle)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2210332,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "8b22e1751f75be137b7b210981baccc1b9ab9222",
            "isKey": false,
            "numCitedBy": 1468,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Texture-map computations can be made tractable through use of precalculated tables which allow computational costs independent of the texture density. The first example of this technique, the \u201cmip\u201d map, uses a set of tables containing successively lower-resolution representations filtered down from the discrete texture function. An alternative method using a single table of values representing the integral over the texture function rather than the function itself may yield superior results at similar cost. The necessary algorithms to support the new technique are explained. Finally, the cost and performance of the new technique is compared to previous techniques."
            },
            "slug": "Summed-area-tables-for-texture-mapping-Crow",
            "title": {
                "fragments": [],
                "text": "Summed-area tables for texture mapping"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "Texture-map computations can be made tractable through use of precalculated tables which allow computational costs independent of the texture density, and the cost and performance of the new technique is compared to previous techniques."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49800308"
                        ],
                        "name": "B. Silverman",
                        "slug": "B.-Silverman",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Silverman",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Silverman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": "We compute the probability of an image window to cover an object using kernel density estimation [44] in the 4D spaceW of all possible windows in an image."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122112046,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "500848eef68baa3b29bee902f643b783ec12ee1c",
            "isKey": false,
            "numCitedBy": 306,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : A class of probability density estimates can be obtained by penalizing the likelihood by a functional which depends on the roughness of the logarithm of the density. The limiting case of the estimates as the amount of smoothing increasing has a natural form which makes the method attractive for data analysis and which provides a rationale for a particular choice of roughness penalty. The estimates are shown to be the solution of an unconstrained convex optimization problem, and mild natural conditions are given for them to exist. Rates of consistency in various norms and conditions for asymptotic normality and approximation by a Gaussian process are given, thus breaking new ground in the theory of maximum penalized likelihood density estimation. (Author)"
            },
            "slug": "On-the-Estimation-of-a-Probability-Density-Function-Silverman",
            "title": {
                "fragments": [],
                "text": "On the Estimation of a Probability Density Function by the Maximum Penalized Likelihood Method"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2856152"
                        ],
                        "name": "E. Parzen",
                        "slug": "E.-Parzen",
                        "structuredName": {
                            "firstName": "Emanuel",
                            "lastName": "Parzen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Parzen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 122932724,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "de28c165623adabcdba0fdb18b65eba685aaf31d",
            "isKey": false,
            "numCitedBy": 9488,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Given a sequence of independent identically distributed random variables with a common probability density function, the problem of the estimation of a probability density function and of determining the mode of a probability function are discussed. Only estimates which are consistent and asymptotically normal are constructed. (Author)"
            },
            "slug": "On-Estimation-of-a-Probability-Density-Function-and-Parzen",
            "title": {
                "fragments": [],
                "text": "On Estimation of a Probability Density Function and Mode"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "95126559"
                        ],
                        "name": "\u6cf0\u660e \u5ca1\u7530",
                        "slug": "\u6cf0\u660e-\u5ca1\u7530",
                        "structuredName": {
                            "firstName": "\u6cf0\u660e",
                            "lastName": "\u5ca1\u7530",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u6cf0\u660e \u5ca1\u7530"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1453922636"
                        ],
                        "name": "\u667a\u4e00 \u5409\u7530",
                        "slug": "\u667a\u4e00-\u5409\u7530",
                        "structuredName": {
                            "firstName": "\u667a\u4e00",
                            "lastName": "\u5409\u7530",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u667a\u4e00 \u5409\u7530"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52621153"
                        ],
                        "name": "\u667a\u6674 \u9577\u5c3e",
                        "slug": "\u667a\u6674-\u9577\u5c3e",
                        "structuredName": {
                            "firstName": "\u667a\u6674",
                            "lastName": "\u9577\u5c3e",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u667a\u6674 \u9577\u5c3e"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "where S\u00f0 SS\u00de is the set of superpixels obtained using [19] with a segmentation scale SS (learned in Section 3)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "We obtain superpixels S\u00f0 SS\u00de using the algorithm of [19] with segmentation scale SS ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 102
                            }
                        ],
                        "text": "A different way of capturing the closed boundary characteristic of objects rests on using superpixels [19] as features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "5 seconds, including the time to compute superpixels using [19]; (LS): around 0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 107326680,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "b3df1ffb8a6856f20be51243406f2fdbe2a335ba",
            "isKey": true,
            "numCitedBy": 1751,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Efficient-Graph-Based-Image-\u6cf0\u660e-\u667a\u4e00",
            "title": {
                "fragments": [],
                "text": "Efficient Graph-Based Image Segmentation\u3092\u7528\u3044\u305f\u5703\u5834\u56f3\u81ea\u52d5\u4f5c\u6210\u624b\u6cd5\u306e\u691c\u8a0e"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2141732326"
                        ],
                        "name": "Jianguo Zhang",
                        "slug": "Jianguo-Zhang",
                        "structuredName": {
                            "firstName": "Jianguo",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianguo Zhang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 179
                            }
                        ],
                        "text": "We compare the 3 object detectors [11, 18, 33] to our algorithm 1 on the entire PASCAL VOC 07 test set (20 object classes over 4952 images) using the standard PASCAL VOC protocol [17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 140
                            }
                        ],
                        "text": "We learn the parameters of the objectness cues from a training dataset T consisting of 1183 images from the PASCAL VOC 07 train+val dataset [17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "2We follow the widespread PASCAL criterion [17], and consider a window w to cover an object o if |w \u22c2 o|/|w \u22c3 o| > 0."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "These approaches do not seem suitable for the PASCAL VOC 07 dataset [17] where many objects are present in an image and they are rarely dominant (fig."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "We evaluate the performance of the objectness measure on the popular PASCAL VOC 07 dataset [17], which is commonly used to evaluate class-specific object detectors [11, 18, 50]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "(b) On the task of detecting objects in the challenging PASCAL VOC 07 dataset [17], we demonstrate that the combined objectness measure performs better than any cue alone, and also outperforms traditional salient blob detectors [27, 28], interest point detectors [41], the Semantic Labeling technique of [22], and a HOG detector [11] trained to detect objects of arbitrary classes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 63925014,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "0ec48ac86456cea3d6d6172ca81ef68e98b21a61",
            "isKey": true,
            "numCitedBy": 3322,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-PASCAL-Visual-Object-Classes-Challenge-Zhang",
            "title": {
                "fragments": [],
                "text": "The PASCAL Visual Object Classes Challenge"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 39
                            }
                        ],
                        "text": "Although a variety of approaches exist [4, 11, 35], most state-of-the-art detectors follow the sliding-window paradigm [11, 12, 18, 25, 33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multi-stage contour based detection"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 119
                            }
                        ],
                        "text": "Although a variety of approaches exist [4, 11, 35], most state-of-the-art detectors follow the sliding-window paradigm [11, 12, 18, 25, 33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 44
                            }
                        ],
                        "text": "Many modern class-specific object detectors [12, 18, 25] are based on sliding windows."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discriminative models for multiclass object layout"
            },
            "venue": {
                "fragments": [],
                "text": "In ICCV,"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "11 no image used to train objectness is in this test set (sec. 3)"
            },
            "venue": {
                "fragments": [],
                "text": "11 no image used to train objectness is in this test set (sec. 3)"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The STAIR Vision Library (v2.4), software available at http://ai"
            },
            "venue": {
                "fragments": [],
                "text": "The STAIR Vision Library (v2.4), software available at http://ai"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The STAIR Vision Library"
            },
            "venue": {
                "fragments": [],
                "text": "The STAIR Vision Library"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Objectness measure, object detection, object recognition\n\u00c7"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 13
                            }
                        ],
                        "text": "In addition to the two applications above, objectness is valuable in others as well."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "What is an object? In CVPR"
            },
            "venue": {
                "fragments": [],
                "text": "What is an object? In CVPR"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Index Terms\u2014Objectness measure, object detection, object recognition\n\u00c7"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Weakly supervised learning of interactions between humans and objects. TPAMI (accepted for publication"
            },
            "venue": {
                "fragments": [],
                "text": "Weakly supervised learning of interactions between humans and objects. TPAMI (accepted for publication"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[1] find salient regions using a frequency-tuned approach."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Susstrunk. Frequancy-tuned salient region detection"
            },
            "venue": {
                "fragments": [],
                "text": "In CVPR,"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 66
                            }
                        ],
                        "text": "In our experiments we use the source code6 provided for SL within [23]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Software available at http://ai.stanford.edu/ sgould/svl"
            },
            "venue": {
                "fragments": [],
                "text": "The STAIR Vision Library"
            },
            "year": 2010
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 36,
            "methodology": 20,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 62,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/Measuring-the-Objectness-of-Image-Windows-Alexe-Deselaers/2eb6caace8296fd4dfd4947efa4fe911c8e133b2?sort=total-citations"
}