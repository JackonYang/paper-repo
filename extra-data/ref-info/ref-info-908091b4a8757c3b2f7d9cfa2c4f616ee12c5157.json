{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7487588,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54d2b5c64a67f65c5dd812b89e07973f97699552",
            "isKey": false,
            "numCitedBy": 1868,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "With the advent of the Internet, billions of images are now freely available online and constitute a dense sampling of the visual world. Using a variety of non-parametric methods, we explore this world with the aid of a large dataset of 79,302,017 images collected from the Internet. Motivated by psychophysical results showing the remarkable tolerance of the human visual system to degradations in image resolution, the images in the dataset are stored as 32 x 32 color images. Each image is loosely labeled with one of the 75,062 non-abstract nouns in English, as listed in the Wordnet lexical database. Hence the image database gives a comprehensive coverage of all object categories and scenes. The semantic information from Wordnet can be used in conjunction with nearest-neighbor methods to perform object classification over a range of semantic levels minimizing the effects of labeling noise. For certain classes that are particularly prevalent in the dataset, such as people, we are able to demonstrate a recognition performance comparable to class-specific Viola-Jones style detectors."
            },
            "slug": "80-Million-Tiny-Images:-A-Large-Data-Set-for-Object-Torralba-Fergus",
            "title": {
                "fragments": [],
                "text": "80 Million Tiny Images: A Large Data Set for Nonparametric Object and Scene Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "For certain classes that are particularly prevalent in the dataset, such as people, this work is able to demonstrate a recognition performance comparable to class-specific Viola-Jones style detectors."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749609"
                        ],
                        "name": "S. Lazebnik",
                        "slug": "S.-Lazebnik",
                        "structuredName": {
                            "firstName": "Svetlana",
                            "lastName": "Lazebnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lazebnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189388"
                        ],
                        "name": "J. Ponce",
                        "slug": "J.-Ponce",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Ponce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ponce"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 463,
                                "start": 437
                            }
                        ],
                        "text": "How many kinds of scenes are there? How can the knowledge about environmental scenes be organized? How do the current state-of-art scene models perform on more realistic and ill-controlled environments, and how does this compare to human performance? To date, computational work on scene and place recognition has classified natural images within a limited number of semantic categories, representing typical indoor and outdoor settings [17, 7, 23, 32, 21, 3, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 198
                            }
                        ],
                        "text": "Previous attempts to characterize environmental scenes have capitalized on uncovering a manageable set of dimensions, features or objects that are correlated with the semantic of purpose of a space [21, 17, 7, 32, 12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 47
                            }
                        ],
                        "text": "Recognition performance on the 15 scene dataset[21, 17, 7], and our SUN database."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 83
                            }
                        ],
                        "text": "Dense SIFT features are also found to perform very well at the 15-category dataset [17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "Dense SIFT: As with HOG2x2, SIFT descriptors are densely extracted [17] using a flat rather than Gaussian window at two scales (4 and 8 pixel radii) on a regular grid at steps of 5 pixels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 115
                            }
                        ],
                        "text": "To better establish comparisons with other papers we start by providing results on the 15 scene categories dataset [21, 17, 7] (Figure 4(a))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "Histogram intersection[17] is used to define the similarity of two histograms at the same pyramid level for two images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 170
                            }
                        ],
                        "text": "The three descriptors are stacked together for each HSV color channels, and quantized into 300 visual words by k-means, and spatial pyramid histograms are used as kernels[17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "the 15 in [17]), but the real world is not so neatly divided."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2421251,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6dbaff29d3898cf60f63f5a34cb9610ebb75220c",
            "isKey": true,
            "numCitedBy": 8328,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method for recognizing scene categories based on approximate global geometric correspondence. This technique works by partitioning the image into increasingly fine sub-regions and computing histograms of local features found inside each sub-region. The resulting \"spatial pyramid\" is a simple and computationally efficient extension of an orderless bag-of-features image representation, and it shows significantly improved performance on challenging scene categorization tasks. Specifically, our proposed method exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories. The spatial pyramid framework also offers insights into the success of several recently proposed image descriptions, including Torralba\u2019s \"gist\" and Lowe\u2019s SIFT descriptors."
            },
            "slug": "Beyond-Bags-of-Features:-Spatial-Pyramid-Matching-Lazebnik-Schmid",
            "title": {
                "fragments": [],
                "text": "Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This paper presents a method for recognizing scene categories based on approximate global geometric correspondence that exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47655614"
                        ],
                        "name": "G. Griffin",
                        "slug": "G.-Griffin",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Griffin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Griffin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144160673"
                        ],
                        "name": "Alex Holub",
                        "slug": "Alex-Holub",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Holub",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Holub"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 139
                            }
                        ],
                        "text": "Whereas the fields of computer vision and cognitive science have developed several databases to organize knowledge about object categories [10, 28], a comprehensive database of real world scenes does not currently exist (the largest available dataset of scene categories contains only 15 classes)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118828957,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a5effa909cdeafaddbbb7855037e02f8e25d632",
            "isKey": false,
            "numCitedBy": 2545,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a challenging set of 256 object categories containing a total of 30607 images. The original Caltech-101 [1] was collected by choosing a set of object categories, downloading examples from Google Images and then manually screening out all images that did not fit the category. Caltech-256 is collected in a similar manner with several improvements: a) the number of categories is more than doubled, b) the minimum number of images in any category is increased from 31 to 80, c) artifacts due to image rotation are avoided and d) a new and larger clutter category is introduced for testing background rejection. We suggest several testing paradigms to measure classification performance, then benchmark the dataset using two simple metrics as well as a state-of-the-art spatial pyramid matching [2] algorithm. Finally we use the clutter category to train an interest detector which rejects uninformative background regions."
            },
            "slug": "Caltech-256-Object-Category-Dataset-Griffin-Holub",
            "title": {
                "fragments": [],
                "text": "Caltech-256 Object Category Dataset"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A challenging set of 256 object categories containing a total of 30607 images is introduced and the clutter category is used to train an interest detector which rejects uninformative background regions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143868587"
                        ],
                        "name": "A. Oliva",
                        "slug": "A.-Oliva",
                        "structuredName": {
                            "firstName": "Aude",
                            "lastName": "Oliva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Oliva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 463,
                                "start": 437
                            }
                        ],
                        "text": "How many kinds of scenes are there? How can the knowledge about environmental scenes be organized? How do the current state-of-art scene models perform on more realistic and ill-controlled environments, and how does this compare to human performance? To date, computational work on scene and place recognition has classified natural images within a limited number of semantic categories, representing typical indoor and outdoor settings [17, 7, 23, 32, 21, 3, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 198
                            }
                        ],
                        "text": "Previous attempts to characterize environmental scenes have capitalized on uncovering a manageable set of dimensions, features or objects that are correlated with the semantic of purpose of a space [21, 17, 7, 32, 12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 47
                            }
                        ],
                        "text": "Recognition performance on the 15 scene dataset[21, 17, 7], and our SUN database."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 115
                            }
                        ],
                        "text": "To better establish comparisons with other papers we start by providing results on the 15 scene categories dataset [21, 17, 7] (Figure 4(a))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "GIST: The GIST descriptor [21] computes the output energy of a bank of 24 filters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "GIST features [21] are proposed specifically for scene recognition tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [21], scenes are represented along continuous dimensions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11664336,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "869171b2f56cfeaa9b81b2626cb4956fea590a57",
            "isKey": false,
            "numCitedBy": 6522,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a computational model of the recognition of real world scenes that bypasses the segmentation and the processing of individual objects or regions. The procedure is based on a very low dimensional representation of the scene, that we term the Spatial Envelope. We propose a set of perceptual dimensions (naturalness, openness, roughness, expansion, ruggedness) that represent the dominant spatial structure of a scene. Then, we show that these dimensions may be reliably estimated using spectral and coarsely localized information. The model generates a multidimensional space in which scenes sharing membership in semantic categories (e.g., streets, highways, coasts) are projected closed together. The performance of the spatial envelope model shows that specific information about object shape or identity is not a requirement for scene categorization and that modeling a holistic representation of the scene informs about its probable semantic category."
            },
            "slug": "Modeling-the-Shape-of-the-Scene:-A-Holistic-of-the-Oliva-Torralba",
            "title": {
                "fragments": [],
                "text": "Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The performance of the spatial envelope model shows that specific information about object shape or identity is not a requirement for scene categorization and that modeling a holistic representation of the scene informs about its probable semantic category."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066819269"
                        ],
                        "name": "James Philbin",
                        "slug": "James-Philbin",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Philbin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Philbin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700928"
                        ],
                        "name": "O. Chum",
                        "slug": "O.-Chum",
                        "structuredName": {
                            "firstName": "Ond\u0159ej",
                            "lastName": "Chum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2090818"
                        ],
                        "name": "M. Isard",
                        "slug": "M.-Isard",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Isard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Isard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782755"
                        ],
                        "name": "Josef Sivic",
                        "slug": "Josef-Sivic",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Sivic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josef Sivic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "An image is represented by two 1,000 dimension histograms where each SIFT is soft-assigned, as in [22], to its nearest cluster centers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6875974,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b6371f8c70c2684faefd99fffcc556c3a75dd7f4",
            "isKey": false,
            "numCitedBy": 1480,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The state of the art in visual object retrieval from large databases is achieved by systems that are inspired by text retrieval. A key component of these approaches is that local regions of images are characterized using high-dimensional descriptors which are then mapped to ldquovisual wordsrdquo selected from a discrete vocabulary.This paper explores techniques to map each visual region to a weighted set of words, allowing the inclusion of features which were lost in the quantization stage of previous systems. The set of visual words is obtained by selecting words based on proximity in descriptor space. We describe how this representation may be incorporated into a standard tf-idf architecture, and how spatial verification is modified in the case of this soft-assignment. We evaluate our method on the standard Oxford Buildings dataset, and introduce a new dataset for evaluation. Our results exceed the current state of the art retrieval performance on these datasets, particularly on queries with poor initial recall where techniques like query expansion suffer. Overall we show that soft-assignment is always beneficial for retrieval with large vocabularies, at a cost of increased storage requirements for the index."
            },
            "slug": "Lost-in-quantization:-Improving-particular-object-Philbin-Chum",
            "title": {
                "fragments": [],
                "text": "Lost in quantization: Improving particular object retrieval in large scale image databases"
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150180356"
                        ],
                        "name": "L. Walker",
                        "slug": "L.-Walker",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Walker",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Walker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9890083,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "174f9066e8a318e7c5c9545396c89cd3beb4a4de",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Subjects were asked to discriminate scenes after very brief exposures (37-69 ms). Their performance was always above chance and increased with exposure duration, confirming that subjects can get the gist of a scene with one fixation. We propose that a simple texture analysis of the image can provide a useful cue towards rapid scene identification. Our model learns texture features across scene categories and then uses this knowledge to categorize new scenes. The texture analysis leads to similar categorizations and confusions as subjects with limited processing time. We conclude that a simple texture discrimination model mostly explains early scene identification."
            },
            "slug": "When-is-scene-recognition-just-texture-recognition-Walker-Malik",
            "title": {
                "fragments": [],
                "text": "When is scene recognition just texture recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The model learns texture features across scene categories and then uses this knowledge to categorize new scenes and conclude that a simple texture discrimination model mostly explains early scene identification."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144430305"
                        ],
                        "name": "Jean-Fran\u00e7ois Lalonde",
                        "slug": "Jean-Fran\u00e7ois-Lalonde",
                        "structuredName": {
                            "firstName": "Jean-Fran\u00e7ois",
                            "lastName": "Lalonde",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Fran\u00e7ois Lalonde"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2433269"
                        ],
                        "name": "Derek Hoiem",
                        "slug": "Derek-Hoiem",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Hoiem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Derek Hoiem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756036"
                        ],
                        "name": "C. Rother",
                        "slug": "C.-Rother",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Rother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rother"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716777"
                        ],
                        "name": "A. Criminisi",
                        "slug": "A.-Criminisi",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Criminisi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Criminisi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 65
                            }
                        ],
                        "text": "Geometry Specific Histograms: Inspired by \u201cIllumination Context\u201d [16], we build color and texton histograms for each geometric class (ground, vertical, porous, and sky)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12312803,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ddf7f6917a3b81f0ba45dba96e60d96a452b5c38",
            "isKey": false,
            "numCitedBy": 219,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a system for inserting new objects into existing photographs by querying a vast image-based object library, pre-computed using a publicly available Internet object database. The central goal is to shield the user from all of the arduous tasks typically involved in image compositing. The user is only asked to do two simple things: 1) pick a 3D location in the scene to place a new object; 2) select an object to insert using a hierarchical menu. We pose the problem of object insertion as a data-driven, 3D-based, context-sensitive object retrieval task. Instead of trying to manipulate the object to change its orientation, color distribution, etc. to fit the new image, we simply retrieve an object of a specified class that has all the required properties (camera pose, lighting, resolution, etc) from our large object library. We present new automatic algorithms for improving object segmentation and blending, estimating true 3D object size and orientation, and estimating scene lighting conditions. We also present an intuitive user interface that makes object insertion fast and simple even for the artistically challenged."
            },
            "slug": "Photo-clip-art-Lalonde-Hoiem",
            "title": {
                "fragments": [],
                "text": "Photo clip art"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A system for inserting new objects into existing photographs by querying a vast image-based object library, pre-computed using a publicly available Internet object database, to shield the user from all of the arduous tasks typically involved in image compositing."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Graph."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2433269"
                        ],
                        "name": "Derek Hoiem",
                        "slug": "Derek-Hoiem",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Hoiem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Derek Hoiem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 198
                            }
                        ],
                        "text": "Previous attempts to characterize environmental scenes have capitalized on uncovering a manageable set of dimensions, features or objects that are correlated with the semantic of purpose of a space [21, 17, 7, 32, 12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206769405,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "ae89592317675c9c7642a3976c3a064cef736f92",
            "isKey": false,
            "numCitedBy": 757,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Many computer vision algorithms limit their performance by ignoring the underlying 3D geometric structure in the image. We show that we can estimate the coarse geometric properties of a scene by learning appearance-based models of geometric classes, even in cluttered natural scenes. Geometric classes describe the 3D orientation of an image region with respect to the camera. We provide a multiple-hypothesis framework for robustly estimating scene structure from a single image and obtaining confidences for each geometric label. These confidences can then be used to improve the performance of many other applications. We provide a thorough quantitative evaluation of our algorithm on a set of outdoor images and demonstrate its usefulness in two applications: object detection and automatic single-view reconstruction."
            },
            "slug": "Geometric-context-from-a-single-image-Hoiem-Efros",
            "title": {
                "fragments": [],
                "text": "Geometric context from a single image"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "This work shows that it can estimate the coarse geometric properties of a scene by learning appearance-based models of geometric classes, even in cluttered natural scenes, and provides a multiple-hypothesis framework for robustly estimating scene structure from a single image and obtaining confidences for each geometric label."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3303774"
                        ],
                        "name": "L. Renninger",
                        "slug": "L.-Renninger",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Renninger",
                            "middleNames": [
                                "Walker"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Renninger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14694860,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "d0632c640086bdde66066542a4670d5e165ef381",
            "isKey": false,
            "numCitedBy": 251,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "When-is-scene-identification-just-texture-Renninger-Malik",
            "title": {
                "fragments": [],
                "text": "When is scene identification just texture recognition?"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40538579"
                        ],
                        "name": "J. Vogel",
                        "slug": "J.-Vogel",
                        "structuredName": {
                            "firstName": "Julia",
                            "lastName": "Vogel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Vogel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12322757,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e264e1e55433f158bf8aa8b260bf430d76d5fa28",
            "isKey": false,
            "numCitedBy": 429,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a novel image representation that renders it possible to access natural scenes by local semantic description. Our work is motivated by the continuing effort in content-based image retrieval to extract and to model the semantic content of images. The basic idea of the semantic modeling is to classify local image regions into semantic concept classes such as water, rocks, or foliage. Images are represented through the frequency of occurrence of these local concepts. Through extensive experiments, we demonstrate that the image representation is well suited for modeling the semantic content of heterogenous scene categories, and thus for categorization and retrieval.The image representation also allows us to rank natural scenes according to their semantic similarity relative to certain scene categories. Based on human ranking data, we learn a perceptually plausible distance measure that leads to a high correlation between the human and the automatically obtained typicality ranking. This result is especially valuable for content-based image retrieval where the goal is to present retrieval results in descending semantic similarity from the query."
            },
            "slug": "Semantic-Modeling-of-Natural-Scenes-for-Image-Vogel-Schiele",
            "title": {
                "fragments": [],
                "text": "Semantic Modeling of Natural Scenes for Content-Based Image Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A novel image representation is presented that renders it possible to access natural scenes by local semantic description by using a perceptually plausible distance measure that leads to a high correlation between the human and the automatically obtained typicality ranking."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6387937,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a2252ccce2b65abc3759149b5c06587cc318e2f",
            "isKey": false,
            "numCitedBy": 3886,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel approach to learn and recognize natural scene categories. Unlike previous work, it does not require experts to annotate the training set. We represent the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning. Each region is represented as part of a \"theme\". In previous work, such themes were learnt from hand-annotations of experts, while our method learns the theme distributions as well as the codewords distribution over the themes without supervision. We report satisfactory categorization performances on a large set of 13 categories of complex scenes."
            },
            "slug": "A-Bayesian-hierarchical-model-for-learning-natural-Fei-Fei-Perona",
            "title": {
                "fragments": [],
                "text": "A Bayesian hierarchical model for learning natural scene categories"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "This work proposes a novel approach to learn and recognize natural scene categories by representing the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 68
                            }
                        ],
                        "text": "This notion of overlap is not symmetric as it is in object detection[6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 66
                            }
                        ],
                        "text": "Our terminology is consistent with the object detection literature[6] where object classification involves classifying entire images, while object detection requires"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4246903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82635fb63640ae95f90ee9bdc07832eb461ca881",
            "isKey": false,
            "numCitedBy": 11683,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension."
            },
            "slug": "The-Pascal-Visual-Object-Classes-(VOC)-Challenge-Everingham-Gool",
            "title": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The state-of-the-art in evaluated methods for both classification and detection are reviewed, whether the methods are statistically different, what they are learning from the images, and what the methods find easy or confuse."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056417995"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054953874"
                        ],
                        "name": "M. Rubin",
                        "slug": "M.-Rubin",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Rubin",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 463,
                                "start": 437
                            }
                        ],
                        "text": "How many kinds of scenes are there? How can the knowledge about environmental scenes be organized? How do the current state-of-art scene models perform on more realistic and ill-controlled environments, and how does this compare to human performance? To date, computational work on scene and place recognition has classified natural images within a limited number of semantic categories, representing typical indoor and outdoor settings [17, 7, 23, 32, 21, 3, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 657043,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37684bb33f21dc70619597498f1b95fb44fbb139",
            "isKey": false,
            "numCitedBy": 977,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "While navigating in an environment, a vision system has to be able to recognize where it is and what the main objects in the scene are. We present a context-based vision system for place and object recognition. The goal is to identify familiar locations (e.g., office 610, conference room 941, main street), to categorize new environments (office, corridor, street) and to use that information to provide contextual priors for object recognition (e.g., tables are more likely in an office than a street). We present a low-dimensional global image representation that provides relevant information for place recognition and categorization, and show how such contextual information introduces strong priors that simplify object recognition. We have trained the system to recognize over 60 locations (indoors and outdoors) and to suggest the presence and locations of more than 20 different object types. The algorithm has been integrated into a mobile system that provides realtime feedback to the user."
            },
            "slug": "Context-based-vision-system-for-place-and-object-Torralba-Murphy",
            "title": {
                "fragments": [],
                "text": "Context-based vision system for place and object recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A low-dimensional global image representation is presented that provides relevant information for place recognition and categorization, and it is shown how such contextual information introduces strong priors that simplify object recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40538579"
                        ],
                        "name": "J. Vogel",
                        "slug": "J.-Vogel",
                        "structuredName": {
                            "firstName": "Julia",
                            "lastName": "Vogel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Vogel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "There are a few exceptions to this trend \u2013 In [31], each image is graded according to different scene categories."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14752064,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "206ff6b47ee55303d3e0d988f0ac824014468568",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an approach to categorize real-world natural scenes based on a semantic typicality measure. The proposed typicality measure allows to grade the similarity of an image with respect to a scene category. We argue that such a graded decision is appropriate and justified both from a human\u2019s perspective as well as from the image-content point of view. The method combines bottom-up information of local semantic concepts with the typical semantic content of an image category. Using this learned category representation the proposed typicality measure also quantifies the semantic transitions between image categories such as coasts, rivers/lakes, forest, plains, mountains or sky/clouds. The method is evaluated quantitatively and qualitatively on a database of natural scenes. The experiments show that the typicality measure well represents the diversity of the given image categories as well as the ambiguity in human judgment of image categorization."
            },
            "slug": "A-Semantic-Typicality-Measure-for-Natural-Scene-Vogel-Schiele",
            "title": {
                "fragments": [],
                "text": "A Semantic Typicality Measure for Natural Scene Categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "An approach to categorize real-world natural scenes based on a semantic typicality measure that represents the diversity of the given image categories as well as the ambiguity in human judgment of image categorization."
            },
            "venue": {
                "fragments": [],
                "text": "DAGM-Symposium"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48950628"
                        ],
                        "name": "N. Dalal",
                        "slug": "N.-Dalal",
                        "structuredName": {
                            "firstName": "Navneet",
                            "lastName": "Dalal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dalal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206590483,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cec734d7097ab6b1e60d95228ffd64248eb89d66",
            "isKey": false,
            "numCitedBy": 29259,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds."
            },
            "slug": "Histograms-of-oriented-gradients-for-human-Dalal-Triggs",
            "title": {
                "fragments": [],
                "text": "Histograms of oriented gradients for human detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection, and the influence of each stage of the computation on performance is studied."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2433269"
                        ],
                        "name": "Derek Hoiem",
                        "slug": "Derek-Hoiem",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Hoiem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Derek Hoiem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 145
                            }
                        ],
                        "text": "To make our color and texton histograms more invariant to scene layout, we also build histograms for specific geometric classes as determined by [13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 37206,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5276705d71e3dac961ab5d06b86a7b806cc9af64",
            "isKey": false,
            "numCitedBy": 718,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans have an amazing ability to instantly grasp the overall 3D structure of a scene\u2014ground orientation, relative positions of major landmarks, etc.\u2014even from a single image. This ability is completely missing in most popular recognition algorithms, which pretend that the world is flat and/or view it through a patch-sized peephole. Yet it seems very likely that having a grasp of this \u201csurface layout\u201d of a scene should be of great assistance for many tasks, including recognition, navigation, and novel view synthesis.In this paper, we take the first step towards constructing the surface layout, a labeling of the image intogeometric classes. Our main insight is to learn appearance-based models of these geometric classes, which coarsely describe the 3D scene orientation of each image region. Our multiple segmentation framework provides robust spatial support, allowing a wide variety of cues (e.g., color, texture, and perspective) to contribute to the confidence in each geometric label. In experiments on a large set of outdoor images, we evaluate the impact of the individual cues and design choices in our algorithm. We further demonstrate the applicability of our method to indoor images, describe potential applications, and discuss extensions to a more complete notion of surface layout."
            },
            "slug": "Recovering-Surface-Layout-from-an-Image-Hoiem-Efros",
            "title": {
                "fragments": [],
                "text": "Recovering Surface Layout from an Image"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper takes the first step towards constructing the surface layout, a labeling of the image intogeometric classes, to learn appearance-based models of these geometric classes, which coarsely describe the 3D scene orientation of each image region."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782755"
                        ],
                        "name": "Josef Sivic",
                        "slug": "Josef-Sivic",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Sivic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josef Sivic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "Sparse SIFT histograms: As in \u201cVideo Google\u201d [27], we build SIFT features at Hessian-affine and MSER [19] interest points."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "We also evaluate sparse SIFTs as used in \u201cVideo Google\u201d [27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 462544,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc56ce42d36bd50de564188424d2a67ff7a44e5f",
            "isKey": false,
            "numCitedBy": 197,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a method for obtaining the principal objects, characters and scenes in a video by measuring the reoccurrence of spatial configurations of viewpoint invariant features. We investigate two aspects of the problem: the scale of the configurations, and the similarity requirements for clustering configurations. The problem is challenging firstly because an object can undergo substantial changes in imaged appearance throughout a video (due to viewpoint and illumination change, and partial occlusion), and secondly because configurations are detected imperfectly, so that inexact patterns must be matched. The novelty of the method is that viewpoint invariant features are used to form the configurations, and that efficient methods from the text analysis literature are employed to reduce the matching complexity. Examples of 'mined' objects are shown for a feature length film and a sitcom."
            },
            "slug": "Video-data-mining-using-configurations-of-viewpoint-Sivic-Zisserman",
            "title": {
                "fragments": [],
                "text": "Video data mining using configurations of viewpoint invariant regions"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A method for obtaining the principal objects, characters and scenes in a video by measuring the reoccurrence of spatial configurations of viewpoint invariant features, and that efficient methods from the text analysis literature are employed to reduce the matching complexity."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145602732"
                        ],
                        "name": "Kobus Barnard",
                        "slug": "Kobus-Barnard",
                        "structuredName": {
                            "firstName": "Kobus",
                            "lastName": "Barnard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kobus Barnard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2446509"
                        ],
                        "name": "P. D. Sahin",
                        "slug": "P.-D.-Sahin",
                        "structuredName": {
                            "firstName": "Pinar",
                            "lastName": "Sahin",
                            "middleNames": [
                                "Duygulu"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Sahin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737568"
                        ],
                        "name": "N. D. Freitas",
                        "slug": "N.-D.-Freitas",
                        "structuredName": {
                            "firstName": "Nando",
                            "lastName": "Freitas",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. D. Freitas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 463,
                                "start": 437
                            }
                        ],
                        "text": "How many kinds of scenes are there? How can the knowledge about environmental scenes be organized? How do the current state-of-art scene models perform on more realistic and ill-controlled environments, and how does this compare to human performance? To date, computational work on scene and place recognition has classified natural images within a limited number of semantic categories, representing typical indoor and outdoor settings [17, 7, 23, 32, 21, 3, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 868535,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a26268d2ba9d34e5b59ae6e5c11a83cdca1a85e",
            "isKey": false,
            "numCitedBy": 1760,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new approach for modeling multi-modal data sets, focusing on the specific case of segmented images with associated text. Learning the joint distribution of image regions and words has many applications. We consider in detail predicting words associated with whole images (auto-annotation) and corresponding to particular image regions (region naming). Auto-annotation might help organize and access large collections of images. Region naming is a model of object recognition as a process of translating image regions to words, much as one might translate from one language to another. Learning the relationships between image regions and semantic correlates (words) is an interesting example of multi-modal data mining, particularly because it is typically hard to apply data mining techniques to collections of images. We develop a number of models for the joint distribution of image regions and words, including several which explicitly learn the correspondence between regions and words. We study multi-modal and correspondence extensions to Hofmann's hierarchical clustering/aspect model, a translation model adapted from statistical machine translation (Brown et al.), and a multi-modal extension to mixture of latent Dirichlet allocation (MoM-LDA). All models are assessed using a large collection of annotated images of real scenes. We study in depth the difficult problem of measuring performance. For the annotation task, we look at prediction performance on held out data. We present three alternative measures, oriented toward different types of task. Measuring the performance of correspondence methods is harder, because one must determine whether a word has been placed on the right region of an image. We can use annotation performance as a proxy measure, but accurate measurement requires hand labeled data, and thus must occur on a smaller scale. We show results using both an annotation proxy, and manually labeled data."
            },
            "slug": "Matching-Words-and-Pictures-Barnard-Sahin",
            "title": {
                "fragments": [],
                "text": "Matching Words and Pictures"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A new approach for modeling multi-modal data sets, focusing on the specific case of segmented images with associated text, is presented, and a number of models for the joint distribution of image regions and words are developed, including several which explicitly learn the correspondence between regions and Words."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48966748"
                        ],
                        "name": "James Hays",
                        "slug": "James-Hays",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hays",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Hays"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "Sparse SIFT histograms: As in \u201cVideo Google\u201d [27], we build SIFT features at Hessian-affine and MSER [19] interest points."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "Tiny Images: The most trivial way to match scenes is to compare them directly in color image space."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2061602,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b48d90cfebb8fbff29d161f6704d31b6909eb7ad",
            "isKey": false,
            "numCitedBy": 894,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Estimating geographic information from an image is an excellent, difficult high-level computer vision problem whose time has come. The emergence of vast amounts of geographically-calibrated image data is a great reason for computer vision to start looking globally - on the scale of the entire planet! In this paper, we propose a simple algorithm for estimating a distribution over geographic locations from a single image using a purely data-driven scene matching approach. For this task, we leverage a dataset of over 6 million GPS-tagged images from the Internet. We represent the estimated image location as a probability distribution over the Earthpsilas surface. We quantitatively evaluate our approach in several geolocation tasks and demonstrate encouraging performance (up to 30 times better than chance). We show that geolocation estimates can provide the basis for numerous other image understanding tasks such as population density estimation, land cover estimation or urban/rural classification."
            },
            "slug": "IM2GPS:-estimating-geographic-information-from-a-Hays-Efros",
            "title": {
                "fragments": [],
                "text": "IM2GPS: estimating geographic information from a single image"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes a simple algorithm for estimating a distribution over geographic locations from a single image using a purely data-driven scene matching approach and shows that geolocation estimates can provide the basis for numerous other image understanding tasks such as population density estimation, land cover estimation or urban/rural classification."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144891282"
                        ],
                        "name": "David R. Martin",
                        "slug": "David-R.-Martin",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Martin",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David R. Martin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143800213"
                        ],
                        "name": "Charless C. Fowlkes",
                        "slug": "Charless-C.-Fowlkes",
                        "structuredName": {
                            "firstName": "Charless",
                            "lastName": "Fowlkes",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charless C. Fowlkes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082299938"
                        ],
                        "name": "D. Tal",
                        "slug": "D.-Tal",
                        "structuredName": {
                            "firstName": "Doron",
                            "lastName": "Tal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "Texton Histograms: We build a 512 entry universal texton dictionary [18] by clustering responses to a bank of filters with 8 orientations, 2 scales, and 2 elongations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 64193,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a1ed876196ec9733acb1daa6d65e35ff0414291",
            "isKey": false,
            "numCitedBy": 6035,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a database containing 'ground truth' segmentations produced by humans for images of a wide variety of natural scenes. We define an error measure which quantifies the consistency between segmentations of differing granularities and find that different human segmentations of the same image are highly consistent. Use of this dataset is demonstrated in two applications: (1) evaluating the performance of segmentation algorithms and (2) measuring probability distributions associated with Gestalt grouping factors as well as statistics of image region properties."
            },
            "slug": "A-database-of-human-segmented-natural-images-and-to-Martin-Fowlkes",
            "title": {
                "fragments": [],
                "text": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A database containing 'ground truth' segmentations produced by humans for images of a wide variety of natural scenes is presented and an error measure is defined which quantifies the consistency between segmentations of differing granularities."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358192"
                        ],
                        "name": "E. Adelson",
                        "slug": "E.-Adelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Adelson",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Adelson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2336823,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "761d0e9f4510bd314a7efd10b9baaf2720b33165",
            "isKey": false,
            "numCitedBy": 308,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The perception of objects is a well-developed field, but the perception of materials has been studied rather little. This is surprising given how important materials are for humans, and how important they must become for intelligent robots. We may learn something by looking at other fields in which material appearance is recognized as important. Classical artists were highly skilled at generating convincing materials. The simulation of material appearance is a topic of great importance in 3D computer graphics. Some fields, such as mineralogy, use the concept of a 'habit' which is a combination of shape and texture, and which may be used for characterizing certain objects or materials. We have recently taken steps toward material recognition by machines, using techniques derived from the domain of texture analysis."
            },
            "slug": "On-seeing-stuff:-the-perception-of-materials-by-and-Adelson",
            "title": {
                "fragments": [],
                "text": "On seeing stuff: the perception of materials by humans and machines"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "The perception of objects is a well-developed field, but the perception of materials has been studied rather little, which is surprising given how important materials are for humans and how important they must become for intelligent robots."
            },
            "venue": {
                "fragments": [],
                "text": "IS&T/SPIE Electronic Imaging"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2177801"
                        ],
                        "name": "E. Shechtman",
                        "slug": "E.-Shechtman",
                        "structuredName": {
                            "firstName": "Eli",
                            "lastName": "Shechtman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Shechtman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144611617"
                        ],
                        "name": "M. Irani",
                        "slug": "M.-Irani",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Irani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Irani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "SSIM: Self-similarity descriptors [26] are computed on a regular grid at steps of five pixels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "Unlike GIST, SIFT, and HOG, which are all local gradient-based approaches, SSIM may provide a distinct, complementary measure of scene layout that is somewhat appearance invariant."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "While SIFT is known to be very good at finding repeated image content, the self-similarity descriptor (SSIM) [26] relates images using their internal layout of local self-similarities."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2341530,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5feb23e3c01c12c797b271fa5cd2a1f2f096130f",
            "isKey": true,
            "numCitedBy": 1062,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an approach for measuring similarity between visual entities (images or videos) based on matching internal self-similarities. What is correlated across images (or across video sequences) is the internal layout of local self-similarities (up to some distortions), even though the patterns generating those local self-similarities are quite different in each of the images/videos. These internal self-similarities are efficiently captured by a compact local \"self-similarity descriptor\"', measured densely throughout the image/video, at multiple scales, while accounting for local and global geometric distortions. This gives rise to matching capabilities of complex visual data, including detection of objects in real cluttered images using only rough hand-sketches, handling textured objects with no clear boundaries, and detecting complex actions in cluttered video data with no prior learning. We compare our measure to commonly used image-based and video-based similarity measures, and demonstrate its applicability to object detection, retrieval, and action detection."
            },
            "slug": "Matching-Local-Self-Similarities-across-Images-and-Shechtman-Irani",
            "title": {
                "fragments": [],
                "text": "Matching Local Self-Similarities across Images and Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "An approach for measuring similarity between visual entities (images or videos) based on matching internal self-similarities, measured densely throughout the image/video, at multiple scales, while accounting for local and global geometric distortions is presented."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685089"
                        ],
                        "name": "Pedro F. Felzenszwalb",
                        "slug": "Pedro-F.-Felzenszwalb",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Felzenszwalb",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro F. Felzenszwalb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689002"
                        ],
                        "name": "David A. McAllester",
                        "slug": "David-A.-McAllester",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McAllester",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. McAllester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "HOG features provide excellent performance for object and human recognition tasks [4,  9 ], so it is interesting to examine their utility for scene recognition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "HOG features are computed using the code available online provided by [ 9 ], which gives a 31dimension descriptor for each node of the grid."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3198903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e79272fe3d65197100eae8be9fec6469107969ae",
            "isKey": false,
            "numCitedBy": 9371,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL data sets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin-sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI--SVM in terms of latent variables. A latent SVM is semiconvex, and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function."
            },
            "slug": "Object-Detection-with-Discriminatively-Trained-Part-Felzenszwalb-Girshick",
            "title": {
                "fragments": [],
                "text": "Object Detection with Discriminatively Trained Part Based Models"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "An object detection system based on mixtures of multiscale deformable part models that is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges is described."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5737624"
                        ],
                        "name": "E. Rosch",
                        "slug": "E.-Rosch",
                        "structuredName": {
                            "firstName": "Eleanor",
                            "lastName": "Rosch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Rosch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3050353"
                        ],
                        "name": "C. Mervis",
                        "slug": "C.-Mervis",
                        "structuredName": {
                            "firstName": "Carolyn",
                            "lastName": "Mervis",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Mervis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2926253"
                        ],
                        "name": "Wayne D. Gray",
                        "slug": "Wayne-D.-Gray",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Gray",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wayne D. Gray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50156221"
                        ],
                        "name": "D. M. Johnson",
                        "slug": "D.-M.-Johnson",
                        "structuredName": {
                            "firstName": "D",
                            "lastName": "Johnson",
                            "middleNames": [
                                "M"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. M. Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1405696055"
                        ],
                        "name": "P. Boyes-Braem",
                        "slug": "P.-Boyes-Braem",
                        "structuredName": {
                            "firstName": "Penny",
                            "lastName": "Boyes-Braem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Boyes-Braem"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 59
                            }
                        ],
                        "text": "Most of the terms referred to basic and entry level places [30, 24, 25, 14] with different semantic descriptions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5612467,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "2c05d131576d01668ef01156ac73e52c3152384a",
            "isKey": false,
            "numCitedBy": 4978,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Basic-objects-in-natural-categories-Rosch-Mervis",
            "title": {
                "fragments": [],
                "text": "Basic objects in natural categories"
            },
            "venue": {
                "fragments": [],
                "text": "Cognitive Psychology"
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144522420"
                        ],
                        "name": "T. Ojala",
                        "slug": "T.-Ojala",
                        "structuredName": {
                            "firstName": "Timo",
                            "lastName": "Ojala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ojala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145962204"
                        ],
                        "name": "M. Pietik\u00e4inen",
                        "slug": "M.-Pietik\u00e4inen",
                        "structuredName": {
                            "firstName": "Matti",
                            "lastName": "Pietik\u00e4inen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pietik\u00e4inen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "26883091"
                        ],
                        "name": "Topi M\u00e4enp\u00e4\u00e4",
                        "slug": "Topi-M\u00e4enp\u00e4\u00e4",
                        "structuredName": {
                            "firstName": "Topi",
                            "lastName": "M\u00e4enp\u00e4\u00e4",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Topi M\u00e4enp\u00e4\u00e4"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 60
                            }
                        ],
                        "text": "We also try the rotation invariant extension version [2] of LBP to examine whether rotation invariance is suitable for scene recognition."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 33
                            }
                        ],
                        "text": "LBP: Local Binary Patterns (LBP) [20] is a powerful texture feature based on occurrence histogram of local binary patterns."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14540685,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f11a7136b6b7854bd0998ef463ffa8e907c411a2",
            "isKey": false,
            "numCitedBy": 13063,
            "numCiting": 95,
            "paperAbstract": {
                "fragments": [],
                "text": "Presents a theoretically very simple, yet efficient, multiresolution approach to gray-scale and rotation invariant texture classification based on local binary patterns and nonparametric discrimination of sample and prototype distributions. The method is based on recognizing that certain local binary patterns, termed \"uniform,\" are fundamental properties of local image texture and their occurrence histogram is proven to be a very powerful texture feature. We derive a generalized gray-scale and rotation invariant operator presentation that allows for detecting the \"uniform\" patterns for any quantization of the angular space and for any spatial resolution and presents a method for combining multiple operators for multiresolution analysis. The proposed approach is very robust in terms of gray-scale variations since the operator is, by definition, invariant against any monotonic transformation of the gray scale. Another advantage is computational simplicity as the operator can be realized with a few operations in a small neighborhood and a lookup table. Experimental results demonstrate that good discrimination can be achieved with the occurrence statistics of simple rotation invariant local binary patterns."
            },
            "slug": "Multiresolution-Gray-Scale-and-Rotation-Invariant-Ojala-Pietik\u00e4inen",
            "title": {
                "fragments": [],
                "text": "Multiresolution Gray-Scale and Rotation Invariant Texture Classification with Local Binary Patterns"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A generalized gray-scale and rotation invariant operator presentation that allows for detecting the \"uniform\" patterns for any quantization of the angular space and for any spatial resolution and presents a method for combining multiple operators for multiresolution analysis."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564537"
                        ],
                        "name": "Jiri Matas",
                        "slug": "Jiri-Matas",
                        "structuredName": {
                            "firstName": "Jiri",
                            "lastName": "Matas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiri Matas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700928"
                        ],
                        "name": "O. Chum",
                        "slug": "O.-Chum",
                        "structuredName": {
                            "firstName": "Ond\u0159ej",
                            "lastName": "Chum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067522034"
                        ],
                        "name": "Martin Urban",
                        "slug": "Martin-Urban",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Urban",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin Urban"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758039"
                        ],
                        "name": "T. Pajdla",
                        "slug": "T.-Pajdla",
                        "structuredName": {
                            "firstName": "Tom\u00e1s",
                            "lastName": "Pajdla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Pajdla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Sparse SIFT histograms: As in \u201cVideo Google\u201d [27], we build SIFT features at Hessian-affine and MSER [ 19 ] interest points."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 41908779,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "996767727f783c1297d1fb0a4fd5e37e9bf0cd1c",
            "isKey": false,
            "numCitedBy": 1247,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Robust-wide-baseline-stereo-from-maximally-stable-Matas-Chum",
            "title": {
                "fragments": [],
                "text": "Robust wide-baseline stereo from maximally stable extremal regions"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564537"
                        ],
                        "name": "Jiri Matas",
                        "slug": "Jiri-Matas",
                        "structuredName": {
                            "firstName": "Jiri",
                            "lastName": "Matas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiri Matas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700928"
                        ],
                        "name": "O. Chum",
                        "slug": "O.-Chum",
                        "structuredName": {
                            "firstName": "Ond\u0159ej",
                            "lastName": "Chum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067522034"
                        ],
                        "name": "Martin Urban",
                        "slug": "Martin-Urban",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Urban",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin Urban"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758039"
                        ],
                        "name": "T. Pajdla",
                        "slug": "T.-Pajdla",
                        "structuredName": {
                            "firstName": "Tom\u00e1s",
                            "lastName": "Pajdla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Pajdla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 101
                            }
                        ],
                        "text": "Sparse SIFT histograms: As in \u201cVideo Google\u201d [27], we build SIFT features at Hessian-affine and MSER [19] interest points."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2104851,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9d5ea177c7fcaf88ec6f56cbeb3e9b74c08e98a3",
            "isKey": false,
            "numCitedBy": 3922,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The wide-baseline stereo problem, i.e. the problem of establishing correspondences between a pair of images taken from different viewpoints is studied. A new set of image elements that are put into correspondence, the so called extremal regions, is introduced. Extremal regions possess highly desirable properties: the set is closed under 1. continuous (and thus projective) transformation of image coordinates and 2. monotonic transformation of image intensities. An efficient (near linear complexity) and practically fast detection algorithm (near frame rate) is presented for an affinely-invariant stable subset of extremal regions, the maximally stable extremal regions (MSER). A new robust similarity measure for establishing tentative correspondences is proposed. The robustness ensures that invariants from multiple measurement regions (regions obtained by invariant constructions from extremal regions), some that are significantly larger (and hence discriminative) than the MSERs, may be used to establish tentative correspondences. The high utility of MSERs, multiple measurement regions and the robust metric is demonstrated in wide-baseline experiments on image pairs from both indoor and outdoor scenes. Significant change of scale (3.5\u00d7), illumination conditions, out-of-plane rotation, occlusion , locally anisotropic scale change and 3D translation of the viewpoint are all present in the test problems. Good estimates of epipolar geometry (average distance from corresponding points to the epipolar line below 0.09 of the inter-pixel distance) are obtained."
            },
            "slug": "Robust-Wide-Baseline-Stereo-from-Maximally-Stable-Matas-Chum",
            "title": {
                "fragments": [],
                "text": "Robust Wide Baseline Stereo from Maximally Stable Extremal Regions"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The wide-baseline stereo problem, i.e. the problem of establishing correspondences between a pair of images taken from different viewpoints, is studied and an efficient and practically fast detection algorithm is presented for an affinely-invariant stable subset of extremal regions, the maximally stable extremal region (MSER)."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743610"
                        ],
                        "name": "B. Tversky",
                        "slug": "B.-Tversky",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Tversky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Tversky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2141671"
                        ],
                        "name": "K. Hemenway",
                        "slug": "K.-Hemenway",
                        "structuredName": {
                            "firstName": "Kathleen",
                            "lastName": "Hemenway",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hemenway"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 59
                            }
                        ],
                        "text": "Most of the terms referred to basic and entry level places [30, 24, 25, 14] with different semantic descriptions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 54413901,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "1c87ac834e820b029c9c4248916520fed0fb4a57",
            "isKey": false,
            "numCitedBy": 302,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Categories-of-environmental-scenes-Tversky-Hemenway",
            "title": {
                "fragments": [],
                "text": "Categories of environmental scenes"
            },
            "venue": {
                "fragments": [],
                "text": "Cognitive Psychology"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743020"
                        ],
                        "name": "J. Kosecka",
                        "slug": "J.-Kosecka",
                        "structuredName": {
                            "firstName": "Jana",
                            "lastName": "Kosecka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kosecka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Wei Zhang",
                        "slug": "Wei-Zhang",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 101
                            }
                        ],
                        "text": "Line Features: We detect straight lines from Canny edges using the method described in Video Compass [15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1413778,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d47bca050a09e69be810fd7c677ed5aefe66c797",
            "isKey": false,
            "numCitedBy": 369,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe a flexible approach for determining the relative orientation of the camera with respect to the scene. The main premise of the approach is the fact that in man-made environments, the majority of lines is aligned with the principal orthogonal directions of the world coordinate frame. We exploit this observation towards efficient detection and estimation of vanishing points, which provide strong constraints on camera parameters and relative orientation of the camera with respect to the scene.By combining efficient image processing techniques in the line detection and initialization stage we demonstrate that simultaneous grouping and estimation of vanishing directions can be achieved in the absence of internal parameters of the camera. Constraints between vanishing points are then used for partial calibration and relative rotation estimation. The algorithm has been tested in a variety of indoors and outdoors scenes and its efficiency and automation makes it amenable for implementation on robotic platforms."
            },
            "slug": "Video-Compass-Kosecka-Zhang",
            "title": {
                "fragments": [],
                "text": "Video Compass"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "A flexible approach for determining the relative orientation of the camera with respect to the scene based on the fact that in man-made environments, the majority of lines is aligned with the principal orthogonal directions of the world coordinate frame is described."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40461178"
                        ],
                        "name": "P. Jolicoeur",
                        "slug": "P.-Jolicoeur",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Jolicoeur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Jolicoeur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738203"
                        ],
                        "name": "M. Gluck",
                        "slug": "M.-Gluck",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Gluck",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gluck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2914641"
                        ],
                        "name": "S. Kosslyn",
                        "slug": "S.-Kosslyn",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Kosslyn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kosslyn"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Most of the terms referred to basic and entry level places [30, 24, 25, 14] with different semantic descriptions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7849920,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "70bbfe1357047bbba560239002c4d2a173f10eae",
            "isKey": false,
            "numCitedBy": 531,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Pictures-and-names:-Making-the-connection-Jolicoeur-Gluck",
            "title": {
                "fragments": [],
                "text": "Pictures and names: Making the connection"
            },
            "venue": {
                "fragments": [],
                "text": "Cognitive Psychology"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48345000"
                        ],
                        "name": "T. Ahonen",
                        "slug": "T.-Ahonen",
                        "structuredName": {
                            "firstName": "Timo",
                            "lastName": "Ahonen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ahonen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564537"
                        ],
                        "name": "Jiri Matas",
                        "slug": "Jiri-Matas",
                        "structuredName": {
                            "firstName": "Jiri",
                            "lastName": "Matas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiri Matas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144775009"
                        ],
                        "name": "Chu He",
                        "slug": "Chu-He",
                        "structuredName": {
                            "firstName": "Chu",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chu He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145962204"
                        ],
                        "name": "M. Pietik\u00e4inen",
                        "slug": "M.-Pietik\u00e4inen",
                        "structuredName": {
                            "firstName": "Matti",
                            "lastName": "Pietik\u00e4inen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pietik\u00e4inen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 53
                            }
                        ],
                        "text": "We also try the rotation invariant extension version [2] of LBP to examine whether rotation invariance is suitable for scene recognition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14605508,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8ba838009fceef57a9937e2439c4499a3d3840e3",
            "isKey": false,
            "numCitedBy": 430,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose Local Binary Pattern Histogram Fourier features (LBP-HF), a novel rotation invariant image descriptor computed from discrete Fourier transforms of local binary pattern (LBP) histograms. Unlike most other histogram based invariant texture descriptors which normalize rotation locally, the proposed invariants are constructed globally for the whole region to be described. In addition to being rotation invariant, the LBP-HF features retain the highly discriminative nature of LBP histograms. In the experiments, it is shown that these features outperform non-invariant and earlier version of rotation invariant LBP and the MR8 descriptor in texture classification, material categorization and face recognition tests."
            },
            "slug": "Rotation-Invariant-Image-Description-with-Local-Ahonen-Matas",
            "title": {
                "fragments": [],
                "text": "Rotation Invariant Image Description with Local Binary Pattern Histogram Fourier Features"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "In the experiments, it is shown that these features outperform non-invariant and earlier version of rotation invariant LBP and the MR8 descriptor in texture classification, material categorization and face recognition tests."
            },
            "venue": {
                "fragments": [],
                "text": "SCIA"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34854785"
                        ],
                        "name": "Russell A. Epstein",
                        "slug": "Russell-A.-Epstein",
                        "structuredName": {
                            "firstName": "Russell",
                            "lastName": "Epstein",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Russell A. Epstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1931482"
                        ],
                        "name": "N. Kanwisher",
                        "slug": "N.-Kanwisher",
                        "structuredName": {
                            "firstName": "Nancy",
                            "lastName": "Kanwisher",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Kanwisher"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 94
                            }
                        ],
                        "text": "However, any restricted set of categories fails to capture the richness and diversity of environments that make up our daily experience."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 920141,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "46ec521e3f4c89218db0292a097f0f44d17c753d",
            "isKey": false,
            "numCitedBy": 2724,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Medial temporal brain regions such as the hippocampal formation and parahippocampal cortex have been generally implicated in navigation and visual memory. However, the specific function of each of these regions is not yet clear. Here we present evidence that a particular area within human parahippocampal cortex is involved in a critical component of navigation: perceiving the local visual environment. This region, which we name the \u2018parahippocampal place area\u2019 (PPA), responds selectively and automatically in functional magnetic resonance imaging (fMRI) to passively viewed scenes, but only weakly to single objects and not at all to faces. The critical factor for this activation appears to be the presence in the stimulus of information about the layout of local space. The response in the PPA to scenes with spatial layout but no discrete objects (empty rooms) is as strong as the response to complex meaningful scenes containing multiple objects (the same rooms furnished) and over twice as strong as the response to arrays of multiple objects without three-dimensional spatial context (the furniture from these rooms on a blank background). This response is reduced if the surfaces in the scene are rearranged so that they no longer define a coherent space. We propose that the PPA represents places by encoding the geometry of the local environment."
            },
            "slug": "A-cortical-representation-of-the-local-visual-Epstein-Kanwisher",
            "title": {
                "fragments": [],
                "text": "A cortical representation of the local visual environment"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Evidence is presented that a particular area within human parahippocampal cortex is involved in a critical component of navigation: perceiving the local visual environment, and it is proposed that the PPA represents places by encoding the geometry of the local environment."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721801"
                        ],
                        "name": "C. Fellbaum",
                        "slug": "C.-Fellbaum",
                        "structuredName": {
                            "firstName": "Christiane",
                            "lastName": "Fellbaum",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Fellbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 74
                            }
                        ],
                        "text": "However, we use this to validate the completeness of the list provided by WordNet."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 105
                            }
                        ],
                        "text": "This gave about 2500 initial terms of space and scene, and after bundling together synonyms (provided by WordNet, and separating scenes with different visual identities such as indoor and outdoor views of churches), the final dataset reaches 899 categories and 130,519 images."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 53
                            }
                        ],
                        "text": "For each scene category, images were retrieved using WordNet terminology from various search engines on the web [28]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 7
                            }
                        ],
                        "text": "To the WordNet collection we added a few categories that seemed like plausible scenes but were missing from WordNet, such as jewelry store and mission1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 114
                            }
                        ],
                        "text": "In order to get a quasi-exhaustive list of environmental categories, we selected from the 70,000 terms of WordNet [8] available on the tiny Images dataset [28] all the terms that described scenes, places, and environments (any concrete noun which could reasonably complete the phrase I am in a place, or Let\u2019s go to the place)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5958691,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d87ceda3042f781c341ac17109d1e94a717f5f60",
            "isKey": true,
            "numCitedBy": 13574,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Part 1 The lexical database: nouns in WordNet, George A. Miller modifiers in WordNet, Katherine J. Miller a semantic network of English verbs, Christiane Fellbaum design and implementation of the WordNet lexical database and searching software, Randee I. Tengi. Part 2: automated discovery of WordNet relations, Marti A. Hearst representing verb alterations in WordNet, Karen T. Kohl et al the formalization of WordNet by methods of relational concept analysis, Uta E. Priss. Part 3 Applications of WordNet: building semantic concordances, Shari Landes et al performance and confidence in a semantic annotation task, Christiane Fellbaum et al WordNet and class-based probabilities, Philip Resnik combining local context and WordNet similarity for word sense identification, Claudia Leacock and Martin Chodorow using WordNet for text retrieval, Ellen M. Voorhees lexical chains as representations of context for the detection and correction of malapropisms, Graeme Hirst and David St-Onge temporal indexing through lexical chaining, Reem Al-Halimi and Rick Kazman COLOR-X - using knowledge from WordNet for conceptual modelling, J.F.M. Burg and R.P. van de Riet knowledge processing on an extended WordNet, Sanda M. Harabagiu and Dan I Moldovan appendix - obtaining and using WordNet."
            },
            "slug": "WordNet-:-an-electronic-lexical-database-Fellbaum",
            "title": {
                "fragments": [],
                "text": "WordNet : an electronic lexical database"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "The lexical database: nouns in WordNet, Katherine J. Miller a semantic network of English verbs, and applications of WordNet: building semantic concordances are presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7007923"
                        ],
                        "name": "G. Dalton",
                        "slug": "G.-Dalton",
                        "structuredName": {
                            "firstName": "Gavin",
                            "lastName": "Dalton",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Dalton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2688477"
                        ],
                        "name": "S. Trager",
                        "slug": "S.-Trager",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Trager",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Trager"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "73150907"
                        ],
                        "name": "D. Abrams",
                        "slug": "D.-Abrams",
                        "structuredName": {
                            "firstName": "Don",
                            "lastName": "Abrams",
                            "middleNames": [
                                "Carlos"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Abrams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50662141"
                        ],
                        "name": "D. Carter",
                        "slug": "D.-Carter",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Carter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Carter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4155680"
                        ],
                        "name": "P. Bonifacio",
                        "slug": "P.-Bonifacio",
                        "structuredName": {
                            "firstName": "Piercarlo",
                            "lastName": "Bonifacio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bonifacio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144104531"
                        ],
                        "name": "J. Aguerri",
                        "slug": "J.-Aguerri",
                        "structuredName": {
                            "firstName": "J",
                            "lastName": "Aguerri",
                            "middleNames": [
                                "A",
                                "L"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Aguerri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15167657"
                        ],
                        "name": "M. MacIntosh",
                        "slug": "M.-MacIntosh",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "MacIntosh",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. MacIntosh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065122015"
                        ],
                        "name": "Christopher H. Evans",
                        "slug": "Christopher-H.-Evans",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Evans",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher H. Evans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145900789"
                        ],
                        "name": "I. Lewis",
                        "slug": "I.-Lewis",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Lewis",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145397285"
                        ],
                        "name": "R. Navarro",
                        "slug": "R.-Navarro",
                        "structuredName": {
                            "firstName": "Ram\u00f3n",
                            "lastName": "Navarro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Navarro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1892901"
                        ],
                        "name": "T. Agocs",
                        "slug": "T.-Agocs",
                        "structuredName": {
                            "firstName": "Tibor",
                            "lastName": "Agocs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Agocs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "134244281"
                        ],
                        "name": "K. Dee",
                        "slug": "K.-Dee",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Dee",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Dee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48986133"
                        ],
                        "name": "Sophie Rousset",
                        "slug": "Sophie-Rousset",
                        "structuredName": {
                            "firstName": "Sophie",
                            "lastName": "Rousset",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sophie Rousset"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "97550082"
                        ],
                        "name": "I. Tosh",
                        "slug": "I.-Tosh",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Tosh",
                            "middleNames": [
                                "A.",
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Tosh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143796734"
                        ],
                        "name": "K. Middleton",
                        "slug": "K.-Middleton",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Middleton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Middleton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69905036"
                        ],
                        "name": "J. Pragt",
                        "slug": "J.-Pragt",
                        "structuredName": {
                            "firstName": "Johan",
                            "lastName": "Pragt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pragt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69373939"
                        ],
                        "name": "D. Terrett",
                        "slug": "D.-Terrett",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Terrett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Terrett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34671764"
                        ],
                        "name": "M. Brock",
                        "slug": "M.-Brock",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Brock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Brock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "88436418"
                        ],
                        "name": "C. Benn",
                        "slug": "C.-Benn",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Benn",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Benn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3832800"
                        ],
                        "name": "M. Verheijen",
                        "slug": "M.-Verheijen",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Verheijen",
                            "middleNames": [
                                "A.",
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Verheijen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71558586"
                        ],
                        "name": "D. Infantes",
                        "slug": "D.-Infantes",
                        "structuredName": {
                            "firstName": "Diego",
                            "lastName": "Infantes",
                            "middleNames": [
                                "Cano"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Infantes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71333004"
                        ],
                        "name": "Craige Bevil",
                        "slug": "Craige-Bevil",
                        "structuredName": {
                            "firstName": "Craige",
                            "lastName": "Bevil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Craige Bevil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145066465"
                        ],
                        "name": "I. Steele",
                        "slug": "I.-Steele",
                        "structuredName": {
                            "firstName": "Iain",
                            "lastName": "Steele",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Steele"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2077649"
                        ],
                        "name": "C. Mottram",
                        "slug": "C.-Mottram",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Mottram",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Mottram"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736626"
                        ],
                        "name": "S. Bates",
                        "slug": "S.-Bates",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Bates",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bates"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "73292794"
                        ],
                        "name": "F. Gribbin",
                        "slug": "F.-Gribbin",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Gribbin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gribbin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48118714"
                        ],
                        "name": "J. Rey",
                        "slug": "J.-Rey",
                        "structuredName": {
                            "firstName": "Juerg",
                            "lastName": "Rey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143977995"
                        ],
                        "name": "L. Rodr\u00edguez",
                        "slug": "L.-Rodr\u00edguez",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Rodr\u00edguez",
                            "middleNames": [
                                "Fernando"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rodr\u00edguez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143652342"
                        ],
                        "name": "J. Delgado",
                        "slug": "J.-Delgado",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Delgado",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Delgado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72665230"
                        ],
                        "name": "I. Guinouard",
                        "slug": "I.-Guinouard",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guinouard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guinouard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3104574"
                        ],
                        "name": "N. Walton",
                        "slug": "N.-Walton",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Walton",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Walton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5058806"
                        ],
                        "name": "M. Irwin",
                        "slug": "M.-Irwin",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Irwin",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Irwin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "98099208"
                        ],
                        "name": "P. Jagourel",
                        "slug": "P.-Jagourel",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Jagourel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Jagourel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72762837"
                        ],
                        "name": "R. Stuik",
                        "slug": "R.-Stuik",
                        "structuredName": {
                            "firstName": "Remko",
                            "lastName": "Stuik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Stuik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9287188"
                        ],
                        "name": "G. Gerlofsma",
                        "slug": "G.-Gerlofsma",
                        "structuredName": {
                            "firstName": "Gerrit",
                            "lastName": "Gerlofsma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Gerlofsma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72604464"
                        ],
                        "name": "Ronald Roelfsma",
                        "slug": "Ronald-Roelfsma",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Roelfsma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald Roelfsma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69033881"
                        ],
                        "name": "I. Skillen",
                        "slug": "I.-Skillen",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Skillen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Skillen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "73141906"
                        ],
                        "name": "Andy Ridings",
                        "slug": "Andy-Ridings",
                        "structuredName": {
                            "firstName": "Andy",
                            "lastName": "Ridings",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andy Ridings"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35045477"
                        ],
                        "name": "M. Balcells",
                        "slug": "M.-Balcells",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Balcells",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Balcells"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "73593825"
                        ],
                        "name": "J. Daban",
                        "slug": "J.-Daban",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Daban",
                            "middleNames": [
                                "Baptiste"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Daban"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66518022"
                        ],
                        "name": "C. Gouvret",
                        "slug": "C.-Gouvret",
                        "structuredName": {
                            "firstName": "Carole",
                            "lastName": "Gouvret",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Gouvret"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33698974"
                        ],
                        "name": "L. Venema",
                        "slug": "L.-Venema",
                        "structuredName": {
                            "firstName": "Lars",
                            "lastName": "Venema",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Venema"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152774144"
                        ],
                        "name": "P. Girard",
                        "slug": "P.-Girard",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Girard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Girard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 67357933,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "714235495a2334435c00d09f0fd34b787f973b86",
            "isKey": false,
            "numCitedBy": 393,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Proceedings-of-the-SPIE-Dalton-Trager",
            "title": {
                "fragments": [],
                "text": "Proceedings of the SPIE"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48950628"
                        ],
                        "name": "N. Dalal",
                        "slug": "N.-Dalal",
                        "structuredName": {
                            "firstName": "Navneet",
                            "lastName": "Dalal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dalal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 82
                            }
                        ],
                        "text": "HOG features provide excellent performance for object and human recognition tasks [4, 9], so it is interesting to examine their utility for scene recognition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 61
                            }
                        ],
                        "text": "HOG2x2: First, histogram of oriented edges (HOG) descriptors [4] are densely extracted on a regular grid at steps of 8 pixels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60565766,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2445f1738413f43c121572e19383af4df909e3d2",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Software-Histogram-of-oriented-gradient-object-Dalal-Triggs",
            "title": {
                "fragments": [],
                "text": "Software - Histogram of oriented gradient object detection"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 59
                            }
                        ],
                        "text": "Most of the terms referred to basic and entry level places [30, 24, 25, 14] with different semantic descriptions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Natural categories"
            },
            "venue": {
                "fragments": [],
                "text": "Cognitive Psychology, 4:328\u2013 350"
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 198
                            }
                        ],
                        "text": "Previous attempts to characterize environmental scenes have capitalized on uncovering a manageable set of dimensions, features or objects that are correlated with the semantic of purpose of a space [21, 17, 7, 32, 12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 463,
                                "start": 437
                            }
                        ],
                        "text": "How many kinds of scenes are there? How can the knowledge about environmental scenes be organized? How do the current state-of-art scene models perform on more realistic and ill-controlled environments, and how does this compare to human performance? To date, computational work on scene and place recognition has classified natural images within a limited number of semantic categories, representing typical indoor and outdoor settings [17, 7, 23, 32, 21, 3, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Semantic model of natural scenes for content-based image retrieval"
            },
            "venue": {
                "fragments": [],
                "text": "Intl. J. Computer Vision, 72:133\u2013157"
            },
            "year": 2007
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 21,
            "methodology": 14
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 37,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/SUN-database:-Large-scale-scene-recognition-from-to-Xiao-Hays/908091b4a8757c3b2f7d9cfa2c4f616ee12c5157?sort=total-citations"
}