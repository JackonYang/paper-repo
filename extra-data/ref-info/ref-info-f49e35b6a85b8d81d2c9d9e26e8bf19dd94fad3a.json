{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110165806"
                        ],
                        "name": "Jiying Wang",
                        "slug": "Jiying-Wang",
                        "structuredName": {
                            "firstName": "Jiying",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiying Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2182929"
                        ],
                        "name": "F. Lochovsky",
                        "slug": "F.-Lochovsky",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Lochovsky",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Lochovsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 3
                            }
                        ],
                        "text": "In [1, 12, 34], two more techniques are proposed."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1951423,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e4ecd6b2429076558b02ddd1b23c6c519eea7c8",
            "isKey": false,
            "numCitedBy": 421,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Many tools have been developed to help users query, extract and integrate data from web pages generated dynamically from databases, i.e., from the Hidden Web. A key prerequisite for such tools is to obtain the schema of the attributes of the retrieved data. In this paper, we describe a system called, DeLa, which reconstructs (part of) a \"hidden\" back-end web database. It does this by sending queries through HTML forms, automatically generating regular expression wrappers to extract data objects from the result pages and restoring the retrieved data into an annotated (labelled) table. The whole process needs no human involvement and proves to be fast (less than one minute for wrapper induction for each site) and accurate (over 90% correctness for data extraction and around 80% correctness for label assignment)."
            },
            "slug": "Data-extraction-and-label-assignment-for-web-Wang-Lochovsky",
            "title": {
                "fragments": [],
                "text": "Data extraction and label assignment for web databases"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A system called, DeLa, which reconstructs (part of) a \"hidden\" back-end web database by sending queries through HTML forms, automatically generating regular expression wrappers to extract data objects from the result pages and restoring the retrieved data into an annotated (labelled) table."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '03"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39665544"
                        ],
                        "name": "D. C. Reis",
                        "slug": "D.-C.-Reis",
                        "structuredName": {
                            "firstName": "Davi",
                            "lastName": "Reis",
                            "middleNames": [
                                "de",
                                "Castro"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. C. Reis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2285220"
                        ],
                        "name": "P. B. Golgher",
                        "slug": "P.-B.-Golgher",
                        "structuredName": {
                            "firstName": "Paulo",
                            "lastName": "Golgher",
                            "middleNames": [
                                "Braz"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. B. Golgher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690426"
                        ],
                        "name": "A. D. Silva",
                        "slug": "A.-D.-Silva",
                        "structuredName": {
                            "firstName": "Altigran",
                            "lastName": "Silva",
                            "middleNames": [
                                "Soares",
                                "da"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. D. Silva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764577"
                        ],
                        "name": "Alberto H. F. Laender",
                        "slug": "Alberto-H.-F.-Laender",
                        "structuredName": {
                            "firstName": "Alberto",
                            "lastName": "Laender",
                            "middleNames": [
                                "H.",
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alberto H. F. Laender"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 84
                            }
                        ],
                        "text": "Finally, tree matching has been used for finding the main contents in news pages in [27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3343581,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "83cc20d078920efa6cc3876a83065beddb592474",
            "isKey": false,
            "numCitedBy": 377,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The Web poses itself as the largest data repository ever available in the history of humankind. Major efforts have been made in order to provide efficient access to relevant information within this huge repository of data. Although several techniques have been developed to the problem of Web data extraction, their use is still not spread, mostly because of the need for high human intervention and the low quality of the extraction results.In this paper, we present a domain-oriented approach to Web data extraction and discuss its application to automatically extracting news from Web sites. Our approach is based on a highly efficient tree structure analysis that produces very effective results. We have tested our approach with several important Brazilian on-line news sites and achieved very precise results, correctly extracting 87.71% of the news in a set of 4088 pages distributed among 35 different sites."
            },
            "slug": "Automatic-web-news-extraction-using-tree-edit-Reis-Golgher",
            "title": {
                "fragments": [],
                "text": "Automatic web news extraction using tree edit distance"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A domain-oriented approach to Web data extraction is presented and its application to automatically extracting news from Web sites is discussed, based on a highly efficient tree structure analysis that produces very effective results."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '04"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145321667"
                        ],
                        "name": "B. Liu",
                        "slug": "B.-Liu",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743874"
                        ],
                        "name": "R. Grossman",
                        "slug": "R.-Grossman",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Grossman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Grossman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40343421"
                        ],
                        "name": "Yanhong Zhai",
                        "slug": "Yanhong-Zhai",
                        "structuredName": {
                            "firstName": "Yanhong",
                            "lastName": "Zhai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yanhong Zhai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "We do not compare it with the method in [5] and the method in [8] here as it is shown in [21] that MDR is already more effective than them."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "However, [21] shows that these methods produce poor results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "We have improved our previous technique MDR [21] for this purpose."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "However, as shown in [21], it performs poorly in finding right data records, and thus could not extract data items well."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 64
                            }
                        ],
                        "text": "Since this step is an improvement to our previous technique MDR [21], below we give a brief overview of the MDR algorithm and present the enhancements made to MDR in this work."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [21], we propose the MDR algorithm, which only identifies data records but does not align or extract data items from the data records."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "However, [21] shows that its performance is also weak."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "The process of identifying data regions is involved; see [21] for more details."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "However, its results are weak as shown in [21]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 191
                            }
                        ],
                        "text": "Below, we only highlight two interesting cases in which a data record is not contained in a contiguous segment of the HTML code in order to show some advanced capabilities of our system (see [21] for more details and other simpler cases)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11383614,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c8cdce10fc4c9505ca22796920d1cd4b0f82dc76",
            "isKey": false,
            "numCitedBy": 556,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "A large amount of information on the Web is contained in regularly structured objects, which we call data records. Such data records are important because they often present the essential information of their host pages, e.g., lists of products or services. It is useful to mine such data records in order to extract information from them to provide value-added services. Existing automatic techniques are not satisfactory because of their poor accuracies. In this paper, we propose a more effective technique to perform the task. The technique is based on two observations about data records on the Web and a string matching algorithm. The proposed technique is able to mine both contiguous and non-contiguous data records. Our experimental results show that the proposed technique outperforms existing techniques substantially."
            },
            "slug": "Mining-data-records-in-Web-pages-Liu-Grossman",
            "title": {
                "fragments": [],
                "text": "Mining data records in Web pages"
            },
            "tldr": {
                "abstractSimilarityScore": 34,
                "text": "The experimental results show that the proposed technique outperforms existing techniques substantially, and is able to mine both contiguous and non-contiguous data records."
            },
            "venue": {
                "fragments": [],
                "text": "KDD '03"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766422"
                        ],
                        "name": "A. Arasu",
                        "slug": "A.-Arasu",
                        "structuredName": {
                            "firstName": "Arvind",
                            "lastName": "Arasu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Arasu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398574232"
                        ],
                        "name": "H. Garcia-Molina",
                        "slug": "H.-Garcia-Molina",
                        "structuredName": {
                            "firstName": "Hector",
                            "lastName": "Garcia-Molina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Garcia-Molina"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 38
                            }
                        ],
                        "text": "We do not compare with the systems in [1][12] as they require multiple pages and all of them contain similar data records to find patterns from the pages to extract data items."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 3
                            }
                        ],
                        "text": "In [1, 12, 34], two more techniques are proposed."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 93
                            }
                        ],
                        "text": "A wrapper is a program that extracts data from a Web site or page and put them in a database [1, 11, 12, 16, 18, 19, 22, 23, 25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 26
                            }
                        ],
                        "text": "In automatic methods, [12][1] find patterns or grammars from multiple pages containing similar data records."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207628158,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "856867d7caeabdd5bba9d13574dd786aa8ee8b30",
            "isKey": true,
            "numCitedBy": 501,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Many web sites contain large sets of pages generated using a common template or layout. For example, Amazon lays out the author, title, comments, etc. in the same way in all its book pages. The values used to generate the pages (e.g., the author, title,...) typically come from a database. In this paper, we study the problem of automatically extracting the database values from such template-generated web pages without any learning examples or other similar human input. We formally define a template, and propose a model that describes how values are encoded into pages using a template. We present an algorithm that takes, as input, a set of template-generated pages, deduces the unknown template used to generate the pages, and extracts, as output, the values encoded in the pages. Experimental evaluation on a large number of real input page collections indicates that our algorithm correctly extracts data in most cases."
            },
            "slug": "Extracting-structured-data-from-Web-pages-Arasu-Garcia-Molina",
            "title": {
                "fragments": [],
                "text": "Extracting structured data from Web pages"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper presents an algorithm that takes, as input, a set of template-generated pages, deduces the unknown template used to generate the pages, and extracts, as output, the values encoded in the pages."
            },
            "venue": {
                "fragments": [],
                "text": "SIGMOD '03"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694279"
                        ],
                        "name": "D. Embley",
                        "slug": "D.-Embley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Embley",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Embley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158090475"
                        ],
                        "name": "Y. S. Jiang",
                        "slug": "Y.-S.-Jiang",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Jiang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. S. Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143743807"
                        ],
                        "name": "Yiu-Kai Ng",
                        "slug": "Yiu-Kai-Ng",
                        "structuredName": {
                            "firstName": "Yiu-Kai",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiu-Kai Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [14], a study is made to automatically identify data record boundaries."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7457781,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "31aa4b4ddfa454bc0626cac4bf092c8d2fe2c852",
            "isKey": false,
            "numCitedBy": 320,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Extraction of information from unstructured or semistructured Web documents often requires a recognition and delimitation of records. (By \u201crecord\u201d we mean a group of information relevant to some entity.) Without first chunking documents that contain multiple records according to record boundaries, extraction of record information will not likely succeed. In this paper we describe a heuristic approach to discovering record boundaries in Web documents. In our approach, we capture the structure of a document as a tree of nested HTML tags, locate the subtree containing the records of interest, identify candidate separator tags within the subtree using five independent heuristics, and select a consensus separator tag based on a combined heuristic. Our approach is fast (runs linearly for practical cases within the context of the larger data-extraction problem) and accurate (100% in the experiments we conducted)."
            },
            "slug": "Record-boundary-discovery-in-Web-documents-Embley-Jiang",
            "title": {
                "fragments": [],
                "text": "Record-boundary discovery in Web documents"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper describes a heuristic approach to discovering record boundaries in Web documents that captures the structure of a document as a tree of nested HTML tags, and locates the subtree containing the records of interest using five independent heuristics and selects a consensus separator tag based on a combined heuristic."
            },
            "venue": {
                "fragments": [],
                "text": "SIGMOD '99"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782658"
                        ],
                        "name": "Kristina Lerman",
                        "slug": "Kristina-Lerman",
                        "structuredName": {
                            "firstName": "Kristina",
                            "lastName": "Lerman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristina Lerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746034"
                        ],
                        "name": "L. Getoor",
                        "slug": "L.-Getoor",
                        "structuredName": {
                            "firstName": "Lise",
                            "lastName": "Getoor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Getoor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "26602711"
                        ],
                        "name": "Steven N. Minton",
                        "slug": "Steven-N.-Minton",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Minton",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven N. Minton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745117"
                        ],
                        "name": "Craig A. Knoblock",
                        "slug": "Craig-A.-Knoblock",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Knoblock",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Craig A. Knoblock"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[20] proposes another method for data extraction."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "The technique in [20] requires the detail page behind the page (to be extracted), and in their experiments, such detail pages are manually identified and downloaded, which is unrealistic in practice."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "Furthermore, the method in [20] assumes that detail pages are given (in their experiments such pages are manually identified), which is not realistic."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[20] proposes a method that tries to explore the detailed information page behind the current page to extract data records."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6642452,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb52373f7266e19efa28043812c9dae96ecd26d1",
            "isKey": true,
            "numCitedBy": 171,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Many Web sites, especially those that dynamically generate HTML pages to display the results of a user's query, present information in the form of list or tables. Current tools that allow applications to programmatically extract this information rely heavily on user input, often in the form of labeled extracted records. The sheer size and rate of growth of the Web make any solution that relies primarily on user input is infeasible in the long term. Fortunately, many Web sites contain much explicit and implicit structure, both in layout and content, that we can exploit for the purpose of information extraction. This paper describes an approach to automatic extraction and segmentation of records from Web tables. Automatic methods do not require any user input, but rely solely on the layout and content of the Web source. Our approach relies on the common structure of many Web sites, which present information as a list or a table, with a link in each entry leading to a detail page containing additional information about that item. We describe two algorithms that use redundancies in the content of table and detail pages to aid in information extraction. The first algorithm encodes additional information provided by detail pages as constraints and finds the segmentation by solving a constraint satisfaction problem. The second algorithm uses probabilistic inference to find the record segmentation. We show how each approach can exploit the web site structure in a general, domain-independent manner, and we demonstrate the effectiveness of each algorithm on a set of twelve Web sites."
            },
            "slug": "Using-the-structure-of-Web-sites-for-automatic-of-Lerman-Getoor",
            "title": {
                "fragments": [],
                "text": "Using the structure of Web sites for automatic segmentation of tables"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper describes an approach to automatic extraction and segmentation of records from Web tables, which relies on the common structure of many Web sites, and describes two algorithms that use redundancies in the content of table and detail pages to aid in information extraction."
            },
            "venue": {
                "fragments": [],
                "text": "SIGMOD '04"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720956"
                        ],
                        "name": "Chia-Hui Chang",
                        "slug": "Chia-Hui-Chang",
                        "structuredName": {
                            "firstName": "Chia-Hui",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chia-Hui Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687115"
                        ],
                        "name": "Shao-Chen Lui",
                        "slug": "Shao-Chen-Lui",
                        "structuredName": {
                            "firstName": "Shao-Chen",
                            "lastName": "Lui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shao-Chen Lui"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[8] proposes a string matching method."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 62
                            }
                        ],
                        "text": "We do not compare it with the method in [5] and the method in [8] here as it is shown in [21] that MDR is already more effective than them."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[8] proposes a method to find patterns from the HTML tag string of a page, and then use the patterns to extract data items."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 39
                            }
                        ],
                        "text": "Center string method, which is used in [8], is a particular heuristic method for multiple sequence alignments, which can also be used for trees."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207646361,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c22e82083b78e4022d5b9d2cd4c2a2f42f73151a",
            "isKey": true,
            "numCitedBy": 527,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "research in information extraction (IE) regards the generation of wrappers that can extract particular information from semi- structured Web documents. Similar to compiler generation, the extractor is actually a driver program, which is accompanied with the generated extraction rule. Previous work in this field aims to learn extraction rules from users' training example. In this paper, we propose IEPAD, a system that automatically discovers extraction rules from Web pages. The system can automatically identify record boundary by repeated pattern mining and multiple sequence alignment. The discovery of repeated patterns are realized through a data structure call PAT trees. Additionally, repeated patterns are further extended by pattern alignment to comprehend all record instances. This new track to IE involves no human effort and content-dependent heuristics. Experimental results show that the constructed extraction rules can achieve 97 percent extraction over fourteen popular search engines."
            },
            "slug": "IEPAD:-information-extraction-based-on-pattern-Chang-Lui",
            "title": {
                "fragments": [],
                "text": "IEPAD: information extraction based on pattern discovery"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "IEPAD is proposed, a system that automatically discovers extraction rules from Web pages that can automatically identify record boundary by repeated pattern mining and multiple sequence alignment and can achieve 97 percent extraction over fourteen popular search engines."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '01"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115625054"
                        ],
                        "name": "Yalin Wang",
                        "slug": "Yalin-Wang",
                        "structuredName": {
                            "firstName": "Yalin",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yalin Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47756656"
                        ],
                        "name": "Jianying Hu",
                        "slug": "Jianying-Hu",
                        "structuredName": {
                            "firstName": "Jianying",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianying Hu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 21
                            }
                        ],
                        "text": "In wrapper induction [11, 19, 23, 25, 33], a set of extraction rules are learnt from a set of manually labeled pages or data records."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2061833,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d89945f77470b6a1dabd1f224f10b7d096fd9435",
            "isKey": false,
            "numCitedBy": 227,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Table is a commonly used presentation scheme, especially for describing relational information. However, table understanding remains an open problem. In this paper, we consider the problem of table detection in web documents. Its potential applications include web mining, knowledge management, and web content summarization and delivery to narrow-bandwidth devices. We describe a machine learning based approach to classify each given table entity as either genuine or non-genuine. Various features reflecting the layout as well as content characteristics of tables are studied.In order to facilitate the training and evaluation of our table classifier, we designed a novel web document table ground truthing protocol and used it to build a large table ground truth database. The database consists of 1,393 HTML files collected from hundreds of different web sites and contains 11,477 leaf TABLE elements, out of which 1,740 are genuine tables. Experiments were conducted using the cross validation method and an F-measure of 95.89% was achieved."
            },
            "slug": "A-machine-learning-based-approach-for-table-on-the-Wang-Hu",
            "title": {
                "fragments": [],
                "text": "A machine learning based approach for table detection on the web"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A machine learning based approach to classify each given table entity as either genuine or non-genuine, and designed a novel web document table ground truthing protocol and used it to build a large table ground truth database."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '02"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145658132"
                        ],
                        "name": "David Pinto",
                        "slug": "David-Pinto",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Pinto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Pinto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144876441"
                        ],
                        "name": "Xing Wei",
                        "slug": "Xing-Wei",
                        "structuredName": {
                            "firstName": "Xing",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xing Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144456145"
                        ],
                        "name": "W. Bruce Croft",
                        "slug": "W.-Bruce-Croft",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Croft",
                            "middleNames": [
                                "Bruce"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Bruce Croft"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1092004,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6991606a1a9d5c285af385ee9159fd46cc14048e",
            "isKey": false,
            "numCitedBy": 440,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability to find tables and extract information from them is a necessary component of data mining, question answering, and other information retrieval tasks. Documents often contain tables in order to communicate densely packed, multi-dimensional information. Tables do this by employing layout patterns to efficiently indicate fields and records in two-dimensional form.Their rich combination of formatting and content present difficulties for traditional language modeling techniques, however. This paper presents the use of conditional random fields (CRFs) for table extraction, and compares them with hidden Markov models (HMMs). Unlike HMMs, CRFs support the use of many rich and overlapping layout and language features, and as a result, they perform significantly better. We show experimental results on plain-text government statistical reports in which tables are located with 92% F1, and their constituent lines are classified into 12 table-related categories with 94% accuracy. We also discuss future work on undirected graphical models for segmenting columns, finding cells, and classifying them as data cells or label cells."
            },
            "slug": "Table-extraction-using-conditional-random-fields-Pinto-McCallum",
            "title": {
                "fragments": [],
                "text": "Table extraction using conditional random fields"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Unlike HMMs, CRFs support the use of many rich and overlapping layout and language features, and as a result, they perform significantly better, and are compared with hidden Markov models (HMMs)."
            },
            "venue": {
                "fragments": [],
                "text": "DG.O"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118682401"
                        ],
                        "name": "Huang Yu",
                        "slug": "Huang-Yu",
                        "structuredName": {
                            "firstName": "Huang",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huang Yu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 93
                            }
                        ],
                        "text": "A wrapper is a program that extracts data from a Web site or page and put them in a database [1, 11, 12, 16, 18, 19, 22, 23, 25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 63362825,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7ad92b1520f574a0ba0b27eb4c753d3d6324c71e",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In order to integrate and query irregular and dynamic information on WEB in a database like fashion, the authors use object exchange model (OEM) to construct information model of WEB in this paper. To express each component of pages as an OEM object, the authors design an algorithm which extracts semi structured data from HTML pages, and the testing results are given. This method can extract structured and semi structured data. It has better applicability than other existing methods."
            },
            "slug": "Extracting-Semi-Structured-Information-from-the-WEB-Yu",
            "title": {
                "fragments": [],
                "text": "Extracting Semi-Structured Information from the WEB"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "To express each component of pages as an OEM object, the authors design an algorithm which extracts semi structured data from HTML pages, and the testing results are given."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3276863"
                        ],
                        "name": "Ion Muslea",
                        "slug": "Ion-Muslea",
                        "structuredName": {
                            "firstName": "Ion",
                            "lastName": "Muslea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ion Muslea"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145293454"
                        ],
                        "name": "Steven Minton",
                        "slug": "Steven-Minton",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Minton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven Minton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745117"
                        ],
                        "name": "Craig A. Knoblock",
                        "slug": "Craig-A.-Knoblock",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Knoblock",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Craig A. Knoblock"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "Example wrapper induction systems include WIEN [19], Softmealy [18], Stalker [23], WL(2) [11], [25], etc."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 69
                            }
                        ],
                        "text": "Example wrapper induction systems \ninclude WIEN [19], Softmealy [18], Stalker [23], WL2 [11], [25], etc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 93
                            }
                        ],
                        "text": "A wrapper is a program that extracts data from a Web site or page and put them in a database [1, 11, 12, 16, 18, 19, 22, 23, 25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 21
                            }
                        ],
                        "text": "In wrapper induction [11, 19, 23, 25, 33], a set of extraction rules are learnt from a set of manually labeled pages or data records."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3514097,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1cc263c84b85027164bd39db169f5d5959ef6822",
            "isKey": true,
            "numCitedBy": 464,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "With the tremendous amount of information that becomes available on the Web on a daily basis, the ability to quickly develop information agents has become a crucial problem. A vital component of any Web-based information agent is a set of wrappers that can extract the relevant data from semistructured information sources. Our novel approach to wrapper induction is based on the idea of hierarchical information extraction, which turns the hard problem of extracting data from an arbitrarily complex document into a series of easier extraction tasks. We introduce an inductive algorithm, STALKER, that generates high accuracy extraction rules based on user-labeled training examples. Labeling the training data represents the major bottleneck in using wrapper induction techniques, and our experimental results show that STALKER does significantly better then other approaches; on one hand, STALKER requires up to two orders of magnitude fewer examples than other algorithms, while on the other hand it can handle information sources that could not be wrapped by existing techniques."
            },
            "slug": "A-hierarchical-approach-to-wrapper-induction-Muslea-Minton",
            "title": {
                "fragments": [],
                "text": "A hierarchical approach to wrapper induction"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work introduces an inductive algorithm, STALKER, that generates high accuracy extraction rules based on user-labeled training examples that can handle information sources that could not be wrapped by existing techniques."
            },
            "venue": {
                "fragments": [],
                "text": "AGENTS '99"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47861681"
                        ],
                        "name": "Binyamin Rosenfeld",
                        "slug": "Binyamin-Rosenfeld",
                        "structuredName": {
                            "firstName": "Binyamin",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Binyamin Rosenfeld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145864794"
                        ],
                        "name": "Ronen Feldman",
                        "slug": "Ronen-Feldman",
                        "structuredName": {
                            "firstName": "Ronen",
                            "lastName": "Feldman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronen Feldman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2090792"
                        ],
                        "name": "Y. Aumann",
                        "slug": "Y.-Aumann",
                        "structuredName": {
                            "firstName": "Yonatan",
                            "lastName": "Aumann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Aumann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 37
                            }
                        ],
                        "text": ", finding different semantics blocks [29, 28]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11008046,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d7fe0bd3854f90c1249062c079e34c30422936d",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Most information extraction systems focus on the textual content of the documents. They treat documents as sequences or of words, disregarding the physical and typographical layout of the information.. While this strategy helps in focusing the extraction process on the key semantic content of the document, much valuable information can also be derived form the document physical appearance. Often, fonts, physical positioning and other graphical characteristics are used to provide additional context to the information. This information is lost with pure-text analysis. In this paper we describe a general procedure for structural extraction, which allows for automatic extraction of entities from the document based on their visual characteristics and relative position in the document layout. Our structural extraction procedure is a learning algorithm, which knows how to automatically generalizes from examples. The procedure is a general one, applicable to any document format with visual and typographical information. We also then describe a specific implementation of the procedure to PDF documents, called PES (PDF Extraction System). PES works with PDF documents and is able to extract such fields such as Author(s), Title, Date, etc. with very high accuracy."
            },
            "slug": "Structural-extraction-from-visual-layout-of-Rosenfeld-Feldman",
            "title": {
                "fragments": [],
                "text": "Structural extraction from visual layout of documents"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A general procedure for structural extraction, which allows for automatic extraction of entities from the document based on their visual characteristics and relative position in the document layout, applicable to any document format with visual and typographical information."
            },
            "venue": {
                "fragments": [],
                "text": "CIKM '02"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690267"
                        ],
                        "name": "David J. Buttler",
                        "slug": "David-J.-Buttler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Buttler",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Buttler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46458150"
                        ],
                        "name": "Ling Liu",
                        "slug": "Ling-Liu",
                        "structuredName": {
                            "firstName": "Ling",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ling Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145932397"
                        ],
                        "name": "C. Pu",
                        "slug": "C.-Pu",
                        "structuredName": {
                            "firstName": "Calton",
                            "lastName": "Pu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Pu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 40
                            }
                        ],
                        "text": "We do not compare it with the method in [5] and the method in [8] here as it is shown in [21] that MDR is already more effective than them."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[5] proposes a few more heuristics to perform the task without using domain ontology."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15556693,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4764b94f2326ba2ea44a9677365aa381657a6388",
            "isKey": false,
            "numCitedBy": 273,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a fully automated object extraction system Omini. A distinct feature of Omini is the suite of algorithms and the automatically learned information extraction rules for discovering and extracting objects from dynamic Web pages or static Web pages that contain multiple object instances. We evaluated the system using more than 2,000 Web pages over 40 sites. It achieves 100% precision (returns only correct objects) and excellent recall (between 99% and 98%, with very few significant objects left out). The object boundary identification algorithms are fast, about 0.1 second per page with a simple optimization."
            },
            "slug": "A-fully-automated-object-extraction-system-for-the-Buttler-Liu",
            "title": {
                "fragments": [],
                "text": "A fully automated object extraction system for the World Wide Web"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A fully automated object extraction system Omini, which achieves 100% precision (returns only correct objects) and excellent recall (between 99% and 98%, with very few significant objects left out)."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 21st International Conference on Distributed Computing Systems"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34607455"
                        ],
                        "name": "Chun-Nan Hsu",
                        "slug": "Chun-Nan-Hsu",
                        "structuredName": {
                            "firstName": "Chun-Nan",
                            "lastName": "Hsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chun-Nan Hsu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2094164369"
                        ],
                        "name": "Ming-Tzung Dung",
                        "slug": "Ming-Tzung-Dung",
                        "structuredName": {
                            "firstName": "Ming-Tzung",
                            "lastName": "Dung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Tzung Dung"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 63
                            }
                        ],
                        "text": "Example wrapper induction systems include WIEN [19], Softmealy [18], Stalker [23], WL(2) [11], [25], etc."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 53
                            }
                        ],
                        "text": "Example wrapper induction systems \ninclude WIEN [19], Softmealy [18], Stalker [23], WL2 [11], [25], etc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 93
                            }
                        ],
                        "text": "A wrapper is a program that extracts data from a Web site or page and put them in a database [1, 11, 12, 16, 18, 19, 22, 23, 25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17895561,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9478265afd280486299a5b8f1dbaaf6769422de",
            "isKey": false,
            "numCitedBy": 507,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generating-Finite-State-Transducers-for-Data-from-Hsu-Dung",
            "title": {
                "fragments": [],
                "text": "Generating Finite-State Transducers for Semi-Structured Data Extraction from the Web"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Syst."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35119829"
                        ],
                        "name": "Ruihua Song",
                        "slug": "Ruihua-Song",
                        "structuredName": {
                            "firstName": "Ruihua",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ruihua Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144321224"
                        ],
                        "name": "Haifeng Liu",
                        "slug": "Haifeng-Liu",
                        "structuredName": {
                            "firstName": "Haifeng",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haifeng Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2259699"
                        ],
                        "name": "Ji-Rong Wen",
                        "slug": "Ji-Rong-Wen",
                        "structuredName": {
                            "firstName": "Ji-Rong",
                            "lastName": "Wen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ji-Rong Wen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712167"
                        ],
                        "name": "Wei-Ying Ma",
                        "slug": "Wei-Ying-Ma",
                        "structuredName": {
                            "firstName": "Wei-Ying",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Ying Ma"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 37
                            }
                        ],
                        "text": ", finding different semantics blocks [29, 28]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6251615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f53944274eea918dbc0e0ee50e2774894eb9a7e",
            "isKey": false,
            "numCitedBy": 311,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous work shows that a web page can be partitioned into multiple segments or blocks, and often the importance of those blocks in a page is not equivalent. Also, it has been proven that differentiating noisy or unimportant blocks from pages can facilitate web mining, search and accessibility. However, no uniform approach and model has been presented to measure the importance of different segments in web pages. Through a user study, we found that people do have a consistent view about the importance of blocks in web pages. In this paper, we investigate how to find a model to automatically assign importance values to blocks in a web page. We define the block importance estimation as a learning problem. First, we use a vision-based page segmentation algorithm to partition a web page into semantic blocks with a hierarchical structure. Then spatial features (such as position and size) and content features (such as the number of images and links) are extracted to construct a feature vector for each block. Based on these features, learning algorithms are used to train a model to assign importance to different segments in the web page. In our experiments, the best model can achieve the performance with Micro-F1 79% and Micro-Accuracy 85.9%, which is quite close to a person's view."
            },
            "slug": "Learning-block-importance-models-for-web-pages-Song-Liu",
            "title": {
                "fragments": [],
                "text": "Learning block importance models for web pages"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper uses a vision-based page segmentation algorithm to partition a web page into semantic blocks with a hierarchical structure, then spatial features and content features are extracted and used to construct a feature vector for each block."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '04"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2127203"
                        ],
                        "name": "Lakshmish Ramaswamy",
                        "slug": "Lakshmish-Ramaswamy",
                        "structuredName": {
                            "firstName": "Lakshmish",
                            "lastName": "Ramaswamy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lakshmish Ramaswamy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144239384"
                        ],
                        "name": "A. Iyengar",
                        "slug": "A.-Iyengar",
                        "structuredName": {
                            "firstName": "Arun",
                            "lastName": "Iyengar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Iyengar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46458150"
                        ],
                        "name": "Ling Liu",
                        "slug": "Ling-Liu",
                        "structuredName": {
                            "firstName": "Ling",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ling Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782350"
                        ],
                        "name": "F. Douglis",
                        "slug": "F.-Douglis",
                        "structuredName": {
                            "firstName": "Fred",
                            "lastName": "Douglis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Douglis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12726241,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7151f607403425f283bcb226f37742e9d3b40ce8",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Dividing web pages into fragments has been shown to provide significant benefits for both content generation and caching. In order for a web site to use fragment-based content generation, however, good methods are needed for dividing web pages into fragments. Manual fragmentation of web pages is expensive, error prone, and unscalable. This paper proposes a novel scheme to automatically detect and flag fragments that are cost-effective cache units in web sites serving dynamic content. We consider the fragments to be interesting if they are shared among multiple documents or they have different lifetime or personalization characteristics. Our approach has three unique features. First, we propose a hierarchical and fragment-aware model of the dynamic web pages and a data structure that is compact and effective for fragment detection. Second, we present an efficient algorithm to detect maximal fragments that are shared among multiple documents. Third, we develop a practical algorithm that effectively detects fragments based on their lifetime and personalization characteristics. We evaluate the proposed scheme through a series of experiments, showing the benefits and costs of the algorithms. We also study the impact of adopting the fragments detected by our system on disk space utilization and network bandwidth consumption."
            },
            "slug": "Automatic-detection-of-fragments-in-dynamically-web-Ramaswamy-Iyengar",
            "title": {
                "fragments": [],
                "text": "Automatic detection of fragments in dynamically generated web pages"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes a novel scheme to automatically detect and flag fragments that are cost-effective cache units in web sites serving dynamic content, and develops a practical algorithm that effectively detects fragments based on their lifetime and personalization characteristics."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '04"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791339"
                        ],
                        "name": "Valter Crescenzi",
                        "slug": "Valter-Crescenzi",
                        "structuredName": {
                            "firstName": "Valter",
                            "lastName": "Crescenzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Valter Crescenzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785690"
                        ],
                        "name": "G. Mecca",
                        "slug": "G.-Mecca",
                        "structuredName": {
                            "firstName": "Giansalvatore",
                            "lastName": "Mecca",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Mecca"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796590"
                        ],
                        "name": "P. Merialdo",
                        "slug": "P.-Merialdo",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Merialdo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Merialdo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "We do not compare with the systems in [1][12] as they require multiple pages and all of them contain similar data records to find patterns from the pages to extract data items."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 3
                            }
                        ],
                        "text": "In [1, 12, 34], two more techniques are proposed."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 93
                            }
                        ],
                        "text": "A wrapper is a program that extracts data from a Web site or page and put them in a database [1, 11, 12, 16, 18, 19, 22, 23, 25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "In automatic methods, [12][1] find patterns or grammars from multiple pages containing similar data records."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15075203,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3dd1f9f7795b31493d98d9f260d37aad07550f6e",
            "isKey": true,
            "numCitedBy": 1157,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper investigates techniques for extracting data from HTML sites through the use of automatically generated wrappers. To automate the wrapper generation and the data extraction process, the paper develops a novel technique to compare HTML pages and generate a wrapper based on their similarities and dierences. Experimental results on real-life data-intensive Web sites confirm the feasibility of the approach."
            },
            "slug": "RoadRunner:-Towards-Automatic-Data-Extraction-from-Crescenzi-Mecca",
            "title": {
                "fragments": [],
                "text": "RoadRunner: Towards Automatic Data Extraction from Large Web Sites"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel technique to compare HTML pages and generate a wrapper based on their similarities and dierences is developed, which confirms the feasibility of the approach on real-life data-intensive Web sites."
            },
            "venue": {
                "fragments": [],
                "text": "VLDB"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1402923066"
                        ],
                        "name": "Ziv Bar-Yossef",
                        "slug": "Ziv-Bar-Yossef",
                        "structuredName": {
                            "firstName": "Ziv",
                            "lastName": "Bar-Yossef",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ziv Bar-Yossef"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145941159"
                        ],
                        "name": "S. Rajagopalan",
                        "slug": "S.-Rajagopalan",
                        "structuredName": {
                            "firstName": "Sridhar",
                            "lastName": "Rajagopalan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Rajagopalan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 87
                            }
                        ],
                        "text": "The simple tree matching algorithm \n (B) N15 N16 N17 N18 N19  N20  N21 N12 N13 N14 N22 (C) M1-15 (D) W1-15 (E) M5-17 (F) W5-17 (G) \nM11-20 (H) W11-20  0 1 (N16) 2 (N16-N17) 0 0 0 0 1 (N2) 0 3 3 2 (N2-N3) 0 3 5 3 (N2-N4) 0 3 5 4 (N2-N5) \n0 3 6 1 (N16) 2 (N17) 1 (N2) 3 0 2 (N3) 0 2 3 (N4) 2 0 4 (N5) 0 3 0 1 (N20) 2 (N20-N21) 0 0 0 0 1 (N11) \n0 2 2 1(N20) 2(N21) 1 (N11) 2 0 0 1 (N22) 0 0 0 1 (N12) 0 1 2 (N12-N13) 0 1 3 (N12-N14) 0 1 1 (N22) \n1 (N12) 1 2 (N13) 0 3 (N14) 0 Figure 10."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 18
                            }
                        ],
                        "text": "For example, W1-15[4, 2] is computed recursively by building the matrices (E)-(H)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "M1-15 matrix is computed based on the W1-15 matrix, and each entry in W1-15, say W1-15[i, \nj], is the maximum matching between the ith and jth first\u00adlevel sub-trees of A and B, which is computed \nrecursively based on its M matrix."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 24
                            }
                        ],
                        "text": "identical symbols, M1-15[4,2]+1 is returned as the maximum matching value between trees A and B (line 11)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 44
                            }
                        ],
                        "text": "Since N1 \nand N15 contain identical symbols, M1-15[4,2]+1 is returned as the maximum matching value between trees \nA and B (line 11)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 13
                            }
                        ],
                        "text": "For example, W1-15[4, 2] is computed recursively by building the matrices \n(E)-(H)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16198840,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b774b700744d7193f48a332086ccb64fd886fa70",
            "isKey": true,
            "numCitedBy": 333,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We formulate and propose the template detection problem, and suggest a practical solution for it based on counting frequent item sets. We show that the use of templates is pervasive on the web. We describe three principles, which characterize the assumptions made by hypertext information retrieval (IR) and data mining (DM) systems, and show that templates are a major source of violation of these principles. As a consequence, basic \"pure\" implementations of simple search algorithms coupled with template detection and elimination show surprising increases in precision at all levels of recall."
            },
            "slug": "Template-detection-via-data-mining-and-its-Bar-Yossef-Rajagopalan",
            "title": {
                "fragments": [],
                "text": "Template detection via data mining and its applications"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that the use of templates is pervasive on the web, and basic \"pure\" implementations of simple search algorithms coupled with template detection and elimination show surprising increases in precision at all levels of recall."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '02"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153924342"
                        ],
                        "name": "Hsin-Hsi Chen",
                        "slug": "Hsin-Hsi-Chen",
                        "structuredName": {
                            "firstName": "Hsin-Hsi",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hsin-Hsi Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072526610"
                        ],
                        "name": "Shih-Chung Tsai",
                        "slug": "Shih-Chung-Tsai",
                        "structuredName": {
                            "firstName": "Shih-Chung",
                            "lastName": "Tsai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shih-Chung Tsai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2949513"
                        ],
                        "name": "Jin-He Tsai",
                        "slug": "Jin-He-Tsai",
                        "structuredName": {
                            "firstName": "Jin-He",
                            "lastName": "Tsai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jin-He Tsai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6844025,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "578c63514136ac5b9af0144bcfe06efcfdd3099c",
            "isKey": false,
            "numCitedBy": 193,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Table is a very common presentation scheme, but few papers touch on table extraction in text data mining. This paper focuses on mining tables from large-scale HTML texts. Table filtering, recognition, interpretation, and presentation are discussed. Heuristic rules and cell similarities are employed to identify tables. The F-measure of table recognition is 86.50%. We also propose an algorithm to capture attribute-value relationships among table cells. Finally, more structured data is extracted and presented."
            },
            "slug": "Mining-Tables-from-Large-Scale-HTML-Texts-Chen-Tsai",
            "title": {
                "fragments": [],
                "text": "Mining Tables from Large Scale HTML Texts"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper focuses on mining tables from large-scale HTML texts by using heuristic rules and cell similarities to identify tables and proposes an algorithm to capture attribute-value relationships among table cells."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8551365"
                        ],
                        "name": "N. Kushmerick",
                        "slug": "N.-Kushmerick",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Kushmerick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Kushmerick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "Example wrapper induction systems include WIEN [19], Softmealy [18], Stalker [23], WL(2) [11], [25], etc."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "Example wrapper induction systems \ninclude WIEN [19], Softmealy [18], Stalker [23], WL2 [11], [25], etc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 93
                            }
                        ],
                        "text": "A wrapper is a program that extracts data from a Web site or page and put them in a database [1, 11, 12, 16, 18, 19, 22, 23, 25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 21
                            }
                        ],
                        "text": "In wrapper induction [11, 19, 23, 25, 33], a set of extraction rules are learnt from a set of manually labeled pages or data records."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11075952,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f052f40a3307de1e45e11a3007a7552b36ebfc8",
            "isKey": true,
            "numCitedBy": 641,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Wrapper-induction:-Efficiency-and-expressiveness-Kushmerick",
            "title": {
                "fragments": [],
                "text": "Wrapper induction: Efficiency and expressiveness"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50056360"
                        ],
                        "name": "William W. Cohen",
                        "slug": "William-W.-Cohen",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Cohen",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William W. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145501780"
                        ],
                        "name": "Matthew F. Hurst",
                        "slug": "Matthew-F.-Hurst",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Hurst",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew F. Hurst"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122689890"
                        ],
                        "name": "Lee S. Jensen",
                        "slug": "Lee-S.-Jensen",
                        "structuredName": {
                            "firstName": "Lee",
                            "lastName": "Jensen",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lee S. Jensen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "Example wrapper induction systems include WIEN [19], Softmealy [18], Stalker [23], WL(2) [11], [25], etc."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 83
                            }
                        ],
                        "text": "Example wrapper induction systems \ninclude WIEN [19], Softmealy [18], Stalker [23], WL2 [11], [25], etc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 93
                            }
                        ],
                        "text": "A wrapper is a program that extracts data from a Web site or page and put them in a database [1, 11, 12, 16, 18, 19, 22, 23, 25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 21
                            }
                        ],
                        "text": "In wrapper induction [11, 19, 23, 25, 33], a set of extraction rules are learnt from a set of manually labeled pages or data records."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2313483,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0909fee90833e20913adb553bf6667c9a3b854b0",
            "isKey": true,
            "numCitedBy": 293,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "A program that makes an existing website look like a database is called a wrapper. Wrapper learning is the problem of learning website wrappers from examples. We present a wrapper-learning system called WL2 that can exploit several different representations of a document. Examples of such different representations include DOM-level and token-level representations, as well as two-dimensional geometric views of the rendered page (for tabular data) and representations of the visual appearance of text asm it will be rendered. Additionally, the learning system is modular, and can be easily adapted to new domains and tasks. The learning system described is part of an \"industrial-strength\" wrapper management system that is in active use at WhizBang Labs. Controlled experiments show that the learner has broader coverage and a faster learning rate than earlier wrapper-learning systems."
            },
            "slug": "A-flexible-learning-system-for-wrapping-tables-and-Cohen-Hurst",
            "title": {
                "fragments": [],
                "text": "A flexible learning system for wrapping tables and lists in HTML documents"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A wrapper-learning system called WL2 that can exploit several different representations of a document, including DOM-level and token-level representations, as well as two-dimensional geometric views of the rendered page and representations of the visual appearance of text asm it will be rendered."
            },
            "venue": {
                "fragments": [],
                "text": "WWW '02"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29829232"
                        ],
                        "name": "J. Wang",
                        "slug": "J.-Wang",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Wang",
                            "middleNames": [
                                "Tsong-Li"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694805"
                        ],
                        "name": "B. Shapiro",
                        "slug": "B.-Shapiro",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Shapiro",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Shapiro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695878"
                        ],
                        "name": "D. Shasha",
                        "slug": "D.-Shasha",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "Shasha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Shasha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698666"
                        ],
                        "name": "Kaizhong Zhang",
                        "slug": "Kaizhong-Zhang",
                        "structuredName": {
                            "firstName": "Kaizhong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaizhong Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2712717"
                        ],
                        "name": "K. Currey",
                        "slug": "K.-Currey",
                        "structuredName": {
                            "firstName": "Kathleen",
                            "lastName": "Currey",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Currey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [32][10], two other algorithms are also presented with similar complexities."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14697648,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4e9d658a686d1e953803779dcf143ea66f9eeee1",
            "isKey": false,
            "numCitedBy": 124,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Ordered, labeled trees are trees in which each node has a label and the left-to-right order of its children (if it has any) is fixed. Such trees have many applications in vision, pattern recognition, molecular biology and natural language processing. We consider a substructure of an ordered labeled tree T to be a connected subgraph of T. Given two ordered labeled trees T/sub 1/ and T/sub 2/ and an integer d, the largest approximately common substructure problem is to find a substructure U/sub 1/ of T/sub 1/ and a substructure U/sub 2/ of T/sub 2/ such that U/sub 1/ is within edit distance d of U/sub 2/ and where there does not exist any other substructure V/sub 1/ of T/sub 1/ and V/sub 2/ of T/sub 2/ such that V/sub 1/ and V/sub 2/ satisfy the distance constraint and the sum of the sizes of V/sub 1/ and V/sub 2/ is greater than the sum of the sizes of U/sub 1/ and U/sub 2/. We present a dynamic programming algorithm to solve this problem, which runs as fast as the fastest known algorithm for computing the edit distance of two trees when the distance allowed in the common substructures is a constant independent of the input trees. To demonstrate the utility of our algorithm, we discuss its application to discovering motifs in multiple RNA secondary structures (which are ordered labeled trees)."
            },
            "slug": "An-Algorithm-for-Finding-the-Largest-Approximately-Wang-Shapiro",
            "title": {
                "fragments": [],
                "text": "An Algorithm for Finding the Largest Approximately Common Substructures of Two Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A dynamic programming algorithm is presented that runs as fast as the fastest known algorithm for computing the edit distance of two trees when the distance allowed in the common substructures is a constant independent of the input trees."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2913159"
                        ],
                        "name": "Robert B. Doorenbos",
                        "slug": "Robert-B.-Doorenbos",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Doorenbos",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert B. Doorenbos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780531"
                        ],
                        "name": "Daniel S. Weld",
                        "slug": "Daniel-S.-Weld",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Weld",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel S. Weld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3531043,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7b3b14828a71c3bd4e56fda87a8c89a72d358c4e",
            "isKey": false,
            "numCitedBy": 608,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The WorldWideWeb is less agent-friendly than we might hope. Most information on the Web is presented in loosely structured natural language text with no agent-readable semantics. HTML annotations structure the display of Web pages, but provide virtually no insight into their content. Thus, the designers of intelligent Web agents need to address the following questions: (1) To what extent can an agent understand information published at Web sites? (2) Is the agent's understanding sufficient to provide genuinely useful assistance to users? (3) Is site-specific hand-coding necessary, or can the agent automatically extract information from unfamiliar Web sites? (4) What aspects of the Web facilitate this competence? In this paper we investigate these issues with a case study using ShopBot, a fully-implemented, domainindependent comparison-shopping agent. Given the home pages of several online stores, ShopBot autonomously learns how to shop at those vendors. After learning, it is able to speedily visit over a dozen software and CD vendors, extract product information, and summarize the results for the user. Preliminary studies show that ShopBot enables users to both find superior prices and substantially reduce Web shopping time. Remarkably, ShopBot achieves this performance without sophisticated natural language processing, and requires only minimal knowledge about different product domains. Instead, ShopBot relies on a combination of heuristic search, pattern matching, and inductive learning techniques. PERMISSION TO COPY WITHOUT FEE ALL OR OR PART OF THIS MATERIAL IS GRANTED PROVIDED THAT THE COPIES ARE NOT MADE OR DISTRIBUTED FOR DIRECT COMMERCIAL ADVANTAGE, THE ACM copyRIGHT NOTICE AND THE TITLE OF THE PUBLICATION AND ITS DATE APPEAR, AND NOTICE IS GIVEN THAT COPYING IS BY PERMISSION OF ACM. To COPY OTHERWISE, OR TO REPUBLISH, REQUIRES A FEE AND/OR SPECIFIC PERMISSION. AGENTS '97 CONFERENCE PROCEEDINGS, COPYRIGHT 1997 ACM."
            },
            "slug": "A-scalable-comparison-shopping-agent-for-the-Web-Doorenbos-Etzioni",
            "title": {
                "fragments": [],
                "text": "A scalable comparison-shopping agent for the World-Wide Web"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "ShopBot, a fully-implemented, domainindependent comparison-shopping agent that relies on a combination of heuristic search, pattern matching, and inductive learning techniques, enables users to both find superior prices and substantially reduce Web shopping time."
            },
            "venue": {
                "fragments": [],
                "text": "AGENTS '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1851421"
                        ],
                        "name": "K. Tai",
                        "slug": "K.-Tai",
                        "structuredName": {
                            "firstName": "Kuo-chung",
                            "lastName": "Tai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Tai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "The concept of mapping [30] is formally defined as:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 73
                            }
                        ],
                        "text": "1 Tree Edit Distance Similar to string edit distance, tree edit distance [31, 30] between two trees A and B (we are only interested in labeled ordered rooted trees) is the cost associated with the minimum set of operations needed to transform A into B."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "Solving the tree edit distance problem is often assisted by finding a minimum-cost mapping between two trees [30]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [30], a solution based on dynamic programming is presented."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8920424,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b3f2bd7f89fdfdc118432f4105e7e6c00a6073f5",
            "isKey": true,
            "numCitedBy": 922,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "The tree-to-tree correctmn problem Is to determine, for two labeled ordered trees T and T', the distance from T to T' as measured by the mlmmum cost sequence of edit operaUons needed to transform T into T' The edit operations investigated allow changing one node of a tree into another node, deleting one node from a tree, or inserting a node into a tree An algorithm Is presented which solves this problem m time O(V* V'*LZ* L'2), where V and V' are the numbers of nodes respectively of T and T', and L and L' are the maximum depths respectively of T and T' Possible apphcatmns are to the problems of measuring the similarity between trees, automatic error recovery and correction for programming languages, and determining the largest common substructure of two trees"
            },
            "slug": "The-Tree-to-Tree-Correction-Problem-Tai",
            "title": {
                "fragments": [],
                "text": "The Tree-to-Tree Correction Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "An algorithm is presented which solves the problem of determining the distance from T to T' as measured by the mlmmum cost sequence of edit operaUons needed to transform T into T'."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37886264"
                        ],
                        "name": "H. Carrillo",
                        "slug": "H.-Carrillo",
                        "structuredName": {
                            "firstName": "Humberto",
                            "lastName": "Carrillo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Carrillo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1816655"
                        ],
                        "name": "D. Lipman",
                        "slug": "D.-Lipman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lipman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lipman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "In [6], a multiple alignment method is proposed using multidimensional dynamic programming."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 119835502,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "daceee6126bc54987b38df6d9fd953445232768d",
            "isKey": false,
            "numCitedBy": 592,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "The study and comparison of sequences of characters from a finite alphabet is relevant to various areas of science, notably molecular biology. The measurement of sequence similarity involves the consideration of the different possible sequence alignments in order to find an optimal one for which the \u201cdistance\u201d between sequences is minimum. By associating a path in a lattice to each alignment, a geometric insight can be brought into the problem of finding an optimal alignment. This problem can then be solved by applying a dynamic programming algorithm. However, the computational effort grows rapidly with the number N of sequences to be compared $(O(l^N ))$, where l is the mean length of the sequences to be compared).It is proved here that knowledge of the measure of an arbitrarily chosen alignment can be used in combination with information from the pairwise alignments to considerably restrict the size of the region of the lattice in consideration. This reduction implies fewer computations and less memory ..."
            },
            "slug": "The-multiple-sequence-alignment-problem-in-biology-Carrillo-Lipman",
            "title": {
                "fragments": [],
                "text": "The multiple sequence alignment problem in biology"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is proved here that knowledge of the measure of an arbitrarily chosen alignment can be used in combination with information from the pairwise alignments to considerably restrict the size of the region of the lattice in consideration."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108952917"
                        ],
                        "name": "Weimin Chen",
                        "slug": "Weimin-Chen",
                        "structuredName": {
                            "firstName": "Weimin",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weimin Chen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 7
                            }
                        ],
                        "text": "In [32][10], two other algorithms are also presented with similar complexities."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 55
                            }
                        ],
                        "text": "All the formulations have complexities above quadratic [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26634788,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "169ecdf2cea4181c60df305b7f8b5e3c51f084e0",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The ordered tree-to-tree correction problem is to compute the minimum edit cost of transforming one ordered tree to another one. This paper presents a new algorithm for this problem. Given two ordered trees S and T, our algorithm runs in O(|S||T|+min{L2S|T|+L2.5SLT,L2T|S|+L2.5TLS) time, where LS denotes the number of leaves of S and DS denotes the depth of S. The previous best algorithms for this problem run in O(|S||T|min{LS,DS}min{LT,DT}) time (K. Zhang and D. Shasha, SIAM J. Comput.18, No. 6 (1989), 1245?1262) and in O(min{|S|2|T|log2|T|,|T|2|S|log2|S|}) time (P. N. Klein, in \u201cAlgorithms?ESA'98, 6th Annual European Symposium\u201d (G. Bilardi, G. F. Italiano, A. Pietracaprina, and G. Pucci, Eds.), Lecture Notes in Computer Science, Vol. 1461, pp. 91?102, Springer-Verlag, Berlin/New York, 1998). As a comparison, our algorithm is asymptotically faster for certain kind of trees."
            },
            "slug": "New-Algorithm-for-Ordered-Tree-to-Tree-Correction-Chen",
            "title": {
                "fragments": [],
                "text": "New Algorithm for Ordered Tree-to-Tree Correction Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "The ordered tree-to-tree correction problem is to compute the minimum edit cost of transforming one ordered tree to another one, and a new algorithm is presented for this problem, which is asymptotically faster for certain kind of trees."
            },
            "venue": {
                "fragments": [],
                "text": "J. Algorithms"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706261"
                        ],
                        "name": "D. Gusfield",
                        "slug": "D.-Gusfield",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Gusfield",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gusfield"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61800864,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7c72b917a38b09e6d3ab19d28a4344ba54edb6ae",
            "isKey": false,
            "numCitedBy": 3108,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Part I. Exact String Matching: The Fundamental String Problem: 1. Exact matching: fundamental preprocessing and first algorithms 2. Exact matching: classical comparison-based methods 3. Exact matching: a deeper look at classical methods 4. Semi-numerical string matching Part II. Suffix Trees and their Uses: 5. Introduction to suffix trees 6. Linear time construction of suffix trees 7. First applications of suffix trees 8. Constant time lowest common ancestor retrieval 9. More applications of suffix trees Part III. Inexact Matching, Sequence Alignment and Dynamic Programming: 10. The importance of (sub)sequence comparison in molecular biology 11. Core string edits, alignments and dynamic programming 12. Refining core string edits and alignments 13. Extending the core problems 14. Multiple string comparison: the Holy Grail 15. Sequence database and their uses: the motherlode Part IV. Currents, Cousins and Cameos: 16. Maps, mapping, sequencing and superstrings 17. Strings and evolutionary trees 18. Three short topics 19. Models of genome-level mutations."
            },
            "slug": "Algorithms-on-Strings,-Trees,-and-Sequences:-and-Gusfield",
            "title": {
                "fragments": [],
                "text": "Algorithms on Strings, Trees, and Sequences: Computer Science and Computational Biology"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The author examines the importance of (sub)sequence comparison in molecular biology, core string edits, alignments and dynamic programming, and a deeper look at classical methods for exact string matching."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764306"
                        ],
                        "name": "G. Barton",
                        "slug": "G.-Barton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Barton",
                            "middleNames": [
                                "John"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Barton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145410834"
                        ],
                        "name": "M. Sternberg",
                        "slug": "M.-Sternberg",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Sternberg",
                            "middleNames": [
                                "J.",
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sternberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 100
                            }
                        ],
                        "text": "Other popular multiple alignment methods include progressive alignment [17] and iterative alignment [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 41
                            }
                        ],
                        "text": "Many heuristic methods are also proposed [24, 17, 3]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11351850,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "95ec97c764eaeaa3c5d34270b402bcc38587da12",
            "isKey": false,
            "numCitedBy": 501,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-strategy-for-the-rapid-multiple-alignment-of-from-Barton-Sternberg",
            "title": {
                "fragments": [],
                "text": "A strategy for the rapid multiple alignment of protein sequences. Confidence levels from tertiary structure comparisons."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of molecular biology"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2861507"
                        ],
                        "name": "P. Hogeweg",
                        "slug": "P.-Hogeweg",
                        "structuredName": {
                            "firstName": "Paulien",
                            "lastName": "Hogeweg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hogeweg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2794871"
                        ],
                        "name": "B. Hesper",
                        "slug": "B.-Hesper",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Hesper",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Hesper"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "Other popular multiple alignment methods include progressive alignment [17] and iterative alignment [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 41
                            }
                        ],
                        "text": "Many heuristic methods are also proposed [24, 17, 3]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 29280525,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "2a79523647031702e4428ec2961b1d0177fb3ecb",
            "isKey": false,
            "numCitedBy": 220,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "SummaryIn this paper we argue that the alignment of sets of sequences and the construction of phyletic trees cannot be treated separately. The concept of \u2018good alignment\u2019 is meaningless without reference to a phyletic tree, and the construction of phyletic trees presupposes alignment of the sequences.We propose an integrated method that generates both an alignment of a set of sequences and a phyletic tree. In this method a putative tree is used to align the sequences and the alignment obtained is used to adjust the tree; this process is iterated. As a demonstration we apply the method to the analysis of the evolution of 5S rRNA sequences in prokaryotes."
            },
            "slug": "The-alignment-of-sets-of-sequences-and-the-of-An-Hogeweg-Hesper",
            "title": {
                "fragments": [],
                "text": "The alignment of sets of sequences and the construction of phyletic trees: An integrated method"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "An integrated method is proposed that generates both an alignment of a set of sequences and a phyletic tree and is applied to the analysis of the evolution of 5S rRNA sequences in prokaryotes."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Molecular Evolution"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46726219"
                        ],
                        "name": "C. Notredame",
                        "slug": "C.-Notredame",
                        "structuredName": {
                            "firstName": "C\u00e9dric",
                            "lastName": "Notredame",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Notredame"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 41
                            }
                        ],
                        "text": "Many heuristic methods are also proposed [24, 17, 3]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15681467,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "045e5afb4640557e1355473ef638c902c0e14296",
            "isKey": false,
            "numCitedBy": 383,
            "numCiting": 105,
            "paperAbstract": {
                "fragments": [],
                "text": "The assembly of a multiple sequence alignment (MSA) has become one of the most common tasks when dealing with sequence analysis. Unfortunately, the wide range of available methods and the differences in the results given by these methods makes it hard for a non-specialist to decide which program is best suited for a given purpose. In this review we briefly describe existing techniques and expose the potential strengths and weaknesses of the most widely used multiple alignment packages."
            },
            "slug": "Recent-progress-in-multiple-sequence-alignment:-a-Notredame",
            "title": {
                "fragments": [],
                "text": "Recent progress in multiple sequence alignment: a survey."
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "In this review, existing techniques are described and the potential strengths and weaknesses of the most widely used multiple alignment packages are exposed."
            },
            "venue": {
                "fragments": [],
                "text": "Pharmacogenomics"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698666"
                        ],
                        "name": "Kaizhong Zhang",
                        "slug": "Kaizhong-Zhang",
                        "structuredName": {
                            "firstName": "Kaizhong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaizhong Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1821243"
                        ],
                        "name": "R. Statman",
                        "slug": "R.-Statman",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Statman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Statman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695878"
                        ],
                        "name": "D. Shasha",
                        "slug": "D.-Shasha",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "Shasha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Shasha"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 73
                            }
                        ],
                        "text": "It has also been shown \nthat if the trees are not ordered, the problem is NP\u00adcomplete [36]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 84
                            }
                        ],
                        "text": "It has also been shown that if the trees are not ordered, the problem is NPcomplete [36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16338028,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "909bd42ec0fcee624e0fbea868ceb8325bf6708b",
            "isKey": false,
            "numCitedBy": 352,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-Editing-Distance-Between-Unordered-Labeled-Zhang-Statman",
            "title": {
                "fragments": [],
                "text": "On the Editing Distance Between Unordered Labeled Trees"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Process. Lett."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71597938"
                        ],
                        "name": "Valiente Feruglio",
                        "slug": "Valiente-Feruglio",
                        "structuredName": {
                            "firstName": "Valiente",
                            "lastName": "Feruglio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Valiente Feruglio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117735545"
                        ],
                        "name": "Gabriel Alejandro",
                        "slug": "Gabriel-Alejandro",
                        "structuredName": {
                            "firstName": "Gabriel",
                            "lastName": "Alejandro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gabriel Alejandro"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 73
                            }
                        ],
                        "text": "1 Tree Edit Distance Similar to string edit distance, tree edit distance [31, 30] between two trees A and B (we are only interested in labeled ordered rooted trees) is the cost associated with the minimum set of operations needed to transform A into B."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 116147478,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0cc76496af5a889259867468ed41366b86a78b88",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The relationship between tree edit distance and maximum common \nsubtrees is established, showing that a tree edit distance constrained \nby insertion and deletions on leaves only and by a simple condition on \nthe cost of the tree edit operations, corresponds to a maximum commonsubtree isomorphism, allowing thus the use of known tree edit distance algorithms to solve the maximum common subtree problem, and viceversa. Further, a tree distance based on the size of a maximum common subtree is introduced."
            },
            "slug": "Tree-edit-distance-and-common-subtrees-Feruglio-Alejandro",
            "title": {
                "fragments": [],
                "text": "Tree edit distance and common subtrees"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The relationship between tree edit distance and maximum common subtrees is established, showing that a tree editdistance constrained by insertion and deletions on leaves only and by a simple condition on the cost of the tree edit operations corresponds to a maximum commonsubtree isomorphism."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37062236"
                        ],
                        "name": "Wuu Yang",
                        "slug": "Wuu-Yang",
                        "structuredName": {
                            "firstName": "Wuu",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wuu Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "We use an example from [35] to explain the algorithm (Figure 10)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "In this work, we use a restricted matching algorithm [35], which was first proposed to compare two computer programs in software engineering."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10853673,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "445e5f03b05a6febf9630bbb0e3f0c72e02ed403",
            "isKey": false,
            "numCitedBy": 411,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Programmers frequently face the need to identify the differences between two programs, usually two different versions of a program. Text\u2010based tools such as the UNIXr\u0300 utility diff often produce unsatisfactory comparisons because they cannot accurately pinpoint the differences and because they sometimes produce irrelevant differences. Since programs have a rigid syntactic structure as described by the grammar of the programming language in which they are written, we develop a comparison algorithm that exploits knowledge of the grammar. The algorithm, which is based on a dynamic programming scheme, can point out the differences between two programs more accurately than previous text comparison tools. Finally, the two programs are pretty\u2010printed \u2018synchronously\u2019 with the differences highlighted so that the differences are easily identified."
            },
            "slug": "Identifying-syntactic-differences-between-two-Yang",
            "title": {
                "fragments": [],
                "text": "Identifying syntactic differences between two programs"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "A comparison algorithm is developed that can point out the differences between two programs more accurately than previous text comparison tools and is based on a dynamic programming scheme."
            },
            "venue": {
                "fragments": [],
                "text": "Softw. Pract. Exp."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398035522"
                        ],
                        "name": "Ricardo Baeza-Yates",
                        "slug": "Ricardo-Baeza-Yates",
                        "structuredName": {
                            "firstName": "Ricardo",
                            "lastName": "Baeza-Yates",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ricardo Baeza-Yates"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16531971,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc2f8507f00a419aebe9d9ccb56a68919cc19b46",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "We survey several algorithms for searching a string in a piece of text. We include theoretical and empirical results, as well as the actual code of each algorithm. An extensive bibliography is also included."
            },
            "slug": "Algorithms-for-string-searching-Baeza-Yates",
            "title": {
                "fragments": [],
                "text": "Algorithms for string searching"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "This work surveys several algorithms for searching a string in a piece of text and includes theoretical and empirical results, as well as the actual code of each algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "SIGF"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 87
                            }
                        ],
                        "text": "The simple tree matching algorithm \n (B) N15 N16 N17 N18 N19  N20  N21 N12 N13 N14 N22 (C) M1-15 (D) W1-15 (E) M5-17 (F) W5-17 (G) \nM11-20 (H) W11-20  0 1 (N16) 2 (N16-N17) 0 0 0 0 1 (N2) 0 3 3 2 (N2-N3) 0 3 5 3 (N2-N4) 0 3 5 4 (N2-N5) \n0 3 6 1 (N16) 2 (N17) 1 (N2) 3 0 2 (N3) 0 2 3 (N4) 2 0 4 (N5) 0 3 0 1 (N20) 2 (N20-N21) 0 0 0 0 1 (N11) \n0 2 2 1(N20) 2(N21) 1 (N11) 2 0 0 1 (N22) 0 0 0 1 (N12) 0 1 2 (N12-N13) 0 1 3 (N12-N14) 0 1 1 (N22) \n1 (N12) 1 2 (N13) 0 3 (N14) 0 Figure 10."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 18
                            }
                        ],
                        "text": "For example, W1-15[4, 2] is computed recursively by building the matrices (E)-(H)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "M1-15 matrix is computed based on the W1-15 matrix, and each entry in W1-15, say W1-15[i, \nj], is the maximum matching between the ith and jth first\u00adlevel sub-trees of A and B, which is computed \nrecursively based on its M matrix."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 151
                            }
                        ],
                        "text": "1 The Basic Idea of MDR The MDR algorithm is based on two observations about data records in a Web page and an edit distance string matching algorithm [2] to find data records."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 24
                            }
                        ],
                        "text": "identical symbols, M1-15[4,2]+1 is returned as the maximum matching value between trees A and B (line 11)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 44
                            }
                        ],
                        "text": "Since N1 \nand N15 contain identical symbols, M1-15[4,2]+1 is returned as the maximum matching value between trees \nA and B (line 11)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 16
                            }
                        ],
                        "text": ", edit distance [2]) to compare different sub-strings to find those similar ones, which may represent similar data records."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 13
                            }
                        ],
                        "text": "For example, W1-15[4, 2] is computed recursively by building the matrices \n(E)-(H)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Algorithms for string matching: A survey"
            },
            "venue": {
                "fragments": [],
                "text": "ACM SIGIR Forum,"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11699416"
                        ],
                        "name": "Chaomei Chen",
                        "slug": "Chaomei-Chen",
                        "structuredName": {
                            "firstName": "Chaomei",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chaomei Chen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 90
                            }
                        ],
                        "text": "(ii) The proposed system identifies data records by analyzing HTML tag trees or DOM trees [7]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6787541,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2fb318e0c3e9c2b11e711d3182c76f3dd39d1322",
            "isKey": false,
            "numCitedBy": 616,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Mining-the-Web:-Discovering-knowledge-from-data-Chen",
            "title": {
                "fragments": [],
                "text": "Mining the Web: Discovering knowledge from hypertext data"
            },
            "venue": {
                "fragments": [],
                "text": "J. Assoc. Inf. Sci. Technol."
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Schema-Guided Wrapper Generator. ICDE-02"
            },
            "venue": {
                "fragments": [],
                "text": "Schema-Guided Wrapper Generator. ICDE-02"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Algorithms on strings, tree, and sequence"
            },
            "venue": {
                "fragments": [],
                "text": "Algorithms on strings, tree, and sequence"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Schema - Guided Wrapper Generator"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 17,
            "methodology": 17,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 39,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Web-data-extraction-based-on-partial-tree-alignment-Zhai-Liu/f49e35b6a85b8d81d2c9d9e26e8bf19dd94fad3a?sort=total-citations"
}