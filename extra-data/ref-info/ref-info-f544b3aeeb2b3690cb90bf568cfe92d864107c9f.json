{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145349582"
                        ],
                        "name": "Sanjiv Kumar",
                        "slug": "Sanjiv-Kumar",
                        "structuredName": {
                            "firstName": "Sanjiv",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sanjiv Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2261707"
                        ],
                        "name": "J. August",
                        "slug": "J.-August",
                        "structuredName": {
                            "firstName": "Jonas",
                            "lastName": "August",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. August"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 174
                            }
                        ],
                        "text": "One way to deal with this is to approximate the gradient by substituting approximate inference in place of exact inference (Wainwright et al., 2003; Sutton & McCallum, 2005; Kumar et al., 2005; Vishwanathan et al., 2006; Levin & Weiss, 2009; Pal et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9643537,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "939b4508f8646998aaf2b977666dfa27ecda1610",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Estimation of parameters of random field models from labeled training data is crucial for their good performance in many image analysis applications. In this paper, we present an approach for approximate maximum likelihood parameter learning in discriminative field models, which is based on approximating true expectations with simple piecewise constant functions constructed using inference techniques. Gradient ascent with these updates exhibits compelling limit cycle behavior which is tied closely to the number of errors made during inference. The performance of various approximations was evaluated with different inference techniques showing that the learned parameters lead to good classification performance so long as the method used for approximating the gradient is consistent with the inference mechanism. The proposed approach is general enough to be used for the training of, e.g., smoothing parameters of conventional Markov Random Fields (MRFs)."
            },
            "slug": "Exploiting-Inference-for-Approximate-Parameter-in-Kumar-August",
            "title": {
                "fragments": [],
                "text": "Exploiting Inference for Approximate Parameter Learning in Discriminative Fields: An Empirical Study"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An approach for approximate maximum likelihood parameter learning in discriminative field models, which is based on approximating true expectations with simple piecewise constant functions constructed using inference techniques is presented."
            },
            "venue": {
                "fragments": [],
                "text": "EMMCVPR"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1972076"
                        ],
                        "name": "C. Pal",
                        "slug": "C.-Pal",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Pal",
                            "middleNames": [
                                "Joseph"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Pal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2862313"
                        ],
                        "name": "J. Weinman",
                        "slug": "J.-Weinman",
                        "structuredName": {
                            "firstName": "Jerod",
                            "lastName": "Weinman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weinman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1859684"
                        ],
                        "name": "Lam C. Tran",
                        "slug": "Lam-C.-Tran",
                        "structuredName": {
                            "firstName": "Lam",
                            "lastName": "Tran",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lam C. Tran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709053"
                        ],
                        "name": "Daniel Scharstein",
                        "slug": "Daniel-Scharstein",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Scharstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Scharstein"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18355221,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03396c4c8240d7d79a343d8a5a15acf28a3aab98",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "Until recently, the lack of ground truth data has hindered the application of discriminative structured prediction techniques to the stereo problem. In this paper we use ground truth data sets that we have recently constructed to explore different model structures and parameter learning techniques. To estimate parameters in Markov random fields (MRFs) via maximum likelihood one usually needs to perform approximate probabilistic inference. Conditional random fields (CRFs) are discriminative versions of traditional MRFs. We explore a number of novel CRF model structures including a CRF for stereo matching with an explicit occlusion model. CRFs require expensive inference steps for each iteration of optimization and inference is particularly slow when there are many discrete states. We explore belief propagation, variational message passing and graph cuts as inference methods during learning and compare with learning via pseudolikelihood. To accelerate approximate inference we have developed a new method called sparse variational message passing which can reduce inference time by an order of magnitude with negligible loss in quality. Learning using sparse variational message passing improves upon previous approaches using graph cuts and allows efficient learning over large data sets when energy functions violate the constraints imposed by graph cuts."
            },
            "slug": "On-Learning-Conditional-Random-Fields-for-Stereo-Pal-Weinman",
            "title": {
                "fragments": [],
                "text": "On Learning Conditional Random Fields for Stereo"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A new method called sparse variational message passing is developed which can reduce inference time by an order of magnitude with negligible loss in quality and allows efficient learning over large data sets when energy functions violate the constraints imposed by graph cuts."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143729959"
                        ],
                        "name": "Vibhav Vineet",
                        "slug": "Vibhav-Vineet",
                        "structuredName": {
                            "firstName": "Vibhav",
                            "lastName": "Vineet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vibhav Vineet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734784"
                        ],
                        "name": "J. Warrell",
                        "slug": "J.-Warrell",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Warrell",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Warrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3274976"
                        ],
                        "name": "Paul Sturgess",
                        "slug": "Paul-Sturgess",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Sturgess",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul Sturgess"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143635540"
                        ],
                        "name": "Philip H. S. Torr",
                        "slug": "Philip-H.-S.-Torr",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Torr",
                            "middleNames": [
                                "H.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip H. S. Torr"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 122
                            }
                        ],
                        "text": "Note that we did not include any additional information, such as dense SiftFlow initialization or bounding box detection (Vineet et al., 2012a;b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 84
                            }
                        ],
                        "text": "Kra\u0308henbu\u0308hl and Koltun (2011) used grid search to estimate the kernel parameters and Vineet et al. (2012a) described a generative approach for this estimation task."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 134
                            }
                        ],
                        "text": "For example, parameters for the unary potentials and for the pairwise potentials are separately estimated (Kra\u0308henbu\u0308hl & Koltun, 2011; Vineet et al., 2012a;b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 0
                            }
                        ],
                        "text": "Vineet et al. (2012a; 2012b) have extended convolutional inference to more general Gaussian kernels and to models with higher-order terms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7636798,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "140dde4a16c69d10d8bffac0b5b03b332079fed1",
            "isKey": true,
            "numCitedBy": 19,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, Krahenbuhl and Koltun proposed an efficient inference method for densely connected pairwise random fields using the mean-field approximation for a Conditional Random Field (CRF). However, they restrict their pairwise weights to take the form of a weighted combination of Gaussian kernels where each Gaussian component is allowed to take only zero mean, and can only be rescaled by a single value for each label pair. Further, their method is sensitive to initialization. In this paper, we propose methods to alleviate these issues. First, we propose a hierarchical mean-field approach where labelling from the coarser level is propagated to the finer level for better initialisation. Further, we use SIFT-flow based label transfer to provide a good initial condition at the coarsest level. Second, we allow our approach to take general Gaussian pairwise weights, where we learn the mean, the co-variance matrix, and the mixing co-efficient for every mixture component. We propose a variation of Expectation Maximization (EM) for piecewise learning of the parameters of the mixture model determined by the maximum likelihood function. Finally, we demonstrate the efficiency and accuracy offered by our method for object class segmentation problems on two challenging datasets: PascalVOC-10 segmentation and CamVid datasets. We show that we are able to achieve state of the art performance on the CamVid dataset, and an almost 3% improvement on the PascalVOC10 dataset compared to baseline graph-cut and mean-field methods, while also reducing the inference time by almost a factor of 3 compared to graph-cuts based methods."
            },
            "slug": "Improved-Initialization-and-Gaussian-Mixture-Terms-Vineet-Warrell",
            "title": {
                "fragments": [],
                "text": "Improved Initialization and Gaussian Mixture Pairwise Terms for Dense Random Fields with Mean-field Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A hierarchical mean-field approach where labelling from the coarser level is propagated to the finer level for better initialisation and a variation of Expectation Maximization (EM) for piecewise learning of the parameters of the mixture model determined by the maximum likelihood function is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2562966"
                        ],
                        "name": "Philipp Kr\u00e4henb\u00fchl",
                        "slug": "Philipp-Kr\u00e4henb\u00fchl",
                        "structuredName": {
                            "firstName": "Philipp",
                            "lastName": "Kr\u00e4henb\u00fchl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philipp Kr\u00e4henb\u00fchl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145231047"
                        ],
                        "name": "V. Koltun",
                        "slug": "V.-Koltun",
                        "structuredName": {
                            "firstName": "Vladlen",
                            "lastName": "Koltun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Koltun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5574079,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c81c20109c809cfc47565a9477c04ee005d424bf",
            "isKey": false,
            "numCitedBy": 2641,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Most state-of-the-art techniques for multi-class image segmentation and labeling use conditional random fields defined over pixels or image regions. While region-level models often feature dense pairwise connectivity, pixel-level models are considerably larger and have only permitted sparse graph structures. In this paper, we consider fully connected CRF models defined on the complete set of pixels in an image. The resulting graphs have billions of edges, making traditional inference algorithms impractical. Our main contribution is a highly efficient approximate inference algorithm for fully connected CRF models in which the pairwise edge potentials are defined by a linear combination of Gaussian kernels. Our experiments demonstrate that dense connectivity at the pixel level substantially improves segmentation and labeling accuracy."
            },
            "slug": "Efficient-Inference-in-Fully-Connected-CRFs-with-Kr\u00e4henb\u00fchl-Koltun",
            "title": {
                "fragments": [],
                "text": "Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper considers fully connected CRF models defined on the complete set of pixels in an image and proposes a highly efficient approximate inference algorithm in which the pairwise edge potentials are defined by a linear combination of Gaussian kernels."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802944"
                        ],
                        "name": "M. Tappen",
                        "slug": "M.-Tappen",
                        "structuredName": {
                            "firstName": "Marshall",
                            "lastName": "Tappen",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Tappen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2468599,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "079f2fb8a288100c574e59c2756a69fc743762f2",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Markov random field, or MRF, models are a powerful tool for modeling images. While much progress has been made in algorithms for inference in MRFs, learning the parameters of an MRF is still a challenging problem. In this paper, we show how variational optimization can be used to learn the parameters of an MRF. This method for learning, which we refer to as variational mode learning, finds the MRF parameters by minimizing a loss function that penalizes the difference between ground-truth images and an approximate, variational solution to the MRF. In particular, we focus on learning parameters for the field of experts model of Roth and Black. In addition to demonstrating the effectiveness of this method, we show that a model based on derivative filters performs quite similarly to the field of experts model. This suggests that the field of experts model, which is difficult to interpret, can be understood as imposing piecewise continuity on the image."
            },
            "slug": "Utilizing-Variational-Optimization-to-Learn-Markov-Tappen",
            "title": {
                "fragments": [],
                "text": "Utilizing Variational Optimization to Learn Markov Random Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper shows how variational optimization can be used to learn the parameters of an MRF, and suggests that the field of experts model, which is difficult to interpret, can be understood as imposing piecewise continuity on the image."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145920814"
                        ],
                        "name": "S. Roth",
                        "slug": "S.-Roth",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13058320,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dfcae80f4d34ac09ba8063c5cfb5be954d0bf5f1",
            "isKey": false,
            "numCitedBy": 704,
            "numCiting": 109,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a framework for learning generic, expressive image priors that capture the statistics of natural scenes and can be used for a variety of machine vision tasks. The approach provides a practical method for learning high-order Markov random field (MRF) models with potential functions that extend over large pixel neighborhoods. These clique potentials are modeled using the Product-of-Experts framework that uses non-linear functions of many linear filter responses. In contrast to previous MRF approaches all parameters, including the linear filters themselves, are learned from training data. We demonstrate the capabilities of this Field-of-Experts model with two example applications, image denoising and image inpainting, which are implemented using a simple, approximate inference scheme. While the model is trained on a generic image database and is not tuned toward a specific application, we obtain results that compete with specialized techniques."
            },
            "slug": "Fields-of-Experts-Roth-Black",
            "title": {
                "fragments": [],
                "text": "Fields of Experts"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The approach provides a practical method for learning high-order Markov random field models with potential functions that extend over large pixel neighborhoods with non-linear functions of many linear filter responses."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37210858"
                        ],
                        "name": "Charles Sutton",
                        "slug": "Charles-Sutton",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Sutton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1549479,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "beb0766d6233836ac1203b224930dc037e2b1dff",
            "isKey": false,
            "numCitedBy": 197,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "For many large undirected models that arise in real-world applications, exact maximum-likelihood training is intractable, because it requires computing marginal distributions of the model. Conditional training is even more difficult, because the partition function depends not only on the parameters, but also on the observed input, requiring repeated inference over each training example. An appealing idea for such models is to independently train a local undirected classifier over each clique, afterwards combining the learned weights into a single global model. In this paper, we show that this piecewise method can be justified as minimizing a new family of upper bounds on the log partition function. On three natural-language data sets, piecewise training is more accurate than pseudolikelihood, and often performs comparably to global training using belief propagation."
            },
            "slug": "Piecewise-Training-for-Undirected-Models-Sutton-McCallum",
            "title": {
                "fragments": [],
                "text": "Piecewise Training for Undirected Models"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper shows that this piecewise method can be justified as minimizing a new family of upper bounds on the log partition function, and on three natural-language data sets, piecewise training is more accurate than pseudolikelihood, and often performs comparably to global training using belief propagation."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709053"
                        ],
                        "name": "Daniel Scharstein",
                        "slug": "Daniel-Scharstein",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Scharstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Scharstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1972076"
                        ],
                        "name": "C. Pal",
                        "slug": "C.-Pal",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Pal",
                            "middleNames": [
                                "Joseph"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Pal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 258,
                                "start": 242
                            }
                        ],
                        "text": "One way to deal with this is to approximate the gradient by substituting approximate inference in place of exact inference (Wainwright et al., 2003; Sutton & McCallum, 2005; Kumar et al., 2005; Vishwanathan et al., 2006; Levin & Weiss, 2009; Pal et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13154466,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8c36b842780c81333cbbb87628b49f9911a5936b",
            "isKey": false,
            "numCitedBy": 818,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "State-of-the-art stereo vision algorithms utilize color changes as important cues for object boundaries. Most methods impose heuristic restrictions or priors on disparities, for example by modulating local smoothness costs with intensity gradients. In this paper we seek to replace such heuristics with explicit probabilistic models of disparities and intensities learned from real images. We have constructed a large number of stereo datasets with ground-truth disparities, and we use a subset of these datasets to learn the parameters of conditional random fields (CRFs). We present experimental results illustrating the potential of our approach for automatically learning the parameters of models with richer structure than standard hand-tuned MRF models."
            },
            "slug": "Learning-Conditional-Random-Fields-for-Stereo-Scharstein-Pal",
            "title": {
                "fragments": [],
                "text": "Learning Conditional Random Fields for Stereo"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper has constructed a large number of stereo datasets with ground-truth disparities, and a subset of these datasets are used to learn the parameters of conditional random fields (CRFs) and presents experimental results illustrating the potential of this approach for automatically learning the Parameters of models with richer structure than standard hand-tuned MRF models."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145500336"
                        ],
                        "name": "Alex Kulesza",
                        "slug": "Alex-Kulesza",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Kulesza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Kulesza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10876177,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "325ea1f2022ee3886a5810df76dcfbe4010ad439",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "In many structured prediction problems, the highest-scoring labeling is hard to compute exactly, leading to the use of approximate inference methods. However, when inference is used in a learning algorithm, a good approximation of the score may not be sufficient. We show in particular that learning can fail even with an approximate inference method with rigorous approximation guarantees. There are two reasons for this. First, approximate methods can effectively reduce the expressivity of an underlying model by making it impossible to choose parameters that reliably give good predictions. Second, approximations can respond to parameter changes in such a way that standard learning algorithms are misled. In contrast, we give two positive results in the form of learning bounds for the use of LP-relaxed inference in structured perceptron and empirical risk minimization settings. We argue that without understanding combinations of inference and learning, such as these, that are appropriately compatible, learning performance under approximate inference cannot be guaranteed."
            },
            "slug": "Structured-Learning-with-Approximate-Inference-Kulesza-Pereira",
            "title": {
                "fragments": [],
                "text": "Structured Learning with Approximate Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is shown in particular that learning can fail even with an approximate inference method with rigorous approximation guarantees, and argued that without understanding combinations of inference and learning, such as these that are appropriately compatible, learning performance under approximate inference cannot be guaranteed."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143729959"
                        ],
                        "name": "Vibhav Vineet",
                        "slug": "Vibhav-Vineet",
                        "structuredName": {
                            "firstName": "Vibhav",
                            "lastName": "Vineet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vibhav Vineet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734784"
                        ],
                        "name": "J. Warrell",
                        "slug": "J.-Warrell",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Warrell",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Warrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143635540"
                        ],
                        "name": "Philip H. S. Torr",
                        "slug": "Philip-H.-S.-Torr",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Torr",
                            "middleNames": [
                                "H.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip H. S. Torr"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 122
                            }
                        ],
                        "text": "Note that we did not include any additional information, such as dense SiftFlow initialization or bounding box detection (Vineet et al., 2012a;b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 84
                            }
                        ],
                        "text": "Kra\u0308henbu\u0308hl and Koltun (2011) used grid search to estimate the kernel parameters and Vineet et al. (2012a) described a generative approach for this estimation task."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 134
                            }
                        ],
                        "text": "For example, parameters for the unary potentials and for the pairwise potentials are separately estimated (Kra\u0308henbu\u0308hl & Koltun, 2011; Vineet et al., 2012a;b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 0
                            }
                        ],
                        "text": "Vineet et al. (2012a; 2012b) have extended convolutional inference to more general Gaussian kernels and to models with higher-order terms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10712991,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9247d8bc88746596e71dc56b7359cb08e03032f1",
            "isKey": true,
            "numCitedBy": 113,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, a number of cross bilateral filtering methods have been proposed for solving multi-label problems in computer vision, such as stereo, optical flow and object class segmentation that show an order of magnitude improvement in speed over previous methods. These methods have achieved good results despite using models with only unary and/or pairwise terms. However, previous work has shown the value of using models with higher-order terms e.g. to represent label consistency over large regions, or global co-occurrence relations. We show how these higher-order terms can be formulated such that filter-based inference remains possible. We demonstrate our techniques on joint stereo and object labelling problems, as well as object class segmentation, showing in addition for joint object-stereo labelling how our method provides an efficient approach to inference in product label-spaces. We show that we are able to speed up inference in these models around 10\u201330 times with respect to competing graph-cut/move-making methods, as well as maintaining or improving accuracy in all cases. We show results on PascalVOC-10 for object class segmentation, and Leuven for joint object-stereo labelling."
            },
            "slug": "Filter-Based-Mean-Field-Inference-for-Random-Fields-Vineet-Warrell",
            "title": {
                "fragments": [],
                "text": "Filter-Based Mean-Field Inference for Random Fields with Higher-Order Terms and Product Label-Spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The techniques are demonstrated on joint stereo and object labelling problems, as well as object class segmentation, showing in addition for joint object-stereo labelling how the method provides an efficient approach to inference in product label-spaces."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33913193"
                        ],
                        "name": "Xuming He",
                        "slug": "Xuming-He",
                        "structuredName": {
                            "firstName": "Xuming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400347470"
                        ],
                        "name": "M. A. Carreira-Perpi\u00f1\u00e1n",
                        "slug": "M.-A.-Carreira-Perpi\u00f1\u00e1n",
                        "structuredName": {
                            "firstName": "Miguel",
                            "lastName": "Carreira-Perpi\u00f1\u00e1n",
                            "middleNames": [
                                "\u00c1."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. A. Carreira-Perpi\u00f1\u00e1n"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 109
                            }
                        ],
                        "text": "Models with hierarchical structure and higher-order potentials have been proposed to reduce these artifacts (He et al., 2004; Roth & Black, 2009; Kohli et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11859305,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "363b56f85e12389017ba8894056a1b309e46a5f7",
            "isKey": false,
            "numCitedBy": 933,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an approach to include contextual features for labeling images, in which each pixel is assigned to one of a finite set of labels. The features are incorporated into a probabilistic framework, which combines the outputs of several components. Components differ in the information they encode. Some focus on the image-label mapping, while others focus solely on patterns within the label field. Components also differ in their scale, as some focus on fine-resolution patterns while others on coarser, more global structure. A supervised version of the contrastive divergence algorithm is applied to learn these features from labeled image data. We demonstrate performance on two real-world image databases and compare it to a classifier and a Markov random field."
            },
            "slug": "Multiscale-conditional-random-fields-for-image-He-Zemel",
            "title": {
                "fragments": [],
                "text": "Multiscale conditional random fields for image labeling"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "An approach to include contextual features for labeling images, in which each pixel is assigned to one of a finite set of labels, are incorporated into a probabilistic framework, which combines the outputs of several components."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721860"
                        ],
                        "name": "M. Wainwright",
                        "slug": "M.-Wainwright",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Wainwright",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wainwright"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701607"
                        ],
                        "name": "A. Willsky",
                        "slug": "A.-Willsky",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Willsky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Willsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 124
                            }
                        ],
                        "text": "One way to deal with this is to approximate the gradient by substituting approximate inference in place of exact inference (Wainwright et al., 2003; Sutton & McCallum, 2005; Kumar et al., 2005; Vishwanathan et al., 2006; Levin & Weiss, 2009; Pal et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15761929,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44bed05c75b761e1ad2a93c0be35a2acb4deb5b9",
            "isKey": false,
            "numCitedBy": 229,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In previous work [10], we presented a class of upper bounds on the log partition function of an arbitrary undirected graphical model based on solving a convex variational problem. Here we develop a class of local message-passing algorithms, which we call tree-reweighted belief propagation, for efficiently computing the value of these upper bounds, as well as the associated pseudomarginals. We also consider the uses of our bounds for the problem of maximum likelihood (ML) parameter estimation. For a completely observed model, our analysis gives rise to a concave lower bound on the log likelihood of the data. Maximizing this lower bound yields an approximate ML estimate which, in analogy to the moment-matching of exact ML estimation, can be interpreted in terms of pseudo-moment-matching. We present preliminary results illustrating the behavior of this approximate ML estimator."
            },
            "slug": "Tree-reweighted-belief-propagation-algorithms-and-Wainwright-Jaakkola",
            "title": {
                "fragments": [],
                "text": "Tree-reweighted belief propagation algorithms and approximate ML estimation by pseudo-moment matching"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A class of local message-passing algorithms is developed, which is called tree-reweighted belief propagation, for efficiently computing the value of these upper bounds on the log partition function of an arbitrary undirected graphical model, as well as the associated pseudomarginals."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143967473"
                        ],
                        "name": "Pushmeet Kohli",
                        "slug": "Pushmeet-Kohli",
                        "structuredName": {
                            "firstName": "Pushmeet",
                            "lastName": "Kohli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pushmeet Kohli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728641"
                        ],
                        "name": "Lubor Ladicky",
                        "slug": "Lubor-Ladicky",
                        "structuredName": {
                            "firstName": "Lubor",
                            "lastName": "Ladicky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lubor Ladicky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143635540"
                        ],
                        "name": "Philip H. S. Torr",
                        "slug": "Philip-H.-S.-Torr",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Torr",
                            "middleNames": [
                                "H.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip H. S. Torr"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 146
                            }
                        ],
                        "text": "Models with hierarchical structure and higher-order potentials have been proposed to reduce these artifacts (He et al., 2004; Roth & Black, 2009; Kohli et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 690715,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "87073fd45685b78cb5a68e5eae331d88f2a2be63",
            "isKey": false,
            "numCitedBy": 1008,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a novel framework for labelling problems which is able to combine multiple segmentations in a principled manner. Our method is based on higher order conditional random fields and uses potentials defined on sets of pixels (image segments) generated using unsupervised segmentation algorithms. These potentials enforce label consistency in image regions and can be seen as a generalization of the commonly used pairwise contrast sensitive smoothness potentials. The higher order potential functions used in our framework take the form of the Robust Pn model and are more general than the Pn Potts model recently proposed by Kohli et al. We prove that the optimal swap and expansion moves for energy functions composed of these potentials can be computed by solving a st-mincut problem. This enables the use of powerful graph cut based move making algorithms for performing inference in the framework. We test our method on the problem of multi-class object segmentation by augmenting the conventional crf used for object segmentation with higher order potentials defined on image regions. Experiments on challenging data sets show that integration of higher order potentials quantitatively and qualitatively improves results leading to much better definition of object boundaries. We believe that this method can be used to yield similar improvements for many other labelling problems."
            },
            "slug": "Robust-Higher-Order-Potentials-for-Enforcing-Label-Kohli-Ladicky",
            "title": {
                "fragments": [],
                "text": "Robust Higher Order Potentials for Enforcing Label Consistency"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "This paper proposes a novel framework for labelling problems which is able to combine multiple segmentations in a principled manner based on higher order conditional random fields and uses potentials defined on sets of pixels generated using unsupervised segmentation algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145162067"
                        ],
                        "name": "A. Blake",
                        "slug": "A.-Blake",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143967473"
                        ],
                        "name": "Pushmeet Kohli",
                        "slug": "Pushmeet-Kohli",
                        "structuredName": {
                            "firstName": "Pushmeet",
                            "lastName": "Kohli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pushmeet Kohli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756036"
                        ],
                        "name": "C. Rother",
                        "slug": "C.-Rother",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Rother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rother"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 143
                            }
                        ],
                        "text": "Random field models are often used to express image coherence priors in computer vision applications, such as segmentation and reconstruction (Blake et al., 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59627074,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "22352ffef6a995aa0b265ba49a92832df09b0734",
            "isKey": false,
            "numCitedBy": 260,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This volume demonstrates the power of the Markov random field (MRF) in vision, treating the MRF both as a tool for modeling image data and, utilizing recently developed algorithms, as a means of making inferences about images. These inferences concern underlying image and scene structure as well as solutions to such problems as image reconstruction, image segmentation, 3D vision, and object labeling. It offers key findings and state-of-the-art research on both algorithms and applications. After an introduction to the fundamental concepts used in MRFs, the book reviews some of the main algorithms for performing inference with MRFs; presents successful applications of MRFs, including segmentation, super-resolution, and image restoration, along with a comparison of various optimization methods; discusses advanced algorithmic topics; addresses limitations of the strong locality assumptions in the MRFs discussed in earlier chapters; and showcases applications that use MRFs in more complex ways, as components in bigger systems or with multiterm energy functions. The book will be an essential guide to current research on these powerful mathematical tools."
            },
            "slug": "Markov-Random-Fields-for-Vision-and-Image-Blake-Kohli",
            "title": {
                "fragments": [],
                "text": "Markov Random Fields for Vision and Image Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "This volume demonstrates the power of the Markov random field in vision, treating the MRF both as a tool for modeling image data and, utilizing recently developed algorithms, as a means of making inferences about images."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721860"
                        ],
                        "name": "M. Wainwright",
                        "slug": "M.-Wainwright",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Wainwright",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wainwright"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123333909"
                        ],
                        "name": "M.I. Jordan",
                        "slug": "M.I.-Jordan",
                        "structuredName": {
                            "firstName": "M.I.",
                            "lastName": "Jordan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M.I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207178945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d98d0d1900b13b87aa4ffd6b69c046beb63f0434",
            "isKey": false,
            "numCitedBy": 3901,
            "numCiting": 303,
            "paperAbstract": {
                "fragments": [],
                "text": "The formalism of probabilistic graphical models provides a unifying framework for capturing complex dependencies among random variables, and building large-scale multivariate statistical models. Graphical models have become a focus of research in many statistical, computational and mathematical fields, including bioinformatics, communication theory, statistical physics, combinatorial optimization, signal and image processing, information retrieval and statistical machine learning. Many problems that arise in specific instances \u2014 including the key problems of computing marginals and modes of probability distributions \u2014 are best studied in the general setting. Working with exponential family representations, and exploiting the conjugate duality between the cumulant function and the entropy for exponential families, we develop general variational representations of the problems of computing likelihoods, marginal probabilities and most probable configurations. We describe how a wide variety of algorithms \u2014 among them sum-product, cluster variational methods, expectation-propagation, mean field methods, max-product and linear programming relaxation, as well as conic programming relaxations \u2014 can all be understood in terms of exact or approximate forms of these variational representations. The variational approach provides a complementary alternative to Markov chain Monte Carlo as a general source of approximation methods for inference in large-scale statistical models."
            },
            "slug": "Graphical-Models,-Exponential-Families,-and-Wainwright-Jordan",
            "title": {
                "fragments": [],
                "text": "Graphical Models, Exponential Families, and Variational Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The variational approach provides a complementary alternative to Markov chain Monte Carlo as a general source of approximation methods for inference in large-scale statistical models."
            },
            "venue": {
                "fragments": [],
                "text": "Found. Trends Mach. Learn."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721860"
                        ],
                        "name": "M. Wainwright",
                        "slug": "M.-Wainwright",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Wainwright",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wainwright"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16451367,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f3d831a9447ae7a8f9aa5c5beef8a28dbfacf352",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Consider the problem of joint parameter estimation and prediction in a Markov random field: that is, the model parameters are estimated on the basis of an initial set of data, and then the fitted model is used to perform prediction (e.g., smoothing, denoising, interpolation) on a new noisy observation. Working under the restriction of limited computation, we analyze a joint method in which the same convex variational relaxation is used to construct an M-estimator for fitting parameters, and to perform approximate marginalization for the prediction step. The key result of this paper is that in the computation-limited setting, using an inconsistent parameter estimator (i.e., an estimator that returns the \"wrong\" model even in the infinite data limit) is provably beneficial, since the resulting errors can partially compensate for errors made by using an approximate prediction technique. En route to this result, we analyze the asymptotic properties of M-estimators based on convex variational relaxations, and establish a Lipschitz stability property that holds for a broad class of convex variational methods. This stability result provides additional incentive, apart from the obvious benefit of unique global optima, for using message-passing methods based on convex variational relaxations. We show that joint estimation/prediction based on the reweighted sum-product algorithm substantially outperforms a commonly used heuristic based on ordinary sum-product."
            },
            "slug": "Estimating-the-\"Wrong\"-Graphical-Model:-Benefits-in-Wainwright",
            "title": {
                "fragments": [],
                "text": "Estimating the \"Wrong\" Graphical Model: Benefits in the Computation-Limited Setting"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The key result of this paper is that in the computation-limited setting, using an inconsistent parameter estimator is provably beneficial, since the resulting errors can partially compensate for errors made by using an approximate prediction technique."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145713874"
                        ],
                        "name": "S. Vishwanathan",
                        "slug": "S.-Vishwanathan",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Vishwanathan",
                            "middleNames": [
                                "V.",
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vishwanathan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739396"
                        ],
                        "name": "N. Schraudolph",
                        "slug": "N.-Schraudolph",
                        "structuredName": {
                            "firstName": "Nicol",
                            "lastName": "Schraudolph",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Schraudolph"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145610994"
                        ],
                        "name": "Mark W. Schmidt",
                        "slug": "Mark-W.-Schmidt",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Schmidt",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark W. Schmidt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056417995"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 194
                            }
                        ],
                        "text": "One way to deal with this is to approximate the gradient by substituting approximate inference in place of exact inference (Wainwright et al., 2003; Sutton & McCallum, 2005; Kumar et al., 2005; Vishwanathan et al., 2006; Levin & Weiss, 2009; Pal et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1978101,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "950e8d556e30d2a873172bfa90f4f36da5286c07",
            "isKey": false,
            "numCitedBy": 358,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We apply Stochastic Meta-Descent (SMD), a stochastic gradient optimization method with gain vector adaptation, to the training of Conditional Random Fields (CRFs). On several large data sets, the resulting optimizer converges to the same quality of solution over an order of magnitude faster than limited-memory BFGS, the leading method reported to date. We report results for both exact and inexact inference techniques."
            },
            "slug": "Accelerated-training-of-conditional-random-fields-Vishwanathan-Schraudolph",
            "title": {
                "fragments": [],
                "text": "Accelerated training of conditional random fields with stochastic gradient methods"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "Stochastic Meta-Descent (SMD), a stochastic gradient optimization method with gain vector adaptation, is applied to the training of Conditional Random Fields (CRFs) and the resulting optimizer converges to the same quality of solution over an order of magnitude faster than limited-memory BFGS."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50785579"
                        ],
                        "name": "N. Friedman",
                        "slug": "N.-Friedman",
                        "structuredName": {
                            "firstName": "Nir",
                            "lastName": "Friedman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14995779,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0a9b181fc252108de45720d4645ac245e1ba463",
            "isKey": false,
            "numCitedBy": 6021,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Most tasks require a person or an automated system to reason -- to reach conclusions based on available information. The framework of probabilistic graphical models, presented in this book, provides a general approach for this task. The approach is model-based, allowing interpretable models to be constructed and then manipulated by reasoning algorithms. These models can also be learned automatically from data, allowing the approach to be used in cases where manually constructing a model is difficult or even impossible. Because uncertainty is an inescapable aspect of most real-world applications, the book focuses on probabilistic models, which make the uncertainty explicit and provide models that are more faithful to reality. Probabilistic Graphical Models discusses a variety of models, spanning Bayesian networks, undirected Markov networks, discrete and continuous models, and extensions to deal with dynamical systems and relational data. For each class of models, the text describes the three fundamental cornerstones: representation, inference, and learning, presenting both basic concepts and advanced techniques. Finally, the book considers the use of the proposed framework for causal reasoning and decision making under uncertainty. The main text in each chapter provides the detailed technical development of the key ideas. Most chapters also include boxes with additional material: skill boxes, which describe techniques; case study boxes, which discuss empirical cases related to the approach described in the text, including applications in computer vision, robotics, natural language understanding, and computational biology; and concept boxes, which present significant concepts drawn from the material in the chapter. Instructors (and readers) can group chapters in various combinations, from core topics to more technically advanced material, to suit their particular needs."
            },
            "slug": "Probabilistic-Graphical-Models-Principles-and-Koller-Friedman",
            "title": {
                "fragments": [],
                "text": "Probabilistic Graphical Models - Principles and Techniques"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The framework of probabilistic graphical models, presented in this book, provides a general approach for causal reasoning and decision making under uncertainty, allowing interpretable models to be constructed and then manipulated by reasoning algorithms."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3313411"
                        ],
                        "name": "Bharath K. Sriperumbudur",
                        "slug": "Bharath-K.-Sriperumbudur",
                        "structuredName": {
                            "firstName": "Bharath",
                            "lastName": "Sriperumbudur",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bharath K. Sriperumbudur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725533"
                        ],
                        "name": "G. Lanckriet",
                        "slug": "G.-Lanckriet",
                        "structuredName": {
                            "firstName": "Gert",
                            "lastName": "Lanckriet",
                            "middleNames": [
                                "R.",
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lanckriet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10354732,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8aabf5ba342d37edc1ebd89ba898c70b0a4ca121",
            "isKey": false,
            "numCitedBy": 393,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "The concave-convex procedure (CCCP) is a majorization-minimization algorithm that solves d.c. (difference of convex functions) programs as a sequence of convex programs. In machine learning, CCCP is extensively used in many learning algorithms like sparse support vector machines (SVMs), transductive SVMs, sparse principal component analysis, etc. Though widely used in many applications, the convergence behavior of CCCP has not gotten a lot of specific attention. Yuille and Rangarajan analyzed its convergence in their original paper, however, we believe the analysis is not complete. Although the convergence of CCCP can be derived from the convergence of the d.c. algorithm (DCA), its proof is more specialized and technical than actually required for the specific case of CCCP. In this paper, we follow a different reasoning and show how Zangwill's global convergence theory of iterative algorithms provides a natural framework to prove the convergence of CCCP, allowing a more elegant and simple proof. This underlines Zangwill's theory as a powerful and general framework to deal with the convergence issues of iterative algorithms, after also being used to prove the convergence of algorithms like expectation-maximization, generalized alternating minimization, etc. In this paper, we provide a rigorous analysis of the convergence of CCCP by addressing these questions: (i) When does CCCP find a local minimum or a stationary point of the d.c. program under consideration? (ii) When does the sequence generated by CCCP converge? We also present an open problem on the issue of local convergence of CCCP."
            },
            "slug": "On-the-Convergence-of-the-Concave-Convex-Procedure-Sriperumbudur-Lanckriet",
            "title": {
                "fragments": [],
                "text": "On the Convergence of the Concave-Convex Procedure"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown how Zangwill's global convergence theory of iterative algorithms provides a natural framework to prove the convergence of CCCP, allowing a more elegant and simple proof."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3295092"
                        ],
                        "name": "S. Chopra",
                        "slug": "S.-Chopra",
                        "structuredName": {
                            "firstName": "Sumit",
                            "lastName": "Chopra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chopra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2315504"
                        ],
                        "name": "R. Hadsell",
                        "slug": "R.-Hadsell",
                        "structuredName": {
                            "firstName": "Raia",
                            "lastName": "Hadsell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hadsell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70478759"
                        ],
                        "name": "Aurelio Ranzato",
                        "slug": "Aurelio-Ranzato",
                        "structuredName": {
                            "firstName": "Aurelio",
                            "lastName": "Ranzato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aurelio Ranzato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13919023"
                        ],
                        "name": "F. Huang",
                        "slug": "F.-Huang",
                        "structuredName": {
                            "firstName": "Fu",
                            "lastName": "Huang",
                            "middleNames": [
                                "Jie"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 122
                            }
                        ],
                        "text": "Algorithms for parameter estimation in random field models minimize a loss function, such as the negative log-likelihood (LeCun et al., 2006; Koller & Friedman, 2009; Tappen, 2011)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8531544,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7fc604e1a3e45cd2d2742f96d62741930a363efa",
            "isKey": false,
            "numCitedBy": 794,
            "numCiting": 107,
            "paperAbstract": {
                "fragments": [],
                "text": "Energy-Based Models (EBMs) capture dependencies between variables by associating a scalar energy to each configuration of the variab les. Inference consists in clamping the value of observed variables and finding config urations of the remaining variables that minimize the energy. Learning consists in finding an energy function in which observed configurations of the variables a re given lower energies than unobserved ones. The EBM approach provides a common theoretical framework for many learning models, including traditional discr iminative and generative approaches, as well as graph-transformer networks, co nditional random fields, maximum margin Markov networks, and several manifold learning methods. Probabilistic models must be properly normalized, which sometimes requires evaluating intractable integrals over the space of all poss ible variable configurations. Since EBMs have no requirement for proper normalization, this problem is naturally circumvented. EBMs can be viewed as a form of non-probabilistic factor graphs, and they provide considerably more flexibility in th e design of architectures and training criteria than probabilistic approaches ."
            },
            "slug": "A-Tutorial-on-Energy-Based-Learning-LeCun-Chopra",
            "title": {
                "fragments": [],
                "text": "A Tutorial on Energy-Based Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The EBM approach provides a common theoretical framework for many learning models, including traditional discr iminative and generative approaches, as well as graph-transformer networks, co nditional random fields, maximum margin Markov networks, and several manifold learning methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143774737"
                        ],
                        "name": "J. Shotton",
                        "slug": "J.-Shotton",
                        "structuredName": {
                            "firstName": "Jamie",
                            "lastName": "Shotton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shotton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756036"
                        ],
                        "name": "C. Rother",
                        "slug": "C.-Rother",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Rother",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rother"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716777"
                        ],
                        "name": "A. Criminisi",
                        "slug": "A.-Criminisi",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Criminisi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Criminisi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 77
                            }
                        ],
                        "text": "(In our experiments, hi is the result of a TextonBoost classifier at pixel i (Shotton et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 78
                            }
                        ],
                        "text": "(In our experiments, hi is the result of a TextonBoost classifier at pixel i (Shotton et al., 2009).)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 242941,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d1bfcdfc90e662defd26b8b0deae6ef6e661b23",
            "isKey": false,
            "numCitedBy": 1084,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\nThis paper details a new approach for learning a discriminative model of object classes, incorporating texture, layout, and context information efficiently. The learned model is used for automatic visual understanding and semantic segmentation of photographs. Our discriminative model exploits texture-layout filters, novel features based on textons, which jointly model patterns of texture and their spatial layout. Unary classification and feature selection is achieved using shared boosting to give an efficient classifier which can be applied to a large number of classes. Accurate image segmentation is achieved by incorporating the unary classifier in a conditional random field, which (i) captures the spatial interactions between class labels of neighboring pixels, and (ii) improves the segmentation of specific object instances. Efficient training of the model on large datasets is achieved by exploiting both random feature selection and piecewise training methods.\n\nHigh classification and segmentation accuracy is demonstrated on four varied databases: (i) the MSRC 21-class database containing photographs of real objects viewed under general lighting conditions, poses and viewpoints, (ii) the 7-class Corel subset and (iii) the 7-class Sowerby database used in He et\u00a0al. (Proceeding of IEEE Conference on Computer Vision and Pattern Recognition, vol.\u00a02, pp.\u00a0695\u2013702, June 2004), and (iv) a set of video sequences of television shows. The proposed algorithm gives competitive and visually pleasing results for objects that are highly textured (grass, trees, etc.), highly structured (cars, faces, bicycles, airplanes, etc.), and even articulated (body, cow, etc.).\n"
            },
            "slug": "TextonBoost-for-Image-Understanding:-Multi-Class-by-Shotton-Winn",
            "title": {
                "fragments": [],
                "text": "TextonBoost for Image Understanding: Multi-Class Object Recognition and Segmentation by Jointly Modeling Texture, Layout, and Context"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A new approach for learning a discriminative model of object classes, incorporating texture, layout, and context information efficiently, which gives competitive and visually pleasing results for objects that are highly textured, highly structured, and even articulated."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722101"
                        ],
                        "name": "Justin Domke",
                        "slug": "Justin-Domke",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Domke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Justin Domke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10880491,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e2b6ecae2b8890d6d798dbee589777cf8ccb2c76",
            "isKey": false,
            "numCitedBy": 181,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Likelihood-based learning of graphical models faces challenges of computational complexity and robustness to model misspecification. This paper studies methods that fit parameters directly to maximize a measure of the accuracy of predicted marginals, taking into account both model and inference approximations at training time. Experiments on imaging problems suggest marginalization-based learning performs better than likelihood-based approximations on difficult problems where the model being fit is approximate in nature."
            },
            "slug": "Learning-Graphical-Model-Parameters-with-Marginal-Domke",
            "title": {
                "fragments": [],
                "text": "Learning Graphical Model Parameters with Approximate Marginal Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Methods that fit parameters directly to maximize a measure of the accuracy of predicted marginals, taking into account both model and inference approximations at training time are studied."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145257017"
                        ],
                        "name": "Anand Rangarajan",
                        "slug": "Anand-Rangarajan",
                        "structuredName": {
                            "firstName": "Anand",
                            "lastName": "Rangarajan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anand Rangarajan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 74
                            }
                        ],
                        "text": "We use the concave-convex procedure (CCCP) to minimize the KL divergence (Yuille & Rangarajan, 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6538459,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f0bf18fc90674fbef616e8c54bdefb3adc237e4d",
            "isKey": false,
            "numCitedBy": 437,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce the Concave-Convex procedure (CCCP) which constructs discrete time iterative dynamical systems which are guaranteed to monotonically decrease global optimization/energy functions. It can be applied to (almost) any optimization problem and many existing algorithms can be interpreted in terms of CCCP. In particular, we prove relationships to some applications of Legendre transform techniques. We then illustrate CCCP by applications to Potts models, linear assignment, EM algorithms, and Generalized Iterative Scaling (GIS). CCCP can be used both as a new way to understand existing optimization algorithms and as a procedure for generating new algorithms."
            },
            "slug": "The-Concave-Convex-Procedure-(CCCP)-Yuille-Rangarajan",
            "title": {
                "fragments": [],
                "text": "The Concave-Convex Procedure (CCCP)"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "This work introduces the Concave-Convex procedure (CCCP) which constructs discrete time iterative dynamical systems which are guaranteed to monotonically decrease global optimization/energy functions and proves relationships to some applications of Legendre transform techniques."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35044492"
                        ],
                        "name": "F. Eaton",
                        "slug": "F.-Eaton",
                        "structuredName": {
                            "firstName": "Frederik",
                            "lastName": "Eaton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Eaton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1751750,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "2ea7719c1a63175a3ccea1b6ef1a1c314e85f7bf",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Choosing-a-Variable-to-Clamp-Eaton-Ghahramani",
            "title": {
                "fragments": [],
                "text": "Choosing a Variable to Clamp"
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning parameters in continuous-valued Markov random fields"
            },
            "venue": {
                "fragments": [],
                "text": "In Markov Random Fields For Vision And Image Processing. MIT Press,"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The concaveconvex procedure (CCCP)"
            },
            "venue": {
                "fragments": [],
                "text": "In NIPS,"
            },
            "year": 2001
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 4,
            "methodology": 10
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 26,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Parameter-Learning-and-Convergent-Inference-for-Kr\u00e4henb\u00fchl-Koltun/f544b3aeeb2b3690cb90bf568cfe92d864107c9f?sort=total-citations"
}