{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3403009"
                        ],
                        "name": "Aude Genevay",
                        "slug": "Aude-Genevay",
                        "structuredName": {
                            "firstName": "Aude",
                            "lastName": "Genevay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aude Genevay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711979"
                        ],
                        "name": "Marco Cuturi",
                        "slug": "Marco-Cuturi",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Cuturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Cuturi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145481009"
                        ],
                        "name": "G. Peyr\u00e9",
                        "slug": "G.-Peyr\u00e9",
                        "structuredName": {
                            "firstName": "Gabriel",
                            "lastName": "Peyr\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Peyr\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144570279"
                        ],
                        "name": "F. Bach",
                        "slug": "F.-Bach",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Bach",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Bach"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6849824,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f840ce7c8373c11f89ea1693d0cab170627d31f",
            "isKey": false,
            "numCitedBy": 300,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Optimal transport (OT) defines a powerful framework to compare probability distributions in a geometrically faithful way. However, the practical impact of OT is still limited because of its computational burden. We propose a new class of stochastic optimization algorithms to cope with large-scale problems routinely encountered in machine learning applications. These methods are able to manipulate arbitrary distributions (either discrete or continuous) by simply requiring to be able to draw samples from them, which is the typical setup in high-dimensional learning problems. This alleviates the need to discretize these densities, while giving access to provably convergent methods that output the correct distance without discretization error. These algorithms rely on two main ideas: (a) the dual OT problem can be re-cast as the maximization of an expectation ; (b) entropic regularization of the primal OT problem results in a smooth dual optimization optimization which can be addressed with algorithms that have a provably faster convergence. We instantiate these ideas in three different setups: (i) when comparing a discrete distribution to another, we show that incremental stochastic optimization schemes can beat Sinkhorn's algorithm, the current state-of-the-art finite dimensional OT solver; (ii) when comparing a discrete distribution to a continuous density, a semi-discrete reformulation of the dual program is amenable to averaged stochastic gradient descent, leading to better performance than approximately solving the problem by discretization ; (iii) when dealing with two continuous densities, we propose a stochastic gradient descent over a reproducing kernel Hilbert space (RKHS). This is currently the only known method to solve this problem, apart from computing OT on finite samples. We backup these claims on a set of discrete, semi-discrete and continuous benchmark problems."
            },
            "slug": "Stochastic-Optimization-for-Large-scale-Optimal-Genevay-Cuturi",
            "title": {
                "fragments": [],
                "text": "Stochastic Optimization for Large-scale Optimal Transport"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A new class of stochastic optimization algorithms to cope with large-scale problems routinely encountered in machine learning applications, based on entropic regularization of the primal OT problem, which results in a smooth dual optimization optimization which can be addressed with algorithms that have a provably faster convergence."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698617"
                        ],
                        "name": "O. Bousquet",
                        "slug": "O.-Bousquet",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bousquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bousquet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802148"
                        ],
                        "name": "S. Gelly",
                        "slug": "S.-Gelly",
                        "structuredName": {
                            "firstName": "Sylvain",
                            "lastName": "Gelly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gelly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3121264"
                        ],
                        "name": "I. Tolstikhin",
                        "slug": "I.-Tolstikhin",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Tolstikhin",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Tolstikhin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1405241841"
                        ],
                        "name": "Carl-Johann Simon-Gabriel",
                        "slug": "Carl-Johann-Simon-Gabriel",
                        "structuredName": {
                            "firstName": "Carl-Johann",
                            "lastName": "Simon-Gabriel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carl-Johann Simon-Gabriel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30927799"
                        ],
                        "name": "B. Schoelkopf",
                        "slug": "B.-Schoelkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Schoelkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schoelkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 40
                            }
                        ],
                        "text": "A very similar conclusion is reached by [5] (see in particular their Proposition 3)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 262,
                                "start": 259
                            }
                        ],
                        "text": "As opposed to this dual way to compute gradients of the fitting energy, we advocate for the use of a primal formulation, which is numerically stable, because it does not involve differentiating the (dual) solution of an OT sub-problem, as also pointed out in [5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 88521144,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a2605ca2c38fe45ac87b1d196a322857d8cb912",
            "isKey": false,
            "numCitedBy": 117,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We study unsupervised generative modeling in terms of the optimal transport (OT) problem between true (but unknown) data distribution $P_X$ and the latent variable model distribution $P_G$. We show that the OT problem can be equivalently written in terms of probabilistic encoders, which are constrained to match the posterior and prior distributions over the latent space. When relaxed, this constrained optimization problem leads to a penalized optimal transport (POT) objective, which can be efficiently minimized using stochastic gradient descent by sampling from $P_X$ and $P_G$. We show that POT for the 2-Wasserstein distance coincides with the objective heuristically employed in adversarial auto-encoders (AAE) (Makhzani et al., 2016), which provides the first theoretical justification for AAEs known to the authors. We also compare POT to other popular techniques like variational auto-encoders (VAE) (Kingma and Welling, 2014). Our theoretical results include (a) a better understanding of the commonly observed blurriness of images generated by VAEs, and (b) establishing duality between Wasserstein GAN (Arjovsky and Bottou, 2017) and POT for the 1-Wasserstein distance."
            },
            "slug": "From-optimal-transport-to-generative-modeling:-the-Bousquet-Gelly",
            "title": {
                "fragments": [],
                "text": "From optimal transport to generative modeling: the VEGAN cookbook"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "It is shown that POT for the 2-Wasserstein distance coincides with the objective heuristically employed in adversarial auto-encoders (AAE) (Makhzani et al., 2016), which provides the first theoretical justification for AAEs known to the authors."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51129938"
                        ],
                        "name": "Espen Bernton",
                        "slug": "Espen-Bernton",
                        "structuredName": {
                            "firstName": "Espen",
                            "lastName": "Bernton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Espen Bernton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32746783"
                        ],
                        "name": "P. Jacob",
                        "slug": "P.-Jacob",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Jacob",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Jacob"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830618"
                        ],
                        "name": "Mathieu Gerber",
                        "slug": "Mathieu-Gerber",
                        "structuredName": {
                            "firstName": "Mathieu",
                            "lastName": "Gerber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mathieu Gerber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145155783"
                        ],
                        "name": "C. Robert",
                        "slug": "C.-Robert",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Robert",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Robert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 85
                            }
                        ],
                        "text": "As (m,n) increases, E(\u00ca\u03b5) approaches E\u03b5, and convergence of minimizers is studied in [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 66
                            }
                        ],
                        "text": "To remedy these issues, and in line with several recent proposals [2, 26, 4, 1], we propose to shift away from information divergence based methods (among which the MLE) and consider instead the geometry of optimal transport [36, 31] to define such a fitting criterion."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 368,
                                "start": 365
                            }
                        ],
                        "text": "Although the use of Wasserstein metrics for inference in generative models was considered over ten years ago in [2], that development remained exclusively theoretical until a recent wave of papers managed to implement that idea more or less faithfully using several workarounds: entropic regularization over a discrete space [26], approximate Bayesian computations [4] and a neural network parameterization of the dual potential arising from the dual OT problem when considering the 1-Wasserstein distance [1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 88520948,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "11fadfe035f08b786279014e82203d0270443165",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "In purely generative models, one can simulate data given parameters but not necessarily evaluate the likelihood. We use Wasserstein distances between empirical distributions of observed data and empirical distributions of synthetic data drawn from such models to estimate their parameters. Previous interest in the Wasserstein distance for statistical inference has been mainly theoretical, due to computational limitations. Thanks to recent advances in numerical transport, the computation of these distances has become feasible, up to controllable approximation errors. We leverage these advances to propose point estimators and quasi-Bayesian distributions for parameter inference, first for independent data. For dependent data, we extend the approach by using delay reconstruction and residual reconstruction techniques. For large data sets, we propose an alternative distance using the Hilbert space-filling curve, which computation scales as nlogn where n is the size of the data. We provide a theoretical study of the proposed estimators, and adaptive Monte Carlo algorithms to approximate them. The approach is illustrated on four examples: a quantile g-and-k distribution, a toggle switch model from systems biology, a Lotka-Volterra model for plankton population sizes and a L\\'evy-driven stochastic volatility model."
            },
            "slug": "Inference-in-generative-models-using-the-distance-Bernton-Jacob",
            "title": {
                "fragments": [],
                "text": "Inference in generative models using the Wasserstein distance"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This work uses Wasserstein distances between empirical distributions of observed data and empirical distribution of synthetic data drawn from such models to estimate their parameters, and proposes an alternative distance using the Hilbert space-filling curve."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153440022"
                        ],
                        "name": "Ian J. Goodfellow",
                        "slug": "Ian-J.-Goodfellow",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Goodfellow",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian J. Goodfellow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403025868"
                        ],
                        "name": "Jean Pouget-Abadie",
                        "slug": "Jean-Pouget-Abadie",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Pouget-Abadie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean Pouget-Abadie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153583218"
                        ],
                        "name": "Mehdi Mirza",
                        "slug": "Mehdi-Mirza",
                        "structuredName": {
                            "firstName": "Mehdi",
                            "lastName": "Mirza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mehdi Mirza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113742925"
                        ],
                        "name": "Bing Xu",
                        "slug": "Bing-Xu",
                        "structuredName": {
                            "firstName": "Bing",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bing Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1393680089"
                        ],
                        "name": "David Warde-Farley",
                        "slug": "David-Warde-Farley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Warde-Farley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Warde-Farley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1955694"
                        ],
                        "name": "Sherjil Ozair",
                        "slug": "Sherjil-Ozair",
                        "structuredName": {
                            "firstName": "Sherjil",
                            "lastName": "Ozair",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sherjil Ozair"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760871"
                        ],
                        "name": "Aaron C. Courville",
                        "slug": "Aaron-C.-Courville",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Courville",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron C. Courville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "Image generating models such as GAN [15] or VAE [21] have become popular in recent years."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 25
                            }
                        ],
                        "text": "The improved Wasserstein GAN approach [17] (which penalizes the squared norm of the gradient of the dual potential) is similar to an MMD (in fact a dual Sobolev norm)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "This matrix, as in MMDGAN\u2019s approach [25] is all we need."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 100
                            }
                        ],
                        "text": "Major approaches include variational autoencoders (VAE) [21], generative adversarial networks (GAN) [15] and several more variations including combinations of both [23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 16
                            }
                        ],
                        "text": "The adversarial GAN approach is implicitly geometric in the sense that it computes the best achievable classification accuracy (taking for granted the training and generated datapoints have opposite labels) for a given class of classifiers as a proxy for the distance between two distributions: If accuracy is high distributions are well separated, if accuracy is low they are difficult to tell apart and lie thus at a very close distance."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 118
                            }
                        ],
                        "text": "Both methods require a second network for the training of the generative network (an adversial network in the case of GANs, an encoding network in the case of VAEs)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1033682,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54e325aee6b2d476bbbb88615ac15e251c6e8214",
            "isKey": true,
            "numCitedBy": 29656,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to \u00bd everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples."
            },
            "slug": "Generative-Adversarial-Nets-Goodfellow-Pouget-Abadie",
            "title": {
                "fragments": [],
                "text": "Generative Adversarial Nets"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A new framework for estimating generative models via an adversarial process, in which two models are simultaneously train: a generative model G that captures the data distribution and a discriminative model D that estimates the probability that a sample came from the training data rather than G."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792298"
                        ],
                        "name": "Marc G. Bellemare",
                        "slug": "Marc-G.-Bellemare",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Bellemare",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc G. Bellemare"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1841008"
                        ],
                        "name": "Ivo Danihelka",
                        "slug": "Ivo-Danihelka",
                        "structuredName": {
                            "firstName": "Ivo",
                            "lastName": "Danihelka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ivo Danihelka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2605877"
                        ],
                        "name": "Will Dabney",
                        "slug": "Will-Dabney",
                        "structuredName": {
                            "firstName": "Will",
                            "lastName": "Dabney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Will Dabney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14594344"
                        ],
                        "name": "S. Mohamed",
                        "slug": "S.-Mohamed",
                        "structuredName": {
                            "firstName": "Shakir",
                            "lastName": "Mohamed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mohamed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40627523"
                        ],
                        "name": "Balaji Lakshminarayanan",
                        "slug": "Balaji-Lakshminarayanan",
                        "structuredName": {
                            "firstName": "Balaji",
                            "lastName": "Lakshminarayanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Balaji Lakshminarayanan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7018631"
                        ],
                        "name": "Stephan Hoyer",
                        "slug": "Stephan-Hoyer",
                        "structuredName": {
                            "firstName": "Stephan",
                            "lastName": "Hoyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephan Hoyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708654"
                        ],
                        "name": "R. Munos",
                        "slug": "R.-Munos",
                        "structuredName": {
                            "firstName": "R\u00e9mi",
                            "lastName": "Munos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Munos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 45
                            }
                        ],
                        "text": "It was also used to fit generative models in [3], while [24] uses MMD with a gaussian kernel."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 27
                            }
                        ],
                        "text": "Note that contrary to what [3] claims, the energy distance cannot be presented as a cure to solve the bias of OT estimation in high-dimension, since the two distances are fundamentally different."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10547012,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cfcf66e4b22dc7671a5941e94e9d4afae75ba2f8",
            "isKey": false,
            "numCitedBy": 248,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "The Wasserstein probability metric has received much attention from the machine learning community. Unlike the Kullback-Leibler divergence, which strictly measures change in probability, the Wasserstein metric reflects the underlying geometry between outcomes. The value of being sensitive to this geometry has been demonstrated, among others, in ordinal regression and generative modelling, and most recently in reinforcement learning. In this paper we describe three natural properties of probability divergences that we believe reflect requirements from machine learning: sum invariance, scale sensitivity, and unbiased sample gradients. The Wasserstein metric possesses the first two properties but, unlike the Kullback-Leibler divergence, does not possess the third. We provide empirical evidence suggesting this is a serious issue in practice. Leveraging insights from probabilistic forecasting we propose an alternative to the Wasserstein metric, the Cramer distance. We show that the Cramer distance possesses all three desired properties, combining the best of the Wasserstein and Kullback-Leibler divergences. We give empirical results on a number of domains comparing these three divergences. To illustrate the practical relevance of the Cramer distance we design a new algorithm, the Cramer Generative Adversarial Network (GAN), and show that it has a number of desirable properties over the related Wasserstein GAN."
            },
            "slug": "The-Cramer-Distance-as-a-Solution-to-Biased-Bellemare-Danihelka",
            "title": {
                "fragments": [],
                "text": "The Cramer Distance as a Solution to Biased Wasserstein Gradients"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper describes three natural properties of probability divergences that it believes reflect requirements from machine learning: sum invariance, scale sensitivity, and unbiased sample gradients and proposes an alternative to the Wasserstein metric, the Cramer distance, which possesses all three desired properties."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3437110"
                        ],
                        "name": "Antoine Rolet",
                        "slug": "Antoine-Rolet",
                        "structuredName": {
                            "firstName": "Antoine",
                            "lastName": "Rolet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antoine Rolet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711979"
                        ],
                        "name": "Marco Cuturi",
                        "slug": "Marco-Cuturi",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Cuturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Cuturi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145481009"
                        ],
                        "name": "G. Peyr\u00e9",
                        "slug": "G.-Peyr\u00e9",
                        "structuredName": {
                            "firstName": "Gabriel",
                            "lastName": "Peyr\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Peyr\u00e9"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 135
                            }
                        ],
                        "text": "The benefits of this regularization has opened the path to many applications of the Wasserstein distance in relevant learning problems [8, 13, 18, 28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1726037,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e7d468e3e848d9c49c2c9bfaf847ffc98e81b627",
            "isKey": false,
            "numCitedBy": 117,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider in this paper the dictionary learning problem when the observations are normalized histograms of features. This problem can be tackled using non-negative matrix factorization approaches, using typically Euclidean or Kullback-Leibler fitting errors. Because these fitting errors are separable and treat each feature on equal footing, they are blind to any similarity the features may share. We assume in this work that we have prior knowledge on these features. To leverage this side-information, we propose to use the Wasserstein (a.k.a. earth mover\u2019s or optimal transport) distance as the fitting error between each original point and its reconstruction, and we propose scalable algorithms to to so. Our methods build upon Fenchel duality and entropic regularization of Wasserstein distances, which improves not only speed but also computational stability. We apply these techniques on face images and text documents. We show in particular that we can learn dictionaries (topics) for bagof-word representations of texts using words that may not have appeared in the original texts, or even words that come from a different language than that used in the texts."
            },
            "slug": "Fast-Dictionary-Learning-with-a-Smoothed-Loss-Rolet-Cuturi",
            "title": {
                "fragments": [],
                "text": "Fast Dictionary Learning with a Smoothed Wasserstein Loss"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work proposes to use the Wasserstein distance as the fitting error between each original point and its reconstruction, and proposes scalable algorithms to do so, which improves not only speed but also computational stability."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47002813"
                        ],
                        "name": "Yujia Li",
                        "slug": "Yujia-Li",
                        "structuredName": {
                            "firstName": "Yujia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yujia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754860"
                        ],
                        "name": "Kevin Swersky",
                        "slug": "Kevin-Swersky",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Swersky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Swersky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 37
                            }
                        ],
                        "text": "This matrix, as in MMDGAN\u2019s approach [25] is all we need."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 80
                            }
                        ],
                        "text": "It was shown in ensuing works that the effectiveness of the MMD in that setting [25, 12] hinges on the ability to find a relevant RKHS bandwidth parameter, which is a highly nontrivial choice."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 72
                            }
                        ],
                        "text": "Using MMD to train generative models has been shown to be successful in [12, 25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 536962,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c8b509be29721ee6b12c880b4d97ed6b60bad217",
            "isKey": false,
            "numCitedBy": 657,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer perceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014). Training a generative adversarial network, however, requires careful optimization of a difficult minimax program. Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation. We further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples. We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database."
            },
            "slug": "Generative-Moment-Matching-Networks-Li-Swersky",
            "title": {
                "fragments": [],
                "text": "Generative Moment Matching Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This work forms a method that generates an independent sample via a single feedforward pass through a multilayer perceptron, as in the recently proposed generative adversarial networks, using MMD to learn to generate codes that can then be decoded to produce samples."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144535526"
                        ],
                        "name": "G. Montavon",
                        "slug": "G.-Montavon",
                        "structuredName": {
                            "firstName": "Gr\u00e9goire",
                            "lastName": "Montavon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Montavon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711979"
                        ],
                        "name": "Marco Cuturi",
                        "slug": "Marco-Cuturi",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Cuturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Cuturi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 66
                            }
                        ],
                        "text": "To remedy these issues, and in line with several recent proposals [2, 26, 4, 1], we propose to shift away from information divergence based methods (among which the MLE) and consider instead the geometry of optimal transport [36, 31] to define such a fitting criterion."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 329,
                                "start": 325
                            }
                        ],
                        "text": "Although the use of Wasserstein metrics for inference in generative models was considered over ten years ago in [2], that development remained exclusively theoretical until a recent wave of papers managed to implement that idea more or less faithfully using several workarounds: entropic regularization over a discrete space [26], approximate Bayesian computations [4] and a neural network parameterization of the dual potential arising from the dual OT problem when considering the 1-Wasserstein distance [1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14390733,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e5abcd3dcc1dad7354c2fa85d7142294860bd83e",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Boltzmann machines are able to learn highly complex, multimodal, structured and multiscale real-world data distributions. Parameters of the model are usually learned by minimizing the Kullback-Leibler (KL) divergence from training samples to the learned model. We propose in this work a novel approach for Boltzmann machine training which assumes that a meaningful metric between observations is known. This metric between observations can then be used to define the Wasserstein distance between the distribution induced by the Boltzmann machine on the one hand, and that given by the training sample on the other hand. We derive a gradient of that distance with respect to the model parameters. Minimization of this new objective leads to generative models with different statistical properties. We demonstrate their practical potential on data completion and denoising, for which the metric between observations plays a crucial role."
            },
            "slug": "Wasserstein-Training-of-Restricted-Boltzmann-Montavon-M\u00fcller",
            "title": {
                "fragments": [],
                "text": "Wasserstein Training of Restricted Boltzmann Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work proposes a novel approach for Boltzmann machine training which assumes that a meaningful metric between observations is known, and derives a gradient of that distance with respect to the model parameters from the Kullback-Leibler divergence."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2139116"
                        ],
                        "name": "Charlie Frogner",
                        "slug": "Charlie-Frogner",
                        "structuredName": {
                            "firstName": "Charlie",
                            "lastName": "Frogner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charlie Frogner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "151505981"
                        ],
                        "name": "Chiyuan Zhang",
                        "slug": "Chiyuan-Zhang",
                        "structuredName": {
                            "firstName": "Chiyuan",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chiyuan Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3232655"
                        ],
                        "name": "H. Mobahi",
                        "slug": "H.-Mobahi",
                        "structuredName": {
                            "firstName": "Hossein",
                            "lastName": "Mobahi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Mobahi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399105101"
                        ],
                        "name": "M. Araya-Polo",
                        "slug": "M.-Araya-Polo",
                        "structuredName": {
                            "firstName": "Mauricio",
                            "lastName": "Araya-Polo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Araya-Polo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 125
                            }
                        ],
                        "text": "It is however important to realize that for large scale and high dimensional learning applications, empirical considerations [9, 22, 13] suggest that, unlike relevant applications of the same scheme in graphics [33], a relatively strong regularization\u2014a large \u03b5\u2014leads not only to more stable results but also faster convergence, so that the value for L can be set quite low."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 135
                            }
                        ],
                        "text": "The benefits of this regularization has opened the path to many applications of the Wasserstein distance in relevant learning problems [8, 13, 18, 28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8649027,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ae23eda2faddd824480de2d90638795f797cc66e",
            "isKey": false,
            "numCitedBy": 390,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning to predict multi-label outputs is challenging, but in many problems there is a natural metric on the outputs that can be used to improve predictions. In this paper we develop a loss function for multi-label learning, based on the Wasserstein distance. The Wasserstein distance provides a natural notion of dissimilarity for probability measures. Although optimizing with respect to the exact Wasserstein distance is costly, recent work has described a regularized approximation that is efficiently computed. We describe an efficient learning algorithm based on this regularization, as well as a novel extension of the Wasserstein distance from probability measures to unnormalized measures. We also describe a statistical learning bound for the loss. The Wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space. We demonstrate this property on a real-data tag prediction problem, using the Yahoo Flickr Creative Commons dataset, outperforming a baseline that doesn't use the metric."
            },
            "slug": "Learning-with-a-Wasserstein-Loss-Frogner-Zhang",
            "title": {
                "fragments": [],
                "text": "Learning with a Wasserstein Loss"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An efficient learning algorithm based on this regularization, as well as a novel extension of the Wasserstein distance from probability measures to unnormalized measures, which can encourage smoothness of the predictions with respect to a chosen metric on the output space."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2533850"
                        ],
                        "name": "G. Dziugaite",
                        "slug": "G.-Dziugaite",
                        "structuredName": {
                            "firstName": "Gintare",
                            "lastName": "Dziugaite",
                            "middleNames": [
                                "Karolina"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Dziugaite"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39331522"
                        ],
                        "name": "Daniel M. Roy",
                        "slug": "Daniel-M.-Roy",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Roy",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel M. Roy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": "We choose the inception score introduced in [29] as it is well spread, and also the reference in [12] agains which we compare our losses."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 80
                            }
                        ],
                        "text": "It was shown in ensuing works that the effectiveness of the MMD in that setting [25, 12] hinges on the ability to find a relevant RKHS bandwidth parameter, which is a highly nontrivial choice."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 72
                            }
                        ],
                        "text": "Using MMD to train generative models has been shown to be successful in [12, 25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9127770,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "4e2f6b4bc889eed1afe5833d5190f6f02e501061",
            "isKey": false,
            "numCitedBy": 396,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider training a deep neural network to generate samples from an unknown distribution given i.i.d. data. We frame learning as an optimization minimizing a two-sample test statistic\u2014informally speaking, a good generator network produces samples that cause a two-sample test to fail to reject the null hypothesis. As our two-sample test statistic, we use an unbiased estimate of the maximum mean discrepancy, which is the centerpiece of the nonparametric kernel two-sample test proposed by Gretton et al. [2]. We compare to the adversarial nets framework introduced by Goodfellow et al. [1], in which learning is a two-player game between a generator network and an adversarial discriminator network, both trained to outwit the other. From this perspective, the MMD statistic plays the role of the discriminator. In addition to empirical comparisons, we prove bounds on the generalization error incurred by optimizing the empirical MMD."
            },
            "slug": "Training-generative-neural-networks-via-Maximum-Dziugaite-Roy",
            "title": {
                "fragments": [],
                "text": "Training generative neural networks via Maximum Mean Discrepancy optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "This work considers training a deep neural network to generate samples from an unknown distribution given i.i.d. data to frame learning as an optimization minimizing a two-sample test statistic, and proves bounds on the generalization error incurred by optimizing the empirical MMD."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726807"
                        ],
                        "name": "Diederik P. Kingma",
                        "slug": "Diederik-P.-Kingma",
                        "structuredName": {
                            "firstName": "Diederik",
                            "lastName": "Kingma",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diederik P. Kingma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2503659"
                        ],
                        "name": "Jimmy Ba",
                        "slug": "Jimmy-Ba",
                        "structuredName": {
                            "firstName": "Jimmy",
                            "lastName": "Ba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jimmy Ba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6628106,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "isKey": false,
            "numCitedBy": 90063,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."
            },
            "slug": "Adam:-A-Method-for-Stochastic-Optimization-Kingma-Ba",
            "title": {
                "fragments": [],
                "text": "Adam: A Method for Stochastic Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This work introduces Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments, and provides a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2887364"
                        ],
                        "name": "Tim Salimans",
                        "slug": "Tim-Salimans",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Salimans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Salimans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153440022"
                        ],
                        "name": "Ian J. Goodfellow",
                        "slug": "Ian-J.-Goodfellow",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Goodfellow",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian J. Goodfellow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2563432"
                        ],
                        "name": "Wojciech Zaremba",
                        "slug": "Wojciech-Zaremba",
                        "structuredName": {
                            "firstName": "Wojciech",
                            "lastName": "Zaremba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wojciech Zaremba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34415167"
                        ],
                        "name": "Vicki Cheung",
                        "slug": "Vicki-Cheung",
                        "structuredName": {
                            "firstName": "Vicki",
                            "lastName": "Cheung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vicki Cheung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38909097"
                        ],
                        "name": "Alec Radford",
                        "slug": "Alec-Radford",
                        "structuredName": {
                            "firstName": "Alec",
                            "lastName": "Radford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alec Radford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41192764"
                        ],
                        "name": "Xi Chen",
                        "slug": "Xi-Chen",
                        "structuredName": {
                            "firstName": "Xi",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xi Chen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "We choose the inception score introduced in [29] as it is well spread, and also the reference in [12] agains which we compare our losses."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1687220,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "571b0750085ae3d939525e62af510ee2cee9d5ea",
            "isKey": false,
            "numCitedBy": 5550,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes."
            },
            "slug": "Improved-Techniques-for-Training-GANs-Salimans-Goodfellow",
            "title": {
                "fragments": [],
                "text": "Improved Techniques for Training GANs"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This work focuses on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic, and presents ImageNet samples with unprecedented resolution and shows that the methods enable the model to learn recognizable features of ImageNet classes."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2708454"
                        ],
                        "name": "Ishaan Gulrajani",
                        "slug": "Ishaan-Gulrajani",
                        "structuredName": {
                            "firstName": "Ishaan",
                            "lastName": "Gulrajani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ishaan Gulrajani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054472270"
                        ],
                        "name": "Faruk Ahmed",
                        "slug": "Faruk-Ahmed",
                        "structuredName": {
                            "firstName": "Faruk",
                            "lastName": "Ahmed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Faruk Ahmed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2877311"
                        ],
                        "name": "Mart\u00edn Arjovsky",
                        "slug": "Mart\u00edn-Arjovsky",
                        "structuredName": {
                            "firstName": "Mart\u00edn",
                            "lastName": "Arjovsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mart\u00edn Arjovsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3074927"
                        ],
                        "name": "Vincent Dumoulin",
                        "slug": "Vincent-Dumoulin",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Dumoulin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent Dumoulin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760871"
                        ],
                        "name": "Aaron C. Courville",
                        "slug": "Aaron-C.-Courville",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Courville",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron C. Courville"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "The improved Wasserstein GAN approach [17] (which penalizes the squared norm of the gradient of the dual potential) is similar to an MMD (in fact a dual Sobolev norm)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 10894094,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "edf73ab12595c6709f646f542a0d2b33eb20a3f4",
            "isKey": false,
            "numCitedBy": 5699,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms."
            },
            "slug": "Improved-Training-of-Wasserstein-GANs-Gulrajani-Ahmed",
            "title": {
                "fragments": [],
                "text": "Improved Training of Wasserstein GANs"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input, which performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2556942"
                        ],
                        "name": "Aaditya Ramdas",
                        "slug": "Aaditya-Ramdas",
                        "structuredName": {
                            "firstName": "Aaditya",
                            "lastName": "Ramdas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaditya Ramdas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2042414"
                        ],
                        "name": "Nicol\u00e1s Garc\u00eda Trillos",
                        "slug": "Nicol\u00e1s-Garc\u00eda-Trillos",
                        "structuredName": {
                            "firstName": "Nicol\u00e1s",
                            "lastName": "Trillos",
                            "middleNames": [
                                "Garc\u00eda"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicol\u00e1s Garc\u00eda Trillos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711979"
                        ],
                        "name": "Marco Cuturi",
                        "slug": "Marco-Cuturi",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Cuturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Cuturi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7725237,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0fa622b2ee89456ff75235fa0bcc642c09076ebd",
            "isKey": false,
            "numCitedBy": 231,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "Nonparametric two sample or homogeneity testing is a decision theoretic problem that involves identifying differences between two random variables without making parametric assumptions about their underlying distributions. The literature is old and rich, with a wide variety of statistics having being intelligently designed and analyzed, both for the unidimensional and the multivariate setting. Our contribution is to tie together many of these tests, drawing connections between seemingly very different statistics. In this work, our central object is the Wasserstein distance, as we form a chain of connections from univariate methods like the Kolmogorov-Smirnov test, PP/QQ plots and ROC/ODC curves, to multivariate tests involving energy statistics and kernel based maximum mean discrepancy. Some connections proceed through the construction of a \\textit{smoothed} Wasserstein distance, and others through the pursuit of a \"distribution-free\" Wasserstein test. Some observations in this chain are implicit in the literature, while others seem to have not been noticed thus far. Given nonparametric two sample testing's classical and continued importance, we aim to provide useful connections for theorists and practitioners familiar with one subset of methods but not others."
            },
            "slug": "On-Wasserstein-Two-Sample-Testing-and-Related-of-Ramdas-Trillos",
            "title": {
                "fragments": [],
                "text": "On Wasserstein Two-Sample Testing and Related Families of Nonparametric Tests"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work forms a chain of connections from univariate methods like the Kolmogorov-Smirnov test, PP/QQ plots and ROC/ODC curves, to multivariate tests involving energy statistics and kernel based maximum mean discrepancy, to provide useful connections for theorists and practitioners familiar with one subset of methods but not others."
            },
            "venue": {
                "fragments": [],
                "text": "Entropy"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711979"
                        ],
                        "name": "Marco Cuturi",
                        "slug": "Marco-Cuturi",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Cuturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Cuturi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 258,
                                "start": 255
                            }
                        ],
                        "text": "We propose a different route, by making two key simplifications: (i) approximate the function E\u03b5(\u03b8) by a size-(m,n) mini-batch sampling \u00ca\u03b5(\u03b8) to make it amenable to stochastic gradient descent ; (ii) approximate \u00ca\u03b5(\u03b8) by L-steps of the Sinkhorn algorithm [9] to obtain an algorithmic loss \u00ca \u03b5 (\u03b8) which is amenable to automatic differentiation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 55
                            }
                        ],
                        "text": "We introduce the regularized optimal transport problem [9, 14] defined by"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 242
                            }
                        ],
                        "text": "One major advantage of regularizing the optimal transport problem is that it becomes solvable efficiently using Sinkhorn\u2019s algorithm [32] (when dealing with discrete measures), and leads to a differentiable loss function (as first noticed in [9, 10])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 125
                            }
                        ],
                        "text": "It is however important to realize that for large scale and high dimensional learning applications, empirical considerations [9, 22, 13] suggest that, unlike relevant applications of the same scheme in graphics [33], a relatively strong regularization\u2014a large \u03b5\u2014leads not only to more stable results but also faster convergence, so that the value for L can be set quite low."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 380,
                                "start": 373
                            }
                        ],
                        "text": "While it was long disregarded because of its computational burden\u2014in its original form solving OT amounts to solving an expensive network flow problem when comparing discrete measures in metric spaces\u2014recent works have shown that this cost can be largely mitigated by settling for cheaper approximations obtained through strongly convex regularizers, in particular entropy [9, 14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15966283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0080118b0eb02af581ff32b85a1bb6aed7081f45",
            "isKey": false,
            "numCitedBy": 1873,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Optimal transport distances are a fundamental family of distances for probability measures and histograms of features. Despite their appealing theoretical properties, excellent performance in retrieval tasks and intuitive formulation, their computation involves the resolution of a linear program whose cost can quickly become prohibitive whenever the size of the support of these measures or the histograms' dimension exceeds a few hundred. We propose in this work a new family of optimal transport distances that look at transport problems from a maximum-entropy perspective. We smooth the classic optimal transport problem with an entropic regularization term, and show that the resulting optimum is also a distance which can be computed through Sinkhorn's matrix scaling algorithm at a speed that is several orders of magnitude faster than that of transport solvers. We also show that this regularized distance improves upon classic optimal transport distances on the MNIST classification problem."
            },
            "slug": "Sinkhorn-Distances:-Lightspeed-Computation-of-Cuturi",
            "title": {
                "fragments": [],
                "text": "Sinkhorn Distances: Lightspeed Computation of Optimal Transport"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work smooths the classic optimal transport problem with an entropic regularization term, and shows that the resulting optimum is also a distance which can be computed through Sinkhorn's matrix scaling algorithm at a speed that is several orders of magnitude faster than that of transport solvers."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109497736"
                        ],
                        "name": "Chun-Liang Li",
                        "slug": "Chun-Liang-Li",
                        "structuredName": {
                            "firstName": "Chun-Liang",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chun-Liang Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702500"
                        ],
                        "name": "Wei-Cheng Chang",
                        "slug": "Wei-Cheng-Chang",
                        "structuredName": {
                            "firstName": "Wei-Cheng",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Cheng Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145215470"
                        ],
                        "name": "Yu Cheng",
                        "slug": "Yu-Cheng",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Cheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu Cheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35729970"
                        ],
                        "name": "Yiming Yang",
                        "slug": "Yiming-Yang",
                        "structuredName": {
                            "firstName": "Yiming",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiming Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719347"
                        ],
                        "name": "B. P\u00f3czos",
                        "slug": "B.-P\u00f3czos",
                        "structuredName": {
                            "firstName": "Barnab\u00e1s",
                            "lastName": "P\u00f3czos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. P\u00f3czos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "The experimental setting is the same as in [24] and we used the same parameters to carry out a fair comparison."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "It was also used to fit generative models in [3], while [24] uses MMD with a gaussian kernel."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "The training procedure is the same as [1],[24] and consists in alterning nc optimisation steps to train the cost function f\u03c6 and an optimisation step to train the generator g\u03b8."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 108
                            }
                        ],
                        "text": "Learning the cost function here is very similar to learning a parametric kernel in an MMD model, as done in [24]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4685015,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd1758d3b86c4f1caf01ec222b45daf15888d1a8",
            "isKey": true,
            "numCitedBy": 490,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Generative moment matching network (GMMN) is a deep generative model that differs from Generative Adversarial Network (GAN) by replacing the discriminator in GAN with a two-sample test based on kernel maximum mean discrepancy (MMD). Although some theoretical guarantees of MMD have been studied, the empirical performance of GMMN is still not as competitive as that of GAN on challenging and large benchmark datasets. The computational efficiency of GMMN is also less desirable in comparison with GAN, partially due to its requirement for a rather large batch size during the training. In this paper, we propose to improve both the model expressiveness of GMMN and its computational efficiency by introducing adversarial kernel learning techniques, as the replacement of a fixed Gaussian kernel in the original GMMN. The new approach combines the key ideas in both GMMN and GAN, hence we name it MMD GAN. The new distance measure in MMD GAN is a meaningful loss that enjoys the advantage of weak topology and can be optimized via gradient descent with relatively small batch sizes. In our evaluation on multiple benchmark datasets, including MNIST, CIFAR- 10, CelebA and LSUN, the performance of MMD-GAN significantly outperforms GMMN, and is competitive with other representative GAN works."
            },
            "slug": "MMD-GAN:-Towards-Deeper-Understanding-of-Moment-Li-Chang",
            "title": {
                "fragments": [],
                "text": "MMD GAN: Towards Deeper Understanding of Moment Matching Network"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "In the evaluation on multiple benchmark datasets, including MNIST, CIFAR- 10, CelebA and LSUN, the performance of MMD-GAN significantly outperforms GMMN, and is competitive with other representative GAN works."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2887364"
                        ],
                        "name": "Tim Salimans",
                        "slug": "Tim-Salimans",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Salimans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Salimans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48213346"
                        ],
                        "name": "Han Zhang",
                        "slug": "Han-Zhang",
                        "structuredName": {
                            "firstName": "Han",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Han Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38909097"
                        ],
                        "name": "Alec Radford",
                        "slug": "Alec-Radford",
                        "structuredName": {
                            "firstName": "Alec",
                            "lastName": "Radford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alec Radford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711560"
                        ],
                        "name": "Dimitris N. Metaxas",
                        "slug": "Dimitris-N.-Metaxas",
                        "structuredName": {
                            "firstName": "Dimitris",
                            "lastName": "Metaxas",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dimitris N. Metaxas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "Shortly after the submission of this work, we came across the recent work by [30] which shares several ideas with our method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3459663,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "69902406e7d08f8865f02185699978db499d25e7",
            "isKey": false,
            "numCitedBy": 207,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We present Optimal Transport GAN (OT-GAN), a variant of generative adversarial nets minimizing a new metric measuring the distance between the generator distribution and the data distribution. This metric, which we call mini-batch energy distance, combines optimal transport in primal form with an energy distance defined in an adversarially learned feature space, resulting in a highly discriminative distance function with unbiased mini-batch gradients. Experimentally we show OT-GAN to be highly stable when trained with large mini-batches, and we present state-of-the-art results on several popular benchmark problems for image generation."
            },
            "slug": "Improving-GANs-Using-Optimal-Transport-Salimans-Zhang",
            "title": {
                "fragments": [],
                "text": "Improving GANs Using Optimal Transport"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "Optimal Transport GAN (OT-GAN), a variant of generative adversarial nets minimizing a new metric measuring the distance between the generator distribution and the data distribution, resulting in a highly discriminative distance function with unbiased mini-batch gradients is presented."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9971430"
                        ],
                        "name": "Guillermo D. Ca\u00f1as",
                        "slug": "Guillermo-D.-Ca\u00f1as",
                        "structuredName": {
                            "firstName": "Guillermo",
                            "lastName": "Ca\u00f1as",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guillermo D. Ca\u00f1as"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690976"
                        ],
                        "name": "L. Rosasco",
                        "slug": "L.-Rosasco",
                        "structuredName": {
                            "firstName": "Lorenzo",
                            "lastName": "Rosasco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rosasco"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 319,
                                "start": 316
                            }
                        ],
                        "text": "If we were to find a probability distribution \u03bc\u03b8 bound to be itself an empirical measure of K atoms (in that case parameter \u03b8 would contain exactly the locations of those K points in addition to their weight), then minimizing the 2-Wasserstein distance of \u03bc\u03b8 to \u03bd would be strictly equivalent to the K-means problem [6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9796093,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "86ea0c2feaf4284de694afe07ff6dd323b77ae63",
            "isKey": false,
            "numCitedBy": 71,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the problem of estimating, in the sense of optimal transport metrics, a measure which is assumed supported on a manifold embedded in a Hilbert space. By establishing a precise connection between optimal transport metrics, optimal quantization, and learning theory, we derive new probabilistic bounds for the performance of a classic algorithm in unsupervised learning (k-means), when used to produce a probability measure derived from the data. In the course of the analysis, we arrive at new lower bounds, as well as probabilistic upper bounds on the convergence rate of the empirical law of large numbers, which, unlike existing bounds, are applicable to a wide class of measures."
            },
            "slug": "Learning-Probability-Measures-with-respect-to-Ca\u00f1as-Rosasco",
            "title": {
                "fragments": [],
                "text": "Learning Probability Measures with respect to Optimal Transport Metrics"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "By establishing a precise connection between optimal transport metrics, optimal quantization, and learning theory, new probabilistic bounds are derived for the performance of a classic algorithm in unsupervised learning (k-means), when used to produce a probability measure derived from the data."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145374281"
                        ],
                        "name": "N. Courty",
                        "slug": "N.-Courty",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Courty",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Courty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145258331"
                        ],
                        "name": "R\u00e9mi Flamary",
                        "slug": "R\u00e9mi-Flamary",
                        "structuredName": {
                            "firstName": "R\u00e9mi",
                            "lastName": "Flamary",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R\u00e9mi Flamary"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2977931"
                        ],
                        "name": "D. Tuia",
                        "slug": "D.-Tuia",
                        "structuredName": {
                            "firstName": "Devis",
                            "lastName": "Tuia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tuia"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 135
                            }
                        ],
                        "text": "The benefits of this regularization has opened the path to many applications of the Wasserstein distance in relevant learning problems [8, 13, 18, 28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18945224,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b52af7de13716ff28a4be0771dafa3fb158ec3f9",
            "isKey": false,
            "numCitedBy": 133,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new and original method to solve the domain adaptation problem using optimal transport. By searching for the best transportation plan between the probability distribution functions of a source and a target domain, a non-linear and invertible transformation of the learning samples can be estimated. Any standard machine learning method can then be applied on the transformed set, which makes our method very generic. We propose a new optimal transport algorithm that incorporates label information in the optimization: this is achieved by combining an efficient matrix scaling technique together with a majoration of a non-convex regularization term. By using the proposed optimal transport with label regularization, we obtain significant increase in performance compared to the original transport solution. The proposed algorithm is computationally efficient and effective, as illustrated by its evaluation on a toy example and a challenging real life vision dataset, against which it achieves competitive results with respect to state-of-the-art methods."
            },
            "slug": "Domain-Adaptation-with-Regularized-Optimal-Courty-Flamary",
            "title": {
                "fragments": [],
                "text": "Domain Adaptation with Regularized Optimal Transport"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new optimal transport algorithm is proposed that incorporates label information in the optimization: this is achieved by combining an efficient matrix scaling technique together with a majoration of a non-convex regularization term."
            },
            "venue": {
                "fragments": [],
                "text": "ECML/PKDD"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3119801"
                        ],
                        "name": "Xavier Glorot",
                        "slug": "Xavier-Glorot",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Glorot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xavier Glorot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": "The parameters \u03b8 are thus the weights of both layers, and are initialized with the Xavier method [14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5575601,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b71ac1e9fb49420d13e084ac67254a0bbd40f83f",
            "isKey": false,
            "numCitedBy": 12433,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence. 1 Deep Neural Networks Deep learning methods aim at learning feature hierarchies with features from higher levels of the hierarchy formed by the composition of lower level features. They include Appearing in Proceedings of the 13 International Conference on Artificial Intelligence and Statistics (AISTATS) 2010, Chia Laguna Resort, Sardinia, Italy. Volume 9 of JMLR: WC Weston et al., 2008). Much attention has recently been devoted to them (see (Bengio, 2009) for a review), because of their theoretical appeal, inspiration from biology and human cognition, and because of empirical success in vision (Ranzato et al., 2007; Larochelle et al., 2007; Vincent et al., 2008) and natural language processing (NLP) (Collobert & Weston, 2008; Mnih & Hinton, 2009). Theoretical results reviewed and discussed by Bengio (2009), suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one may need deep architectures. Most of the recent experimental results with deep architecture are obtained with models that can be turned into deep supervised neural networks, but with initialization or training schemes different from the classical feedforward neural networks (Rumelhart et al., 1986). Why are these new algorithms working so much better than the standard random initialization and gradient-based optimization of a supervised training criterion? Part of the answer may be found in recent analyses of the effect of unsupervised pretraining (Erhan et al., 2009), showing that it acts as a regularizer that initializes the parameters in a \u201cbetter\u201d basin of attraction of the optimization procedure, corresponding to an apparent local minimum associated with better generalization. But earlier work (Bengio et al., 2007) had shown that even a purely supervised but greedy layer-wise procedure would give better results. So here instead of focusing on what unsupervised pre-training or semi-supervised criteria bring to deep architectures, we focus on analyzing what may be going wrong with good old (but deep) multilayer neural networks. Our analysis is driven by investigative experiments to monitor activations (watching for saturation of hidden units) and gradients, across layers and across training iterations. We also evaluate the effects on these of choices of activation function (with the idea that it might affect saturation) and initialization procedure (since unsupervised pretraining is a particular form of initialization and it has a drastic impact)."
            },
            "slug": "Understanding-the-difficulty-of-training-deep-Glorot-Bengio",
            "title": {
                "fragments": [],
                "text": "Understanding the difficulty of training deep feedforward neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46560485"
                        ],
                        "name": "Anders Boesen Lindbo Larsen",
                        "slug": "Anders-Boesen-Lindbo-Larsen",
                        "structuredName": {
                            "firstName": "Anders",
                            "lastName": "Larsen",
                            "middleNames": [
                                "Boesen",
                                "Lindbo"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anders Boesen Lindbo Larsen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388358166"
                        ],
                        "name": "S\u00f8ren Kaae S\u00f8nderby",
                        "slug": "S\u00f8ren-Kaae-S\u00f8nderby",
                        "structuredName": {
                            "firstName": "S\u00f8ren",
                            "lastName": "S\u00f8nderby",
                            "middleNames": [
                                "Kaae"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S\u00f8ren Kaae S\u00f8nderby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724252"
                        ],
                        "name": "O. Winther",
                        "slug": "O.-Winther",
                        "structuredName": {
                            "firstName": "Ole",
                            "lastName": "Winther",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Winther"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 164
                            }
                        ],
                        "text": "Major approaches include variational autoencoders (VAE) [21], generative adversarial networks (GAN) [15] and several more variations including combinations of both [23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8785311,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e8b8a7778ace2a02f8db6fe321a54520c6b283ca",
            "isKey": false,
            "numCitedBy": 1438,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an autoencoder that leverages learned representations to better measure similarities in data space. By combining a variational autoencoder with a generative adversarial network we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective. Thereby, we replace element-wise errors with feature-wise errors to better capture the data distribution while offering invariance towards e.g. translation. We apply our method to images of faces and show that it outperforms VAEs with element-wise similarity measures in terms of visual fidelity. Moreover, we show that the method learns an embedding in which high-level abstract visual features (e.g. wearing glasses) can be modified using simple arithmetic."
            },
            "slug": "Autoencoding-beyond-pixels-using-a-learned-metric-Larsen-S\u00f8nderby",
            "title": {
                "fragments": [],
                "text": "Autoencoding beyond pixels using a learned similarity metric"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "An autoencoder that leverages learned representations to better measure similarities in data space is presented and it is shown that the method learns an embedding in which high-level abstract visual features (e.g. wearing glasses) can be modified using simple arithmetic."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2006888"
                        ],
                        "name": "G. Carlier",
                        "slug": "G.-Carlier",
                        "structuredName": {
                            "firstName": "Guillaume",
                            "lastName": "Carlier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Carlier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145626265"
                        ],
                        "name": "V. Duval",
                        "slug": "V.-Duval",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Duval",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Duval"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145481009"
                        ],
                        "name": "G. Peyr\u00e9",
                        "slug": "G.-Peyr\u00e9",
                        "structuredName": {
                            "firstName": "Gabriel",
                            "lastName": "Peyr\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Peyr\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2092372"
                        ],
                        "name": "Bernhard Schmitzer",
                        "slug": "Bernhard-Schmitzer",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Schmitzer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bernhard Schmitzer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 65
                            }
                        ],
                        "text": "The first part of the assumption is well known, see for instance [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 46156175,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0ea842f015af02a06ae47a7db60804b400e9b7b4",
            "isKey": false,
            "numCitedBy": 116,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Replacing positivity constraints by an entropy barrier is popular to approximate solutions of linear programs. In the special case of the optimal transport problem, this technique dates back to the early work of Schr\\\"odinger. This approach has recently been used successfully to solve optimal transport related problems in several applied fields such as imaging sciences, machine learning and social sciences. The main reason for this success is that, in contrast to linear programming solvers, the resulting algorithms are highly parallelizable and take advantage of the geometry of the computational grid (e.g. an image or a triangulated mesh). The first contribution of this article is the proof of the $\\Gamma$-convergence of the entropic regularized optimal transport problem towards the Monge-Kantorovich problem for the squared Euclidean norm cost function. This implies in particular the convergence of the optimal entropic regularized transport plan towards an optimal transport plan as the entropy vanishes. Optimal transport distances are also useful to define gradient flows as a limit of implicit Euler steps according to the transportation distance. Our second contribution is a proof that implicit steps according to the entropic regularized distance converge towards the original gradient flow when both the step size and the entropic penalty vanish (in some controlled way)."
            },
            "slug": "Convergence-of-Entropic-Schemes-for-Optimal-and-Carlier-Duval",
            "title": {
                "fragments": [],
                "text": "Convergence of Entropic Schemes for Optimal Transport and Gradient Flows"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The first contribution of this article is the proof of the $\\Gamma$-convergence of the entropic regularized optimal transport problem towards the Monge-Kantorovich problem for the squared Euclidean norm cost function."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Math. Anal."
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143983679"
                        ],
                        "name": "Gao Huang",
                        "slug": "Gao-Huang",
                        "structuredName": {
                            "firstName": "Gao",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gao Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144993411"
                        ],
                        "name": "Chuan Guo",
                        "slug": "Chuan-Guo",
                        "structuredName": {
                            "firstName": "Chuan",
                            "lastName": "Guo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chuan Guo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1940272"
                        ],
                        "name": "Matt J. Kusner",
                        "slug": "Matt-J.-Kusner",
                        "structuredName": {
                            "firstName": "Matt",
                            "lastName": "Kusner",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matt J. Kusner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117103358"
                        ],
                        "name": "Yu Sun",
                        "slug": "Yu-Sun",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145757665"
                        ],
                        "name": "Fei Sha",
                        "slug": "Fei-Sha",
                        "structuredName": {
                            "firstName": "Fei",
                            "lastName": "Sha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fei Sha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7446832"
                        ],
                        "name": "Kilian Q. Weinberger",
                        "slug": "Kilian-Q.-Weinberger",
                        "structuredName": {
                            "firstName": "Kilian",
                            "lastName": "Weinberger",
                            "middleNames": [
                                "Q."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kilian Q. Weinberger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 135
                            }
                        ],
                        "text": "The benefits of this regularization has opened the path to many applications of the Wasserstein distance in relevant learning problems [8, 13, 18, 28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1762262,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a896b4fd83aeb55a58b2a7313ee6e10576dbf28a",
            "isKey": false,
            "numCitedBy": 126,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, a new document metric called the word mover's distance (WMD) has been proposed with unprecedented results on kNN-based document classification. The WMD elevates high-quality word embeddings to a document metric by formulating the distance between two documents as an optimal transport problem between the embedded words. However, the document distances are entirely un-supervised and lack a mechanism to incorporate supervision when available. In this paper we propose an efficient technique to learn a supervised metric, which we call the Supervised-WMD (S-WMD) metric. The supervised training minimizes the stochastic leave-one-out nearest neighbor classification error on a per-document level by updating an affine transformation of the underlying word embedding space and a word-imporance weight vector. As the gradient of the original WMD distance would result in an inefficient nested optimization problem, we provide an arbitrarily close approximation that results in a practical and efficient update rule. We evaluate S-WMD on eight real-world text classification tasks on which it consistently outperforms almost all of our 26 competitive baselines."
            },
            "slug": "Supervised-Word-Mover's-Distance-Huang-Guo",
            "title": {
                "fragments": [],
                "text": "Supervised Word Mover's Distance"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes an efficient technique to learn a supervised metric, which it is called the Supervised-WMD (S-W MD) metric, and provides an arbitrarily close approximation of the original WMD distance that results in a practical and efficient update rule."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711979"
                        ],
                        "name": "Marco Cuturi",
                        "slug": "Marco-Cuturi",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Cuturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Cuturi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701800"
                        ],
                        "name": "A. Doucet",
                        "slug": "A.-Doucet",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Doucet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Doucet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 242
                            }
                        ],
                        "text": "One major advantage of regularizing the optimal transport problem is that it becomes solvable efficiently using Sinkhorn\u2019s algorithm [32] (when dealing with discrete measures), and leads to a differentiable loss function (as first noticed in [9, 10])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16786361,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c697fd0b73ec7aaeb2e75cb89cf4b8b020dd9556",
            "isKey": false,
            "numCitedBy": 515,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "We present new algorithms to compute the mean of a set of empirical probability measures under the optimal transport metric. This mean, known as the Wasserstein barycenter, is the measure that minimizes the sum of its Wasserstein distances to each element in that set. We propose two original algorithms to compute Wasserstein barycenters that build upon the subgradient method. A direct implementation of these algorithms is, however, too costly because it would require the repeated resolution of large primal and dual optimal transport problems to compute subgradients. Extending the work of Cuturi (2013), we propose to smooth the Wasserstein distance used in the definition of Wasserstein barycenters with an entropic regularizer and recover in doing so a strictly convex objective whose gradients can be computed for a considerably cheaper computational cost using matrix scaling algorithms. We use these algorithms to visualize a large family of images and to solve a constrained clustering problem."
            },
            "slug": "Fast-Computation-of-Wasserstein-Barycenters-Cuturi-Doucet",
            "title": {
                "fragments": [],
                "text": "Fast Computation of Wasserstein Barycenters"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The Wasserstein distance is proposed to be smoothed with an entropic regularizer and recover in doing so a strictly convex objective whose gradients can be computed for a considerably cheaper computational cost using matrix scaling algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1940272"
                        ],
                        "name": "Matt J. Kusner",
                        "slug": "Matt-J.-Kusner",
                        "structuredName": {
                            "firstName": "Matt",
                            "lastName": "Kusner",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matt J. Kusner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117103358"
                        ],
                        "name": "Yu Sun",
                        "slug": "Yu-Sun",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1971973"
                        ],
                        "name": "Nicholas I. Kolkin",
                        "slug": "Nicholas-I.-Kolkin",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Kolkin",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicholas I. Kolkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7446832"
                        ],
                        "name": "Kilian Q. Weinberger",
                        "slug": "Kilian-Q.-Weinberger",
                        "structuredName": {
                            "firstName": "Kilian",
                            "lastName": "Weinberger",
                            "middleNames": [
                                "Q."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kilian Q. Weinberger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 125
                            }
                        ],
                        "text": "It is however important to realize that for large scale and high dimensional learning applications, empirical considerations [9, 22, 13] suggest that, unlike relevant applications of the same scheme in graphics [33], a relatively strong regularization\u2014a large \u03b5\u2014leads not only to more stable results but also faster convergence, so that the value for L can be set quite low."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14674248,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66021a920001bc3e6258bffe7076d647614147b7",
            "isKey": false,
            "numCitedBy": 1516,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "We present the Word Mover's Distance (WMD), a novel distance function between text documents. Our work is based on recent results in word embeddings that learn semantically meaningful representations for words from local cooccurrences in sentences. The WMD distance measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to \"travel\" to reach the embedded words of another document. We show that this distance metric can be cast as an instance of the Earth Mover's Distance, a well studied transportation problem for which several highly efficient solvers have been developed. Our metric has no hyperparameters and is straight-forward to implement. Further, we demonstrate on eight real world document classification data sets, in comparison with seven state-of-the-art baselines, that the WMD metric leads to unprecedented low k-nearest neighbor document classification error rates."
            },
            "slug": "From-Word-Embeddings-To-Document-Distances-Kusner-Sun",
            "title": {
                "fragments": [],
                "text": "From Word Embeddings To Document Distances"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is demonstrated on eight real world document classification data sets, in comparison with seven state-of-the-art baselines, that the Word Mover's Distance metric leads to unprecedented low k-nearest neighbor document classification error rates."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31889131"
                        ],
                        "name": "G. Sz\u00e9kely",
                        "slug": "G.-Sz\u00e9kely",
                        "structuredName": {
                            "firstName": "G\u00e1bor",
                            "lastName": "Sz\u00e9kely",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Sz\u00e9kely"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144466895"
                        ],
                        "name": "M. L. Rizzo",
                        "slug": "M.-L.-Rizzo",
                        "structuredName": {
                            "firstName": "Maria",
                            "lastName": "Rizzo",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. L. Rizzo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 47751,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ad5e91905a85d6f671c04a67779fd1377e86d199",
            "isKey": false,
            "numCitedBy": 229,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new nonparametric test for equality of two or more multivariate distributions based on Euclidean distance between sample elements. Several consistent tests for comparing multivariate distributions can be developed from the underlying theoretical results. The test procedure for the multisample problem is developed and applied for testing the composite hypothesis of equal distributions, when distributions are unspecified. The proposed test is universally consistent against all fixed alternatives (not necessarily continuous) with finite second moments. The test is implemented by conditioning on the pooled sample to obtain an approximate permutation test, which is distribution free. Our Monte Carlo power study suggests that the new test may be much more sensitive than tests based on nearest neighbors against several classes of alternatives, and performs particularly well in high dimension. Computational complexity of our test procedure is independent of dimension and number of populations sampled. The test is applied in a high dimensional problem, testing microarray data from cancer samples."
            },
            "slug": "TESTING-FOR-EQUAL-DISTRIBUTIONS-IN-HIGH-DIMENSION-Sz\u00e9kely-Rizzo",
            "title": {
                "fragments": [],
                "text": "TESTING FOR EQUAL DISTRIBUTIONS IN HIGH DIMENSION"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51071691"
                        ],
                        "name": "J. Weed",
                        "slug": "J.-Weed",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Weed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144570279"
                        ],
                        "name": "F. Bach",
                        "slug": "F.-Bach",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Bach",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Bach"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 87
                            }
                        ],
                        "text": "In contrast, the unregularized OT loss suffers from a sample complexity of O(1/n), see [37] for a recent account on this point."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 51919254,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "1fc6290fa3e6784501ca67cbef33a6a8edcbdb9e",
            "isKey": false,
            "numCitedBy": 265,
            "numCiting": 84,
            "paperAbstract": {
                "fragments": [],
                "text": "The Wasserstein distance between two probability measures on a metric space is a measure of closeness with applications in statistics, probability, and machine learning. In this work, we consider the fundamental question of how quickly the empirical measure obtained from $n$ independent samples from $\\mu$ approaches $\\mu$ in the Wasserstein distance of any order. We prove sharp asymptotic and finite-sample results for this rate of convergence for general measures on general compact metric spaces. Our finite-sample results show the existence of multi-scale behavior, where measures can exhibit radically different rates of convergence as $n$ grows."
            },
            "slug": "Sharp-asymptotic-and-finite-sample-rates-of-of-in-Weed-Bach",
            "title": {
                "fragments": [],
                "text": "Sharp asymptotic and finite-sample rates of convergence of empirical measures in Wasserstein distance"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work considers the fundamental question of how quickly the empirical measure obtained from independent samples from $\\mu$ approaches $n$ in the Wasserstein distance of any order and proves sharp asymptotic and finite-sample results for this rate of convergence for general measures on general compact metric spaces."
            },
            "venue": {
                "fragments": [],
                "text": "Bernoulli"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708497"
                        ],
                        "name": "A. Gretton",
                        "slug": "A.-Gretton",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Gretton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gretton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704422"
                        ],
                        "name": "K. Borgwardt",
                        "slug": "K.-Borgwardt",
                        "structuredName": {
                            "firstName": "Karsten",
                            "lastName": "Borgwardt",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Borgwardt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733256"
                        ],
                        "name": "M. Rasch",
                        "slug": "M.-Rasch",
                        "structuredName": {
                            "firstName": "Malte",
                            "lastName": "Rasch",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rasch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 63
                            }
                        ],
                        "text": "The latter define the class of Maximum Mean Discrepency losses [16] defined by"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 134
                            }
                        ],
                        "text": "Geometry was also explicitly considered when trying to minimize a flexible metric between distributions: the maximal mean discrepancy [16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1993257,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "9bca4d7b932e0854c3325f1578cfd17341dd8ea8",
            "isKey": false,
            "numCitedBy": 1547,
            "numCiting": 93,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose two statistical tests to determine if two samples are from different distributions. Our test statistic is in both cases the distance between the means of the two samples mapped into a reproducing kernel Hilbert space (RKHS). The first test is based on a large deviation bound for the test statistic, while the second is based on the asymptotic distribution of this statistic. The test statistic can be computed in O(m2) time. We apply our approach to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where our test performs strongly. We also demonstrate excellent performance when comparing distributions over graphs, for which no alternative tests currently exist."
            },
            "slug": "A-Kernel-Method-for-the-Two-Sample-Problem-Gretton-Borgwardt",
            "title": {
                "fragments": [],
                "text": "A Kernel Method for the Two-Sample-Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "This work proposes two statistical tests to determine if two samples are from different distributions, and applies this approach to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where the test performs strongly."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2493456"
                        ],
                        "name": "F. Bassetti",
                        "slug": "F.-Bassetti",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Bassetti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Bassetti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2328362"
                        ],
                        "name": "A. Bodini",
                        "slug": "A.-Bodini",
                        "structuredName": {
                            "firstName": "Antonella",
                            "lastName": "Bodini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bodini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2735098"
                        ],
                        "name": "E. Regazzini",
                        "slug": "E.-Regazzini",
                        "structuredName": {
                            "firstName": "Eugenio",
                            "lastName": "Regazzini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Regazzini"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 66
                            }
                        ],
                        "text": "To remedy these issues, and in line with several recent proposals [2, 26, 4, 1], we propose to shift away from information divergence based methods (among which the MLE) and consider instead the geometry of optimal transport [36, 31] to define such a fitting criterion."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 112
                            }
                        ],
                        "text": "Although the use of Wasserstein metrics for inference in generative models was considered over ten years ago in [2], that development remained exclusively theoretical until a recent wave of papers managed to implement that idea more or less faithfully using several workarounds: entropic regularization over a discrete space [26], approximate Bayesian computations [4] and a neural network parameterization of the dual potential arising from the dual OT problem when considering the 1-Wasserstein distance [1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122521102,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "59350bfcaafb7e2f943db50558965e7df62a3ba8",
            "isKey": false,
            "numCitedBy": 71,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-minimum-Kantorovich-distance-estimators-Bassetti-Bodini",
            "title": {
                "fragments": [],
                "text": "On minimum Kantorovich distance estimators"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3313411"
                        ],
                        "name": "Bharath K. Sriperumbudur",
                        "slug": "Bharath-K.-Sriperumbudur",
                        "structuredName": {
                            "firstName": "Bharath",
                            "lastName": "Sriperumbudur",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bharath K. Sriperumbudur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693668"
                        ],
                        "name": "K. Fukumizu",
                        "slug": "K.-Fukumizu",
                        "structuredName": {
                            "firstName": "Kenji",
                            "lastName": "Fukumizu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukumizu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708497"
                        ],
                        "name": "A. Gretton",
                        "slug": "A.-Gretton",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Gretton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gretton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30927799"
                        ],
                        "name": "B. Schoelkopf",
                        "slug": "B.-Schoelkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Schoelkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schoelkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725533"
                        ],
                        "name": "G. Lanckriet",
                        "slug": "G.-Lanckriet",
                        "structuredName": {
                            "firstName": "Gert",
                            "lastName": "Lanckriet",
                            "middleNames": [
                                "R.",
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lanckriet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 2
                            }
                        ],
                        "text": "g [34]), namely by considering a dual norm L(\u03bc, \u03bd) = ||\u03bc \u2212 \u03bd||B where ||\u03be||B = sup {\u222b X h(x)d\u03be(x) ; h \u2208 B } ."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14221171,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "7a9b9857eab95db5818889e857e356379bc9f603",
            "isKey": false,
            "numCitedBy": 217,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "Given two probability measures, P and Q defined on a measurable space, S, the integral probability metric (IPM) is defined as \u03b3F(P,Q) = sup {\u2223\u2223\u2223\u2223 \u222b"
            },
            "slug": "On-the-empirical-estimation-of-integral-probability-Sriperumbudur-Fukumizu",
            "title": {
                "fragments": [],
                "text": "On the empirical estimation of integral probability metrics"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "Given two probability measures, P and Q defined on a measurable space, S, the integral probability metric (IPM) is defined as \u03b3F(P,Q) = sup {\u2223\u2223 \u2223\u2226\u2226 \u2223 \u222b}."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "Image generating models such as GAN [15] or VAE [21] have become popular in recent years."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 179
                            }
                        ],
                        "text": "(1)\nWhile we focus here for simplicity on the case of deterministic encoding functions g\u03b8 between \u03b6 and \u00b5\u03b8, our method extends to more general probabilistic generative models, such as VAE [21]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 203
                            }
                        ],
                        "text": "Since the dataset is relatively simple, learning the cost is superfluous here and we use the ground cost c(x, y) = ||x\u2212 y||2, which is sufficient for these low resolution images and also the baseline in [21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "Major approaches include variational autoencoders (VAE) [21], generative adversarial networks (GAN) [15] and several more variations including combinations of both [23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 159
                            }
                        ],
                        "text": "Both methods require a second network for the training of the generative network (an adversial network in the case of GANs, an encoding network in the case of VAEs)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 184
                            }
                        ],
                        "text": "While we focus here for simplicity on the case of deterministic encoding functions g\u03b8 between \u03b6 and \u03bc\u03b8, our method extends to more general probabilistic generative models, such as VAE [21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Autoencoding variational bayes"
            },
            "venue": {
                "fragments": [],
                "text": "arXiv preprint arXiv:1312.6114,"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102566654"
                        ],
                        "name": "Richard Sinkhorn",
                        "slug": "Richard-Sinkhorn",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sinkhorn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard Sinkhorn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 133
                            }
                        ],
                        "text": "One major advantage of regularizing the optimal transport problem is that it becomes solvable efficiently using Sinkhorn\u2019s algorithm [32] (when dealing with discrete measures), and leads to a differentiable loss function (as first noticed in [9, 10])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 120846714,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "441f33fab0614fa0696be54a046cbc692b7e70a2",
            "isKey": false,
            "numCitedBy": 829,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Relationship-Between-Arbitrary-Positive-Matrices-Sinkhorn",
            "title": {
                "fragments": [],
                "text": "A Relationship Between Arbitrary Positive Matrices and Doubly Stochastic Matrices"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "Formula (3) corresponds to the celebrated Kantorovitch formulation [19] of OT (see [31] for a detailed account on the theory)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the transfer of masses (in Russian)"
            },
            "venue": {
                "fragments": [],
                "text": "Doklady Akademii Nauk,"
            },
            "year": 1942
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 83
                            }
                        ],
                        "text": "Formula (3) corresponds to the celebrated Kantorovitch formulation [19] of OT (see [31] for a detailed account on the theory)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 225
                            }
                        ],
                        "text": "To remedy these issues, and in line with several recent proposals [2, 26, 4, 1], we propose to shift away from information divergence based methods (among which the MLE) and consider instead the geometry of optimal transport [36, 31] to define such a fitting criterion."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimal Transport for applied mathematicians, volume 87 of Progress in Nonlinear Differential Equations and their applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 225
                            }
                        ],
                        "text": "To remedy these issues, and in line with several recent proposals [2, 26, 4, 1], we propose to shift away from information divergence based methods (among which the MLE) and consider instead the geometry of optimal transport [36, 31] to define such a fitting criterion."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Topics in C. Transportation"
            },
            "venue": {
                "fragments": [],
                "text": "Graduate studies in Math. AMS,"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 211
                            }
                        ],
                        "text": "It is however important to realize that for large scale and high dimensional learning applications, empirical considerations [9, 22, 13] suggest that, unlike relevant applications of the same scheme in graphics [33], a relatively strong regularization\u2014a large \u03b5\u2014leads not only to more stable results but also faster convergence, so that the value for L can be set quite low."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Convolutional Wasserstein distances: Efficient optimal transportation on geometric domains"
            },
            "venue": {
                "fragments": [],
                "text": "ACM Transactions on Graphics (Proc. SIGGRAPH 2015),"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "proved techniques for training GANs"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems"
            },
            "year": 2016
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 18,
            "methodology": 21,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 37,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Learning-Generative-Models-with-Sinkhorn-Genevay-Peyr\u00e9/a1bc7d90564c342beb75cedf36fd921de89d94ad?sort=total-citations"
}