{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3227843"
                        ],
                        "name": "M. Meteer",
                        "slug": "M.-Meteer",
                        "structuredName": {
                            "firstName": "Marie",
                            "lastName": "Meteer",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Meteer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35442155"
                        ],
                        "name": "R. Schwartz",
                        "slug": "R.-Schwartz",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732071"
                        ],
                        "name": "R. Weischedel",
                        "slug": "R.-Weischedel",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Weischedel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Weischedel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14999150,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "493b63168e2778d64934f9027a3dc74f8f9f746f",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We report here on our experiments with POST (Part of Speech Tagger) to address problems of ambiguity and of understanding unknown words. Part of speech tagging, perse, is a well understood problem. Our paper reports experiments in three important areas: handling unknown words, limiting the size of the training set, and returning a set of the most likely tags for each word rather than a single tag. We describe the algorithms that we used and the specific results of our experiments on Wall Street Journal articles and on MUC terrorist messages."
            },
            "slug": "POST:-Using-Probabilities-in-Language-Processing-Meteer-Schwartz",
            "title": {
                "fragments": [],
                "text": "POST: Using Probabilities in Language Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper reports experiments in three important areas: handling unknown words, limiting the size of the training set, and returning a set of the most likely tags for each word rather than a single tag."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32326549"
                        ],
                        "name": "J. Kupiec",
                        "slug": "J.-Kupiec",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Kupiec",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kupiec"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 97
                            }
                        ],
                        "text": "Probabalistic predictions of a word's category can be made by analyzing su xes in untagged text [Kupiec, 1992, Meteer et al., 1991]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 131
                            }
                        ],
                        "text": "In this manner, the vocabulary of 50,000 words in the Brown corpus can be reduced to approximately 400 distinct ambiguity classes [Kupiec, 1992]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 150
                            }
                        ],
                        "text": "\u2026maintains a mapping from states i to non-zero symbol probabilities and simply avoids, in the inner iteration, computing products which must be zero [Kupiec, 1992].\nstates are indistinguishable (in the sense that they had the same transition probabilities and the same symbol probabilities at\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 183
                            }
                        ],
                        "text": "As the resources required are simply a lexicon and a suitably large sample of ordinary text, taggers can be built with minimal e ort, even for other languages, such as French (e.g., [Kupiec, 1992])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Reusable The e ort required to retarget a tagger to new corpora, new tagsets, and new languages should be minimal."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62680996,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "352dbd26580856ba4b9877d43aeba304343af66d",
            "isKey": false,
            "numCitedBy": 492,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Robust-part-of-speech-tagging-using-a-hidden-Markov-Kupiec",
            "title": {
                "fragments": [],
                "text": "Robust part-of-speech tagging using a hidden Markov model"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2687013"
                        ],
                        "name": "A. Derouault",
                        "slug": "A.-Derouault",
                        "structuredName": {
                            "firstName": "Anne-Marie",
                            "lastName": "Derouault",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Derouault"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686820"
                        ],
                        "name": "B. M\u00e9rialdo",
                        "slug": "B.-M\u00e9rialdo",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "M\u00e9rialdo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. M\u00e9rialdo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 60
                            }
                        ],
                        "text": "Derouault and Merialdo use a bootstrap method for training [Derouault and Merialdo, 1986]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3173459,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "85da46e840ab8ea85759f4308aa195bde2aaadab",
            "isKey": false,
            "numCitedBy": 126,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper relates different kinds of language modeling methods that can be applied to the linguistic decoding part of a speech recognition system with a very large vocabulary. These models are studied experimentally on a pseudophonetic input arising from French stenotypy. We propose a model which combines the advantages of a statistical modeling with information theoretic tools, and those of a grammatical approach."
            },
            "slug": "Natural-Language-Modeling-for-Phoneme-to-Text-Derouault-M\u00e9rialdo",
            "title": {
                "fragments": [],
                "text": "Natural Language Modeling for Phoneme-to-Text Transcription"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This paper relates different kinds of language modeling methods that can be applied to the linguistic decoding part of a speech recognition system with a very large vocabulary and proposes a model which combines the advantages of a statistical modeling with information theoretic tools, and those of a grammatical approach."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32326549"
                        ],
                        "name": "J. Kupiec",
                        "slug": "J.-Kupiec",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Kupiec",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kupiec"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2094488,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14bb02f0a0555b3d9aaf8f3f130516dd70bf19d9",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper describes refinements that are currently being investigated in a model for part-of-speech assignment to words in unrestricted text. The model has the advantage that a pre-tagged training corpus is not required. Words are represented by equivalence classes to reduce the number of parameters required and provide an essentially vocabulary-independent model. State chains are used to model selective higher-order conditioning in the model, which obviates the proliferation of parameters attendant in uniformly higher-order models. The structure of the state chains is based on both an analysis of errors and linguistic knowledge. Examples show how word dependency across phrases can be modeled."
            },
            "slug": "Augmenting-a-Hidden-Markov-Model-for-Word-Tagging-Kupiec",
            "title": {
                "fragments": [],
                "text": "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging"
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686820"
                        ],
                        "name": "B. M\u00e9rialdo",
                        "slug": "B.-M\u00e9rialdo",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "M\u00e9rialdo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. M\u00e9rialdo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 90
                            }
                        ],
                        "text": "This level of accuracy is comparable to the best achieved by other taggers [Church, 1988, Merialdo, 1991]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 61014458,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8a9b6828c5e4339025bb78af6b025d21b4830800",
            "isKey": false,
            "numCitedBy": 99,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Experiments on the use of a probabilistic model to tag English text, that is, to assign to each word the correct tag (part of speech) in the context of the sentence, are presented. A simple triclass Markov model is used, and the best way to estimate the parameters of this model, depending on the kind and amount of training data that is provided, is found. Two approaches are compared: the use of text that has been tagged by hand and comparing relative frequency counts; and use text without tags and training the model as a hidden Markov process, according to a maximum likelihood principle. Experiments show that the best training is obtained by using as much tagged text as is available, a maximum likelihood training may improve the accuracy of the tagging.<<ETX>>"
            },
            "slug": "Tagging-text-with-a-probabilistic-model-M\u00e9rialdo",
            "title": {
                "fragments": [],
                "text": "Tagging text with a probabilistic model"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Experiments show that the best training is obtained by using as much tagged text as is available, and a maximum likelihood training may improve the accuracy of the tagging."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings] ICASSP 91: 1991 International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3166885,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10",
            "isKey": false,
            "numCitedBy": 1058,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A program that tags each word in an input sentence with the most likely part of speech has been written. The program uses a linear-time dynamic programming algorithm to find an assignment of parts of speech to words that optimizes the product of (a) lexical probabilities (probability of observing part of speech i given word i) and (b) contextual probabilities (probability of observing part of speech i given n following parts of speech). Program performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct.<<ETX>>"
            },
            "slug": "A-Stochastic-Parts-Program-and-Noun-Phrase-Parser-Church",
            "title": {
                "fragments": [],
                "text": "A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A program that tags each word in an input sentence with the most likely part of speech has been written and performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32326549"
                        ],
                        "name": "J. Kupiec",
                        "slug": "J.-Kupiec",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Kupiec",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kupiec"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14679951,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "343c8af478f7703459b0e390e888efe723f15e31",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This article describes two complementary models that represent dependencies between words in local and non-local contexts. The type of local dependencies considered are sequences of part of speech categories for words. The non-local context of word dependency considered here is that of word recurrence, which is typical in a text. Both are models of phenomena that are to a reasonable extent domain independent, and thus are useful for doing prediction in systems using large vocabularies."
            },
            "slug": "Probabilistic-Models-of-Short-and-Long-Distance-in-Kupiec",
            "title": {
                "fragments": [],
                "text": "Probabilistic Models of Short and Long Distance Word Dependencies in Running Text"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "Two complementary models that represent dependencies between words in local and non-local contexts are described, which are useful for doing prediction in systems using large vocabularies."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 56
                            }
                        ],
                        "text": "Jelinek has used this method for training a text tagger [Jelinek, 1985]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60744956,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d64cae6538bb3b8e595007585477a9fdef106602",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "A language model is a conceptual device which, given a string of past words, provides an estimate of the probability that any given word from an allowed vocabulary will follow the string. In speech recognition, a language model is used to direct the hypothesis search for the sentence that was spoken. In fact, its probability assignments are a factor in evaluating the likelihood that any word string was uttered, and in ordering the word string hypotheses to be examined (1). Language models are also useful in text encoding for compression or transmission, in character recognition of printed or handwritten text, etc. Ideally, the probability assigned by a language model to any word should depend on the entire past sequence of words. However, it is practically impossible to implement such a dependence: 1) The probability estimate involved would have to be extracted from a giant sample of training text containing billions of words; 2) If the probabilities were extractable, sufficient space in computer memory could not be found to store them."
            },
            "slug": "Markov-Source-Modeling-of-Text-Generation-Jelinek",
            "title": {
                "fragments": [],
                "text": "Markov Source Modeling of Text Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A language model is a conceptual device which, given a string of past words, provides an estimate of the probability that any given word from an allowed vocabulary will follow the string."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2466657"
                        ],
                        "name": "K. Koskenniemi",
                        "slug": "K.-Koskenniemi",
                        "structuredName": {
                            "firstName": "Kimmo",
                            "lastName": "Koskenniemi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Koskenniemi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 97
                            }
                        ],
                        "text": "More recently, Koskenniemi also used a rule-based approach implemented with nite-state machines [Koskenniemi, 1990]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 19487388,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "1ef69ff7760477d26d6c8d06f07dd021d60f9413",
            "isKey": false,
            "numCitedBy": 119,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "A language-independent method of finite-state surface syntactic parsing and word-disambiguation is discussed. Input sentences are represented as finite-state networks already containing all possible roles and interpretations of its units. Also syntactic constraint rules are represented as finite-state machines where each constraint excludes certain types of ungrammatical readings. The whole grammar is an intersection of its constraint rules and excludes all ungrammatical possibilities leaving the correct interpretation(s) of the sentence. The method is being tested for Finnish, Swedish and English."
            },
            "slug": "Finite-State-Parsing-and-Disambiguation-Koskenniemi",
            "title": {
                "fragments": [],
                "text": "Finite-State Parsing and Disambiguation"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A language-independent method of finite-state surface syntactic parsing and word-disambiguation is discussed, which excludes all ungrammatical possibilities leaving the correct interpretation of the sentence."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7510969"
                        ],
                        "name": "S. DeRose",
                        "slug": "S.-DeRose",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "DeRose",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. DeRose"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1275545,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3a8de92b304729f15d9bd6c3d22a56ab9b31e212",
            "isKey": false,
            "numCitedBy": 430,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Several algorithms have been developed in the past that attempt to resolve categorial ambiguities in natural language text without recourse to syntactic or semantic level information. An innovative method (called \"CLAWS\") was recently developed by those working with the Lancaster-Oslo/Bergen Corpus of British English. This algorithm uses a systematic calculation based upon the probabilities of co-occurrence of particular tags. Its accuracy is high, but it is very slow, and it has been manually augmented in a number of ways. The effects upon accuracy of this manual augmentation are not individually known.The current paper presents an algorithm for disambiguation that is similar to CLAWS but that operates in linear rather than in exponential time and space, and which minimizes the unsystematic augments. Tests of the algorithm using the million words of the Brown Standard Corpus of English are reported; the overall accuracy is 96%. This algorithm can provide a fast and accurate front end to any parsing or natural language processing system for English."
            },
            "slug": "Grammatical-Category-Disambiguation-by-Statistical-DeRose",
            "title": {
                "fragments": [],
                "text": "Grammatical Category Disambiguation by Statistical Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An algorithm for disambiguation that is similar to CLAWS but that operates in linear rather than in exponential time and space, and which minimizes the unsystematic augments is presented."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716902"
                        ],
                        "name": "Marti A. Hearst",
                        "slug": "Marti-A.-Hearst",
                        "structuredName": {
                            "firstName": "Marti",
                            "lastName": "Hearst",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marti A. Hearst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8112529,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f041c48820535bfaf374b162cfc6acfc52ff87d",
            "isKey": false,
            "numCitedBy": 149,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an accurate, relatively inexpensive method for the disambiguation of noun homographs using large text corpora. The algorithm checks the context surrounding the target noun against that of previously observed instances and chooses the sense for which the most evidence is found, where evidence consists of a set of orthographic, syntactic, and lexical features. Because the sense distinctions made are coarse, the disambiguation can be accomplished without the expense of knowledge bases or inference mechanisms. An implementation of the algorithm is described which, starting with a small set of hand-labeled instances, improves its results automatically via unsupervised training. The approach is compared to other attempts at homograph disambiguation using both machine readable dictionaries and unrestricted text and the use of training instances is determined to be a crucial di erence."
            },
            "slug": "Noun-Homograph-Disambiguation-Using-Local-Context-Hearst",
            "title": {
                "fragments": [],
                "text": "Noun Homograph Disambiguation Using Local Context in Large Text Corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "An accurate, relatively inexpensive method for the disambiguation of noun homographs using large text corpora using both machine readable dictionaries and unrestricted text and the use of training instances is determined to be a crucial di erence."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759195"
                        ],
                        "name": "S. Levinson",
                        "slug": "S.-Levinson",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Levinson",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Levinson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34830449"
                        ],
                        "name": "M. Sondhi",
                        "slug": "M.-Sondhi",
                        "structuredName": {
                            "firstName": "Man",
                            "lastName": "Sondhi",
                            "middleNames": [
                                "Mohan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sondhi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 46254718,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "090f3ea5bc188bbb03aec02aba9ed9c7b38ff870",
            "isKey": false,
            "numCitedBy": 1082,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present several of the salient theoretical and practical issues associated with modeling a speech signal as a probabilistic function of a (hidden) Markov chain. First we give a concise review of the literature with emphasis on the Baum-Welch algorithm. This is followed by a detailed discussion of three issues not treated in the literature: alternatives to the Baum-Welch algorithm; critical facets of the implementation of the algorithms, with emphasis on their numerical properties; and behavior of Markov models on certain artificial but realistic problems. Special attention is given to a particular class of Markov models, which we call \u201cleft-to-right\u201d models. This class of models is especially appropriate for isolated word recognition. The results of the application of these methods to an isolated word, speaker-independent speech recognition experiment are given in a companion paper."
            },
            "slug": "An-introduction-to-the-application-of-the-theory-of-Levinson-Rabiner",
            "title": {
                "fragments": [],
                "text": "An introduction to the application of the theory of probabilistic functions of a Markov process to automatic speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This paper presents several of the salient theoretical and practical issues associated with modeling a speech signal as a probabilistic function of a (hidden) Markov chain, and focuses on a particular class of Markov models, which are especially appropriate for isolated word recognition."
            },
            "venue": {
                "fragments": [],
                "text": "The Bell System Technical Journal"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3133664"
                        ],
                        "name": "D. Cutting",
                        "slug": "D.-Cutting",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Cutting",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cutting"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34165212"
                        ],
                        "name": "Jan O. Pedersen",
                        "slug": "Jan-O.-Pedersen",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Pedersen",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan O. Pedersen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066217332"
                        ],
                        "name": "Per Christian Halvorsen",
                        "slug": "Per-Christian-Halvorsen",
                        "structuredName": {
                            "firstName": "Per",
                            "lastName": "Halvorsen",
                            "middleNames": [
                                "Christian"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Per Christian Halvorsen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59556,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0558d4b776eedb9956b0ec5c14b23f4f81870533",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "For almost all aspects of information access systems it is still the case that their optimal composition and functionality is hotly debated. Moreover, diierent application scenarios put diierent demands on individual components. It is therefore of the essence to be able to quickly build systems that permit exploration of diierent designs and implementation strategies. This paper presents a software implementation architecture for text retrieval systems that facilitates (a) functional modularization (b) mix-and-match combination of module implementations and (c) deenition of inter-module protocols. We show how an object-oriented approach easily accommodates this type of architecture. The design principles are exempliied by code examples in Common Lisp. Taken together these code examples constitute an operational retrieval system. The design principles and protocols implemented have also been instantiated in a large scale retrieval prototype in our research laboratory."
            },
            "slug": "An-object-oriented-architecture-for-text-retrieval-Cutting-Pedersen",
            "title": {
                "fragments": [],
                "text": "An object-oriented architecture for text retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A software implementation architecture for text retrieval systems that facilitates functional modularization, a mix-and-match combination of module implementations and a deenition of inter-module protocols is presented."
            },
            "venue": {
                "fragments": [],
                "text": "RIAO"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3142223"
                        ],
                        "name": "M. Lesk",
                        "slug": "M.-Lesk",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lesk",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lesk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2084325087"
                        ],
                        "name": "E. Schmidt",
                        "slug": "E.-Schmidt",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Schmidt",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Schmidt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7900881,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d9c4b8a9a40fdf274b1314a94f9a2ea1f6b7645",
            "isKey": false,
            "numCitedBy": 705,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Lex helps write programs whose control flow is directed by instances of regular expressions in the input stream. It is well suited for editor-script type transformations and for segmenting input in preparation for a parsing routine. Lex source is a table of regular expressions and corresponding program fragments. The table is translated to a program which reads an input stream, copying it to an output stream and partitioning the input into strings which match the given expressions. As each such string is recognized the corresponding program fragment is executed. The recognition of the expressions is performed by a deterministic finite automaton generated by Lex. The program fragments written by the user are executed in the order in which the corresponding regular expressions occur in the input stream. The lexical analysis programs written with Lex accept ambiguous specifications and choose the longest match possible at each input point. If necessary, substantial lookahead is performed on the input, but the input stream will be backed up to the end of the current partition, so that the user has general freedom to manipulate it. Lex can generate analyzers in either C or Ratfor, a language which can be translated automatically to portable Fortran. It is available on the PDP-11 UNIX, Honeywell GCOS, and IBM OS systems. This manual, however, will only discuss generating analyzers in C on the UNIX system, which is the only supported form of Lex under UNIX Version 7. Lex is designed to simplify interfacing with Yacc, for those with access to this compiler-compiler system."
            },
            "slug": "Lex\u2014a-lexical-analyzer-generator-Lesk-Schmidt",
            "title": {
                "fragments": [],
                "text": "Lex\u2014a lexical analyzer generator"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This manual will only discuss generating analyzers in C on the UNIX system, which is the only supported form of Lex under UNIX Version 7.0, and is designed to simplify interfacing with Yacc, for those with access to this compiler-compiler system."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715952"
                        ],
                        "name": "A. Aho",
                        "slug": "A.-Aho",
                        "structuredName": {
                            "firstName": "Alfred",
                            "lastName": "Aho",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Aho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144281449"
                        ],
                        "name": "R. Sethi",
                        "slug": "R.-Sethi",
                        "structuredName": {
                            "firstName": "Ravi",
                            "lastName": "Sethi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sethi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742391"
                        ],
                        "name": "J. Ullman",
                        "slug": "J.-Ullman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Ullman",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ullman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 42981739,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7f33d55d94e75a554251fe7dc07f1d7b4db8e1a",
            "isKey": false,
            "numCitedBy": 9130,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1 Introduction 1.1 Language Processors 1.2 The Structure of a Compiler 1.3 The Evolution of Programming Languages 1.4 The Science of Building a Compiler 1.5 Applications of Compiler Technology 1.6 Programming Language Basics 1.7 Summary of Chapter 1 1.8 References for Chapter 1 2 A Simple Syntax-Directed Translator 2.1 Introduction 2.2 Syntax Definition 2.3 Syntax-Directed Translation 2.4 Parsing 2.5 A Translator for Simple Expressions 2.6 Lexical Analysis 2.7 Symbol Tables 2.8 Intermediate Code Generation 2.9 Summary of Chapter 2 3 Lexical Analysis 3.1 The Role of the Lexical Analyzer 3.2 Input Buffering 3.3 Specification of Tokens 3.4 Recognition of Tokens 3.5 The Lexical-Analyzer Generator Lex 3.6 Finite Automata 3.7 From Regular Expressions to Automata 3.8 Design of a Lexical-Analyzer Generator 3.9 Optimization of DFA-Based Pattern Matchers 3.10 Summary of Chapter 3 3.11 References for Chapter 3 4 Syntax Analysis 4.1 Introduction 4.2 Context-Free Grammars 4.3 Writing a Grammar 4.4 Top-Down Parsing 4.5 Bottom-Up Parsing 4.6 Introduction to LR Parsing: Simple LR 4.7 More Powerful LR Parsers 4.8 Using Ambiguous Grammars 4.9 Parser Generators 4.10 Summary of Chapter 4 4.11 References for Chapter 4 5 Syntax-Directed Translation 5.1 Syntax-Directed Definitions 5.2 Evaluation Orders for SDD's 5.3 Applications of Syntax-Directed Translation 5.4 Syntax-Directed Translation Schemes 5.5 Implementing L-Attributed SDD's 5.6 Summary of Chapter 5 5.7 References for Chapter 5 6 Intermediate-Code Generation 6.1 Variants of Syntax Trees 6.2 Three-Address Code 6.3 Types and Declarations 6.4 Translation of Expressions 6.5 Type Checking 6.6 Control Flow 6.7 Backpatching 6.8 Switch-Statements 6.9 Intermediate Code for Procedures 6.10 Summary of Chapter 6 6.11 References for Chapter 6 7 Run-Time Environments 7.1 Storage Organization 7.2 Stack Allocation of Space 7.3 Access to Nonlocal Data on the Stack 7.4 Heap Management 7.5 Introduction to Garbage Collection 7.6 Introduction to Trace-Based Collection 7.7 Short-Pause Garbage Collection 7.8 Advanced Topics in Garbage Collection 7.9 Summary of Chapter 7 7.10 References for Chapter 7 8 Code Generation 8.1 Issues in the Design of a Code Generator 8.2 The Target Language 8.3 Addresses in the Target Code 8.4 Basic Blocks and Flow Graphs 8.5 Optimization of Basic Blocks 8.6 A Simple Code Generator 8.7 Peephole Optimization 8.8 Register Allocation and Assignment 8.9 Instruction Selection by Tree Rewriting 8.10 Optimal Code Generation for Expressions 8.11 Dynamic Programming Code-Generation 8.12 Summary of Chapter 8 8.13 References for Chapter 8 9 Machine-Independent Optimizations 9.1 The Principal Sources of Optimization 9.2 Introduction to Data-Flow Analysis 9.3 Foundations of Data-Flow Analysis 9.4 Constant Propagation 9.5 Partial-Redundancy Elimination 9.6 Loops in Flow Graphs 9.7 Region-Based Analysis 9.8 Symbolic Analysis 9.9 Summary of Chapter 9 9.10 References for Chapter 9 10 Instruction-Level Parallelism 10.1 Processor Architectures 10.2 Code-Scheduling Constraints 10.3 Basic-Block Scheduling 10.4 Global Code Scheduling 10.5 Software Pipelining 10.6 Summary of Chapter 10 10.7 References for Chapter 10 11 Optimizing for Parallelism and Locality 11.1 Basic Concepts 11.2 Matrix Multiply: An In-Depth Example 11.3 Iteration Spaces 11.4 Affine Array Indexes 11.5 Data Reuse 11.6 Array Data-Dependence Analysis 11.7 Finding Synchronization-Free Parallelism 11.8 Synchronization Between Parallel Loops 11.9 Pipelining 11.10 Locality Optimizations 11.11 Other Uses of Affine Transforms 11.12 Summary of Chapter 11 11.13 References for Chapter 11 12 Interprocedural Analysis 12.1 Basic Concepts 12.2 Why Interprocedural Analysis? 12.3 A Logical Representation of Data Flow 12.4 A Simple Pointer-Analysis Algorithm 12.5 Context-Insensitive Interprocedural Analysis 12.6 Context-Sensitive Pointer Analysis 12.7 Datalog Implementation by BDD's 12.8 Summary of Chapter 12 12.9 References for Chapter 12 A A Complete Front End A.1 The Source Language A.2 Main A.3 Lexical Analyzer A.4 Symbol Tables and Types A.5 Intermediate Code for Expressions A.6 Jumping Code for Boolean Expressions A.7 Intermediate Code for Statements A.8 Parser A.9 Creating the Front End B Finding Linearly Independent Solutions Index"
            },
            "slug": "Compilers:-Principles,-Techniques,-and-Tools-Aho-Sethi",
            "title": {
                "fragments": [],
                "text": "Compilers: Principles, Techniques, and Tools"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "This book discusses the design of a Code Generator, the role of the Lexical Analyzer, and other topics related to code generation and optimization."
            },
            "venue": {
                "fragments": [],
                "text": "Addison-Wesley series in computer science / World student series edition"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145500689"
                        ],
                        "name": "A. Viterbi",
                        "slug": "A.-Viterbi",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Viterbi",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Viterbi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15843983,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "145c0b53514b02bdc3dadfb2e1cea124f2abd99b",
            "isKey": false,
            "numCitedBy": 5209,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates above R_{0} , the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R_{0} and whose performance bears certain similarities to that of sequential decoding algorithms."
            },
            "slug": "Error-bounds-for-convolutional-codes-and-an-optimum-Viterbi",
            "title": {
                "fragments": [],
                "text": "Error bounds for convolutional codes and an asymptotically optimum decoding algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R_{0} and whose performance bears certain similarities to that of sequential decoding algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2091810890"
                        ],
                        "name": "Shirley Dex",
                        "slug": "Shirley-Dex",
                        "structuredName": {
                            "firstName": "Shirley",
                            "lastName": "Dex",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shirley Dex"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 41
                            }
                        ],
                        "text": "The system is implemented in CommonLisp [Steele, 1990]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62267652,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5240a29f967fb85a84fb5609e453a58970ca73f5",
            "isKey": false,
            "numCitedBy": 17515,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "1. If 49 is divisible by 7, then 7 is a factor of 49. 2. An odd number multiplied by an odd number is odd. 3. An even number divided by an even number is even. 4. The number 123,456 is divisible by 8. 5. If a divides 4 and a divides 6, then a divides 24. 6. If a divides d, then divides d. 2 a 7. The sum of three consecutive numbers is divisible by 3. 8. The product of any three consecutive integers is divisible by 6. 9. The product of any four consecutive numbers is divisible by 24. 10. If a number is divisible by 3, then every digit of that number is divisible by 3."
            },
            "slug": "JR-\u65c5\u5ba2\u8ca9\u58f2\u7dcf\u5408\u30b7\u30b9\u30c6\u30e0\uff08\u30de\u30eb\u30b9\uff09\u306b\u304a\u3051\u308b\u904b\u7528\u53ca\u3073\u7ba1\u7406\u306b\u3064\u3044\u3066-Dex",
            "title": {
                "fragments": [],
                "text": "JR \u65c5\u5ba2\u8ca9\u58f2\u7dcf\u5408\u30b7\u30b9\u30c6\u30e0\uff08\u30de\u30eb\u30b9\uff09\u306b\u304a\u3051\u308b\u904b\u7528\u53ca\u3073\u7ba1\u7406\u306b\u3064\u3044\u3066"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A dictionary definition of divisible number: a number is divisible by a number so that every digit of that number isdivisible by 3."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717349"
                        ],
                        "name": "D. Knuth",
                        "slug": "D.-Knuth",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Knuth",
                            "middleNames": [
                                "Ervin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Knuth"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 88
                            }
                        ],
                        "text": "Dictionaries and su x tables are both e ciently implementable as letter trees, or tries [Knuth, 1973], which require that each character of a word be examined only once during a lookup."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 65140997,
            "fieldsOfStudy": [
                "Engineering",
                "Physics"
            ],
            "id": "748f27587c37d8b0882a20692967901b81429fce",
            "isKey": false,
            "numCitedBy": 11594,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A fuel pin hold-down and spacing apparatus for use in nuclear reactors is disclosed. Fuel pins forming a hexagonal array are spaced apart from each other and held-down at their lower end, securely attached at two places along their length to one of a plurality of vertically disposed parallel plates arranged in horizontally spaced rows. These plates are in turn spaced apart from each other and held together by a combination of spacing and fastening means. The arrangement of this invention provides a strong vibration free hold-down mechanism while avoiding a large pressure drop to the flow of coolant fluid. This apparatus is particularly useful in connection with liquid cooled reactors such as liquid metal cooled fast breeder reactors."
            },
            "slug": "The-Art-of-Computer-Programming-Knuth",
            "title": {
                "fragments": [],
                "text": "The Art of Computer Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The arrangement of this invention provides a strong vibration free hold-down mechanism while avoiding a large pressure drop to the flow of coolant fluid."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059699652"
                        ],
                        "name": "D. Stevenson",
                        "slug": "D.-Stevenson",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Stevenson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Stevenson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15523399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f5d268a4b418a67b689519289fd28eb50ad3861d",
            "isKey": false,
            "numCitedBy": 113,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Offered here for public comment, this proposed standard facilitates transportation of numerically oriented programs and encourages development of high-quality numerical software."
            },
            "slug": "A-Proposed-Standard-for-Binary-Floating-Point-Stevenson",
            "title": {
                "fragments": [],
                "text": "A Proposed Standard for Binary Floating-Point Arithmetic"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "This proposed standard facilitates transportation of numerically oriented programs and encourages development of high-quality numerical software."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39361090"
                        ],
                        "name": "L. Baum",
                        "slug": "L.-Baum",
                        "structuredName": {
                            "firstName": "Leonard",
                            "lastName": "Baum",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Baum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 94
                            }
                        ],
                        "text": "It can be shown that this algorithm will converge, although possibly to a non-global maximum [Baum, 1972]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 103
                            }
                        ],
                        "text": "In this situation the Baum-Welch algorithm (also known as the forward-backward algorithm) can be used [Baum, 1972]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 231
                            }
                        ],
                        "text": "Maximum likelihood estimates (that is, estimates that maximize the probability of the training set) can be found through application of alternating expectation in a procedure known as the BaumWelch, or forward-backward, algorithm [Baum, 1972]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60804212,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "539036ab9e8f038c8a948596e77cc0dfcfa91fb3",
            "isKey": true,
            "numCitedBy": 1785,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-inequality-and-associated-maximization-technique-Baum",
            "title": {
                "fragments": [],
                "text": "An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143812958"
                        ],
                        "name": "M. Wilkes",
                        "slug": "M.-Wilkes",
                        "structuredName": {
                            "firstName": "Maurice",
                            "lastName": "Wilkes",
                            "middleNames": [
                                "Vincent"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wilkes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61642229,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a77f8b3c58a9186298e35913cd180c51c83c82ad",
            "isKey": false,
            "numCitedBy": 1723,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Art-of-Computer-Programming,-Volume-3,-Sorting-Wilkes",
            "title": {
                "fragments": [],
                "text": "The Art of Computer Programming, Volume 3, Sorting and Searching"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27412411"
                        ],
                        "name": "J. Gosling",
                        "slug": "J.-Gosling",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Gosling",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gosling"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60633876,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "69a9f571ccc711797ba12677b926853f6c1d25ec",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Tutorial-on-proposed-IEEE-standard-P754-on-binary-Gosling",
            "title": {
                "fragments": [],
                "text": "Tutorial on proposed IEEE standard P754 on binary floating-point arithmetic"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61012010,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a923c9f89ed53b6e835b3807c0c1bd8d532687b",
            "isKey": false,
            "numCitedBy": 1037,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Interpolated-estimation-of-Markov-source-parameters-Jelinek",
            "title": {
                "fragments": [],
                "text": "Interpolated estimation of Markov source parameters from sparse data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 92
                            }
                        ],
                        "text": "In support of this and other work, we have developed a system architecture for text access [Cutting et al., 1991]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "In Conference Proceedings of RIAO'91"
            },
            "venue": {
                "fragments": [],
                "text": "Intelligent Text and Image Handling, Barcelona, Spain, pages 285{298, April"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "IEEE Task P754. A proposed standard for binary floating-point arithmetic"
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 115
                            }
                        ],
                        "text": "However, when a word appears in the context of other words, the ambiguity is often reduced: in \\a tag is a part-of-speech label,\" the word \\tag\" can only be a noun."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Frequency Analysis of English Usage"
            },
            "venue": {
                "fragments": [],
                "text": "Frequency Analysis of English Usage"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Robust part-of-speech t a g ging using a hidden markov model. submitted to Computer Speech and Language"
            },
            "venue": {
                "fragments": [],
                "text": "Robust part-of-speech t a g ging using a hidden markov model. submitted to Computer Speech and Language"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 61
                            }
                        ],
                        "text": "The phrase recognizers also provide input to a system, Sopa [Sibun, 1991], which recognizes nominal arguments of verbs, speci cally, Subject, Object, and Predicative Arguments."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "Sopa does not rely on information (such as arity or voice) speci c to the particular verbs involved."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Grammatical function assignment in unrestricted text"
            },
            "venue": {
                "fragments": [],
                "text": "internal report, Xerox Palo Alto Research Center"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Computational Analysis of English"
            },
            "venue": {
                "fragments": [],
                "text": "The Computational Analysis of English"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 50
                            }
                        ],
                        "text": "For an introduction to hidden Markov modeling see [Rabiner and Juang, 1986]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "IEEE ASSP Magazine"
            },
            "venue": {
                "fragments": [],
                "text": "January"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 73
                            }
                        ],
                        "text": "An e cient dynamic programming procedure, known as the Viterbi algorithm [Viterbi, 1967], arranges for this computation to proceed in time proportional to T ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "pages 260{269"
            },
            "venue": {
                "fragments": [],
                "text": "April"
            },
            "year": 1967
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tagging text with a probablistic model"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of ICASSP-91"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Derouault and B. Merialdo. Natural language modeling for phonemeto-text transcription"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 3
                            }
                        ],
                        "text": "In [Kupiec, 1989a], networks are used to selectively augment the context in a basic rstorder model, rather than using uniformly second-order dependencies."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 101
                            }
                        ],
                        "text": "Now, adequate training requires processing from tens of thousands to hundreds of thousands of tokens [Kupiec, 1989a]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "pages 92{98"
            },
            "venue": {
                "fragments": [],
                "text": "Cape Cod, MA,"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 138
                            }
                        ],
                        "text": "Kupiec used word equivalence classes (referred to here as ambiguity classes) based on parts of speech, to pool data from individual words [Kupiec, 1989b]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "In Proceedings of the 1989 DARPA Speech and Natural Language Workshop"
            },
            "venue": {
                "fragments": [],
                "text": "pages 290{295, Philadelphia,"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Frequency Analysis of English Usage. Houghton Miiin"
            },
            "venue": {
                "fragments": [],
                "text": "Frequency Analysis of English Usage. Houghton Miiin"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Computational Analysis of English. Long.man"
            },
            "venue": {
                "fragments": [],
                "text": "The Computational Analysis of English. Long.man"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 66
                            }
                        ],
                        "text": "Greene and Rubin used a rule-based approach in the TAGGIT program [Greene and Rubin, 1971], which was an aid in tagging the Brown corpus [Francis and Ku cera, 1982]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic grammatical tagging of English"
            },
            "venue": {
                "fragments": [],
                "text": "Automatic grammatical tagging of English"
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sampson. The Computational Analysis of English"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Grammatical function assignment in unrestricted text. internal report"
            },
            "venue": {
                "fragments": [],
                "text": "Grammatical function assignment in unrestricted text. internal report"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 200
                            }
                        ],
                        "text": "Parameter smoothing can be conveniently achieved using the method of deleted interpolation in which weighted estimates are taken from secondand rst-order models and a uniform probability distribution [Jelinek and Mercer, 1980]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "pages 381{397"
            },
            "venue": {
                "fragments": [],
                "text": "Amsterdam,"
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 114
                            }
                        ],
                        "text": "However the addition of a simple lookahead mechanism which allows speci cation of right context ameliorates this [Aho et al., 1986, Lesk, 1975]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 127
                            }
                        ],
                        "text": "These may be compiled into a single deterministic nite state automaton which partitions character streams into labeled tokens [Aho et al., 1986, Lesk, 1975]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Compilers: Principles"
            },
            "venue": {
                "fragments": [],
                "text": "Techniques and Tools. AddisonWesley"
            },
            "year": 1986
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 7,
            "methodology": 12,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 42,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/A-Practical-Part-of-Speech-Tagger-Cutting-Kupiec/68c2263ceef50530996f4807da8d9a0e835905e8?sort=total-citations"
}