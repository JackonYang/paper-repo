{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148896777"
                        ],
                        "name": "Kai Wang",
                        "slug": "Kai-Wang",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2490700"
                        ],
                        "name": "Boris Babenko",
                        "slug": "Boris-Babenko",
                        "structuredName": {
                            "firstName": "Boris",
                            "lastName": "Babenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Boris Babenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[31], we ignore images that either contain non-alphanumeric characters or have less than three characters, and get a test set with 860 cropped text images."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 7
                            }
                        ],
                        "text": "IIIT5k SVT IC03 IC13 50 1k None 50 None 50 Full 50k None None\nABBYY [34] 24.3 - - 35.0 - 56.0 55.0 - - - Wang et al. [34] - - - 57.0 - 76.0 62.0 - - - Mishra et al. [28] 64.1 57.5 - 73.2 - 81.8 67.8 - - - Wang et al. [35] - - - 70.0 - 90.0 84.0 - - - Goel et al. [13] - - - 77.3 - 89.7 - - - - Bissacco et al. [8] - - - 90.4 78.0 - - - - 87.6 Alsharif and Pineau [6] - - - 74.3 - 93.1 88.6 85.1 - - Almaza\u0301n et al. [5] 91.2 82.1 - 89.2 - - - - - - Yao et al. [36] 80.2 69.3 - 75.9 - 88.5 80.3 - - - Rodrguez-Serrano et al. [30] 76.1 57.4 - 70.0 - - - - - - Jaderberg et al. [23] - - - 86.1 - 96.2 91.5 - - - Su and Lu [33] - - - 83.0 - 92.0 82.0 - - - Gordo [14] 93.3 86.6 - 91.8 - - - - - - Jaderberg et al. [22] 97.1 92.7 - 95.4 80.7* 98.7 98.6 93.3 93.1* 90.8* Jaderberg et al. [21] 95.5 89.6 - 93.2 71.7 97.8 97.0 93.4 89.6 81.8\nCRNN 97.6 94.4 78.2 96.4 80.8 98.7 97.6 95.5 89.4 86.7\nNotice that though the recent models learned by label embedding [5, 14] and incremental learning [22] achieved highly competitive performance, they are constrained to a specific dictionary."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 178
                            }
                        ],
                        "text": "Four popular benchmarks for scene text recognition are used for performance evaluation, namely ICDAR 2003 (IC03), ICDAR 2013 (IC13), IIIT 5k-word (IIIT5k), and Street View Text (SVT)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 80
                            }
                        ],
                        "text": "In the unconstrained lexicon cases, our method achieves the best performance on SVT, yet, is still behind some approaches [8, 22] on IC03 and IC13."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "SVT [31] test dataset consists of 249 street view images collected from Google Street View."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 60
                            }
                        ],
                        "text": "Specifically, we obtain superior performance on IIIT5k, and SVT compared to [22], only achieved lower performance on IC03 with the \u201cFull\u201d lexicon."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "SVT [34] test dataset consists of 249 street view images collected from Google Street View."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14136313,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32b8f58a038df83138435b12a499c8bf0de13811",
            "isKey": true,
            "numCitedBy": 904,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper focuses on the problem of word detection and recognition in natural images. The problem is significantly more challenging than reading text in scanned documents, and has only recently gained attention from the computer vision community. Sub-components of the problem, such as text detection and cropped image word recognition, have been studied in isolation [7, 4, 20]. However, what is unclear is how these recent approaches contribute to solving the end-to-end problem of word recognition. We fill this gap by constructing and evaluating two systems. The first, representing the de facto state-of-the-art, is a two stage pipeline consisting of text detection followed by a leading OCR engine. The second is a system rooted in generic object recognition, an extension of our previous work in [20]. We show that the latter approach achieves superior performance. While scene text recognition has generally been treated with highly domain-specific methods, our results demonstrate the suitability of applying generic computer vision methods. Adopting this approach opens the door for real world scene text recognition to benefit from the rapid advances that have been taking place in object recognition."
            },
            "slug": "End-to-end-scene-text-recognition-Wang-Babenko",
            "title": {
                "fragments": [],
                "text": "End-to-end scene text recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "While scene text recognition has generally been treated with highly domain-specific methods, the results demonstrate the suitability of applying generic computer vision methods."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2156632012"
                        ],
                        "name": "Tao Wang",
                        "slug": "Tao-Wang",
                        "structuredName": {
                            "firstName": "Tao",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tao Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "25629078"
                        ],
                        "name": "David J. Wu",
                        "slug": "David-J.-Wu",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wu",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144638694"
                        ],
                        "name": "Adam Coates",
                        "slug": "Adam-Coates",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Coates",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Coates"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 31
                            }
                        ],
                        "text": "For example, the algorithms in [4], [5] first detect individual characters and then recognize these detected characters with DCNN models, which are trained using labeled character images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3126988,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "26cb14c9d22cf946314d685fe3541ef9f641e429",
            "isKey": false,
            "numCitedBy": 791,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Full end-to-end text recognition in natural images is a challenging problem that has received much attention recently. Traditional systems in this area have relied on elaborate models incorporating carefully hand-engineered features or large amounts of prior knowledge. In this paper, we take a different route and combine the representational power of large, multilayer neural networks together with recent developments in unsupervised feature learning, which allows us to use a common framework to train highly-accurate text detector and character recognizer modules. Then, using only simple off-the-shelf methods, we integrate these two modules into a full end-to-end, lexicon-driven, scene text recognition system that achieves state-of-the-art performance on standard benchmarks, namely Street View Text and ICDAR 2003."
            },
            "slug": "End-to-end-text-recognition-with-convolutional-Wang-Wu",
            "title": {
                "fragments": [],
                "text": "End-to-end text recognition with convolutional neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper combines the representational power of large, multilayer neural networks together with recent developments in unsupervised feature learning, which allows them to use a common framework to train highly-accurate text detector and character recognizer modules."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795547"
                        ],
                        "name": "Bolan Su",
                        "slug": "Bolan-Su",
                        "structuredName": {
                            "firstName": "Bolan",
                            "lastName": "Su",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bolan Su"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771189"
                        ],
                        "name": "Shijian Lu",
                        "slug": "Shijian-Lu",
                        "structuredName": {
                            "firstName": "Shijian",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shijian Lu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 90
                            }
                        ],
                        "text": "[7] extract a set of geometrical or image features from handwritten text, while Su and Lu [8] convert word images into sequential HOG features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18948351,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03f2fc62d66fd579f234dec51e8c5bf737a7bfa2",
            "isKey": false,
            "numCitedBy": 180,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Scene text recognition is a useful but very challenging task due to uncontrolled condition of text in natural scenes. This paper presents a novel approach to recognize text in scene images. In the proposed technique, a word image is first converted into a sequential column vectors based on Histogram of Oriented Gradient (HOG). The Recurrent Neural Network (RNN) is then adapted to classify the sequential feature vectors into the corresponding word. Compared with most of the existing methods that follow a bottom-up approach to form words by grouping the recognized characters, our proposed method is able to recognize the whole word images without character-level segmentation and recognition. Experiments on a number of publicly available datasets show that the proposed method outperforms the state-of-the-art techniques significantly. In addition, the recognition results on publicly available datasets provide a good benchmark for the future research in this area."
            },
            "slug": "Accurate-Scene-Text-Recognition-Based-on-Recurrent-Su-Lu",
            "title": {
                "fragments": [],
                "text": "Accurate Scene Text Recognition Based on Recurrent Neural Network"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper presents a novel approach to recognize text in scene images that outperforms the state-of-the-art techniques significantly and is able to recognize the whole word images without character-level segmentation and recognition."
            },
            "venue": {
                "fragments": [],
                "text": "ACCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3093886"
                        ],
                        "name": "Max Jaderberg",
                        "slug": "Max-Jaderberg",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Jaderberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Max Jaderberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34838386"
                        ],
                        "name": "K. Simonyan",
                        "slug": "K.-Simonyan",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Simonyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Simonyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 76
                            }
                        ],
                        "text": "generally outperformed by previous algorithms based on neural networks [5], [6], aswell as the approach proposed in this paper."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[6] benefits from its large dictionary, however, it is not a model strictly unconstrained to a lexicon as mentioned before."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 158
                            }
                        ],
                        "text": "In the constrained-lexicon case, our method consistently outperforms most state-of-the-arts approaches, and in average beats the best text reader proposed in [6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 32
                            }
                        ],
                        "text": "Yet, our method is still behind [6] on IC13."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 7
                            }
                        ],
                        "text": "Unlike [6], CRNN is not limited to recognize a word in a known dictionary, and able to handle TABLE 1 Network Configuration Summary"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 76
                            }
                        ],
                        "text": "Specifically, we obtain superior performance on IIIT5k, and SVT compared to [6], only achieved lower performance on IC03 with the \u201cFull\u201d lexicon."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 66
                            }
                        ],
                        "text": "the-arts techniques including the approaches based on deep models [6], [18], [19], are shown in Table 2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 26
                            }
                        ],
                        "text": "other approaches (such as [6]) treat scene text recognition as an image classification problem, and assign a class label to each English word (90K words in total)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "Note that the model in[6] is trained on a specific dictionary, namely that each word is associated to a class label."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 122
                            }
                        ],
                        "text": "Some previous approaches have employed DCNN to learn a robust representation for sequence-like objects such as scene text [6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207252329,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5ae7436b5946bd37d17fc1ed26374389a86deff",
            "isKey": true,
            "numCitedBy": 884,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we present an end-to-end system for text spotting\u2014localising and recognising text in natural scene images\u2014and text based image retrieval. This system is based on a region proposal mechanism for detection and deep convolutional neural networks for recognition. Our pipeline uses a novel combination of complementary proposal generation techniques to ensure high recall, and a fast subsequent filtering stage for improving precision. For the recognition and ranking of proposals, we train very large convolutional neural networks to perform word recognition on the whole proposal region at the same time, departing from the character classifier based systems of the past. These networks are trained solely on data produced by a synthetic text generation engine, requiring no human labelled data. Analysing the stages of our pipeline, we show state-of-the-art performance throughout. We perform rigorous experiments across a number of standard end-to-end text spotting benchmarks and text-based image retrieval datasets, showing a large improvement over all previous methods. Finally, we demonstrate a real-world application of our text spotting system to allow thousands of hours of news footage to be instantly searchable via a text query."
            },
            "slug": "Reading-Text-in-the-Wild-with-Convolutional-Neural-Jaderberg-Simonyan",
            "title": {
                "fragments": [],
                "text": "Reading Text in the Wild with Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "An end-to-end system for text spotting\u2014localising and recognising text in natural scene images\u2014and text based image retrieval and a real-world application to allow thousands of hours of news footage to be instantly searchable via a text query is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399142941"
                        ],
                        "name": "Jos\u00e9 A. Rodr\u00edguez-Serrano",
                        "slug": "Jos\u00e9-A.-Rodr\u00edguez-Serrano",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Rodr\u00edguez-Serrano",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jos\u00e9 A. Rodr\u00edguez-Serrano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1821267"
                        ],
                        "name": "Albert Gordo",
                        "slug": "Albert-Gordo",
                        "structuredName": {
                            "firstName": "Albert",
                            "lastName": "Gordo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Albert Gordo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723883"
                        ],
                        "name": "F. Perronnin",
                        "slug": "F.-Perronnin",
                        "structuredName": {
                            "firstName": "Florent",
                            "lastName": "Perronnin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Perronnin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[12] proposed to embed word images and text strings in a common vectorial subspace, converting word recog-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17934318,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c08694d52741fe68a2010c0f836f24d1bfa0ab04",
            "isKey": false,
            "numCitedBy": 87,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "The standard approach to recognizing text in images consists in first classifying local image regions into candidate characters and then combining them with high-level word models such as conditional random fields. This paper explores a new paradigm that departs from this bottom-up view. We propose to embed word labels and word images into a common Euclidean space. Given a word image to be recognized, the text recognition problem is cast as one of retrieval: find the closest word label in this space. This common space is learned using the Structured SVM framework by enforcing matching label-image pairs to be closer than non-matching pairs. This method presents several advantages: it does not require ad-hoc or costly pre-/post-processing operations, it can build on top of any state-of-the-art image descriptor (Fisher vectors in our case), it allows for the recognition of never-seen-before words (zero-shot recognition) and the recognition process is simple and efficient, as it amounts to a nearest neighbor search. Experiments are performed on challenging datasets of license plates and scene text. The main conclusion of the paper is that with such a frugal approach it is possible to obtain results which are competitive with standard bottom-up approaches, thus establishing label embedding as an interesting and simple to compute baseline for text recognition."
            },
            "slug": "Label-Embedding:-A-Frugal-Baseline-for-Text-Rodr\u00edguez-Serrano-Gordo",
            "title": {
                "fragments": [],
                "text": "Label Embedding: A Frugal Baseline for Text Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The main conclusion of the paper is that with such a frugal approach it is possible to obtain results which are competitive with standard bottom-up approaches, thus establishing label embedding as an interesting and simple to compute baseline for text recognition."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50521003"
                        ],
                        "name": "Chen-Yu Lee",
                        "slug": "Chen-Yu-Lee",
                        "structuredName": {
                            "firstName": "Chen-Yu",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chen-Yu Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709243"
                        ],
                        "name": "Anurag Bhardwaj",
                        "slug": "Anurag-Bhardwaj",
                        "structuredName": {
                            "firstName": "Anurag",
                            "lastName": "Bhardwaj",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anurag Bhardwaj"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47046569"
                        ],
                        "name": "Wei Di",
                        "slug": "Wei-Di",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Di",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Di"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679794"
                        ],
                        "name": "V. Jagadeesh",
                        "slug": "V.-Jagadeesh",
                        "structuredName": {
                            "firstName": "Vignesh",
                            "lastName": "Jagadeesh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Jagadeesh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3221010"
                        ],
                        "name": "Robinson Piramuthu",
                        "slug": "Robinson-Piramuthu",
                        "structuredName": {
                            "firstName": "Robinson",
                            "lastName": "Piramuthu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robinson Piramuthu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[13] proposed a discriminative feature pooling method that learns informative sub-regions of characters."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10557766,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2945dee6f78a3e29cd5ea2f135c0bace3def5d4d",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new feature representation method for scene text recognition problem, particularly focusing on improving scene character recognition. Many existing methods rely on Histogram of Oriented Gradient (HOG) or part-based models, which do not span the feature space well for characters in natural scene images, especially given large variation in fonts with cluttered backgrounds. In this work, we propose a discriminative feature pooling method that automatically learns the most informative sub-regions of each scene character within a multi-class classification framework, whereas each sub-region seamlessly integrates a set of low-level image features through integral images. The proposed feature representation is compact, computationally efficient, and able to effectively model distinctive spatial structures of each individual character class. Extensive experiments conducted on challenging datasets (Chars74K, ICDAR'03, ICDAR'11, SVT) show that our method significantly outperforms existing methods on scene character classification and scene text recognition tasks."
            },
            "slug": "Region-Based-Discriminative-Feature-Pooling-for-Lee-Bhardwaj",
            "title": {
                "fragments": [],
                "text": "Region-Based Discriminative Feature Pooling for Scene Text Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work proposes a discriminative feature pooling method that automatically learns the most informative sub-regions of each scene character within a multi-class classification framework, whereas each sub-region seamlessly integrates a set of low-level image features through integral images."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3093886"
                        ],
                        "name": "Max Jaderberg",
                        "slug": "Max-Jaderberg",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Jaderberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Max Jaderberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34838386"
                        ],
                        "name": "K. Simonyan",
                        "slug": "K.-Simonyan",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Simonyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Simonyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11072772,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c0b8aad30d8dfd08535f361864f064b2fbbc9a75",
            "isKey": false,
            "numCitedBy": 683,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we present a framework for the recognition of natural scene text. Our framework does not require any human-labelled data, and performs word recognition on the whole image holistically, departing from the character based recognition systems of the past. The deep neural network models at the centre of this framework are trained solely on data produced by a synthetic text generation engine -- synthetic data that is highly realistic and sufficient to replace real data, giving us infinite amounts of training data. This excess of data exposes new possibilities for word recognition models, and here we consider three models, each one \"reading\" words in a different way: via 90k-way dictionary encoding, character sequence encoding, and bag-of-N-grams encoding. In the scenarios of language based and completely unconstrained text recognition we greatly improve upon state-of-the-art performance on standard datasets, using our fast, simple machinery and requiring zero data-acquisition costs."
            },
            "slug": "Synthetic-Data-and-Artificial-Neural-Networks-for-Jaderberg-Simonyan",
            "title": {
                "fragments": [],
                "text": "Synthetic Data and Artificial Neural Networks for Natural Scene Text Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "This work presents a framework for the recognition of natural scene text that does not require any human-labelled data, and performs word recognition on the whole image holistically, departing from the character based recognition systems of the past."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3093886"
                        ],
                        "name": "Max Jaderberg",
                        "slug": "Max-Jaderberg",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Jaderberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Max Jaderberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34838386"
                        ],
                        "name": "K. Simonyan",
                        "slug": "K.-Simonyan",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Simonyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Simonyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "only the models based on deep neural networks including [6], [18] as well as CRNN have this property."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 113
                            }
                        ],
                        "text": "Consequently, the number of parameters of CRNN is much less than the models learned on the variants of DCNN [6], [18], resulting in a much smaller model compared with [6], [18]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 74
                            }
                        ],
                        "text": "As a result, fully-connected layers for producing structured outputs like [18] are not essential in ourmethod, and themodel size is greatly reduced."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "the-arts techniques including the approaches based on deep models [6], [18], [19], are shown in Table 2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16734174,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "21da448e7c31e1ff6cc3b7155a9c9c49a0138060",
            "isKey": true,
            "numCitedBy": 198,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a representation suitable for the unconstrained recognition of words in natural images: the general case of no fixed lexicon and unknown length. \nTo this end we propose a convolutional neural network (CNN) based architecture which incorporates a Conditional Random Field (CRF) graphical model, taking the whole word image as a single input. The unaries of the CRF are provided by a CNN that predicts characters at each position of the output, while higher order terms are provided by another CNN that detects the presence of N-grams. We show that this entire model (CRF, character predictor, N-gram predictor) can be jointly optimised by back-propagating the structured output loss, essentially requiring the system to perform multi-task learning, and training uses purely synthetically generated data. The resulting model is a more accurate system on standard real-world text recognition benchmarks than character prediction alone, setting a benchmark for systems that have not been trained on a particular lexicon. In addition, our model achieves state-of-the-art accuracy in lexicon-constrained scenarios, without being specifically modelled for constrained recognition. To test the generalisation of our model, we also perform experiments with random alpha-numeric strings to evaluate the method when no visual language model is applicable."
            },
            "slug": "Deep-Structured-Output-Learning-for-Unconstrained-Jaderberg-Simonyan",
            "title": {
                "fragments": [],
                "text": "Deep Structured Output Learning for Unconstrained Text Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A convolutional neural network based architecture which incorporates a Conditional Random Field graphical model, taking the whole word image as a single input, which achieves state-of-the-art accuracy in lexicon-constrained scenarios, without being specifically modelled for constrained recognition."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20790397"
                        ],
                        "name": "Vibhor Goel",
                        "slug": "Vibhor-Goel",
                        "structuredName": {
                            "firstName": "Vibhor",
                            "lastName": "Goel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vibhor Goel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39719398"
                        ],
                        "name": "Anand Mishra",
                        "slug": "Anand-Mishra",
                        "structuredName": {
                            "firstName": "Anand",
                            "lastName": "Mishra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anand Mishra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72492981"
                        ],
                        "name": "Alahari Karteek",
                        "slug": "Alahari-Karteek",
                        "structuredName": {
                            "firstName": "Alahari",
                            "lastName": "Karteek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alahari Karteek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694502"
                        ],
                        "name": "C. Jawahar",
                        "slug": "C.-Jawahar",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Jawahar",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jawahar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13960775,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "817f83b13229f603d5a241812e81059d01e71c7a",
            "isKey": false,
            "numCitedBy": 87,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Recognizing text in images taken in the wild is a challenging problem that has received great attention in recent years. Previous methods addressed this problem by first detecting individual characters, and then forming them into words. Such approaches often suffer from weak character detections, due to large intra-class variations, even more so than characters from scanned documents. We take a different view of the problem and present a holistic word recognition framework. In this, we first represent the scene text image and synthetic images generated from lexicon words using gradient-based features. We then recognize the text in the image by matching the scene and synthetic image features with our novel weighted Dynamic Time Warping (wDTW) approach. We perform experimental analysis on challenging public datasets, such as Street View Text and ICDAR 2003. Our proposed method significantly outperforms our earlier work in Mishra et al. (CVPR 2012), as well as many other recent works, such as Novikova et al. (ECCV 2012), Wang et al. al.(ICPR 2012), Wang et al.(ICCV 2011)."
            },
            "slug": "Whole-is-Greater-than-Sum-of-Parts:-Recognizing-Goel-Mishra",
            "title": {
                "fragments": [],
                "text": "Whole is Greater than Sum of Parts: Recognizing Scene Text Words"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work presents a holistic word recognition framework that represents the scene text image and synthetic images generated from lexicon words using gradient-based features, and recognizes the text in the image by matching the scene and synthetic image features with the novel weighted Dynamic Time Warping (wDTW) approach."
            },
            "venue": {
                "fragments": [],
                "text": "2013 12th International Conference on Document Analysis and Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3093886"
                        ],
                        "name": "Max Jaderberg",
                        "slug": "Max-Jaderberg",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Jaderberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Max Jaderberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "highly competitive performance on scene text (word recognition) than the prior arts [5], [19]; 6) It contains much less parameters than a standard DCNNmodel, consuming less storage space."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "the-arts techniques including the approaches based on deep models [6], [18], [19], are shown in Table 2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13072702,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b4738809317259d2b49017203da512b21ea51ed",
            "isKey": false,
            "numCitedBy": 561,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of this work is text spotting in natural images. This is divided into two sequential tasks: detecting words regions in the image, and recognizing the words within these regions. We make the following contributions: first, we develop a Convolutional Neural Network (CNN) classifier that can be used for both tasks. The CNN has a novel architecture that enables efficient feature sharing (by using a number of layers in common) for text detection, character case-sensitive and insensitive classification, and bigram classification. It exceeds the state-of-the-art performance for all of these. Second, we make a number of technical changes over the traditional CNN architectures, including no downsampling for a per-pixel sliding window, and multi-mode learning with a mixture of linear models (maxout). Third, we have a method of automated data mining of Flickr, that generates word and character level annotations. Finally, these components are used together to form an end-to-end, state-of-the-art text spotting system. We evaluate the text-spotting system on two standard benchmarks, the ICDAR Robust Reading data set and the Street View Text data set, and demonstrate improvements over the state-of-the-art on multiple measures."
            },
            "slug": "Deep-Features-for-Text-Spotting-Jaderberg-Vedaldi",
            "title": {
                "fragments": [],
                "text": "Deep Features for Text Spotting"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A Convolutional Neural Network classifier is developed that can be used for text spotting in natural images and a method of automated data mining of Flickr, that generates word and character level annotations is used to form an end-to-end, state-of-the-art text spotting system."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39719398"
                        ],
                        "name": "Anand Mishra",
                        "slug": "Anand-Mishra",
                        "structuredName": {
                            "firstName": "Anand",
                            "lastName": "Mishra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anand Mishra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72492981"
                        ],
                        "name": "Alahari Karteek",
                        "slug": "Alahari-Karteek",
                        "structuredName": {
                            "firstName": "Alahari",
                            "lastName": "Karteek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alahari Karteek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694502"
                        ],
                        "name": "C. Jawahar",
                        "slug": "C.-Jawahar",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Jawahar",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jawahar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 7
                            }
                        ],
                        "text": "IIIT5k [33] contains 3,000 cropped word test images collected from the Internet."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9695967,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb5b2df137a4d54c3a9145fa363e66531b491580",
            "isKey": false,
            "numCitedBy": 548,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of recognizing text in images taken in the wild has gained significant attention from the computer vision community in recent years. Contrary to recognition of printed documents, recognizing scene text is a challenging problem. We focus on the problem of recognizing text extracted from natural scene images and the web. Significant attempts have been made to address this problem in the recent past. However, many of these works benefit from the availability of strong context, which naturally limits their applicability. In this work we present a framework that uses a higher order prior computed from an English dictionary to recognize a word, which may or may not be a part of the dictionary. We show experimental results on publicly available datasets. Furthermore, we introduce a large challenging word dataset with five thousand words to evaluate various steps of our method exhaustively. The main contributions of this work are: (1) We present a framework, which incorporates higher order statistical language models to recognize words in an unconstrained manner (i.e. we overcome the need for restricted word lists, and instead use an English dictionary to compute the priors). (2) We achieve significant improvement (more than 20%) in word recognition accuracies without using a restricted word list. (3) We introduce a large word recognition dataset (atleast 5 times larger than other public datasets) with character level annotation and benchmark it."
            },
            "slug": "Scene-Text-Recognition-using-Higher-Order-Language-Mishra-Karteek",
            "title": {
                "fragments": [],
                "text": "Scene Text Recognition using Higher Order Language Priors"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A framework is presented that uses a higher order prior computed from an English dictionary to recognize a word, which may or may not be a part of the dictionary, and achieves significant improvement in word recognition accuracies without using a restricted word list."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145905113"
                        ],
                        "name": "X. Bai",
                        "slug": "X.-Bai",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Bai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146721"
                        ],
                        "name": "C. Yao",
                        "slug": "C.-Yao",
                        "structuredName": {
                            "firstName": "Cong",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743698"
                        ],
                        "name": "Wenyu Liu",
                        "slug": "Wenyu-Liu",
                        "structuredName": {
                            "firstName": "Wenyu",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenyu Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[14] and Gordo [15] used mid-level features for scene text recognition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5922947,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f7a0ba33e13de1095258bf58390bcaeb60516877",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 101,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we are concerned with the problem of automatic scene text recognition, which involves localizing and reading characters in natural images. We investigate this problem from the perspective of representation and propose a novel multi-scale representation, which leads to accurate, robust character identification and recognition. This representation consists of a set of mid-level primitives, termed strokelets, which capture the underlying substructures of characters at different granularities. The Strokelets possess four distinctive advantages: 1) usability: automatically learned from character level annotations; 2) robustness: insensitive to interference factors; 3) generality: applicable to variant languages; and 4) expressivity: effective at describing characters. Extensive experiments on standard benchmarks verify the advantages of the strokelets and demonstrate the effectiveness of the text recognition algorithm built upon the strokelets. Moreover, we show the method to incorporate the strokelets to improve the performance of scene text detection."
            },
            "slug": "Strokelets:-A-Learned-Multi-Scale-Mid-Level-for-Bai-Yao",
            "title": {
                "fragments": [],
                "text": "Strokelets: A Learned Multi-Scale Mid-Level Representation for Scene Text Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper proposes a novel multi-scale representation, which leads to accurate, robust character identification and recognition, which consists of a set of mid-level primitives, termed strokelets, which capture the underlying substructures of characters at different granularities."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146721"
                        ],
                        "name": "C. Yao",
                        "slug": "C.-Yao",
                        "structuredName": {
                            "firstName": "Cong",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145905113"
                        ],
                        "name": "X. Bai",
                        "slug": "X.-Bai",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Bai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743698"
                        ],
                        "name": "Wenyu Liu",
                        "slug": "Wenyu-Liu",
                        "structuredName": {
                            "firstName": "Wenyu",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenyu Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[16] unified text detection and recognition by sharing features between the two tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14457729,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2950be66e7b4c94dbb16e3319d8bece5da4e799f",
            "isKey": false,
            "numCitedBy": 222,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "High level semantics embodied in scene texts are both rich and clear and thus can serve as important cues for a wide range of vision applications, for instance, image understanding, image indexing, video search, geolocation, and automatic navigation. In this paper, we present a unified framework for text detection and recognition in natural images. The contributions of this paper are threefold: 1) text detection and recognition are accomplished concurrently using exactly the same features and classification scheme; 2) in contrast to methods in the literature, which mainly focus on horizontal or near-horizontal texts, the proposed system is capable of localizing and reading texts of varying orientations; and 3) a new dictionary search method is proposed, to correct the recognition errors usually caused by confusions among similar yet different characters. As an additional contribution, a novel image database with texts of different scales, colors, fonts, and orientations in diverse real-world scenarios, is generated and released. Extensive experiments on standard benchmarks as well as the proposed database demonstrate that the proposed system achieves highly competitive performance, especially on multioriented texts."
            },
            "slug": "A-Unified-Framework-for-Multioriented-Text-and-Yao-Bai",
            "title": {
                "fragments": [],
                "text": "A Unified Framework for Multioriented Text Detection and Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A unified framework for text detection and recognition in natural images using exactly the same features and classification scheme and a new dictionary search method is proposed, to correct the recognition errors usually caused by confusions among similar yet different characters."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146721"
                        ],
                        "name": "C. Yao",
                        "slug": "C.-Yao",
                        "structuredName": {
                            "firstName": "Cong",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145905113"
                        ],
                        "name": "X. Bai",
                        "slug": "X.-Bai",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Bai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276155"
                        ],
                        "name": "Baoguang Shi",
                        "slug": "Baoguang-Shi",
                        "structuredName": {
                            "firstName": "Baoguang",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Baoguang Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743698"
                        ],
                        "name": "Wenyu Liu",
                        "slug": "Wenyu-Liu",
                        "structuredName": {
                            "firstName": "Wenyu",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenyu Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11341313,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca0eb5d81484f62af7b10f18aa4ed65d7856c106",
            "isKey": false,
            "numCitedBy": 244,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Driven by the wide range of applications, scene text detection and recognition have become active research topics in computer vision. Though extensively studied, localizing and reading text in uncontrolled environments remain extremely challenging, due to various interference factors. In this paper, we propose a novel multi-scale representation for scene text recognition. This representation consists of a set of detectable primitives, termed as strokelets, which capture the essential substructures of characters at different granularities. Strokelets possess four distinctive advantages: (1) Usability: automatically learned from bounding box labels, (2) Robustness: insensitive to interference factors, (3) Generality: applicable to variant languages, and (4) Expressivity: effective at describing characters. Extensive experiments on standard benchmarks verify the advantages of strokelets and demonstrate the effectiveness of the proposed algorithm for text recognition."
            },
            "slug": "Strokelets:-A-Learned-Multi-scale-Representation-Yao-Bai",
            "title": {
                "fragments": [],
                "text": "Strokelets: A Learned Multi-scale Representation for Scene Text Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes a novel multi-scale representation for scene text recognition that consists of a set of detectable primitives, termed as strokelets, which capture the essential substructures of characters at different granularities."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2511744"
                        ],
                        "name": "O. Alsharif",
                        "slug": "O.-Alsharif",
                        "structuredName": {
                            "firstName": "Ouais",
                            "lastName": "Alsharif",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Alsharif"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145134886"
                        ],
                        "name": "Joelle Pineau",
                        "slug": "Joelle-Pineau",
                        "structuredName": {
                            "firstName": "Joelle",
                            "lastName": "Pineau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joelle Pineau"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17140888,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b1fa02bc9d34fa827694a6bf79253db37ec928eb",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of detecting and recognizing text in natural scenes has proved to be more challenging than its counterpart in documents, with most of the previous work focusing on a single part of the problem. In this work, we propose new solutions to the character and word recognition problems and then show how to combine these solutions in an end-to-end text-recognition system. We do so by leveraging the recently introduced Maxout networks along with hybrid HMM models that have proven useful for voice recognition. Using these elements, we build a tunable and highly accurate recognition system that beats state-of-the-art results on all the sub-problems for both the ICDAR 2003 and SVT benchmark datasets."
            },
            "slug": "End-to-End-Text-Recognition-with-Hybrid-HMM-Maxout-Alsharif-Pineau",
            "title": {
                "fragments": [],
                "text": "End-to-End Text Recognition with Hybrid HMM Maxout Models"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work proposes new solutions to the character and word recognition problems and shows how to combine these solutions in an end-to-end text-recognition system that beats state-of-the-art results on all the sub-problems for both the ICDAR 2003 and SVT benchmark datasets."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1821267"
                        ],
                        "name": "Albert Gordo",
                        "slug": "Albert-Gordo",
                        "structuredName": {
                            "firstName": "Albert",
                            "lastName": "Gordo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Albert Gordo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 20
                            }
                        ],
                        "text": "Yao et al. [36] and Gordo et al. [14] used mid-level features for scene text recognition."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 656,
                                "start": 651
                            }
                        ],
                        "text": "IIIT5k SVT IC03 IC13 50 1k None 50 None 50 Full 50k None None\nABBYY [34] 24.3 - - 35.0 - 56.0 55.0 - - - Wang et al. [34] - - - 57.0 - 76.0 62.0 - - - Mishra et al. [28] 64.1 57.5 - 73.2 - 81.8 67.8 - - - Wang et al. [35] - - - 70.0 - 90.0 84.0 - - - Goel et al. [13] - - - 77.3 - 89.7 - - - - Bissacco et al. [8] - - - 90.4 78.0 - - - - 87.6 Alsharif and Pineau [6] - - - 74.3 - 93.1 88.6 85.1 - - Almaza\u0301n et al. [5] 91.2 82.1 - 89.2 - - - - - - Yao et al. [36] 80.2 69.3 - 75.9 - 88.5 80.3 - - - Rodrguez-Serrano et al. [30] 76.1 57.4 - 70.0 - - - - - - Jaderberg et al. [23] - - - 86.1 - 96.2 91.5 - - - Su and Lu [33] - - - 83.0 - 92.0 82.0 - - - Gordo [14] 93.3 86.6 - 91.8 - - - - - - Jaderberg et al. [22] 97.1 92.7 - 95.4 80.7* 98.7 98.6 93.3 93.1* 90.8* Jaderberg et al. [21] 95.5 89.6 - 93.2 71.7 97.8 97.0 93.4 89.6 81.8\nCRNN 97.6 94.4 78.2 96.4 80.8 98.7 97.6 95.5 89.4 86.7\nNotice that though the recent models learned by label embedding [5, 14] and incremental learning [22] achieved highly competitive performance, they are constrained to a specific dictionary."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "[14] and Gordo [15] used mid-level features for scene text recognition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 70
                            }
                        ],
                        "text": "Notice that though the recent models learned by label embedding [11], [15] and incre-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14980039,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "384c841250a2689fc66fc93216d208b5e48edf1c",
            "isKey": true,
            "numCitedBy": 90,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of learning word image representations: given the cropped image of a word, we are interested in finding a descriptive, robust, and compact fixed-length representation. Machine learning techniques can then be supplied with these representations to produce models useful for word retrieval or recognition tasks. Although many works have focused on the machine learning aspect once a global representation has been produced, little work has been devoted to the construction of those base image representations: most works use standard coding and aggregation techniques directly on top of standard computer vision features such as SIFT or HOG. We propose to learn local mid-level features suitable for building word image representations. These features are learnt by leveraging character bounding box annotations on a small set of training images. However, contrary to other approaches that use character bounding box information, our approach does not rely on detecting the individual characters explicitly at testing time. Our local midlevel features can then be aggregated to produce a global word image signature. When pairing these features with the recent word attributes framework of [4], we obtain results comparable with or better than the state-of-the-art on matching and recognition tasks using global descriptors of only 96 dimensions."
            },
            "slug": "Supervised-mid-level-features-for-word-image-Gordo",
            "title": {
                "fragments": [],
                "text": "Supervised mid-level features for word image representation"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper proposes to learn local mid-level features suitable for building word image representations by leveraging character bounding box annotations on a small set of training images, and achieves results comparable with or better than the state-of-the-art on matching and recognition tasks using global descriptors of only 96 dimensions."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721248"
                        ],
                        "name": "P. Haffner",
                        "slug": "P.-Haffner",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Haffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haffner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 58
                            }
                        ],
                        "text": "Consequently, the most popular deep models like DCNN [2], [3] cannot be directly applied to sequence prediction, since DCNN models often operate on inputs and outputs with fixed dimensions, and thus are incapable of producing a variable-length label sequence."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14542261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "162d958ff885f1462aeda91cd72582323fd6a1f4",
            "isKey": false,
            "numCitedBy": 35158,
            "numCiting": 248,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day."
            },
            "slug": "Gradient-based-learning-applied-to-document-LeCun-Bottou",
            "title": {
                "fragments": [],
                "text": "Gradient-based learning applied to document recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task, and Convolutional neural networks are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467588"
                        ],
                        "name": "Jon Almaz\u00e1n",
                        "slug": "Jon-Almaz\u00e1n",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Almaz\u00e1n",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jon Almaz\u00e1n"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1821267"
                        ],
                        "name": "Albert Gordo",
                        "slug": "Albert-Gordo",
                        "structuredName": {
                            "firstName": "Albert",
                            "lastName": "Gordo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Albert Gordo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686569"
                        ],
                        "name": "A. Forn\u00e9s",
                        "slug": "A.-Forn\u00e9s",
                        "structuredName": {
                            "firstName": "Alicia",
                            "lastName": "Forn\u00e9s",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Forn\u00e9s"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2864362"
                        ],
                        "name": "Ernest Valveny",
                        "slug": "Ernest-Valveny",
                        "structuredName": {
                            "firstName": "Ernest",
                            "lastName": "Valveny",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ernest Valveny"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 64
                            }
                        ],
                        "text": "Notice that though the recent models learned by label embedding [11], [15] and incre-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10057476,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e61061b2ddd5e789a071e0681f4eb405bf811339",
            "isKey": false,
            "numCitedBy": 383,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problems of word spotting and word recognition on images. In word spotting, the goal is to find all instances of a query word in a dataset of images. In recognition, the goal is to recognize the content of the word image, usually aided by a dictionary or lexicon. We describe an approach in which both word images and text strings are embedded in a common vectorial subspace. This is achieved by a combination of label embedding and attributes learning, and a common subspace regression. In this subspace, images and strings that represent the same word are close together, allowing one to cast recognition and retrieval tasks as a nearest neighbor problem. Contrary to most other existing methods, our representation has a fixed length, is low dimensional, and is very fast to compute and, especially, to compare. We test our approach on four public datasets of both handwritten documents and natural images showing results comparable or better than the state-of-the-art on spotting and recognition tasks."
            },
            "slug": "Word-Spotting-and-Recognition-with-Embedded-Almaz\u00e1n-Gordo",
            "title": {
                "fragments": [],
                "text": "Word Spotting and Recognition with Embedded Attributes"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An approach in which both word images and text strings are embedded in a common vectorial subspace, allowing one to cast recognition and retrieval tasks as a nearest neighbor problem and is very fast to compute and, especially, to compare."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144079770"
                        ],
                        "name": "Yingying Zhu",
                        "slug": "Yingying-Zhu",
                        "structuredName": {
                            "firstName": "Yingying",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yingying Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146721"
                        ],
                        "name": "C. Yao",
                        "slug": "C.-Yao",
                        "structuredName": {
                            "firstName": "Cong",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145905113"
                        ],
                        "name": "X. Bai",
                        "slug": "X.-Bai",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Bai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "Comprehensive surveys of related works were given in [9], [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3405510,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bf3ca2a672298b65a47741c429baa29bb567e38c",
            "isKey": false,
            "numCitedBy": 273,
            "numCiting": 103,
            "paperAbstract": {
                "fragments": [],
                "text": "Text, as one of the most influential inventions of humanity, has played an important role in human life, so far from ancient times. The rich and precise information embodied in text is very useful in a wide range of vision-based applications, therefore text detection and recognition in natural scenes have become important and active research topics in computer vision and document analysis. Especially in recent years, the community has seen a surge of research efforts and substantial progresses in these fields, though a variety of challenges (e.g. noise, blur, distortion, occlusion and variation) still remain. The purposes of this survey are three-fold: 1) introduce up-to-date works, 2) identify state-of-the-art algorithms, and 3) predict potential research directions in the future. Moreover, this paper provides comprehensive links to publicly available resources, including benchmark datasets, source codes, and online demos. In summary, this literature review can serve as a good reference for researchers in the areas of scene text detection and recognition."
            },
            "slug": "Scene-text-detection-and-recognition:-recent-and-Zhu-Yao",
            "title": {
                "fragments": [],
                "text": "Scene text detection and recognition: recent advances and future trends"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This literature review can serve as a good reference for researchers in the areas of scene text detection and recognition and identify state-of-the-art algorithms, and predict potential research directions in the future."
            },
            "venue": {
                "fragments": [],
                "text": "Frontiers of Computer Science"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735300"
                        ],
                        "name": "S. Haykin",
                        "slug": "S.-Haykin",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Haykin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Haykin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713858"
                        ],
                        "name": "B. Kosko",
                        "slug": "B.-Kosko",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Kosko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Kosko"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 65
                            }
                        ],
                        "text": "Consequently, the most popular deep models like DCNN [25, 26] cannot be directly applied to sequence prediction, since DCNN models often operate on inputs and outputs with fixed dimensions, and thus are incapable of producing a variable-length label sequence."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 64294544,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f42b865e20e61a954239f421b42007236e671f19",
            "isKey": false,
            "numCitedBy": 3504,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer Neural Networks trained with the backpropagation algorithm constitute the best example of a successful Gradient-Based Learning technique. Given an appropriate network architecture, Gradient-Based Learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional Neural Networks, that are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called Graph Transformer Networks (GTN), allows such multi-module systems to be trained globally using Gradient-Based methods so as to minimize an overall performance measure. Two systems for on-line handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of Graph Transformer Networks. A Graph Transformer Network for reading bank check is also described. It uses Convolutional Neural Network character recognizers combined with global training techniques to provides record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day."
            },
            "slug": "GradientBased-Learning-Applied-to-Document-Haykin-Kosko",
            "title": {
                "fragments": [],
                "text": "GradientBased Learning Applied to Document Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Various methods applied to handwritten character recognition are reviewed and compared and Convolutional Neural Networks, that are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7408951"
                        ],
                        "name": "Jeff Donahue",
                        "slug": "Jeff-Donahue",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Donahue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Donahue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652147"
                        ],
                        "name": "J. Malik",
                        "slug": "J.-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 135
                            }
                        ],
                        "text": "However, majority of the recent works related to deep neural networks have devoted to detection or classification of object categories [1], [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 135
                            }
                        ],
                        "text": "Being robust, rich and trainable, deep convolutional features have been widely adopted for different kinds of visual recognition tasks [1], [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 215827080,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f4df08d9072fc2ac181b7fced6a245315ce05c8",
            "isKey": false,
            "numCitedBy": 16987,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn."
            },
            "slug": "Rich-Feature-Hierarchies-for-Accurate-Object-and-Girshick-Donahue",
            "title": {
                "fragments": [],
                "text": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper proposes a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726358"
                        ],
                        "name": "A. Bissacco",
                        "slug": "A.-Bissacco",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Bissacco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bissacco"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152710625"
                        ],
                        "name": "M. Cummins",
                        "slug": "M.-Cummins",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Cummins",
                            "middleNames": [
                                "Joseph"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Cummins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34180232"
                        ],
                        "name": "Yuval Netzer",
                        "slug": "Yuval-Netzer",
                        "structuredName": {
                            "firstName": "Yuval",
                            "lastName": "Netzer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuval Netzer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2665814"
                        ],
                        "name": "H. Neven",
                        "slug": "H.-Neven",
                        "structuredName": {
                            "firstName": "Hartmut",
                            "lastName": "Neven",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Neven"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 71
                            }
                        ],
                        "text": "generally outperformed by previous algorithms based on neural networks [5], [6], aswell as the approach proposed in this paper."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 99
                            }
                        ],
                        "text": "Our method uses only synthetic text with word level labels as the training data, very different to PhotoOCR [8] which used 7.9 millions of real word images with character-level annotations for training."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 36
                            }
                        ],
                        "text": "For example, the algorithms in [4], [5] first detect individual characters and then recognize these detected characters with DCNN models, which are trained using labeled character images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 84
                            }
                        ],
                        "text": "highly competitive performance on scene text (word recognition) than the prior arts [5], [19]; 6) It contains much less parameters than a standard DCNNmodel, consuming less storage space."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 45
                            }
                        ],
                        "text": "Particularly, our model outperforms PhotoOCR [5] which used 7."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3149088,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "31a8803d7e2618bfa44c472d003055bb5961b9de",
            "isKey": true,
            "numCitedBy": 401,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe Photo OCR, a system for text extraction from images. Our particular focus is reliable text extraction from smartphone imagery, with the goal of text recognition as a user input modality similar to speech recognition. Commercially available OCR performs poorly on this task. Recent progress in machine learning has substantially improved isolated character classification, we build on this progress by demonstrating a complete OCR system using these techniques. We also incorporate modern data center-scale distributed language modelling. Our approach is capable of recognizing text in a variety of challenging imaging conditions where traditional OCR systems fail, notably in the presence of substantial blur, low resolution, low contrast, high image noise and other distortions. It also operates with low latency, mean processing time is 600 ms per image. We evaluate our system on public benchmark datasets for text extraction and outperform all previously reported results, more than halving the error rate on multiple benchmarks. The system is currently in use in many applications at Google, and is available as a user input modality in Google Translate for Android."
            },
            "slug": "PhotoOCR:-Reading-Text-in-Uncontrolled-Conditions-Bissacco-Cummins",
            "title": {
                "fragments": [],
                "text": "PhotoOCR: Reading Text in Uncontrolled Conditions"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "This work describes Photo OCR, a system for text extraction from images that is capable of recognizing text in a variety of challenging imaging conditions where traditional OCR systems fail, notably in the presence of substantial blur, low resolution, low contrast, high image noise and other distortions."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Computer Vision"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 37
                            }
                        ],
                        "text": "In summary, current systems based on DCNN can not be directly used for image-based sequence recognition."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 53
                            }
                        ],
                        "text": "Consequently, the most popular deep models like DCNN [2], [3] cannot be directly applied to sequence prediction, since DCNN models often operate on inputs and outputs with fixed dimensions, and thus are incapable of producing a variable-length label sequence."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "Consequently, the most popular deep models like DCNN [25, 26] cannot be directly applied to sequence prediction, since DCNN models often operate on inputs and outputs with fixed dimensions, and thus are incapable of producing a variable-length label sequence."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 126
                            }
                        ],
                        "text": "For example, the algorithms in [35, 8] firstly detect individual characters and then recognize these detected characters with DCNN models, which are trained using labeled character images."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 140
                            }
                        ],
                        "text": "Being robust, rich and trainable, deep convolutional features have been widely adopted for different kinds of visual recognition tasks [1], [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 122
                            }
                        ],
                        "text": "The proposed neural network model is named as Convolutional Recurrent Neural Network (CRNN), since it is a combination of DCNN and RNN."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 285,
                                "start": 281
                            }
                        ],
                        "text": "For sequence-like objects, CRNN possesses several distinctive advantages over conventional neural network models: 1) It can be directly learned from sequence labels (for instance, words), requiring no detailed annotations (for instance, characters); 2) It has the same property of DCNN on learning informative representations directly from image data, requiring neither hand-craft features nor preprocessing steps, including binarization/segmentation, component localization, etc.; 3) It has the same property of RNN, being able to produce a sequence of labels; 4) It is unconstrained to the lengths of sequence-like objects, requiring only height normalization in both training and testing phases; 5) It achieves better or highly competitive performance on scene texts (word recognition) than the prior arts [23, 8]; 6) It contains much less parameters than a standard DCNN model, consuming less storage space."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 198
                            }
                        ],
                        "text": "Recently, the community has seen a strong revival of neural networks, which is mainly stimulated by the great success of deep neural network models, specifically Deep Convolutional Neural Networks (DCNN), in various vision tasks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 140
                            }
                        ],
                        "text": "However, majority of the recent works related to deep neural networks have devoted to detection or classification of object categories [1], [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 195908774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "isKey": true,
            "numCitedBy": 80692,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry."
            },
            "slug": "ImageNet-classification-with-deep-convolutional-Krizhevsky-Sutskever",
            "title": {
                "fragments": [],
                "text": "ImageNet classification with deep convolutional neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A large, deep convolutional neural network was trained to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes and employed a recently developed regularization method called \"dropout\" that proved to be very effective."
            },
            "venue": {
                "fragments": [],
                "text": "Commun. ACM"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40360972"
                        ],
                        "name": "Abdel-rahman Mohamed",
                        "slug": "Abdel-rahman-Mohamed",
                        "structuredName": {
                            "firstName": "Abdel-rahman",
                            "lastName": "Mohamed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abdel-rahman Mohamed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "Therefore, we follow [23] and combine two LSTMs, one forward and one backward, into a bidi-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "ments in the task of speech recognition [23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206741496,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d",
            "isKey": false,
            "numCitedBy": 6879,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score."
            },
            "slug": "Speech-recognition-with-deep-recurrent-neural-Graves-Mohamed",
            "title": {
                "fragments": [],
                "text": "Speech recognition with deep recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34838386"
                        ],
                        "name": "K. Simonyan",
                        "slug": "K.-Simonyan",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Simonyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Simonyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "The architecture of the convolutional layers is derived from the VGG-VeryDeep architectures [34]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14124313,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb42cf88027de515750f230b23b1a057dc782108",
            "isKey": false,
            "numCitedBy": 62016,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision."
            },
            "slug": "Very-Deep-Convolutional-Networks-for-Large-Scale-Simonyan-Zisserman",
            "title": {
                "fragments": [],
                "text": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This work investigates the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting using an architecture with very small convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145084658"
                        ],
                        "name": "Zhen Zuo",
                        "slug": "Zhen-Zuo",
                        "structuredName": {
                            "firstName": "Zhen",
                            "lastName": "Zuo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhen Zuo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2521776"
                        ],
                        "name": "Bing Shuai",
                        "slug": "Bing-Shuai",
                        "structuredName": {
                            "firstName": "Bing",
                            "lastName": "Shuai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bing Shuai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2096527"
                        ],
                        "name": "G. Wang",
                        "slug": "G.-Wang",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111312232"
                        ],
                        "name": "Xiao Liu",
                        "slug": "Xiao-Liu",
                        "structuredName": {
                            "firstName": "Xiao",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiao Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144802333"
                        ],
                        "name": "Xingxing Wang",
                        "slug": "Xingxing-Wang",
                        "structuredName": {
                            "firstName": "Xingxing",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xingxing Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145952970"
                        ],
                        "name": "B. Wang",
                        "slug": "B.-Wang",
                        "structuredName": {
                            "firstName": "Bing",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2597809"
                        ],
                        "name": "Yushi Chen",
                        "slug": "Yushi-Chen",
                        "structuredName": {
                            "firstName": "Yushi",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yushi Chen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 106
                            }
                        ],
                        "text": "Note that integrating DCNN and RNN in an end-to-end manner has also been explored in image classification [17], where the RNN is onlyused formodeling spatial dependen-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16889475,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ad3b3d00d3601190dd04155d69c343fb03bfe704",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "In existing convolutional neural networks (CNNs), both convolution and pooling are locally performed for image regions separately, no contextual dependencies between different image regions have been taken into consideration. Such dependencies represent useful spatial structure information in images. Whereas recurrent neural networks (RNNs) are designed for learning contextual dependencies among sequential data by using the recurrent (feedback) connections. In this work, we propose the convolutional recurrent neural network (C-RNN), which learns the spatial dependencies between image regions to enhance the discriminative power of image representation. The C-RNN is trained in an end-to-end manner from raw pixel images. CNN layers are firstly processed to generate middle level features. RNN layer is then learned to encode spatial dependencies. The C-RNN can learn better image representation, especially for images with obvious spatial contextual dependencies. Our method achieves competitive performance on ILSVRC 2012, SUN 397, and MIT indoor."
            },
            "slug": "Convolutional-recurrent-neural-networks:-Learning-Zuo-Shuai",
            "title": {
                "fragments": [],
                "text": "Convolutional recurrent neural networks: Learning spatial dependencies for image representation"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The convolutional recurrent neural network (C-RNN) is proposed, which learns the spatial dependencies between image regions to enhance the discriminative power of image representation and achieves competitive performance on ILSVRC 2012, SUN 397, and MIT indoor."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145815031"
                        ],
                        "name": "S. Lucas",
                        "slug": "S.-Lucas",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Lucas",
                            "middleNames": [
                                "M.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lucas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87531536"
                        ],
                        "name": "A. Panaretos",
                        "slug": "A.-Panaretos",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Panaretos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Panaretos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073684197"
                        ],
                        "name": "Luis Sosa",
                        "slug": "Luis-Sosa",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Sosa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luis Sosa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052189571"
                        ],
                        "name": "Anthony Tang",
                        "slug": "Anthony-Tang",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anthony Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108862960"
                        ],
                        "name": "Shirley Wong",
                        "slug": "Shirley-Wong",
                        "structuredName": {
                            "firstName": "Shirley",
                            "lastName": "Wong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shirley Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114080648"
                        ],
                        "name": "Robert Young",
                        "slug": "Robert-Young",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Young",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Young"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065499715"
                        ],
                        "name": "Kazuki Ashida",
                        "slug": "Kazuki-Ashida",
                        "structuredName": {
                            "firstName": "Kazuki",
                            "lastName": "Ashida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kazuki Ashida"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055905787"
                        ],
                        "name": "Hiroki Nagai",
                        "slug": "Hiroki-Nagai",
                        "structuredName": {
                            "firstName": "Hiroki",
                            "lastName": "Nagai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hiroki Nagai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47471571"
                        ],
                        "name": "Masayuki Okamoto",
                        "slug": "Masayuki-Okamoto",
                        "structuredName": {
                            "firstName": "Masayuki",
                            "lastName": "Okamoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Masayuki Okamoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152933693"
                        ],
                        "name": "Hiroaki Yamamoto",
                        "slug": "Hiroaki-Yamamoto",
                        "structuredName": {
                            "firstName": "Hiroaki",
                            "lastName": "Yamamoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hiroaki Yamamoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34593830"
                        ],
                        "name": "H. Miyao",
                        "slug": "H.-Miyao",
                        "structuredName": {
                            "firstName": "Hidetoshi",
                            "lastName": "Miyao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Miyao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146280843"
                        ],
                        "name": "JunMin Zhu",
                        "slug": "JunMin-Zhu",
                        "structuredName": {
                            "firstName": "JunMin",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "JunMin Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2873679"
                        ],
                        "name": "WuWen Ou",
                        "slug": "WuWen-Ou",
                        "structuredName": {
                            "firstName": "WuWen",
                            "lastName": "Ou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "WuWen Ou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144899680"
                        ],
                        "name": "Christian Wolf",
                        "slug": "Christian-Wolf",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Wolf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680935"
                        ],
                        "name": "J. Jolion",
                        "slug": "J.-Jolion",
                        "structuredName": {
                            "firstName": "Jean-Michel",
                            "lastName": "Jolion",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Jolion"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1844464"
                        ],
                        "name": "L. Todoran",
                        "slug": "L.-Todoran",
                        "structuredName": {
                            "firstName": "Leon",
                            "lastName": "Todoran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Todoran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717056"
                        ],
                        "name": "M. Worring",
                        "slug": "M.-Worring",
                        "structuredName": {
                            "firstName": "Marcel",
                            "lastName": "Worring",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Worring"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46520089"
                        ],
                        "name": "Xiaofan Lin",
                        "slug": "Xiaofan-Lin",
                        "structuredName": {
                            "firstName": "Xiaofan",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaofan Lin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 5
                            }
                        ],
                        "text": "IC03 [30] test dataset contains 251 scene images with labeled text bounding boxes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "The average testing time is 0.16s/sample, as measured on IC03 without a lexicon."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "IIIT5k SVT IC03 IC13 50 1k None 50 None 50 Full 50k None None\nABBYY [34] 24.3 - - 35.0 - 56.0 55.0 - - - Wang et al. [34] - - - 57.0 - 76.0 62.0 - - - Mishra et al. [28] 64.1 57.5 - 73.2 - 81.8 67.8 - - - Wang et al. [35] - - - 70.0 - 90.0 84.0 - - - Goel et al. [13] - - - 77.3 - 89.7 - - - - Bissacco et al. [8] - - - 90.4 78.0 - - - - 87.6 Alsharif and Pineau [6] - - - 74.3 - 93.1 88.6 85.1 - - Almaza\u0301n et al. [5] 91.2 82.1 - 89.2 - - - - - - Yao et al. [36] 80.2 69.3 - 75.9 - 88.5 80.3 - - - Rodrguez-Serrano et al. [30] 76.1 57.4 - 70.0 - - - - - - Jaderberg et al. [23] - - - 86.1 - 96.2 91.5 - - - Su and Lu [33] - - - 83.0 - 92.0 82.0 - - - Gordo [14] 93.3 86.6 - 91.8 - - - - - - Jaderberg et al. [22] 97.1 92.7 - 95.4 80.7* 98.7 98.6 93.3 93.1* 90.8* Jaderberg et al. [21] 95.5 89.6 - 93.2 71.7 97.8 97.0 93.4 89.6 81.8\nCRNN 97.6 94.4 78.2 96.4 80.8 98.7 97.6 95.5 89.4 86.7\nNotice that though the recent models learned by label embedding [5, 14] and incremental learning [22] achieved highly competitive performance, they are constrained to a specific dictionary."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "IC03 [27] test dataset contains 251 scene images with labeled text bounding boxes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 107
                            }
                        ],
                        "text": "Four popular benchmarks for scene text recognition are used for performance evaluation, namely ICDAR 2003 (IC03), ICDAR 2013 (IC13), IIIT 5k-word (IIIT5k), and Street View Text (SVT)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 133
                            }
                        ],
                        "text": "In the unconstrained lexicon cases, our method achieves the best performance on SVT, yet, is still behind some approaches [8, 22] on IC03 and IC13."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "IC13 [24] test dataset inherits most of its data from IC03."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 117
                            }
                        ],
                        "text": "Specifically, we obtain superior performance on IIIT5k, and SVT compared to [22], only achieved lower performance on IC03 with the \u201cFull\u201d lexicon."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 64
                            }
                        ],
                        "text": "The approximate lexicon search is applied to the 50k lexicon of IC03, with the parameter \u03b4 set to 3."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2250003,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a01deac56a81646e8d84cb7bf2d905714ff00808",
            "isKey": true,
            "numCitedBy": 235,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract.This paper describes the robust reading competitions for ICDAR 2003. With the rapid growth in research over the last few years on recognizing text in natural scenes, there is an urgent need to establish some common benchmark datasets and gain a clear understanding of the current state of the art. We use the term \u2018robust reading\u2019 to refer to text images that are beyond the capabilities of current commercial OCR packages. We chose to break down the robust reading problem into three subproblems and run competitions for each stage, and also a competition for the best overall system. The subproblems we chose were text locating, character recognition and word recognition. By breaking down the problem in this way, we hoped to gain a better understanding of the state of the art in each of the subproblems. Furthermore, our methodology involved storing detailed results of applying each algorithm to each image in the datasets, allowing researchers to study in depth the strengths and weaknesses of each algorithm. The text-locating contest was the only one to have any entries. We give a brief description of each entry and present the results of this contest, showing cases where the leading entries succeed and fail. We also describe an algorithm for combining the outputs of the individual text locators and show how the combination scheme improves on any of the individual systems."
            },
            "slug": "ICDAR-2003-robust-reading-competitions:-entries,-Lucas-Panaretos",
            "title": {
                "fragments": [],
                "text": "ICDAR 2003 robust reading competitions: entries, results, and future directions"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper broke down the robust reading problem into three subproblems and run competitions for each stage, and also a competition for the best overall system, and described an algorithm for combining the outputs of the individual text locators and showed how the combination scheme improves on any of theindividual systems."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Document Analysis and Recognition (IJDAR)"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143616982"
                        ],
                        "name": "Ana Rebelo",
                        "slug": "Ana-Rebelo",
                        "structuredName": {
                            "firstName": "Ana",
                            "lastName": "Rebelo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ana Rebelo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2190426"
                        ],
                        "name": "Ichiro Fujinaga",
                        "slug": "Ichiro-Fujinaga",
                        "structuredName": {
                            "firstName": "Ichiro",
                            "lastName": "Fujinaga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ichiro Fujinaga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2828598"
                        ],
                        "name": "F. Paszkiewicz",
                        "slug": "F.-Paszkiewicz",
                        "structuredName": {
                            "firstName": "Filipe",
                            "lastName": "Paszkiewicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Paszkiewicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2136076"
                        ],
                        "name": "A. Mar\u00e7al",
                        "slug": "A.-Mar\u00e7al",
                        "structuredName": {
                            "firstName": "Andr\u00e9",
                            "lastName": "Mar\u00e7al",
                            "middleNames": [
                                "R.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mar\u00e7al"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144453899"
                        ],
                        "name": "C. Guedes",
                        "slug": "C.-Guedes",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Guedes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Guedes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3698192"
                        ],
                        "name": "Jaime S. Cardoso",
                        "slug": "Jaime-S.-Cardoso",
                        "structuredName": {
                            "firstName": "Jaime",
                            "lastName": "Cardoso",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jaime S. Cardoso"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 130
                            }
                        ],
                        "text": "Previous methods often requires image preprocessing (mostly binirization), staff lines detection and individual notes recognition [39]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12964479,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46ec9789856ddab19e3d0011cadc08b419b533f3",
            "isKey": false,
            "numCitedBy": 243,
            "numCiting": 144,
            "paperAbstract": {
                "fragments": [],
                "text": "For centuries, music has been shared and remembered by two traditions: aural transmission and in the form of written documents normally called musical scores. Many of these scores exist in the form of unpublished manuscripts and hence they are in danger of being lost through the normal ravages of time. To preserve the music some form of typesetting or, ideally, a computer system that can automatically decode the symbolic images and create new scores is required. Programs analogous to optical character recognition systems called optical music recognition (OMR) systems have been under intensive development for many years. However, the results to date are far from ideal. Each of the proposed methods emphasizes different properties and therefore makes it difficult to effectively evaluate its competitive advantages. This article provides an overview of the literature concerning the automatic analysis of images of printed and handwritten musical scores. For self-containment and for the benefit of the reader, an introduction to OMR processing systems precedes the literature overview. The following study presents a reference scheme for any researcher wanting to compare new OMR algorithms against well-known ones."
            },
            "slug": "Optical-music-recognition:-state-of-the-art-and-Rebelo-Fujinaga",
            "title": {
                "fragments": [],
                "text": "Optical music recognition: state-of-the-art and open issues"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An overview of the literature concerning the automatic analysis of images of printed and handwritten musical scores and a reference scheme for any researcher wanting to compare new OMR algorithms against well-known ones is presented."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Multimedia Information Retrieval"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143913738"
                        ],
                        "name": "Santiago Fern\u00e1ndez",
                        "slug": "Santiago-Fern\u00e1ndez",
                        "structuredName": {
                            "firstName": "Santiago",
                            "lastName": "Fern\u00e1ndez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Santiago Fern\u00e1ndez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145842938"
                        ],
                        "name": "Faustino J. Gomez",
                        "slug": "Faustino-J.-Gomez",
                        "structuredName": {
                            "firstName": "Faustino",
                            "lastName": "Gomez",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Faustino J. Gomez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 132
                            }
                        ],
                        "text": "particular, in the transcription layer, error differentials are backpropagated with the forward-backward algorithm, as described in [24]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "efficiently computed using the forward-backward algorithm described in [24]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9901844,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96494e722f58705fa20302fe6179d483f52705b4",
            "isKey": false,
            "numCitedBy": 3456,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN."
            },
            "slug": "Connectionist-temporal-classification:-labelling-Graves-Fern\u00e1ndez",
            "title": {
                "fragments": [],
                "text": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems of sequence learning and post-processing."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054165706"
                        ],
                        "name": "S. Ioffe",
                        "slug": "S.-Ioffe",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Ioffe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ioffe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574060"
                        ],
                        "name": "Christian Szegedy",
                        "slug": "Christian-Szegedy",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Szegedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Szegedy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 37
                            }
                        ],
                        "text": "We find that the batch normalization [35] technique is critical for training a network of such depth."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5808102,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d376d6978dad0374edfa6709c9556b42d3594d3",
            "isKey": false,
            "numCitedBy": 29123,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters."
            },
            "slug": "Batch-Normalization:-Accelerating-Deep-Network-by-Ioffe-Szegedy",
            "title": {
                "fragments": [],
                "text": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694936"
                        ],
                        "name": "Qixiang Ye",
                        "slug": "Qixiang-Ye",
                        "structuredName": {
                            "firstName": "Qixiang",
                            "lastName": "Ye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qixiang Ye"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 53
                            }
                        ],
                        "text": "Comprehensive surveys of related works were given in [9], [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5729190,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "caec97674544a4948a1b0ec2b9f6c624b87b647b",
            "isKey": false,
            "numCitedBy": 590,
            "numCiting": 230,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper analyzes, compares, and contrasts technical challenges, methods, and the performance of text detection and recognition research in color imagery. It summarizes the fundamental problems and enumerates factors that should be considered when addressing these problems. Existing techniques are categorized as either stepwise or integrated and sub-problems are highlighted including text localization, verification, segmentation and recognition. Special issues associated with the enhancement of degraded text and the processing of video text, multi-oriented, perspectively distorted and multilingual text are also addressed. The categories and sub-categories of text are illustrated, benchmark datasets are enumerated, and the performance of the most representative approaches is compared. This review provides a fundamental comparison and analysis of the remaining problems in the field."
            },
            "slug": "Text-Detection-and-Recognition-in-Imagery:-A-Survey-Ye-Doermann",
            "title": {
                "fragments": [],
                "text": "Text Detection and Recognition in Imagery: A Survey"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This review provides a fundamental comparison and analysis of the remaining problems in the field and summarizes the fundamental problems and enumerates factors that should be considered when addressing these problems."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694974"
                        ],
                        "name": "Dimosthenis Karatzas",
                        "slug": "Dimosthenis-Karatzas",
                        "structuredName": {
                            "firstName": "Dimosthenis",
                            "lastName": "Karatzas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dimosthenis Karatzas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688013"
                        ],
                        "name": "F. Shafait",
                        "slug": "F.-Shafait",
                        "structuredName": {
                            "firstName": "Faisal",
                            "lastName": "Shafait",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shafait"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809705"
                        ],
                        "name": "S. Uchida",
                        "slug": "S.-Uchida",
                        "structuredName": {
                            "firstName": "Seiichi",
                            "lastName": "Uchida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Uchida"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35613969"
                        ],
                        "name": "M. Iwamura",
                        "slug": "M.-Iwamura",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Iwamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Iwamura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2730090"
                        ],
                        "name": "L. G. I. Bigorda",
                        "slug": "L.-G.-I.-Bigorda",
                        "structuredName": {
                            "firstName": "Llu\u00eds",
                            "lastName": "Bigorda",
                            "middleNames": [
                                "G\u00f3mez",
                                "i"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. G. I. Bigorda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2729896"
                        ],
                        "name": "Sergi Robles Mestre",
                        "slug": "Sergi-Robles-Mestre",
                        "structuredName": {
                            "firstName": "Sergi",
                            "lastName": "Mestre",
                            "middleNames": [
                                "Robles"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sergi Robles Mestre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40016884"
                        ],
                        "name": "J. M. Romeu",
                        "slug": "J.-M.-Romeu",
                        "structuredName": {
                            "firstName": "Joan",
                            "lastName": "Romeu",
                            "middleNames": [
                                "Mas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. M. Romeu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50313341"
                        ],
                        "name": "D. F. Mota",
                        "slug": "D.-F.-Mota",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mota",
                            "middleNames": [
                                "Fern\u00e1ndez"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. F. Mota"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467588"
                        ],
                        "name": "Jon Almaz\u00e1n",
                        "slug": "Jon-Almaz\u00e1n",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Almaz\u00e1n",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jon Almaz\u00e1n"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2578506"
                        ],
                        "name": "Llu\u00eds-Pere de las Heras",
                        "slug": "Llu\u00eds-Pere-de-las-Heras",
                        "structuredName": {
                            "firstName": "Llu\u00eds-Pere",
                            "lastName": "Heras",
                            "middleNames": [
                                "de",
                                "las"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Llu\u00eds-Pere de las Heras"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "IIIT5k SVT IC03 IC13 50 1k None 50 None 50 Full 50k None None\nABBYY [34] 24.3 - - 35.0 - 56.0 55.0 - - - Wang et al. [34] - - - 57.0 - 76.0 62.0 - - - Mishra et al. [28] 64.1 57.5 - 73.2 - 81.8 67.8 - - - Wang et al. [35] - - - 70.0 - 90.0 84.0 - - - Goel et al. [13] - - - 77.3 - 89.7 - - - - Bissacco et al. [8] - - - 90.4 78.0 - - - - 87.6 Alsharif and Pineau [6] - - - 74.3 - 93.1 88.6 85.1 - - Almaza\u0301n et al. [5] 91.2 82.1 - 89.2 - - - - - - Yao et al. [36] 80.2 69.3 - 75.9 - 88.5 80.3 - - - Rodrguez-Serrano et al. [30] 76.1 57.4 - 70.0 - - - - - - Jaderberg et al. [23] - - - 86.1 - 96.2 91.5 - - - Su and Lu [33] - - - 83.0 - 92.0 82.0 - - - Gordo [14] 93.3 86.6 - 91.8 - - - - - - Jaderberg et al. [22] 97.1 92.7 - 95.4 80.7* 98.7 98.6 93.3 93.1* 90.8* Jaderberg et al. [21] 95.5 89.6 - 93.2 71.7 97.8 97.0 93.4 89.6 81.8\nCRNN 97.6 94.4 78.2 96.4 80.8 98.7 97.6 95.5 89.4 86.7\nNotice that though the recent models learned by label embedding [5, 14] and incremental learning [22] achieved highly competitive performance, they are constrained to a specific dictionary."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 126
                            }
                        ],
                        "text": "Four popular benchmarks for scene text recognition are used for performance evaluation, namely ICDAR 2003 (IC03), ICDAR 2013 (IC13), IIIT 5k-word (IIIT5k), and Street View Text (SVT)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 142
                            }
                        ],
                        "text": "In the unconstrained lexicon cases, our method achieves the best performance on SVT, yet, is still behind some approaches [8, 22] on IC03 and IC13."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "IC13 [24] test dataset inherits most of its data from IC03."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 5
                            }
                        ],
                        "text": "IC13 [32] test dataset inherits most of its data from IC03."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206777226,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fcd7b547bf0a6646a282f521db880e74974aa838",
            "isKey": true,
            "numCitedBy": 879,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "This report presents the final results of the ICDAR 2013 Robust Reading Competition. The competition is structured in three Challenges addressing text extraction in different application domains, namely born-digital images, real scene images and real-scene videos. The Challenges are organised around specific tasks covering text localisation, text segmentation and word recognition. The competition took place in the first quarter of 2013, and received a total of 42 submissions over the different tasks offered. This report describes the datasets and ground truth specification, details the performance evaluation protocols used and presents the final results along with a brief summary of the participating methods."
            },
            "slug": "ICDAR-2013-Robust-Reading-Competition-Karatzas-Shafait",
            "title": {
                "fragments": [],
                "text": "ICDAR 2013 Robust Reading Competition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The datasets and ground truth specification are described, the performance evaluation protocols used are details, and the final results are presented along with a brief summary of the participating methods."
            },
            "venue": {
                "fragments": [],
                "text": "2013 12th International Conference on Document Analysis and Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308557"
                        ],
                        "name": "S. Hochreiter",
                        "slug": "S.-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hochreiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 11
                            }
                        ],
                        "text": "Long-Short Term Memory [18, 11] (LSTM) is a type of RNN unit that is specially designed to address this problem."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "Long-Short Term Memory [21], [22] (LSTM) is a type of RNN unit that is specially designed to address this problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1915014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44d2abe2175df8153f465f6c39b68b76a0d40ab9",
            "isKey": false,
            "numCitedBy": 51557,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms."
            },
            "slug": "Long-Short-Term-Memory-Hochreiter-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Long Short-Term Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel, efficient, gradient based method called long short-term memory (LSTM) is introduced, which can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "Traditional RNN unit, however, suffers from the vanishing gradient problem [20], which limits the range of context it can store, and adds burden to the training process."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206457500,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0be39ee052d246ae99c082a565aba25b811be2d",
            "isKey": false,
            "numCitedBy": 6130,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered."
            },
            "slug": "Learning-long-term-dependencies-with-gradient-is-Bengio-Simard",
            "title": {
                "fragments": [],
                "text": "Learning long-term dependencies with gradient descent is difficult"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work shows why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases, and exposes a trade-off between efficient learning by gradient descent and latching on information for long periods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48799969"
                        ],
                        "name": "Matthew D. Zeiler",
                        "slug": "Matthew-D.-Zeiler",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Zeiler",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew D. Zeiler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 29
                            }
                        ],
                        "text": "For optimization, we use the ADADELTA [37] to automatically calculate per-dimension learning rates."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "Networks are trained with ADADELTA [27], setting the parameter r to 0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "For optimization, we use the ADADELTA [27] to automatically calculate per-dimension learning rates."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 53
                            }
                        ],
                        "text": "Compared with the conventional momentum [31] method, ADADELTA requires no manual setting of a learning rate."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 50
                            }
                        ],
                        "text": "More importantly, we find that optimization using ADADELTA converges faster than the momentum method."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 26
                            }
                        ],
                        "text": "Networks are trained with ADADELTA, setting the parameter \u03c1 to 0.9."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7365802,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8729441d734782c3ed532a7d2d9611b438c0a09a",
            "isKey": true,
            "numCitedBy": 5458,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
            },
            "slug": "ADADELTA:-An-Adaptive-Learning-Rate-Method-Zeiler",
            "title": {
                "fragments": [],
                "text": "ADADELTA: An Adaptive Learning Rate Method"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A novel per-dimension learning rate method for gradient descent called ADADELTA that dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent is presented."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "Compared with the conventional momentum [28] method, ADADELTA requires no manual setting of a learning rate."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 205001834,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "052b1d8ce63b07fec3de9dbb583772d860b7c769",
            "isKey": false,
            "numCitedBy": 20289,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1."
            },
            "slug": "Learning-representations-by-back-propagating-errors-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning representations by back-propagating errors"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "Back-propagation repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector, which helps to represent important features of the task domain."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47022813"
                        ],
                        "name": "J.A. Anderson",
                        "slug": "J.A.-Anderson",
                        "structuredName": {
                            "firstName": "J.A.",
                            "lastName": "Anderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J.A. Anderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061358113"
                        ],
                        "name": "Edward Rosenfeld",
                        "slug": "Edward-Rosenfeld",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward Rosenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "Compared with the conventional momentum [31] method, ADADELTA requires no manual setting of a learning rate."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8160958,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d0e5a21512d2aea34026f83b1ff86ea30b8c0d6",
            "isKey": false,
            "numCitedBy": 983,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "ion for Knowledge Acquisition\u201d by T. Bylander and B. Chadrasekaran. Chandrasekaran\u2019s papers are usually illuminating, and this one does not fail: He and Bylander re-examine such traditional beliefs as knowledge should be uniformly represented and controlled and the knowledge base should be separated from the inference engine. The final 10 papers in volume 1 discuss generalized learning and ruleinduction techniques. They are interesting and informative, particularly \u201cGeneralization and Noise\u201d by Y. Kodratoff and M. Manango, which discusses symbolic and numeric rule induction. Most rule-induction techniques focus on the use of examples and numeric analysis such as repertory grids. Kodratoff\u2019s and Manango\u2019s exploration of how the two complement each other is refreshing. Because of their technical nature and the amount of work it would take to put their content to use, most of the papers in this section of the volume are more appropriate for a specialized or research-oriented group. For those just getting involved in knowledge-based\u2013systems development, Knowledge Acquisition Tools for Expert Systems is the more useful volume. In addition to discussing the tools themselves, most of the papers contain details of the knowledgeacquisition techniques that are automated, thus providing much of the same information which is available in the first volume. As an added benefit, they also often discuss the underlying architectures for solving domain-specific problems. For instance, the details of the medical diagnostic architecture laid out in \u201cDesign for Acquisition: Principles of Knowledge System Design to Facilitate Knowledge Acquisition\u201d by T. R. Gruber and P. R. Cohen are almost as useful as the discussion of how to build a knowledge-acquisition system. Volume 2 is particularly germane given the rise in commercial interest about automated knowledge acquisition following this year\u2019s introduction of Neuron Data\u2019s NEXTRATM product and last year\u2019s introduction of Test Bench by Texas Instruments. Test Bench is actually discussed in \u201cA Mixed-Initiative Workbench for Knowledge Acquisition\u201d by G. S. Kahn, E. H. Breaux, P. De Klerk, and R. L. Joseph. This volume provides the background necessary to evaluate knowledge-acquisition tools such as NEXTRA, Test Bench, and AutoIntelligence (IntelligenceWare). The vendors of knowledge-based\u2013systems development tools, for example, Inference, IntelliCorp, Aion, AI Corp., and IBM, would do well to pay heed to these books because they point the way to removing the knowledge bottleneck from knowledge-based\u2013systems development. Overall, the papers in both volumes are comprehensive and well integrated, a sometimes difficult state to achieve when compiling a collection of papers resulting from a small conference. The collection is comparable to Anna Hart\u2019s Knowledge Acquisition for Expert Systems (McGraw-Hill, 1986), but it is broader in scope and not as structured. The arrangement of the papers is marred only by an overly brief index. Few readers can be expected to read a collection from beginning to end, and a better index would facilitate more enlightened use. Less important\u2014but nevertheless distracting\u2014is the large number of typographical errors in both volumes. In conclusion, the set is recommended for both the commercial and research knowledge-based\u2013systems practitioner. Reading the volumes in reverse order might be more useful to the commercial developer given the extra information available in volume 2. Neurocomputing: Foundations of Research"
            },
            "slug": "Neurocomputing:-Foundations-of-Research-Anderson-Rosenfeld",
            "title": {
                "fragments": [],
                "text": "Neurocomputing: Foundations of Research"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The set is recommended for both the commercial and research knowledge-based\u2013systems practitioner and provides the background necessary to evaluate knowledge-acquisition tools such as NEXTRA, Test Bench, and AutoIntelligence (IntelligenceWare)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2088368"
                        ],
                        "name": "F. Gers",
                        "slug": "F.-Gers",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Gers",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739396"
                        ],
                        "name": "N. Schraudolph",
                        "slug": "N.-Schraudolph",
                        "structuredName": {
                            "firstName": "Nicol",
                            "lastName": "Schraudolph",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Schraudolph"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "Long-Short Term Memory [21], [22] (LSTM) is a type of RNN unit that is specially designed to address this problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 474078,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "047655e733a9eed9a500afd916efa566915b9110",
            "isKey": false,
            "numCitedBy": 1269,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The temporal distance between events conveys information essential for numerous sequential tasks such as motor control and rhythm detection. While Hidden Markov Models tend to ignore this information, recurrent neural networks (RNNs) can in principle learn to make use of it. We focus on Long Short-Term Memory (LSTM) because it has been shown to outperform other RNNs on tasks involving long time lags. We find that LSTM augmented by \"peephole connections\" from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes spaced either 50 or 49 time steps apart without the help of any short training exemplars. Without external resets or teacher forcing, our LSTM variant also learns to generate stable streams of precisely timed spikes and other highly nonlinear periodic patterns. This makes LSTM a promising approach for tasks that require the accurate measurement or generation of time intervals."
            },
            "slug": "Learning-Precise-Timing-with-LSTM-Recurrent-Gers-Schraudolph",
            "title": {
                "fragments": [],
                "text": "Learning Precise Timing with LSTM Recurrent Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work finds that LSTM augmented by \"peephole connections\" from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes spaced either 50 or 49 time steps apart without the help of any short training exemplars."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939803"
                        ],
                        "name": "Ronan Collobert",
                        "slug": "Ronan-Collobert",
                        "structuredName": {
                            "firstName": "Ronan",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronan Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256269"
                        ],
                        "name": "C. Farabet",
                        "slug": "C.-Farabet",
                        "structuredName": {
                            "firstName": "Cl\u00e9ment",
                            "lastName": "Farabet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Farabet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 36
                            }
                        ],
                        "text": "We implement the network within the Torch7 [10] framework, with custom implementations for the LSTM units (in Torch7/CUDA), the transcription layer (in C++) and the BK-tree data structure (in C++)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "We implement the network within the Torch7 [36] framework, with custom implementations for the LSTM units (in Torch7/ CUDA), the transcription layer (in C++) and the BK-tree data structure (in C++)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14365368,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3449b65008b27f6e60a73d80c1fd990f0481126b",
            "isKey": false,
            "numCitedBy": 1490,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Torch7 is a versatile numeric computing framework and machine learning library that extends Lua. Its goal is to provide a flexible environment to design and train learning machines. Flexibility is obtained via Lua, an extremely lightweight scripting language. High performance is obtained via efficient OpenMP/SSE and CUDA implementations of low-level numeric routines. Torch7 can easily be interfaced to third-party software thanks to Lua\u2019s light interface."
            },
            "slug": "Torch7:-A-Matlab-like-Environment-for-Machine-Collobert-Kavukcuoglu",
            "title": {
                "fragments": [],
                "text": "Torch7: A Matlab-like Environment for Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "Torch7 is a versatile numeric computing framework and machine learning library that extends Lua that can easily be interfaced to third-party software thanks to Lua\u2019s light interface."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1829673"
                        ],
                        "name": "W. Burkhard",
                        "slug": "W.-Burkhard",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Burkhard",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Burkhard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8571043"
                        ],
                        "name": "R. Keller",
                        "slug": "R.-Keller",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Keller",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Keller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 80
                            }
                        ],
                        "text": "The candidates N d\u00f0l0\u00de can be found efficiently with the BK-tree data structure [26], which is a metric tree specifically adapted to discrete metric spaces."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17978145,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "86837dcbebc36ddffd97e1493060e9b58e337abc",
            "isKey": false,
            "numCitedBy": 412,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of searching the set of keys in a file to find a key which is closest to a given query key is discussed. After \u201cclosest,\u201d in terms of a metric on the the key space, is suitably defined, three file structures are presented together with their corresponding search algorithms, which are intended to reduce the number of comparisons required to achieve the desired result. These methods are derived using certain inequalities satisfied by metrics and by graph-theoretic concepts. Some empirical results are presented which compare the efficiency of the methods."
            },
            "slug": "Some-approaches-to-best-match-file-searching-Burkhard-Keller",
            "title": {
                "fragments": [],
                "text": "Some approaches to best-match file searching"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Three file structures are presented together with their corresponding search algorithms, which are intended to reduce the number of comparisons required to achieve the desired result."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743758"
                        ],
                        "name": "M. Liwicki",
                        "slug": "M.-Liwicki",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Liwicki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Liwicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143913738"
                        ],
                        "name": "Santiago Fern\u00e1ndez",
                        "slug": "Santiago-Fern\u00e1ndez",
                        "structuredName": {
                            "firstName": "Santiago",
                            "lastName": "Fern\u00e1ndez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Santiago Fern\u00e1ndez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2168488"
                        ],
                        "name": "Roman Bertolami",
                        "slug": "Roman-Bertolami",
                        "structuredName": {
                            "firstName": "Roman",
                            "lastName": "Bertolami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roman Bertolami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720945"
                        ],
                        "name": "H. Bunke",
                        "slug": "H.-Bunke",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bunke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bunke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[7] extract a set of geometrical or image features from handwritten text, while Su and Lu [8] convert word images into sequential HOG features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14635907,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "375214ac340226e23ec428e92ec499fb89f508b8",
            "isKey": false,
            "numCitedBy": 1586,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "Recognizing lines of unconstrained handwritten text is a challenging task. The difficulty of segmenting cursive or overlapping characters, combined with the need to exploit surrounding context, has led to low recognition rates for even the best current recognizers. Most recent progress in the field has been made either through improved preprocessing or through advances in language modeling. Relatively little work has been done on the basic recognition algorithms. Indeed, most systems rely on the same hidden Markov models that have been used for decades in speech and handwriting recognition, despite their well-known shortcomings. This paper proposes an alternative approach based on a novel type of recurrent neural network, specifically designed for sequence labeling tasks where the data is hard to segment and contains long-range bidirectional interdependencies. In experiments on two large unconstrained handwriting databases, our approach achieves word recognition accuracies of 79.7 percent on online data and 74.1 percent on offline data, significantly outperforming a state-of-the-art HMM-based system. In addition, we demonstrate the network's robustness to lexicon size, measure the individual influence of its hidden layers, and analyze its use of context. Last, we provide an in-depth discussion of the differences between the network and HMMs, suggesting reasons for the network's superior performance."
            },
            "slug": "A-Novel-Connectionist-System-for-Unconstrained-Graves-Liwicki",
            "title": {
                "fragments": [],
                "text": "A Novel Connectionist System for Unconstrained Handwriting Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper proposes an alternative approach based on a novel type of recurrent neural network, specifically designed for sequence labeling tasks where the data is hard to segment and contains long-range bidirectional interdependencies, significantly outperforming a state-of-the-art HMM-based system."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Almaz\u00e1n, and L. de las Heras. ICDAR 2013 robust reading competition. In ICDAR"
            },
            "venue": {
                "fragments": [],
                "text": "Almaz\u00e1n, and L. de las Heras. ICDAR 2013 robust reading competition. In ICDAR"
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reading text in uncontrolled conditions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "robust reading competition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "http://www.capella.de/us/index. cfm/products/capella-scan/ info-capella-scan"
            },
            "venue": {
                "fragments": [],
                "text": "http://www.capella.de/us/index. cfm/products/capella-scan/ info-capella-scan"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 78
                            }
                        ],
                        "text": "The candidates N\u03b4(l) can be found efficiently with the BK-tree data structure [9], which is a metric tree specifically adapted to discrete metric spaces."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some approaches to bestmatch file searching"
            },
            "venue": {
                "fragments": [],
                "text": "Commun. ACM, 16(4):230\u2013236,"
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "robust reading competitions : entries , results , and future directions"
            },
            "venue": {
                "fragments": [],
                "text": "IJDAR"
            },
            "year": 2003
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 26,
            "methodology": 17,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 47,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/An-End-to-End-Trainable-Neural-Network-for-Sequence-Shi-Bai/8e9149ab00236d04db23394774e716c4f1d89231?sort=total-citations"
}