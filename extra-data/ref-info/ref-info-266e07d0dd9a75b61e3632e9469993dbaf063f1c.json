{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47055692"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14060545,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c33b70cf34814fdfe045026cc2a39fb9636d1b4a",
            "isKey": false,
            "numCitedBy": 283,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Some scientists have concluded that backpropagation is a specialized method for pattern classification, of little relevance to broader problems, to parallel computing, or to our understanding of the human brain. The author questions these beliefs and proposes development of a general theory of intelligence in which backpropagation and comparisons to the brain play a central role. He also points to a series of intermediate steps and applications leading up to the construction of such generalized systems, including past applications to social science which in some ways go beyond the work in AI as such. The author presents a condensed mathematical summary of that work. He begins by summarizing a generalized formulation of backpropagation, and then discusses network architectures and applications which it opens up.<<ETX>>"
            },
            "slug": "Backpropagation:-past-and-future-Werbos",
            "title": {
                "fragments": [],
                "text": "Backpropagation: past and future"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The author proposes development of a general theory of intelligence in which backpropagation and comparisons to the brain play a central role, and points to a series of intermediate steps and applications leading up to the construction of such generalized systems."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE 1988 International Conference on Neural Networks"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47055692"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35067038,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c7f9bddb6773be57ab671ebcbffcaaf039ad4cd0",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Generalized methods that are commonly used in neural-network research have made it possible for the US Energy Information Administration (EIA) to solve a gas-industry optimization problem on a personal computer that would previously have required a mainframe computer because of the run time required. The resulting model was used to produce EIA's official energy forecasts published in 1988. It is shown how backpropagation can be used by modelers with no special training in neurocomputing. Earlier applications of backpropagation to modeling and to EIA problems are reviewed that antedate the practical applications to neural networks. Finally, the relations between backpropagation, the current EIA model, and economic issues related to modeling and the gas industry are discussed. Among these issues are optimization subject to constraints, and competition and efficiency in gas supply. It is also shown how more recent formulations of backpropagation are a special case of the proposed formulation. >"
            },
            "slug": "Maximizing-long-term-gas-industry-profits-in-two-in-Werbos",
            "title": {
                "fragments": [],
                "text": "Maximizing long-term gas industry profits in two minutes in Lotus using neural network methods"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The relations between backpropagation, the current EIA model, and economic issues related to modeling and the gas industry are discussed and it is shown how more recent formulations of back Propagation are a special case of the proposed formulation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7840452,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a57c6d627ffc667ae3547073876c35d6420accff",
            "isKey": false,
            "numCitedBy": 1574,
            "numCiting": 122,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Connectionist-Learning-Procedures-Hinton",
            "title": {
                "fragments": [],
                "text": "Connectionist Learning Procedures"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144826602"
                        ],
                        "name": "C. Anderson",
                        "slug": "C.-Anderson",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Anderson",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Anderson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1522994,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8a7acaf6469c06ae5876d92f013184db5897bb13",
            "isKey": false,
            "numCitedBy": 3237,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem. The task is to balance a pole that is hinged to a movable cart by applying forces to the cart's base. It is argued that the learning problems faced by adaptive elements that are components of adaptive networks are at least as difficult as this version of the pole-balancing problem. The learning system consists of a single associative search element (ASE) and a single adaptive critic element (ACE). In the course of learning to balance the pole, the ASE constructs associations between input and output by searching under the influence of reinforcement feedback, and the ACE constructs a more informative evaluation function than reinforcement feedback alone can provide. The differences between this approach and other attempts to solve problems using neurolike elements are discussed, as is the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences."
            },
            "slug": "Neuronlike-adaptive-elements-that-can-solve-control-Barto-Sutton",
            "title": {
                "fragments": [],
                "text": "Neuronlike adaptive elements that can solve difficult learning control problems"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem and the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man, and Cybernetics"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2319833"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064905984"
                        ],
                        "name": "Jim Titus",
                        "slug": "Jim-Titus",
                        "structuredName": {
                            "firstName": "Jim",
                            "lastName": "Titus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jim Titus"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 24278842,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "9c790c8e2d3bc565d59a91600dac0d8b4d1eedc4",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "The \"compromise\" method is a new computer-based forecasting tool, available within the conversational CS package on the MIT Multics. Like regression (least squares) or new forms of Box-Jenkins methods, it estimates the parameters of a multivariate dynamic model and may be used for causal analysis or policy impact analysis. Unlike those maximum-likelihood methods, it does not assume that errors are \"white noise,\" random and normal. It follows the newer robust philosophy of trying to minimize estimation errors on the assumption that noise will be inextricably dirty. In the case of \"strong\" dynamic models\u00bfmodels which predict that changes in present variable values lead to comparable changes in future variable values it may reduce parameter errors by an order of magnitude. Forecasting errors will also be reduced, although the degree of reduction depends on how much randomness exists in the process. When we used the compromise method according to the new \"bias\" procedure, in order to reestimate the J-5 model (a nonlinear multiequation model used by the Department of Defense in long-range forecasting), forecasting errors were reduced by between 0 and 45 percent (with a median of about 20 percent) across different variables, as compared with regression. With simultaneous-equation econometric models, it has reduced them by 50 percent. The procedure has been documented for use by nonprogrammers [1]; it incorporates a new quasi-Newtonian method which can handle many parameters."
            },
            "slug": "An-Empirical-Test-of-New-Forecasting-Methods-from-a-Werbos-Titus",
            "title": {
                "fragments": [],
                "text": "An Empirical Test of New Forecasting Methods Derived from a Theory of Intelligence: The Prediction of Conflict in Latin America"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "The compromise method is a new computer-based forecasting tool, available within the conversational CS package on the MIT Multics, and incorporates a new quasi-Newtonian method which can handle many parameters."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man, and Cybernetics"
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2319833"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7587835,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "3abac8d1bf1a6c69805e8aa6f0335b66f39ca999",
            "isKey": false,
            "numCitedBy": 252,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Successes with expert systems and other specialized systems have revived hopes for factory automation and productivity growth. A full realization of these potentials will require conscious effort to overcome obsolete rigidities, to develop unified and adaptive methods for integrating complex systems, and to increase our understanding of these systems (understanding which is vital to human productivity in developing software). How adaptive systems may be built and understood by extending control theory and statistics is discussed. Adaptive systems, like human infants, are less agile than young monkeys but have something important to contribute as they mature. It argues that the old dream of understanding intelligence in generalized terms, permitting a unified understanding of adaptive systems and of the human mind, was not incorrect; rather, the early attempts in that direction failed because they did not make full use of research possibilities in statistics, control theory, and numerical analysis (many of which are still unexploited) and were limited by hardware costs which are now coming down. A basic adaptive system derived from this approach fits the fundamental, qualitative empirical facts of human brain physiology in some detail (unlike the usual \"general neuron models,\" which rarely even discriminate between basic components of the brain), and offers opportunities for further research; it can even translate certain fundamental ideas of Freud into something more mathematical and scientific. The mathematics of the basic system, and the fit to the brain, are described in detail."
            },
            "slug": "Building-and-Understanding-Adaptive-Systems:-A-to-Werbos",
            "title": {
                "fragments": [],
                "text": "Building and Understanding Adaptive Systems: A Statistical/Numerical Approach to Factory Automation and Brain Research"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is argued that the old dream of understanding intelligence in generalized terms, permitting a unified understanding of adaptive systems and of the human mind, was not incorrect; rather, the early attempts failed because they did not make full use of research possibilities in statistics, control theory, and numerical analysis and were limited by hardware costs which are now coming down."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man, and Cybernetics"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516142"
                        ],
                        "name": "D. Tank",
                        "slug": "D.-Tank",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Tank",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tank"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10096429,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "4517ca6110dfb5de3f004f67653de3c33b8d6234",
            "isKey": false,
            "numCitedBy": 1934,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "A new conceptual framework and a minimization principle together provide an understanding of computation in model neural circuits. The circuits consist of nonlinear graded-response model neurons organized into networks with effectively symmetric synaptic connections. The neurons represent an approximation to biological neurons in which a simplified set of important computational properties is retained. Complex circuits solving problems similar to those essential in biology can be analyzed and understood without the need to follow the circuit dynamics in detail. Implementation of the model with electronic devices will provide a class of electronic circuits of novel form and function."
            },
            "slug": "Computing-with-neural-circuits:-a-model.-Hopfield-Tank",
            "title": {
                "fragments": [],
                "text": "Computing with neural circuits: a model."
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A new conceptual framework and a minimization principle together provide an understanding of computation in model neural circuits that represent an approximation to biological neurons in which a simplified set of important computational properties is retained."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47055692"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59694629,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa277dfe3645463a25432282563fca4891d846ea",
            "isKey": false,
            "numCitedBy": 261,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The following paper summarizes the major properties and applications of a collection of algorithms involving differentiation and optimization at minimum cost. The areas of application include the sensitivity analysis of models, new work in statistical or econometric estimation, optimization, artificial intelligence and neuron modelling. The details, references and derivations can be obtained by requesting \u201eSensitivity Analysis Methods for Nonlinear Systems\u201c from Forecast Analysis and Evaluation Team, Quality Assurance, OSS/EIA, Room 7413, Department of Energy, Washington, DC 20461."
            },
            "slug": "Applications-of-advances-in-nonlinear-sensitivity-Werbos",
            "title": {
                "fragments": [],
                "text": "Applications of advances in nonlinear sensitivity analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "The following paper summarizes the major properties and applications of a collection of algorithms involving differentiation and optimization at minimum cost, including the sensitivity analysis of models, new work in statistical or econometric estimation, optimization, artificial intelligence and neuron modelling."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3242249"
                        ],
                        "name": "S. Foote",
                        "slug": "S.-Foote",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Foote",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Foote"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143829154"
                        ],
                        "name": "J. Morrison",
                        "slug": "J.-Morrison",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Morrison",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Morrison"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16700881,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "64726fa31a87d79d3c3112c435368b30dae647b9",
            "isKey": false,
            "numCitedBy": 457,
            "numCiting": 105,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : The goal of the proposed studies is to characterize the effects of noradrenergic (NA) afferents on cortical information processing. our previous studies indicate that the primate locus coeruleus (LC) system, originating in the pontine brainstem, innervates neocortex more densely than previously thought, exhibiting highly specific patterns in terms of the regional and laminar distribution of its axons. our previous neurophysiological observations suggest that this system imposes state-related modulatory effects on thalamo- cortical and cortico-cortical systems. The proposed studies have the following Specific Aims: (1) To examine, in monkeys, the effects of manipulating the LC-NA system on ERPs, EEG characteristics, and associated behaviors in operant paradigms that utilize visual or auditory cues; (2) To correlate the activities of individual monkey LC-NA neurons with cortical neuronal activity and the measures utilized in Aim 1; (3) To extend our preliminary observation that activation of the LC by local drug infusion, in halothane-anesthetized rats, produces EEG signs of cortical and hippocampal activation; (4) To examine the relationship between the intensity of LC neuronal activity and rates of norepinephrine release in neocortex and hippocampus by performing microdialysis in these forebrain terminal regions in anesthetized rats during manipulation of LC activity."
            },
            "slug": "Extrathalamic-modulation-of-cortical-function.-Foote-Morrison",
            "title": {
                "fragments": [],
                "text": "Extrathalamic modulation of cortical function."
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The goal of the proposed studies is to characterize the effects of noradrenergic (NA) afferents on cortical information processing and examine the relationship between the intensity of LC neuronal activity and rates of norepinephrine release in neocortex and hippocampus during manipulation of LC activity."
            },
            "venue": {
                "fragments": [],
                "text": "Annual review of neuroscience"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682174"
                        ],
                        "name": "S. Grossberg",
                        "slug": "S.-Grossberg",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Grossberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Grossberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3103321"
                        ],
                        "name": "D. Levine",
                        "slug": "D.-Levine",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Levine",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Levine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2744613"
                        ],
                        "name": "N. Schmajuk",
                        "slug": "N.-Schmajuk",
                        "structuredName": {
                            "firstName": "Nestor",
                            "lastName": "Schmajuk",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Schmajuk"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 24119274,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "acd7c1ea64f8fc9753c653ea442c6e8f28205b0b",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "At least four types of learning processes are relevant in the present paper: learning of conditioned reinforcement, incentive motivation, sensory expectancy, and motor command. These several types of learning processes, which operate on a slow time scale, regulate and are regulated by rapidly fluctuating limited capacity STM representations of sensory events. The theory suggest how nonlinear feedback interactions among these fast information processing mechanisms and slow learning mechanisms participate in different conditioning paradigms, and actively regulate learning and memory to generate predictive internal representations of external environmental contingencies."
            },
            "slug": "Predictive-regulation-of-associative-learning-in-a-Grossberg-Levine",
            "title": {
                "fragments": [],
                "text": "Predictive regulation of associative learning in a neural network by reinforcement and attentive feedback."
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The theory suggest how nonlinear feedback interactions among these fast information processing mechanisms and slow learning mechanisms participate in different conditioning paradigms, and actively regulate learning and memory to generate predictive internal representations of external environmental contingencies."
            },
            "venue": {
                "fragments": [],
                "text": "International journal of neurology"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800149"
                        ],
                        "name": "J. Dennis",
                        "slug": "J.-Dennis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Dennis",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dennis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795397"
                        ],
                        "name": "Bobby Schnabel",
                        "slug": "Bobby-Schnabel",
                        "structuredName": {
                            "firstName": "Bobby",
                            "lastName": "Schnabel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bobby Schnabel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 27578127,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e1053197256c6c3c0631377ec23a3f7dc1cb4781",
            "isKey": false,
            "numCitedBy": 7615,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface 1. Introduction. Problems to be considered Characteristics of 'real-world' problems Finite-precision arithmetic and measurement of error Exercises 2. Nonlinear Problems in One Variable. What is not possible Newton's method for solving one equation in one unknown Convergence of sequences of real numbers Convergence of Newton's method Globally convergent methods for solving one equation in one uknown Methods when derivatives are unavailable Minimization of a function of one variable Exercises 3. Numerical Linear Algebra Background. Vector and matrix norms and orthogonality Solving systems of linear equations-matrix factorizations Errors in solving linear systems Updating matrix factorizations Eigenvalues and positive definiteness Linear least squares Exercises 4. Multivariable Calculus Background Derivatives and multivariable models Multivariable finite-difference derivatives Necessary and sufficient conditions for unconstrained minimization Exercises 5. Newton's Method for Nonlinear Equations and Unconstrained Minimization. Newton's method for systems of nonlinear equations Local convergence of Newton's method The Kantorovich and contractive mapping theorems Finite-difference derivative methods for systems of nonlinear equations Newton's method for unconstrained minimization Finite difference derivative methods for unconstrained minimization Exercises 6. Globally Convergent Modifications of Newton's Method. The quasi-Newton framework Descent directions Line searches The model-trust region approach Global methods for systems of nonlinear equations Exercises 7. Stopping, Scaling, and Testing. Scaling Stopping criteria Testing Exercises 8. Secant Methods for Systems of Nonlinear Equations. Broyden's method Local convergence analysis of Broyden's method Implementation of quasi-Newton algorithms using Broyden's update Other secant updates for nonlinear equations Exercises 9. Secant Methods for Unconstrained Minimization. The symmetric secant update of Powell Symmetric positive definite secant updates Local convergence of positive definite secant methods Implementation of quasi-Newton algorithms using the positive definite secant update Another convergence result for the positive definite secant method Other secant updates for unconstrained minimization Exercises 10. Nonlinear Least Squares. The nonlinear least-squares problem Gauss-Newton-type methods Full Newton-type methods Other considerations in solving nonlinear least-squares problems Exercises 11. Methods for Problems with Special Structure. The sparse finite-difference Newton method Sparse secant methods Deriving least-change secant updates Analyzing least-change secant methods Exercises Appendix A. A Modular System of Algorithms for Unconstrained Minimization and Nonlinear Equations (by Robert Schnabel) Appendix B. Test Problems (by Robert Schnabel) References Author Index Subject Index."
            },
            "slug": "Numerical-methods-for-unconstrained-optimization-Dennis-Schnabel",
            "title": {
                "fragments": [],
                "text": "Numerical methods for unconstrained optimization and nonlinear equations"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Newton's Method for Nonlinear Equations and Unconstrained Minimization and methods for solving nonlinear least-squares problems with Special Structure."
            },
            "venue": {
                "fragments": [],
                "text": "Prentice Hall series in computational mathematics"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "115269432"
                        ],
                        "name": "R. Godson",
                        "slug": "R.-Godson",
                        "structuredName": {
                            "firstName": "Roy",
                            "lastName": "Godson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Godson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 183738004,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "69e9a9c26dedf487a5c57c65e05db410881ffbc6",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Elements-of-intelligence-Godson",
            "title": {
                "fragments": [],
                "text": "Elements of intelligence"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48949688"
                        ],
                        "name": "D. Kirk",
                        "slug": "D.-Kirk",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Kirk",
                            "middleNames": [
                                "E."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kirk"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 133570466,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "2a9662fcfeed7bab9f3adf870374bdbe9a3fae18",
            "isKey": false,
            "numCitedBy": 848,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Optimal-Control-Theory-Kirk",
            "title": {
                "fragments": [],
                "text": "Optimal Control Theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2074066293"
                        ],
                        "name": "J. Uhlig",
                        "slug": "J.-Uhlig",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Uhlig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Uhlig"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122236249,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "08706dedd8bc167084a60e8d811d89348331bf35",
            "isKey": false,
            "numCitedBy": 252,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "C.-Forsythe-and-C.-B.-Moler,-Computer-Solution-of-+-Uhlig",
            "title": {
                "fragments": [],
                "text": "C. Forsythe and C. B. Moler, Computer Solution of Linear Algebraic Systems. (Series in Automatic Computation) XI + 148 S. Englewood Cliffs, N.J. 1967. Prentice-Hall, Inc. Preis geb. 54 s. net"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "97073827"
                        ],
                        "name": "Werbos",
                        "slug": "Werbos",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Werbos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Werbos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 107685209,
            "fieldsOfStudy": [
                "Environmental Science",
                "Engineering"
            ],
            "id": "c0d3cf8e0bb6c35b12d1d328a5d32fff0ec6fa0c",
            "isKey": false,
            "numCitedBy": 1,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Natural-Gas-Market-Model:-Equations-and-data-Werbos",
            "title": {
                "fragments": [],
                "text": "The Natural Gas Market Model: Equations and data sources"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30320952"
                        ],
                        "name": "F. Pineda",
                        "slug": "F.-Pineda",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pineda",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Pineda"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 196070964,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d18b17417128322f86528f60d652a5b998a51409",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "GENERALIZATION-OF-BACKPROPAGATION-TO-RECURRENT-AND-Pineda",
            "title": {
                "fragments": [],
                "text": "GENERALIZATION OF BACKPROPAGATION TO RECURRENT AND HIGH-ORDER NETWORKS."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3547091"
                        ],
                        "name": "D. Hebb",
                        "slug": "D.-Hebb",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Hebb",
                            "middleNames": [
                                "Olding"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hebb"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62285311,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2760c82b71bbcd586f786ca8017d5916f2a7c8ec",
            "isKey": false,
            "numCitedBy": 4625,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-organization-of-behavior-Hebb",
            "title": {
                "fragments": [],
                "text": "The organization of behavior"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2319833"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 207975157,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "56623a496727d5c71491850e04512ddf4152b487",
            "isKey": false,
            "numCitedBy": 4468,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Beyond-Regression-:-\"New-Tools-for-Prediction-and-Werbos",
            "title": {
                "fragments": [],
                "text": "Beyond Regression : \"New Tools for Prediction and Analysis in the Behavioral Sciences"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068289963"
                        ],
                        "name": "L. B. Almeida",
                        "slug": "L.-B.-Almeida",
                        "structuredName": {
                            "firstName": "Lu\u00eds",
                            "lastName": "Almeida",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. B. Almeida"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58820035,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8be3f21ab796bd9811382b560507c1c679fae37f",
            "isKey": false,
            "numCitedBy": 325,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-learning-rule-for-asynchronous-perceptrons-with-a-Almeida",
            "title": {
                "fragments": [],
                "text": "A learning rule for asynchronous perceptrons with feedback in a combinatorial environment"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalization ofbackpropagation to recurrent and higher order networks"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE Conference on Neural Information Processing Systems"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Second - order backpropagation : Implementing an optimal O ( n ) approximation to Newton ' s method in an artificial neural network"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "cL Backpropagation versus content - addressable memory : Applications , evaluation , and synthesis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "bL Learning how the worl \u00a2 ~ works : Specifications for predictive networks in robots and brains"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings ( ~ [ the 1987 IEEE International Conferen ( ~ ~ , n Systems . Man and Cybernetics"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Changes in global policy analysis procedures suggested by new methods of optimization"
            },
            "venue": {
                "fragments": [],
                "text": "Policy Analys ; s aria InJbrmation Systems"
            },
            "year": 1979
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {},
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 24,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Generalization-of-backpropagation-with-application-Werbos/266e07d0dd9a75b61e3632e9469993dbaf063f1c?sort=total-citations"
}