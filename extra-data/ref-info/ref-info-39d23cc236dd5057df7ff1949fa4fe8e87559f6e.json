{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054135353"
                        ],
                        "name": "Wonseok Hwang",
                        "slug": "Wonseok-Hwang",
                        "structuredName": {
                            "firstName": "Wonseok",
                            "lastName": "Hwang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wonseok Hwang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49841374"
                        ],
                        "name": "Jinyeong Yim",
                        "slug": "Jinyeong-Yim",
                        "structuredName": {
                            "firstName": "Jinyeong",
                            "lastName": "Yim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jinyeong Yim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7882243"
                        ],
                        "name": "Seunghyun Park",
                        "slug": "Seunghyun-Park",
                        "structuredName": {
                            "firstName": "Seunghyun",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seunghyun Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16110760"
                        ],
                        "name": "Sohee Yang",
                        "slug": "Sohee-Yang",
                        "structuredName": {
                            "firstName": "Sohee",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sohee Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4418074"
                        ],
                        "name": "Minjoon Seo",
                        "slug": "Minjoon-Seo",
                        "structuredName": {
                            "firstName": "Minjoon",
                            "lastName": "Seo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minjoon Seo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 119
                            }
                        ],
                        "text": "2D Transformer Encoder We use Transformer (Vaswani et al., 2017) with the following modification for encoding 2D text (Hwang et al., 2020)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 61
                            }
                        ],
                        "text": ", 2017) with the following modification for encoding 2D text (Hwang et al., 2020)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Hwang et al., 2020 formulates the task as spatial dependency parsing which is essentially an another tagging approach where the inter-text segment relations are tagged."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 222103842,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20d0564fd3fdbc24f266ca2076826a2271c3ea08",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Information Extraction (IE) for semi-structured document images is often approached as a sequence tagging problem by classifying each recognized input token into one of the IOB (Inside, Outside, and Beginning) categories. However, such problem setup has two inherent limitations that (1) it cannot easily handle complex spatial relationships and (2) it is not suitable for highly structured information, which are nevertheless frequently observed in real-world document images. To tackle these issues, we first formulate the IE task as spatial dependency parsing problem that focuses on the relationship among text segment nodes in the documents. Under this setup, we then propose SPADE (SPAtial DEpendency parser) that models highly complex spatial relationships and an arbitrary number of information layers in the documents in an end-to-end manner. We evaluate it on various kinds of documents such as receipts, name cards, forms, and invoices, and show that it achieves a similar or better performance compared to strong baselines including BERT-based IOB taggger, with up to 37.7% improvement."
            },
            "slug": "Spatial-Dependency-Parsing-for-Semi-Structured-Hwang-Yim",
            "title": {
                "fragments": [],
                "text": "Spatial Dependency Parsing for Semi-Structured Document Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "SPADE (SPAtial DEpendency parser) is proposed that models highly complex spatial relationships and an arbitrary number of information layers in the documents in an end-to-end manner and achieves a similar or better performance compared to strong baselines including BERT-based IOB taggger."
            },
            "venue": {
                "fragments": [],
                "text": "FINDINGS"
            },
            "year": 2021
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3032611"
                        ],
                        "name": "Yiheng Xu",
                        "slug": "Yiheng-Xu",
                        "structuredName": {
                            "firstName": "Yiheng",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiheng Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123545597"
                        ],
                        "name": "Minghao Li",
                        "slug": "Minghao-Li",
                        "structuredName": {
                            "firstName": "Minghao",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minghao Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145500855"
                        ],
                        "name": "Lei Cui",
                        "slug": "Lei-Cui",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Cui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Cui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110003"
                        ],
                        "name": "Shaohan Huang",
                        "slug": "Shaohan-Huang",
                        "structuredName": {
                            "firstName": "Shaohan",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaohan Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49807919"
                        ],
                        "name": "Furu Wei",
                        "slug": "Furu-Wei",
                        "structuredName": {
                            "firstName": "Furu",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Furu Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92660691"
                        ],
                        "name": "Ming Zhou",
                        "slug": "Ming-Zhou",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming Zhou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 300,
                                "start": 103
                            }
                        ],
                        "text": "Related Work Most of previous works formulate semi-structured document IE task as text tagging problem (Palm et al., 2017; Katti et al., 2018; Zhao et al., 2019; Xu et al., 2019; Denk and Reisswig, 2019; Majumder et al., 2020; Liu et al., 2019; Yu et al., 2020; Qian et al., 2019; Hwang et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 155
                            }
                        ],
                        "text": "\u2026Work Most of previous works formulate semi-structured document IE task as text tagging problem (Palm et al., 2017; Katti et al., 2018; Zhao et al., 2019; Xu et al., 2019; Denk and Reisswig, 2019; Majumder et al., 2020; Liu et al., 2019; Yu et al., 2020; Qian et al., 2019; Hwang et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 209515395,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3465c06c872d8c48d628c5fc2d484087719351b6",
            "isKey": false,
            "numCitedBy": 164,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Pre-training techniques have been verified successfully in a variety of NLP tasks in recent years. Despite the widespread use of pre-training models for NLP applications, they almost exclusively focus on text-level manipulation, while neglecting layout and style information that is vital for document image understanding. In this paper, we propose the LayoutLM to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents. Furthermore, we also leverage image features to incorporate words' visual information into LayoutLM. To the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for document-level pre-training. It achieves new state-of-the-art results in several downstream tasks, including form understanding (from 70.72 to 79.27), receipt understanding (from 94.02 to 95.24) and document image classification (from 93.07 to 94.42). The code and pre-trained LayoutLM models are publicly available at https://aka.ms/layoutlm."
            },
            "slug": "LayoutLM:-Pre-training-of-Text-and-Layout-for-Image-Xu-Li",
            "title": {
                "fragments": [],
                "text": "LayoutLM: Pre-training of Text and Layout for Document Image Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The LayoutLM is proposed to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2048569409"
                        ],
                        "name": "Wenwen Yu",
                        "slug": "Wenwen-Yu",
                        "structuredName": {
                            "firstName": "Wenwen",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenwen Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054645836"
                        ],
                        "name": "Ning Lu",
                        "slug": "Ning-Lu",
                        "structuredName": {
                            "firstName": "Ning",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ning Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2689287"
                        ],
                        "name": "Xianbiao Qi",
                        "slug": "Xianbiao-Qi",
                        "structuredName": {
                            "firstName": "Xianbiao",
                            "lastName": "Qi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xianbiao Qi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2124651409"
                        ],
                        "name": "Ping Gong",
                        "slug": "Ping-Gong",
                        "structuredName": {
                            "firstName": "Ping",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ping Gong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069928447"
                        ],
                        "name": "Rong Xiao",
                        "slug": "Rong-Xiao",
                        "structuredName": {
                            "firstName": "Rong",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rong Xiao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 215786577,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba5ed98c4546fada5c732bced4a1c1615f1a4c16",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Computer vision with state-of-the-art deep learning models has achieved huge success in the field of Optical Character Recognition (OCR) including text detection and recognition tasks recently. However, Key Information Extraction (KIE) from documents as the downstream task of OCR, having a large number of use scenarios in real-world, remains a challenge because documents not only have textual features extracting from OCR systems but also have semantic visual features that are not fully exploited and play a critical role in KIE. Too little work has been devoted to efficiently make full use of both textual and visual features of the documents. In this paper, we introduce PICK, a framework that is effective and robust in handling complex documents layout for KIE by combining graph learning with graph convolution operation, yielding a richer semantic representation containing the textual and visual features and global layout without ambiguity. Extensive experiments on realworld datasets have been conducted to show that our method outperforms baselines methods by significant margins. Our code is available at https://github.com/wenwenyu/PICK-pytorch."
            },
            "slug": "PICK:-Processing-Key-Information-Extraction-from-Yu-Lu",
            "title": {
                "fragments": [],
                "text": "PICK: Processing Key Information Extraction from Documents using Improved Graph Learning-Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "PICK is introduced, a framework that is effective and robust in handling complex documents layout for KIE by combining graph learning with graph convolution operation, yielding a richer semantic representation containing the textual and visual features and global layout without ambiguity."
            },
            "venue": {
                "fragments": [],
                "text": "2020 25th International Conference on Pattern Recognition (ICPR)"
            },
            "year": 2021
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113309578"
                        ],
                        "name": "Xu Zhong",
                        "slug": "Xu-Zhong",
                        "structuredName": {
                            "firstName": "Xu",
                            "lastName": "Zhong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xu Zhong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9585722"
                        ],
                        "name": "Elaheh Shafieibavani",
                        "slug": "Elaheh-Shafieibavani",
                        "structuredName": {
                            "firstName": "Elaheh",
                            "lastName": "Shafieibavani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Elaheh Shafieibavani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399097376"
                        ],
                        "name": "Antonio Jimeno-Yepes",
                        "slug": "Antonio-Jimeno-Yepes",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Jimeno-Yepes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antonio Jimeno-Yepes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 48
                            }
                        ],
                        "text": "A similar score has been recently suggested by (Zhong et al., 2020) for a table recognition task."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 208267858,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "99b2a05177c17dd86627863608ac45dc4cfe0446",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Important information that relates to a specific topic in a document is often organized in tabular format to assist readers with information retrieval and comparison, which may be difficult to provide in natural language. However, tabular data in unstructured digital documents, e.g., Portable Document Format (PDF) and images, are difficult to parse into structured machine-readable format, due to complexity and diversity in their structure and style. To facilitate image-based table recognition with deep learning, we develop the largest publicly available table recognition dataset PubTabNet (this https URL), containing 568k table images with corresponding structured HTML representation. PubTabNet is automatically generated by matching the XML and PDF representations of the scientific articles in PubMed Central Open Access Subset (PMCOA). We also propose a novel attention-based encoder-dual-decoder (EDD) architecture that converts images of tables into HTML code. The model has a structure decoder which reconstructs the table structure and helps the cell decoder to recognize cell content. In addition, we propose a new Tree-Edit-Distance-based Similarity (TEDS) metric for table recognition, which more appropriately captures multi-hop cell misalignment and OCR errors than the pre-established metric. The experiments demonstrate that the EDD model can accurately recognize complex tables solely relying on the image representation, outperforming the state-of-the-art by 9.7% absolute TEDS score."
            },
            "slug": "Image-based-table-recognition:-data,-model,-and-Zhong-Shafieibavani",
            "title": {
                "fragments": [],
                "text": "Image-based table recognition: data, model, and evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The largest publicly available table recognition dataset PubTabNet is developed, containing 568k table images with corresponding structured HTML representation, and a new Tree-Edit-Distance-based Similarity (TEDS) metric for table recognition is proposed, which more appropriately captures multi-hop cell misalignment and OCR errors than the pre-established metric."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110965354"
                        ],
                        "name": "Xiaojing Liu",
                        "slug": "Xiaojing-Liu",
                        "structuredName": {
                            "firstName": "Xiaojing",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojing Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112256"
                        ],
                        "name": "Feiyu Gao",
                        "slug": "Feiyu-Gao",
                        "structuredName": {
                            "firstName": "Feiyu",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Feiyu Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112207346"
                        ],
                        "name": "Qiong Zhang",
                        "slug": "Qiong-Zhang",
                        "structuredName": {
                            "firstName": "Qiong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiong Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36530509"
                        ],
                        "name": "Huasha Zhao",
                        "slug": "Huasha-Zhao",
                        "structuredName": {
                            "firstName": "Huasha",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huasha Zhao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 85528598,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04df8c70257b5280b9d303502c9d7ddf946f181b",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Visually rich documents (VRDs) are ubiquitous in daily business and life. Examples are purchase receipts, insurance policy documents, custom declaration forms and so on. In VRDs, visual and layout information is critical for document understanding, and texts in such documents cannot be serialized into the one-dimensional sequence without losing information. Classic information extraction models such as BiLSTM-CRF typically operate on text sequences and do not incorporate visual features. In this paper, we introduce a graph convolution based model to combine textual and visual information presented in VRDs. Graph embeddings are trained to summarize the context of a text segment in the document, and further combined with text embeddings for entity extraction. Extensive experiments have been conducted to show that our method outperforms BiLSTM-CRF baselines by significant margins, on two real-world datasets. Additionally, ablation studies are also performed to evaluate the effectiveness of each component of our model."
            },
            "slug": "Graph-Convolution-for-Multimodal-Information-from-Liu-Gao",
            "title": {
                "fragments": [],
                "text": "Graph Convolution for Multimodal Information Extraction from Visually Rich Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper introduces a graph convolution based model to combine textual and visual information presented in VRDs and outperforms BiLSTM-CRF baselines by significant margins, on two real-world datasets."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108812052"
                        ],
                        "name": "Xiaohui Zhao",
                        "slug": "Xiaohui-Zhao",
                        "structuredName": {
                            "firstName": "Xiaohui",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaohui Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109569771"
                        ],
                        "name": "Zhuo Wu",
                        "slug": "Zhuo-Wu",
                        "structuredName": {
                            "firstName": "Zhuo",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhuo Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108004518"
                        ],
                        "name": "Xiaoguang Wang",
                        "slug": "Xiaoguang-Wang",
                        "structuredName": {
                            "firstName": "Xiaoguang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoguang Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 300,
                                "start": 103
                            }
                        ],
                        "text": "Related Work Most of previous works formulate semi-structured document IE task as text tagging problem (Palm et al., 2017; Katti et al., 2018; Zhao et al., 2019; Xu et al., 2019; Denk and Reisswig, 2019; Majumder et al., 2020; Liu et al., 2019; Yu et al., 2020; Qian et al., 2019; Hwang et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 143
                            }
                        ],
                        "text": "Related Work Most of previous works formulate semi-structured document IE task as text tagging problem (Palm et al., 2017; Katti et al., 2018; Zhao et al., 2019; Xu et al., 2019; Denk and Reisswig, 2019; Majumder et al., 2020; Liu et al., 2019; Yu et al., 2020; Qian et al., 2019; Hwang et al.,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 88518818,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "024aea24cc4d0949d6fe1482591d2429a9d8bbeb",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Extracting key information from documents, such as receipts or invoices, and preserving the interested texts to structured data is crucial in the document-intensive streamline processes of office automation in areas that includes but not limited to accounting, financial, and taxation areas. To avoid designing expert rules for each specific type of document, some published works attempt to tackle the problem by learning a model to explore the semantic context in text sequences based on the Named Entity Recognition (NER) method in the NLP field. In this paper, we propose to harness the effective information from both semantic meaning and spatial distribution of texts in documents. Specifically, our proposed model, Convolutional Universal Text Information Extractor (CUTIE), applies convolutional neural networks on gridded texts where texts are embedded as features with semantical connotations. We further explore the effect of employing different structures of convolutional neural network and propose a fast and portable structure. We demonstrate the effectiveness of the proposed method on a dataset with up to $4,484$ labelled receipts, without any pre-training or post-processing, achieving state of the art performance that is much better than the NER based methods in terms of either speed and accuracy. Experimental results also demonstrate that the proposed CUTIE model being able to achieve good performance with a much smaller amount of training data."
            },
            "slug": "CUTIE:-Learning-to-Understand-Documents-with-Text-Zhao-Wu",
            "title": {
                "fragments": [],
                "text": "CUTIE: Learning to Understand Documents with Convolutional Universal Text Information Extractor"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The proposed model, Convolutional Universal Text Information Extractor (CUTIE), applies convolutional neural networks on gridded texts where texts are embedded as features with semantical connotations and aims to harness the effective information from both semantic meaning and spatial distribution of texts in documents."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719068"
                        ],
                        "name": "Anoop R. Katti",
                        "slug": "Anoop-R.-Katti",
                        "structuredName": {
                            "firstName": "Anoop",
                            "lastName": "Katti",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anoop R. Katti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9992847"
                        ],
                        "name": "C. Reisswig",
                        "slug": "C.-Reisswig",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Reisswig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Reisswig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39387393"
                        ],
                        "name": "Cordula Guder",
                        "slug": "Cordula-Guder",
                        "structuredName": {
                            "firstName": "Cordula",
                            "lastName": "Guder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cordula Guder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14334250"
                        ],
                        "name": "Sebastian Brarda",
                        "slug": "Sebastian-Brarda",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Brarda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Brarda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2704747"
                        ],
                        "name": "S. Bickel",
                        "slug": "S.-Bickel",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Bickel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bickel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2216963"
                        ],
                        "name": "J. H\u00f6hne",
                        "slug": "J.-H\u00f6hne",
                        "structuredName": {
                            "firstName": "Johannes",
                            "lastName": "H\u00f6hne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H\u00f6hne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2803968"
                        ],
                        "name": "J. Faddoul",
                        "slug": "J.-Faddoul",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Faddoul",
                            "middleNames": [
                                "Baptiste"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Faddoul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 52815006,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15aae08159856cdbf0ce539357d473a04dcbb7f3",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a novel type of text representation that preserves the 2D layout of a document. This is achieved by encoding each document page as a two-dimensional grid of characters. Based on this representation, we present a generic document understanding pipeline for structured documents. This pipeline makes use of a fully convolutional encoder-decoder network that predicts a segmentation mask and bounding boxes. We demonstrate its capabilities on an information extraction task from invoices and show that it significantly outperforms approaches based on sequential text or document images."
            },
            "slug": "Chargrid:-Towards-Understanding-2D-Documents-Katti-Reisswig",
            "title": {
                "fragments": [],
                "text": "Chargrid: Towards Understanding 2D Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A novel type of text representation is introduced that preserves the 2D layout of a document by encoding each document page as a two-dimensional grid of characters and it is shown that it significantly outperforms approaches based on sequential text or document images."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13070498"
                        ],
                        "name": "A. See",
                        "slug": "A.-See",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "See",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. See"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35025299"
                        ],
                        "name": "Peter J. Liu",
                        "slug": "Peter-J.-Liu",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Liu",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter J. Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 95
                            }
                        ],
                        "text": "Decoder We use Transformer decoder equipped with the gated copying mechanism (Gu et al., 2016; See et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8314118,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "668db48c6a79826456341680ee1175dfc4cced71",
            "isKey": false,
            "numCitedBy": 2464,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points."
            },
            "slug": "Get-To-The-Point:-Summarization-with-Networks-See-Liu",
            "title": {
                "fragments": [],
                "text": "Get To The Point: Summarization with Pointer-Generator Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways, using a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153368526"
                        ],
                        "name": "Timo I. Denk",
                        "slug": "Timo-I.-Denk",
                        "structuredName": {
                            "firstName": "Timo",
                            "lastName": "Denk",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timo I. Denk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9992847"
                        ],
                        "name": "C. Reisswig",
                        "slug": "C.-Reisswig",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Reisswig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Reisswig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 300,
                                "start": 103
                            }
                        ],
                        "text": "Related Work Most of previous works formulate semi-structured document IE task as text tagging problem (Palm et al., 2017; Katti et al., 2018; Zhao et al., 2019; Xu et al., 2019; Denk and Reisswig, 2019; Majumder et al., 2020; Liu et al., 2019; Yu et al., 2020; Qian et al., 2019; Hwang et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 202558968,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2fbda89395f993040b7665730c64182ade3be195",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "For understanding generic documents, information like font sizes, column layout, and generally the positioning of words may carry semantic information that is crucial for solving a downstream document intelligence task. Our novel BERTgrid, which is based on Chargrid by Katti et al. (2018), represents a document as a grid of contextualized word piece embedding vectors, thereby making its spatial structure and semantics accessible to the processing neural network. The contextualized embedding vectors are retrieved from a BERT language model. We use BERTgrid in combination with a fully convolutional network on a semantic instance segmentation task for extracting fields from invoices. We demonstrate its performance on tabulated line item and document header field extraction."
            },
            "slug": "BERTgrid:-Contextualized-Embedding-for-2D-Document-Denk-Reisswig",
            "title": {
                "fragments": [],
                "text": "BERTgrid: Contextualized Embedding for 2D Document Representation and Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The novel BERTgrid, which is based on Chargrid, represents a document as a grid of contextualized word piece embedding vectors, thereby making its spatial structure and semantics accessible to the processing neural network."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5606742"
                        ],
                        "name": "Yujie Qian",
                        "slug": "Yujie-Qian",
                        "structuredName": {
                            "firstName": "Yujie",
                            "lastName": "Qian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yujie Qian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2628786"
                        ],
                        "name": "Enrico Santus",
                        "slug": "Enrico-Santus",
                        "structuredName": {
                            "firstName": "Enrico",
                            "lastName": "Santus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Enrico Santus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8752221"
                        ],
                        "name": "Zhijing Jin",
                        "slug": "Zhijing-Jin",
                        "structuredName": {
                            "firstName": "Zhijing",
                            "lastName": "Jin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhijing Jin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144084849"
                        ],
                        "name": "Jiang Guo",
                        "slug": "Jiang-Guo",
                        "structuredName": {
                            "firstName": "Jiang",
                            "lastName": "Guo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiang Guo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741283"
                        ],
                        "name": "R. Barzilay",
                        "slug": "R.-Barzilay",
                        "structuredName": {
                            "firstName": "Regina",
                            "lastName": "Barzilay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Barzilay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 53109320,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1da8e1ad1814d81f69433ac877ef70caa950e4e6",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Most modern Information Extraction (IE) systems are implemented as sequential taggers and only model local dependencies. Non-local and non-sequential context is, however, a valuable source of information to improve predictions. In this paper, we introduce GraphIE, a framework that operates over a graph representing a broad set of dependencies between textual units (i.e. words or sentences). The algorithm propagates information between connected nodes through graph convolutions, generating a richer representation that can be exploited to improve word-level predictions. Evaluation on three different tasks \u2014 namely textual, social media and visual information extraction \u2014 shows that GraphIE consistently outperforms the state-of-the-art sequence tagging model by a significant margin."
            },
            "slug": "GraphIE:-A-Graph-Based-Framework-for-Information-Qian-Santus",
            "title": {
                "fragments": [],
                "text": "GraphIE: A Graph-Based Framework for Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Evaluation on three different tasks shows that GraphIE consistently outperforms the state-of-the-art sequence tagging model by a significant margin, and generates a richer representation that can be exploited to improve word-level predictions."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3165738"
                        ],
                        "name": "Bodhisattwa Prasad Majumder",
                        "slug": "Bodhisattwa-Prasad-Majumder",
                        "structuredName": {
                            "firstName": "Bodhisattwa",
                            "lastName": "Majumder",
                            "middleNames": [
                                "Prasad"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bodhisattwa Prasad Majumder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2406599"
                        ],
                        "name": "Navneet Potti",
                        "slug": "Navneet-Potti",
                        "structuredName": {
                            "firstName": "Navneet",
                            "lastName": "Potti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Navneet Potti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2519906"
                        ],
                        "name": "Sandeep Tata",
                        "slug": "Sandeep-Tata",
                        "structuredName": {
                            "firstName": "Sandeep",
                            "lastName": "Tata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sandeep Tata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796372"
                        ],
                        "name": "James Bradley Wendt",
                        "slug": "James-Bradley-Wendt",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Wendt",
                            "middleNames": [
                                "Bradley"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Bradley Wendt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110560772"
                        ],
                        "name": "Qi Zhao",
                        "slug": "Qi-Zhao",
                        "structuredName": {
                            "firstName": "Qi",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qi Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763978"
                        ],
                        "name": "Marc Najork",
                        "slug": "Marc-Najork",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Najork",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc Najork"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 300,
                                "start": 103
                            }
                        ],
                        "text": "Related Work Most of previous works formulate semi-structured document IE task as text tagging problem (Palm et al., 2017; Katti et al., 2018; Zhao et al., 2019; Xu et al., 2019; Denk and Reisswig, 2019; Majumder et al., 2020; Liu et al., 2019; Yu et al., 2020; Qian et al., 2019; Hwang et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 197
                            }
                        ],
                        "text": "\u2026Work Most of previous works formulate semi-structured document IE task as text tagging problem (Palm et al., 2017; Katti et al., 2018; Zhao et al., 2019; Xu et al., 2019; Denk and Reisswig, 2019; Majumder et al., 2020; Liu et al., 2019; Yu et al., 2020; Qian et al., 2019; Hwang et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 219732851,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "58877aa9aa2d09585a4ff4881b02cb1c7ff9bc28",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel approach using representation learning for tackling the problem of extracting structured information from form-like document images. We propose an extraction system that uses knowledge of the types of the target fields to generate extraction candidates and a neural network architecture that learns a dense representation of each candidate based on neighboring words in the document. These learned representations are not only useful in solving the extraction task for unseen document templates from two different domains but are also interpretable, as we show using loss cases."
            },
            "slug": "Representation-Learning-for-Information-Extraction-Majumder-Potti",
            "title": {
                "fragments": [],
                "text": "Representation Learning for Information Extraction from Form-like Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An extraction system that uses knowledge of the types of the target fields to generate extraction candidates and a neural network architecture that learns a dense representation of each candidate based on neighboring words in the document is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054135353"
                        ],
                        "name": "Wonseok Hwang",
                        "slug": "Wonseok-Hwang",
                        "structuredName": {
                            "firstName": "Wonseok",
                            "lastName": "Hwang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wonseok Hwang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109603647"
                        ],
                        "name": "Seonghyeon Kim",
                        "slug": "Seonghyeon-Kim",
                        "structuredName": {
                            "firstName": "Seonghyeon",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seonghyeon Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4418074"
                        ],
                        "name": "Minjoon Seo",
                        "slug": "Minjoon-Seo",
                        "structuredName": {
                            "firstName": "Minjoon",
                            "lastName": "Seo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minjoon Seo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49841374"
                        ],
                        "name": "Jinyeong Yim",
                        "slug": "Jinyeong-Yim",
                        "structuredName": {
                            "firstName": "Jinyeong",
                            "lastName": "Yim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jinyeong Yim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7882243"
                        ],
                        "name": "Seunghyun Park",
                        "slug": "Seunghyun-Park",
                        "structuredName": {
                            "firstName": "Seunghyun",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seunghyun Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71537829"
                        ],
                        "name": "Sungrae Park",
                        "slug": "Sungrae-Park",
                        "structuredName": {
                            "firstName": "Sungrae",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sungrae Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39924323"
                        ],
                        "name": "Junyeop Lee",
                        "slug": "Junyeop-Lee",
                        "structuredName": {
                            "firstName": "Junyeop",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junyeop Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2722273"
                        ],
                        "name": "Bado Lee",
                        "slug": "Bado-Lee",
                        "structuredName": {
                            "firstName": "Bado",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bado Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72152162"
                        ],
                        "name": "Hwalsuk Lee",
                        "slug": "Hwalsuk-Lee",
                        "structuredName": {
                            "firstName": "Hwalsuk",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hwalsuk Lee"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 300,
                                "start": 103
                            }
                        ],
                        "text": "Related Work Most of previous works formulate semi-structured document IE task as text tagging problem (Palm et al., 2017; Katti et al., 2018; Zhao et al., 2019; Xu et al., 2019; Denk and Reisswig, 2019; Majumder et al., 2020; Liu et al., 2019; Yu et al., 2020; Qian et al., 2019; Hwang et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 37
                            }
                        ],
                        "text": "Here, we explain each module of POT (Hwang et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 85
                            }
                        ],
                        "text": "For example, our currently-deployed IE system for name card and receipt images, POT (Hwang et al., 2019),1 consists of three manually engineered modules and one data-driven module (Fig."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 292,
                                "start": 274
                            }
                        ],
                        "text": "\u2026Work Most of previous works formulate semi-structured document IE task as text tagging problem (Palm et al., 2017; Katti et al., 2018; Zhao et al., 2019; Xu et al., 2019; Denk and Reisswig, 2019; Majumder et al., 2020; Liu et al., 2019; Yu et al., 2020; Qian et al., 2019; Hwang et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207910450,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "da5d93e2931c12b81774a6857db0175875fdf71a",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Parsing textual information embedded in images is important for various downstream tasks. However, many previously developed parsers are limited to handling the information presented in one dimensional sequence format. Here, we present POST OCR TAGGING BASED PARSER (POT), a simple and robust parser that can parse visually embedded texts by BIO-tagging the output of optical character recognition (OCR) task. Our shallow parsing approach enables building robust neural parser with less than a thousand labeled data. POT is validated on receipt and namecard parsing tasks."
            },
            "slug": "Post-OCR-parsing:-building-simple-and-robust-parser-Hwang-Kim",
            "title": {
                "fragments": [],
                "text": "Post-OCR parsing: building simple and robust parser via BIO tagging"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work presents POST OCR TAGGING BASED PARSER (POT), a simple and robust parser that can parse visually embedded texts by BIO-tagging the output of optical character recognition (OCR) task."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50335211"
                        ],
                        "name": "Thomas Wolf",
                        "slug": "Thomas-Wolf",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Wolf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1380459402"
                        ],
                        "name": "Lysandre Debut",
                        "slug": "Lysandre-Debut",
                        "structuredName": {
                            "firstName": "Lysandre",
                            "lastName": "Debut",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lysandre Debut"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51918868"
                        ],
                        "name": "Victor Sanh",
                        "slug": "Victor-Sanh",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Sanh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Sanh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40811585"
                        ],
                        "name": "Julien Chaumond",
                        "slug": "Julien-Chaumond",
                        "structuredName": {
                            "firstName": "Julien",
                            "lastName": "Chaumond",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julien Chaumond"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40899333"
                        ],
                        "name": "Clement Delangue",
                        "slug": "Clement-Delangue",
                        "structuredName": {
                            "firstName": "Clement",
                            "lastName": "Delangue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Clement Delangue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1382164294"
                        ],
                        "name": "Anthony Moi",
                        "slug": "Anthony-Moi",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Moi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anthony Moi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1382164165"
                        ],
                        "name": "Pierric Cistac",
                        "slug": "Pierric-Cistac",
                        "structuredName": {
                            "firstName": "Pierric",
                            "lastName": "Cistac",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierric Cistac"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1382164170"
                        ],
                        "name": "T. Rault",
                        "slug": "T.-Rault",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Rault",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Rault"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2185329"
                        ],
                        "name": "R\u00e9mi Louf",
                        "slug": "R\u00e9mi-Louf",
                        "structuredName": {
                            "firstName": "R\u00e9mi",
                            "lastName": "Louf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R\u00e9mi Louf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "97662964"
                        ],
                        "name": "Morgan Funtowicz",
                        "slug": "Morgan-Funtowicz",
                        "structuredName": {
                            "firstName": "Morgan",
                            "lastName": "Funtowicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Morgan Funtowicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1383218348"
                        ],
                        "name": "Jamie Brew",
                        "slug": "Jamie-Brew",
                        "structuredName": {
                            "firstName": "Jamie",
                            "lastName": "Brew",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jamie Brew"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 208117506,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2",
            "isKey": false,
            "numCitedBy": 2300,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers."
            },
            "slug": "Transformers:-State-of-the-Art-Natural-Language-Wolf-Debut",
            "title": {
                "fragments": [],
                "text": "Transformers: State-of-the-Art Natural Language Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Transformers is an open-source library that consists of carefully engineered state-of-the art Transformer architectures under a unified API and a curated collection of pretrained models made by and available for the community."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3016273"
                        ],
                        "name": "Jiatao Gu",
                        "slug": "Jiatao-Gu",
                        "structuredName": {
                            "firstName": "Jiatao",
                            "lastName": "Gu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiatao Gu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11955007"
                        ],
                        "name": "Zhengdong Lu",
                        "slug": "Zhengdong-Lu",
                        "structuredName": {
                            "firstName": "Zhengdong",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhengdong Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49404233"
                        ],
                        "name": "Hang Li",
                        "slug": "Hang-Li",
                        "structuredName": {
                            "firstName": "Hang",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hang Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052674293"
                        ],
                        "name": "V. Li",
                        "slug": "V.-Li",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Li",
                            "middleNames": [
                                "O.",
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 78
                            }
                        ],
                        "text": "Decoder We use Transformer decoder equipped with the gated copying mechanism (Gu et al., 2016; See et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8174613,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba30df190664193514d1d309cb673728ed48f449",
            "isKey": false,
            "numCitedBy": 1194,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which certain segments in the input sequence are selectively replicated in the output sequence. A similar phenomenon is observable in human language communication. For example, humans tend to repeat entity names or even long phrases in conversation. The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation. In this paper, we incorporate copying into neural network-based Seq2Seq learning and propose a new model called CopyNet with encoder-decoder structure. CopyNet can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose sub-sequences in the input sequence and put them at proper places in the output sequence. Our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of CopyNet. For example, CopyNet can outperform regular RNN-based model with remarkable margins on text summarization tasks."
            },
            "slug": "Incorporating-Copying-Mechanism-in-Learning-Gu-Lu",
            "title": {
                "fragments": [],
                "text": "Incorporating Copying Mechanism in Sequence-to-Sequence Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper incorporates copying into neural network-based Seq2Seq learning and proposes a new model called CopyNet with encoder-decoder structure which can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose sub-sequences in the input sequence and put them at proper places in the output sequence."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067170682"
                        ],
                        "name": "Rasmus Berg Palm",
                        "slug": "Rasmus-Berg-Palm",
                        "structuredName": {
                            "firstName": "Rasmus",
                            "lastName": "Palm",
                            "middleNames": [
                                "Berg"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rasmus Berg Palm"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724252"
                        ],
                        "name": "O. Winther",
                        "slug": "O.-Winther",
                        "structuredName": {
                            "firstName": "Ole",
                            "lastName": "Winther",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Winther"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805808"
                        ],
                        "name": "Florian Laws",
                        "slug": "Florian-Laws",
                        "structuredName": {
                            "firstName": "Florian",
                            "lastName": "Laws",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Florian Laws"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 300,
                                "start": 103
                            }
                        ],
                        "text": "Related Work Most of previous works formulate semi-structured document IE task as text tagging problem (Palm et al., 2017; Katti et al., 2018; Zhao et al., 2019; Xu et al., 2019; Denk and Reisswig, 2019; Majumder et al., 2020; Liu et al., 2019; Yu et al., 2020; Qian et al., 2019; Hwang et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 104
                            }
                        ],
                        "text": "Related Work Most of previous works formulate semi-structured document IE task as text tagging problem (Palm et al., 2017; Katti et al., 2018; Zhao et al., 2019; Xu et al., 2019; Denk and Reisswig, 2019; Majumder et al., 2020; Liu et al., 2019; Yu et al., 2020; Qian et al., 2019; Hwang et al.,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 30210556,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01a50b9662a59070c4ff53873453d8854c15ade1",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We present CloudScan; an invoice analysis system that requires zero configuration or upfront annotation. In contrast to previous work, CloudScan does not rely on templates of invoice layout, instead it learns a single global model of invoices that naturally generalizes to unseen invoice layouts. The model is trained using data automatically extracted from end-user provided feedback. This automatic training data extraction removes the requirement for users to annotate the data precisely. We describe a recurrent neural network model that can capture long range context and compare it to a baseline logistic regression model corresponding to the current CloudScan production system. We train and evaluate the system on 8 important fields using a dataset of 326,471 invoices. The recurrent neural network and baseline model achieve 0.891 and 0.887 average F1 scores respectively on seen invoice layouts. For the harder task of unseen invoice layouts, the recurrent neural network model outperforms the baseline with 0.840 average F1 compared to 0.788."
            },
            "slug": "CloudScan-A-Configuration-Free-Invoice-Analysis-Palm-Winther",
            "title": {
                "fragments": [],
                "text": "CloudScan - A Configuration-Free Invoice Analysis System Using Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A recurrent neural network model that can capture long range context and compare it to a baseline logistic regression model corresponding to the current CloudScan production system are described."
            },
            "venue": {
                "fragments": [],
                "text": "2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145307652"
                        ],
                        "name": "Li Dong",
                        "slug": "Li-Dong",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Dong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Dong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 44167998,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd04e31c25451f9103a0ac2220ac8d7e7884c343",
            "isKey": false,
            "numCitedBy": 285,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic parsing aims at mapping natural language utterances into structured meaning representations. In this work, we propose a structure-aware neural architecture which decomposes the semantic parsing process into two stages. Given an input utterance, we first generate a rough sketch of its meaning, where low-level information (such as variable names and arguments) is glossed over. Then, we fill in missing details by taking into account the natural language input and the sketch itself. Experimental results on four datasets characteristic of different domains and meaning representations show that our approach consistently improves performance, achieving competitive results despite the use of relatively simple decoders."
            },
            "slug": "Coarse-to-Fine-Decoding-for-Neural-Semantic-Parsing-Dong-Lapata",
            "title": {
                "fragments": [],
                "text": "Coarse-to-Fine Decoding for Neural Semantic Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "This work proposes a structure-aware neural architecture which decomposes the semantic parsing process into two stages, and shows that this approach consistently improves performance, achieving competitive results despite the use of relatively simple decoders."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40348417"
                        ],
                        "name": "Ashish Vaswani",
                        "slug": "Ashish-Vaswani",
                        "structuredName": {
                            "firstName": "Ashish",
                            "lastName": "Vaswani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashish Vaswani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846258"
                        ],
                        "name": "Noam M. Shazeer",
                        "slug": "Noam-M.-Shazeer",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Shazeer",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noam M. Shazeer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3877127"
                        ],
                        "name": "Niki Parmar",
                        "slug": "Niki-Parmar",
                        "structuredName": {
                            "firstName": "Niki",
                            "lastName": "Parmar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Niki Parmar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39328010"
                        ],
                        "name": "Jakob Uszkoreit",
                        "slug": "Jakob-Uszkoreit",
                        "structuredName": {
                            "firstName": "Jakob",
                            "lastName": "Uszkoreit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jakob Uszkoreit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145024664"
                        ],
                        "name": "Llion Jones",
                        "slug": "Llion-Jones",
                        "structuredName": {
                            "firstName": "Llion",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Llion Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "19177000"
                        ],
                        "name": "Aidan N. Gomez",
                        "slug": "Aidan-N.-Gomez",
                        "structuredName": {
                            "firstName": "Aidan",
                            "lastName": "Gomez",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aidan N. Gomez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40527594"
                        ],
                        "name": "Lukasz Kaiser",
                        "slug": "Lukasz-Kaiser",
                        "structuredName": {
                            "firstName": "Lukasz",
                            "lastName": "Kaiser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lukasz Kaiser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3443442"
                        ],
                        "name": "Illia Polosukhin",
                        "slug": "Illia-Polosukhin",
                        "structuredName": {
                            "firstName": "Illia",
                            "lastName": "Polosukhin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Illia Polosukhin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 62
                            }
                        ],
                        "text": "WYVERN consists of the following three major modules: (1) the Transformer encoder that accepts 2D text, (2) the decoder for sequence generation, and (3) the transition module that converts the generated sequence into parse tree."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 43
                            }
                        ],
                        "text": "2D Transformer Encoder We use Transformer (Vaswani et al., 2017) with the following modification for encoding 2D text (Hwang et al., 2020)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 15
                            }
                        ],
                        "text": "Decoder We use Transformer decoder equipped with the gated copying mechanism (Gu et al., 2016; See et al., 2017)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 148
                            }
                        ],
                        "text": "To achieve service-quality accuracy and stability with a sequence generation model while minimizing domain-specific engineering, we start from a 2D Transformer baseline and experiment with the following components: (1) copying mechanism, (2) transition module, and (3) employing preexisting weak-label data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 42
                            }
                        ],
                        "text": "2D Transformer Encoder We use Transformer (Vaswani et al., 2017) with the following modification for encoding 2D text (Hwang et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 33
                            }
                        ],
                        "text": "Note that a naive application of Transformer encoderdecoder model shows dramatic performance\nWYVERN w/ weak-label pretraining X 8.1 5.93 \u2020 Transformer model with minimal modification to encode 2D texts (Sec."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 147
                            }
                        ],
                        "text": "At each time t, the probability of copying individual input tokens is calculated via inner product between the contextualized inputs from the last Transformer layers of the encoder ({he}) and the decoder (hd(t))."
                    },
                    "intents": []
                }
            ],
            "corpusId": 13756489,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "isKey": true,
            "numCitedBy": 35148,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
            },
            "slug": "Attention-is-All-you-Need-Vaswani-Shazeer",
            "title": {
                "fragments": [],
                "text": "Attention is All you Need"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely is proposed, which generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39172707"
                        ],
                        "name": "Jacob Devlin",
                        "slug": "Jacob-Devlin",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Devlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacob Devlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744179"
                        ],
                        "name": "Ming-Wei Chang",
                        "slug": "Ming-Wei-Chang",
                        "structuredName": {
                            "firstName": "Ming-Wei",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Wei Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2544107"
                        ],
                        "name": "Kenton Lee",
                        "slug": "Kenton-Lee",
                        "structuredName": {
                            "firstName": "Kenton",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenton Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3259253"
                        ],
                        "name": "Kristina Toutanova",
                        "slug": "Kristina-Toutanova",
                        "structuredName": {
                            "firstName": "Kristina",
                            "lastName": "Toutanova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristina Toutanova"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 5
                            }
                        ],
                        "text": "Like BERT, each feature is represented as integers and maps to a trainable embedding vector."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 28
                            }
                        ],
                        "text": "The model is based on BERT (Devlin et al., 2018) except that 2D coordinate embeddings are added to input vectors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 80
                            }
                        ],
                        "text": "Experimental setup The model is initialized with pretrained multi-lingual BERT (Devlin et al., 2018; Wolf et al., 2020) and trained by Adam optimizer (Kingma and Ba, 2015) with learning rate 2e-5 or 3e-5."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 52967399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "isKey": true,
            "numCitedBy": 33744,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
            },
            "slug": "BERT:-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang",
            "title": {
                "fragments": [],
                "text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A new language representation model, BERT, designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers, which can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38253388"
                        ],
                        "name": "Pengcheng Yin",
                        "slug": "Pengcheng-Yin",
                        "structuredName": {
                            "firstName": "Pengcheng",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pengcheng Yin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700325"
                        ],
                        "name": "Graham Neubig",
                        "slug": "Graham-Neubig",
                        "structuredName": {
                            "firstName": "Graham",
                            "lastName": "Neubig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Graham Neubig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 288,
                                "start": 190
                            }
                        ],
                        "text": "One can consider replacing the entire pipeline into a single end-to-end model but such model is known to show relatively low accuracy in structured prediction tasks without careful modeling (Dyer et al., 2016; Dong and Lapata, 2018; Yin and Neubig, 2018; Fernandez Astudillo et al., 2020)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 58
                            }
                        ],
                        "text": "NT(key), GEN(token), and REDUCE based on previous studies (Yin and Neubig, 2018; Dyer et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52926380,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "649c1148439a4e875dab414ba67bf8c80350af4a",
            "isKey": false,
            "numCitedBy": 108,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We present TRANX, a transition-based neural semantic parser that maps natural language (NL) utterances into formal meaning representations (MRs). TRANX uses a transition system based on the abstract syntax description language for the target MR, which gives it two major advantages: (1) it is highly accurate, using information from the syntax of the target MR to constrain the output space and model the information flow, and (2) it is highly generalizable, and can easily be applied to new types of MR by just writing a new abstract syntax description corresponding to the allowable structures in the MR. Experiments on four different semantic parsing and code generation tasks show that our system is generalizable, extensible, and effective, registering strong results compared to existing neural semantic parsers."
            },
            "slug": "TRANX:-A-Transition-based-Neural-Abstract-Syntax-Yin-Neubig",
            "title": {
                "fragments": [],
                "text": "TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "TRANX is a transition-based neural semantic parser that maps natural language utterances into formal meaning representations (MRs) and is highly generalizable, extensible, and effective, registering strong results compared to existing neural semantic parsers."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3394760"
                        ],
                        "name": "Ram\u00f3n Fern\u00e1ndez Astudillo",
                        "slug": "Ram\u00f3n-Fern\u00e1ndez-Astudillo",
                        "structuredName": {
                            "firstName": "Ram\u00f3n",
                            "lastName": "Astudillo",
                            "middleNames": [
                                "Fern\u00e1ndez"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ram\u00f3n Fern\u00e1ndez Astudillo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143668305"
                        ],
                        "name": "Miguel Ballesteros",
                        "slug": "Miguel-Ballesteros",
                        "structuredName": {
                            "firstName": "Miguel",
                            "lastName": "Ballesteros",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Miguel Ballesteros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2524647"
                        ],
                        "name": "Tahira Naseem",
                        "slug": "Tahira-Naseem",
                        "structuredName": {
                            "firstName": "Tahira",
                            "lastName": "Naseem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tahira Naseem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145079952"
                        ],
                        "name": "Austin Blodgett",
                        "slug": "Austin-Blodgett",
                        "structuredName": {
                            "firstName": "Austin",
                            "lastName": "Blodgett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Austin Blodgett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707117"
                        ],
                        "name": "Radu Florian",
                        "slug": "Radu-Florian",
                        "structuredName": {
                            "firstName": "Radu",
                            "lastName": "Florian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radu Florian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 219963738,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb28e163fe721d2334b211356aba71b2a76e9d9a",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Modeling the parser state is key to good performance in transition-based parsing. Recurrent Neural Networks considerably improved the performance of transition-based systems by modelling the global state, e.g. stack-LSTM parsers, or local state modeling of contextualized features, e.g. Bi-LSTM parsers. Given the success of Transformer architectures in recent parsing systems, this work explores modifications of the sequence-to-sequence Transformer architecture to model either global or local parser states in transition-based parsing. We show that modifications of the cross attention mechanism of the Transformer considerably strengthen performance both on dependency and Abstract Meaning Representation (AMR) parsing tasks, particularly for smaller models or limited training data."
            },
            "slug": "Transition-based-Parsing-with-Stack-Transformers-Astudillo-Ballesteros",
            "title": {
                "fragments": [],
                "text": "Transition-based Parsing with Stack-Transformers"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Modifications of the cross attention mechanism of the Transformer considerably strengthen performance both on dependency and Abstract Meaning Representation (AMR) parsing tasks, particularly for smaller models or limited training data."
            },
            "venue": {
                "fragments": [],
                "text": "FINDINGS"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698666"
                        ],
                        "name": "Kaizhong Zhang",
                        "slug": "Kaizhong-Zhang",
                        "structuredName": {
                            "firstName": "Kaizhong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaizhong Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695878"
                        ],
                        "name": "D. Shasha",
                        "slug": "D.-Shasha",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "Shasha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Shasha"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10970317,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "277ff0c74cc72663d0aabbeae25a3e97b245457c",
            "isKey": false,
            "numCitedBy": 1292,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Ordered labeled trees are trees in which the left-to-right order among siblings is significant. The distance between two ordered trees is considered to be the weighted number of edit operations (insert, delete, and modify) to transform one tree to another. The problem of approximate tree matching is also considered. Specifically, algorithms are designed to answer the following kinds of questions:1. What is the distance between two trees? 2. What is the minimum distance between $T_1 $ and $T_2 $ when zero or more subtrees can be removed from $T_2 $? 3. Let the pruning of a tree at node n mean removing all the descendants of node n. The analogous question for prunings as for subtrees is answered.A dynamic programming algorithm is presented to solve the three questions in sequential time $O(|T_1 | \\times |T_2 | \\times \\min ({\\textit{depth}}(T_1 ),{\\textit{leaves}}(T_1 )) \\times \\min ({\\textit{depth}}(T_2 ),{\\textit{leaves}}(T_2 )))$ and space $O(|T_1 | \\times |T_2 |)$ compared with $O(|T_1 | \\times |T_2 | \\t..."
            },
            "slug": "Simple-Fast-Algorithms-for-the-Editing-Distance-and-Zhang-Shasha",
            "title": {
                "fragments": [],
                "text": "Simple Fast Algorithms for the Editing Distance Between Trees and Related Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Algorithms are designed to answer the following kinds of questions about trees: what is the distance between two trees, and the analogous question for prunings as for subtrees."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Comput."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726807"
                        ],
                        "name": "Diederik P. Kingma",
                        "slug": "Diederik-P.-Kingma",
                        "structuredName": {
                            "firstName": "Diederik",
                            "lastName": "Kingma",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diederik P. Kingma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2503659"
                        ],
                        "name": "Jimmy Ba",
                        "slug": "Jimmy-Ba",
                        "structuredName": {
                            "firstName": "Jimmy",
                            "lastName": "Ba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jimmy Ba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 38
                            }
                        ],
                        "text": ", 2020) and trained by Adam optimizer (Kingma and Ba, 2015) with learning rate 2e-5 or 3e-5."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6628106,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "isKey": false,
            "numCitedBy": 90052,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."
            },
            "slug": "Adam:-A-Method-for-Stochastic-Optimization-Kingma-Ba",
            "title": {
                "fragments": [],
                "text": "Adam: A Method for Stochastic Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This work introduces Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments, and provides a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745899"
                        ],
                        "name": "Chris Dyer",
                        "slug": "Chris-Dyer",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Dyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Dyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3376845"
                        ],
                        "name": "A. Kuncoro",
                        "slug": "A.-Kuncoro",
                        "structuredName": {
                            "firstName": "Adhiguna",
                            "lastName": "Kuncoro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kuncoro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143668305"
                        ],
                        "name": "Miguel Ballesteros",
                        "slug": "Miguel-Ballesteros",
                        "structuredName": {
                            "firstName": "Miguel",
                            "lastName": "Ballesteros",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Miguel Ballesteros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144365875"
                        ],
                        "name": "Noah A. Smith",
                        "slug": "Noah-A.-Smith",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Smith",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah A. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 280
                            }
                        ],
                        "text": "\u2026types of actions\nb\na\nS\nAST\n+1 TED\nprediction\n[store]\n[list] [list]\n[item]\n[name]\n105 h-street\n[address]\n[menu] [menu]\nS\n[store]\n[name]\nHappy store\nHappy store\n105\n[price]\nS\n[store]\n[name]\n[address]\nNT(key), GEN(token), and REDUCE based on previous studies (Yin and Neubig, 2018; Dyer et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 191
                            }
                        ],
                        "text": "One can consider replacing the entire pipeline into a single end-to-end model but such model is known to show relatively low accuracy in structured prediction tasks without careful modeling (Dyer et al., 2016; Dong and Lapata, 2018; Yin and Neubig, 2018; Fernandez Astudillo et al., 2020)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 58
                            }
                        ],
                        "text": "NT(key), GEN(token), and REDUCE based on previous studies (Yin and Neubig, 2018; Dyer et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1949831,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7345843e87c81e24e42264859b214d26042f8d51",
            "isKey": false,
            "numCitedBy": 424,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce recurrent neural network grammars, probabilistic models of sentences with explicit phrase structure. We explain efficient inference procedures that allow application to both parsing and language modeling. Experiments show that they provide better parsing in English than any single previously published supervised generative model and better language modeling than state-of-the-art sequential RNNs in English and Chinese."
            },
            "slug": "Recurrent-Neural-Network-Grammars-Dyer-Kuncoro",
            "title": {
                "fragments": [],
                "text": "Recurrent Neural Network Grammars"
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Transformers: Stateof-the-art natural language processing"
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP System Demonstrations, Online. Association for"
            },
            "year": 2020
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 7,
            "methodology": 12
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 24,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Cost-effective-End-to-end-Information-Extraction-Hwang-Lee/39d23cc236dd5057df7ff1949fa4fe8e87559f6e?sort=total-citations"
}