{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2657884"
                        ],
                        "name": "M. Cal\u00ed",
                        "slug": "M.-Cal\u00ed",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Cal\u00ed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Cal\u00ed"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 214
                            }
                        ],
                        "text": "Most systems that generate extraction patterns automatically use special training resources, such as texts annotated with domain-specific tags (e.g., AutoSlog (Riloff 1993; 1996a), CRYSTAL (Soderland et al. 1995), RAPIER (Califf 1998), SRV (Freitag 1998), WHISK (Soderland 1999)) or manually fined keywords, frames, or object recognizers (e.g., PALKA (Kim & Moldovan 1993) and LIEP (Huffman 1996))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 148
                            }
                        ],
                        "text": "\u2026training resources, such as texts annotated with domain-specific tags (e.g., AutoSlog (Riloff 1993; 1996a), CRYSTAL (Soderland et al. 1995), RAPIER (Califf 1998), SRV (Freitag 1998), WHISK (Soderland 1999)) or manually fined keywords, frames, or object recognizers (e.g., PALKA (Kim & Moldovan\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8126183,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6958e58fcaf3f45572bc4e7cf7d45798f0cad175",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 129,
            "paperAbstract": {
                "fragments": [],
                "text": "The recent growth of online information available in the form of natural language documents creates a greater need for computing systems with the ability to process those documents to simplify access to the information. One type of processing appropriate for many tasks is information extraction, a type of text skimming that retrieves speci c types of information from text. Although information extraction systems have existed for two decades, these systems have generally been built by hand and contain domain speci c information, making them di cult to port to other domains. A few researchers have begun to apply machine learning to information extraction tasks, but most of this work has involved applying learning to pieces of a much larger system. This paper presents a novel rule representation speci c to natural language and a learning system, Rapier, which learns information extraction rules. Rapier takes pairs of documents and lled templates indicating the information to be extracted and learns patterns to extract llers for the slots in the template. This proposal presents initial results on a small corpus of computer-related job postings with a preliminary version of Rapier. Future research will involve several enhancements to Rapier as well as more thorough testing on several domains and extension to additional natural language processing tasks. We intend to extend the rule representation and algorithm to allow for more types of constraints than are currently supported. We also plan to incorporate active learning, or sample selection, methods, speci cally query by committee, into Rapier. These methods have the potential to substantially reduce the amount of annotation required. We will explore the issue of distinguishing relevant and irrelevant messages, since currently Rapier only extracts from the any messages given to it, assuming that all are relevant. We also intend to run much larger tests with Rapier on multiple domains including the terrorism domain from the third and fourth Message Uncderstanding Conferences, which will allow comparison against other systems. Finally, we plan to demonstrate the generality of Rapier`s representation and algorithm by applying it to other natural language processing tasks such as word sense disambiguation."
            },
            "slug": "Relational-learning-techniques-for-natural-language-Cal\u00ed",
            "title": {
                "fragments": [],
                "text": "Relational learning techniques for natural language information extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A novel rule representation speci c to natural language and a learning system, Rapier, which learns information extraction rules, and initial results on a small corpus of computer-related job postings with a preliminary version of Rapier are presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2605572"
                        ],
                        "name": "S. Huffman",
                        "slug": "S.-Huffman",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Huffman",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Huffman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14690792,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8dadbf2dfebe794ad4fc5022f8bb65195c8f0d5a",
            "isKey": false,
            "numCitedBy": 193,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "A growing population of users want to extract a growing variety of information from on-line texts. Unfortunately, current information extraction systems typically require experts to hand-build dictionaries of extraction patterns for each new type of information to be extracted. This paper presents a system that can learn dictionaries of extraction patterns directly from user-provided examples of texts and events to be extracted from them. The system, called LIEP, learns patterns that recognize relationships between key constituents based on local syntax. Sets of patterns learned by LIEP for a sample extraction task perform nearly at the level of a hand-built dictionary of patterns."
            },
            "slug": "Learning-information-extraction-patterns-from-Huffman",
            "title": {
                "fragments": [],
                "text": "Learning information extraction patterns from examples"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A system that can learn dictionaries of extraction patterns directly from user-provided examples of texts and events to be extracted from them, and learns patterns that recognize relationships between key constituents based on local syntax."
            },
            "venue": {
                "fragments": [],
                "text": "Learning for Natural Language Processing"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691993"
                        ],
                        "name": "E. Riloff",
                        "slug": "E.-Riloff",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Riloff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riloff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 363940,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97a3d56d74575aae5d563ab2a0438a25ffbb69ae",
            "isKey": false,
            "numCitedBy": 168,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-Empirical-Study-of-Automated-Dictionary-for-in-Riloff",
            "title": {
                "fragments": [],
                "text": "An Empirical Study of Automated Dictionary Construction for Information Extraction in Three Domains"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144295318"
                        ],
                        "name": "S. Soderland",
                        "slug": "S.-Soderland",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Soderland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Soderland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145262164"
                        ],
                        "name": "David Fisher",
                        "slug": "David-Fisher",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Fisher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Fisher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51080653"
                        ],
                        "name": "J. Aseltine",
                        "slug": "J.-Aseltine",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Aseltine",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Aseltine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1925215"
                        ],
                        "name": "W. Lehnert",
                        "slug": "W.-Lehnert",
                        "structuredName": {
                            "firstName": "Wendy",
                            "lastName": "Lehnert",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Lehnert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 181
                            }
                        ],
                        "text": "Most systems that generate extraction patterns automatically use special training resources, such as texts annotated with domain-specific tags (e.g., AutoSlog (Riloff 1993; 1996a), CRYSTAL (Soderland et al. 1995), RAPIER (Califf 1998), SRV (Freitag 1998), WHISK (Soderland 1999)) or manually fined keywords, frames, or object recognizers (e.g., PALKA (Kim & Moldovan 1993) and LIEP (Huffman 1996))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 144
                            }
                        ],
                        "text": "\u2026automatically use special training resources, such as texts annotated with domain-specific tags (e.g., AutoSlog (Riloff 1993; 1996a), CRYSTAL (Soderland et al. 1995), RAPIER (Califf 1998), SRV (Freitag 1998), WHISK (Soderland 1999)) or manually fined keywords, frames, or object recognizers\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9168228,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8245f6099f547008522ebbe6fb813d8132085746",
            "isKey": false,
            "numCitedBy": 409,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the central knowledge sources of an information extraction (IE) system IS a dictionary of linguistic patterns that can be used to identify references to relevant information in a text Automatic creation of conceptual dictionaries is important for portability and scalability of an IE system This paper describes CRYSTAL, a system which automatically induces a dictionary of \"concept-node definitions\" sufficient to identify relevant information from a training corpus Each of these concept-node definitions is generalized as far as possible without producing errors, so that a minimum number of dictionary entries cover the positive training instances Because it tests the accuracy of each proposed definition, CRYSTAL can often surpass human intuitions in creating reliable extraction rules."
            },
            "slug": "CRYSTAL:-Inducing-a-Conceptual-Dictionary-Soderland-Fisher",
            "title": {
                "fragments": [],
                "text": "CRYSTAL: Inducing a Conceptual Dictionary"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "CRYSTAL is described, a system which automatically induces a dictionary of \"concept-node definitions\" sufficient to identify relevant information from a training corpus that can often surpass human intuitions in creating reliable extraction rules."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691993"
                        ],
                        "name": "E. Riloff",
                        "slug": "E.-Riloff",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Riloff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riloff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 12
                            }
                        ],
                        "text": "Our stopword list contained 35 words, mainly pronouns, determiners, and quantifiers. only the most reliable extraction patterns but also patterns that will frequently extract relevant information (even if irrelevant information will also be extracted)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15894892,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "acec622ca4fb7e01a56116522d35ded149969d0a",
            "isKey": false,
            "numCitedBy": 762,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Many corpus-based natural language processing systems rely on text corpora that have been manually annotated with syntactic or semantic tags. In particular, all previous dictionary construction systems for information extraction have used an annotated training corpus or some form of annotated input. We have developed a system called AutoSlog-TS that creates dictionaries of extraction patterns using only untagged text. AutoSlog-TS is based on the AutoSlog system, which generated extraction patterns using annotated text and a set of heuristic rules. By adapting AutoSlog and combining it with statistical techniques, we eliminated its dependency on tagged text. In experiments with the MUG-4 terrorism domain, AutoSlog-TS created a dictionary of extraction patterns that performed comparably to a dictionary created by AutoSlog, using only preclassified texts as input."
            },
            "slug": "Automatically-Generating-Extraction-Patterns-from-Riloff",
            "title": {
                "fragments": [],
                "text": "Automatically Generating Extraction Patterns from Untagged Text"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work has developed a system called AutoSlog-TS that creates dictionaries of extraction patterns using only untagged text, and in experiments with the MUG-4 terrorism domain, created a dictionary of extraction pattern that performed comparably to a dictionary created by autoSlog, using only preclassified texts as input."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI, Vol. 2"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691993"
                        ],
                        "name": "E. Riloff",
                        "slug": "E.-Riloff",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Riloff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riloff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 147
                            }
                        ],
                        "text": "\u2026that generate extraction patterns automatically use special training resources, such as texts annotated with domain-specific tags (e.g., AutoSlog (Riloff 1993; 1996a), CRYSTAL (Soderland et al. 1995), RAPIER (Califf 1998), SRV (Freitag 1998), WHISK (Soderland 1999)) or manually fined keywords,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "AutoSlog-TS (Riloff 1996b) takes a ferent approach by using a preclassified training corpus in which texts only need to be labeled as relevant Copyright Q1999, American Association for Artificial Intelligence (www.aaai.org)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 8
                            }
                        ],
                        "text": "We used AutoSlog (Riloff 1993; 1996a) in an exhaustive fashion to generate extraction patterns for every noun phrase in the corpus."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 19
                            }
                        ],
                        "text": "Because we applied AutoSlog exhaustively , the complete set of extraction patterns that it produced is capable of extracting every noun phrase in the training corpus."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 230
                            }
                        ],
                        "text": "We hypothesize that the set of possible company names is much larger than the set of 5Due to time constraints, we only hand-labeled the noun phrases that were extracted by at least one of the 19,690 candidate patterns produced by AutoSlog. locations and titles in corporate web pages, so we probably need to generate a much larger lexicon of company names to achieve good results for this category."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 7
                            }
                        ],
                        "text": "Before bootstrapping begins, the text corpus is used to generate a set of candidate extraction patterns."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 1
                            }
                        ],
                        "text": "4AutoSlog actually generated many more extraction patterns, but for practical reasons we only used the patterns that appeared with frequency > 2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 98
                            }
                        ],
                        "text": "Using these criteria, we scored each extraction pattern using the RlogF metric used previously by AutoSlog-TS (Riloff 1996b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 150
                            }
                        ],
                        "text": "Most systems that generate extraction patterns automatically use special training resources, such as texts annotated with domain-specific tags (e.g., AutoSlog (Riloff 1993; 1996a), CRYSTAL (Soderland et al. 1995), RAPIER (Califf 1998), SRV (Freitag 1998), WHISK (Soderland 1999)) or manually fined keywords, frames, or object recognizers (e.g., PALKA (Kim & Moldovan 1993) and LIEP (Huffman 1996))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 74
                            }
                        ],
                        "text": "Generate all candidate extraction patterns from the training corpus using AutoSlog."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 32
                            }
                        ],
                        "text": "Given a noun phrase to extract, AutoSlog uses heuristics to generate a linguistic expression that represents relevant context for extracting the NP."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 2
                            }
                        ],
                        "text": "~ AutoSlog generated 19,690 candidate extraction patterns from the web page training set, and 14,064 candidate extraction patterns from the terrorism training set."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2257053,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bdc08721414c972ab451f8ef3ef39d63c741b324",
            "isKey": false,
            "numCitedBy": 556,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Knowledge-based natural language processing systems have achieved good success with certain tasks but they are often criticized because they depend on a domain-specific dictionary that requires a great deal of manual knowledge engineering. This knowledge engineering bottleneck makes knowledge-based NLP systems impractical for real-world applications because they cannot be easily scaled up or ported to new domains. In response to this problem, we developed a system called AutoSlog that automatically builds a domain-specific dictionary of concepts for extracting information from text. Using AutoSlog, we constructed a dictionary for the domain of terrorist event descriptions in only 5 person-hours. We then compared the AutoSlog dictionary with a hand-crafted dictionary that was built by two highly skilled graduate students and required approximately 1500 person-hours of effort. We evaluated the two dictionaries using two blind test sets of 100 texts each. Overall, the AutoSlog dictionary achieved 98% of the performance of the hand-crafted dictionary. On the first test set, the AutoSlog dictionary obtained 96.3% of the performance of the hand-crafted dictionary. On the second test set, the overall scores were virtually indistinguishable with the AutoSlog dictionary achieving 99.7% of the performance of the handcrafted dictionary."
            },
            "slug": "Automatically-Constructing-a-Dictionary-for-Tasks-Riloff",
            "title": {
                "fragments": [],
                "text": "Automatically Constructing a Dictionary for Information Extraction Tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Using AutoSlog, a system that automatically builds a domain-specific dictionary of concepts for extracting information from text, a dictionary for the domain of terrorist event descriptions was constructed in only 5 person-hours and the overall scores were virtually indistinguishable."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758106"
                        ],
                        "name": "Dayne Freitag",
                        "slug": "Dayne-Freitag",
                        "structuredName": {
                            "firstName": "Dayne",
                            "lastName": "Freitag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dayne Freitag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 149
                            }
                        ],
                        "text": "\u2026such as texts annotated with domain-specific tags (e.g., AutoSlog (Riloff 1993; 1996a), CRYSTAL (Soderland et al. 1995), RAPIER (Califf 1998), SRV (Freitag 1998), WHISK (Soderland 1999)) or manually fined keywords, frames, or object recognizers (e.g., PALKA (Kim & Moldovan 1993) and LIEP (Huffman\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 236
                            }
                        ],
                        "text": "Most systems that generate extraction patterns automatically use special training resources, such as texts annotated with domain-specific tags (e.g., AutoSlog (Riloff 1993; 1996a), CRYSTAL (Soderland et al. 1995), RAPIER (Califf 1998), SRV (Freitag 1998), WHISK (Soderland 1999)) or manually fined keywords, frames, or object recognizers (e.g., PALKA (Kim & Moldovan 1993) and LIEP (Huffman 1996))."
                    },
                    "intents": []
                }
            ],
            "corpusId": 46248,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9272961455fa09b5f561e55638621f11a5883345",
            "isKey": false,
            "numCitedBy": 101,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Two trends are evident in the recent evolution of the field of information extraction: a preference for simple, often corpus-driven techniques over linguistically sophisticated ones; and a broadening of the central problem definition to include many non-traditional text domains. This development calls for information extraction systems which are as retargetable and general as possible. Here, we describe SRV, a learning architecture for information extraction which is designed for maximum generality and flexibility. SRV can exploit domain-specific information, including linguistic syntax and lexical information, in the form of features provided to the system explicitly as input for training. This process is illustrated using a domain created from Reuters corporate acquisitions articles. Features are derived from two general-purpose NLP systems, Sleator and Temperly's link grammar parser and Wordnet. Experiments compare the learner's performance with and without such linguistic information. Surprisingly, in many cases, the system performs as well without this information as with it."
            },
            "slug": "Toward-General-Purpose-Learning-for-Information-Freitag",
            "title": {
                "fragments": [],
                "text": "Toward General-Purpose Learning for Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "SRV is described, a learning architecture for information extraction which is designed for maximum generality and flexibility and can exploit domain-specific information, including linguistic syntax and lexical information, in the form of features provided to the system explicitly as input for training."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691993"
                        ],
                        "name": "E. Riloff",
                        "slug": "E.-Riloff",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Riloff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riloff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068932662"
                        ],
                        "name": "J. Shepherd",
                        "slug": "J.-Shepherd",
                        "structuredName": {
                            "firstName": "Jessica",
                            "lastName": "Shepherd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shepherd"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1437,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "b3e9130ecab419f8267fccadf80c1ee2489be793",
            "isKey": false,
            "numCitedBy": 257,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic knowledge can be a great asset to natural language processing systems, but it is usually hand-coded for each application. Although some semantic information is available in general-purpose knowledge bases such as WordNet and Cyc, many applications require domain-specific lexicons that represent words and categories for a particular topic. In this paper, we present a corpus-based method that can be used to build semantic lexicons for specific categories. The input to the system is a small set of seed words for a category and a representative text corpus. The output is a ranked list of words that are associated with the category. A user then reviews the top-ranked words and decides which ones should be entered in the semantic lexicon. In experiments with five categories, users typically found about 60 words per category in 10-15 minutes to build a core semantic lexicon."
            },
            "slug": "A-Corpus-Based-Approach-for-Building-Semantic-Riloff-Shepherd",
            "title": {
                "fragments": [],
                "text": "A Corpus-Based Approach for Building Semantic Lexicons"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper presents a corpus-based method that can be used to build semantic lexicons for specific categories using a small set of seed words for a category and a representative text corpus."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2169401916"
                        ],
                        "name": "J.-T. Kim",
                        "slug": "J.-T.-Kim",
                        "structuredName": {
                            "firstName": "J.-T.",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J.-T. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40497400"
                        ],
                        "name": "D. Moldovan",
                        "slug": "D.-Moldovan",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Moldovan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Moldovan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 350,
                                "start": 345
                            }
                        ],
                        "text": "Most systems that generate extraction patterns automatically use special training resources, such as texts annotated with domain-specific tags (e.g., AutoSlog (Riloff 1993; 1996a), CRYSTAL (Soderland et al. 1995), RAPIER (Califf 1998), SRV (Freitag 1998), WHISK (Soderland 1999)) or manually fined keywords, frames, or object recognizers (e.g., PALKA (Kim & Moldovan 1993) and LIEP (Huffman 1996))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Most systems that generate extraction patterns automatically use special training resources, such as texts annotated with domain-specific tags (e.g., AutoSlog (Riloff 1993; 1996a), CRYSTAL (Soderland et al. 1995), RAPIER (Califf 1998), SRV (Freitag 1998), WHISK (Soderland 1999)) or manually fined keywords, frames, or object recognizers (e.g., PALKA ( Kim & Moldovan 1993 ) and LIEP (Huffman 1996))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 271,
                                "start": 252
                            }
                        ],
                        "text": "\u2026texts annotated with domain-specific tags (e.g., AutoSlog (Riloff 1993; 1996a), CRYSTAL (Soderland et al. 1995), RAPIER (Califf 1998), SRV (Freitag 1998), WHISK (Soderland 1999)) or manually fined keywords, frames, or object recognizers (e.g., PALKA (Kim & Moldovan 1993) and LIEP (Huffman 1996))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59002598,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e80b34a55aa56578f9a4f27ea207f8c42c93a378",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "A knowledge acquisition tool to extract semantic patterns for a memory-based information retrieval system is presented. The major goal of this tool is to facilitate the construction of a large knowledge base of semantic patterns. The system acquires semantic patterns from texts with a small amount of user interaction. It acquires new phrasal patterns from the input text, maps each element of the pattern to a meaning frame, generalizes the acquired pattern, and merges it into the current knowledge base. Interaction with the user is introduced at some decision points, where the ambiguity cannot be resolved automatically without other pieces of predefined knowledge. The acquisition process is described in detail, and a preliminary experimental result is discussed.<<ETX>>"
            },
            "slug": "Acquisition-of-semantic-patterns-for-information-Kim-Moldovan",
            "title": {
                "fragments": [],
                "text": "Acquisition of semantic patterns for information extraction from corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A knowledge acquisition tool to extract semantic patterns for a memory-based information retrieval system is presented to facilitate the construction of a large knowledge base of semantic patterns."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 9th IEEE Conference on Artificial Intelligence for Applications"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144557047"
                        ],
                        "name": "M. Craven",
                        "slug": "M.-Craven",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Craven",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Craven"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2922396"
                        ],
                        "name": "Dan DiPasquo",
                        "slug": "Dan-DiPasquo",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "DiPasquo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan DiPasquo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758106"
                        ],
                        "name": "Dayne Freitag",
                        "slug": "Dayne-Freitag",
                        "structuredName": {
                            "firstName": "Dayne",
                            "lastName": "Freitag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dayne Freitag"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145172877"
                        ],
                        "name": "K. Nigam",
                        "slug": "K.-Nigam",
                        "structuredName": {
                            "firstName": "Kamal",
                            "lastName": "Nigam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Nigam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682522"
                        ],
                        "name": "Se\u00e1n Slattery",
                        "slug": "Se\u00e1n-Slattery",
                        "structuredName": {
                            "firstName": "Se\u00e1n",
                            "lastName": "Slattery",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Se\u00e1n Slattery"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 151
                            }
                        ],
                        "text": "To evaluate the meta-bootstrapping algorithm, we performed experiments with two text collections: corporate web pages collected for the WebKB project (Craven et al. 1998) and terrorism news articles from the MUC-4 corpus (MUC-4 Proceedings 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "To evaluate the meta-bootstrapping algorithm, we performed experiments with two text collections: corporate web pages collected for the WebKB project ( Craven et al. 1998 ) and terrorism news articles from the MUC-4 corpus (MUC-4 Proceedings 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2312137,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8446830f3c05b97c4d12a0751c022d1ae6a5115b",
            "isKey": false,
            "numCitedBy": 800,
            "numCiting": 89,
            "paperAbstract": {
                "fragments": [],
                "text": "The World Wide Web is a vast source of information accessible to computers, but understandable only to humans. The goal of the research described here is to automatically create a computer understandable world wide knowledge base whose content mirrors that of the World Wide Web. Such a knowledge base would enable much more effective retrieval of Web information, and promote new uses of the Web to support knowledge-based inference and problem solving. Our approach is to develop a trainable information extraction system that takes two inputs: an ontology defining the classes and relations of interest, and a set of training data consisting of labeled regions of hypertext representing instances of these classes and relations. Given these inputs, the system learns to extract information from other pages and hyperlinks on the Web. This paper describes our general approach, several machine learning algorithms for this task, and promising initial results with a prototype system."
            },
            "slug": "Learning-to-Extract-Symbolic-Knowledge-from-the-Web-Craven-DiPasquo",
            "title": {
                "fragments": [],
                "text": "Learning to Extract Symbolic Knowledge from the World Wide Web"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "The goal of the research described here is to automatically create a computer understandable world wide knowledge base whose content mirrors that of the World Wide Web, and several machine learning algorithms for this task are described."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794100"
                        ],
                        "name": "Brian Roark",
                        "slug": "Brian-Roark",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Roark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Roark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749837"
                        ],
                        "name": "Eugene Charniak",
                        "slug": "Eugene-Charniak",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Charniak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Charniak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 137
                            }
                        ],
                        "text": "However there have been recent efforts to automate the construction of domainspecific semantic lexicons as well (Riloff & Shepherd 1997; Roark & Charniak 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 1
                            }
                        ],
                        "text": "(Roark & Charniak 1998)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 219307649,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e0415088488704d05f2cfacdff3b480129e7f0c",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Generating semantic lexicons semi-automatically could be a great time saver, relative to creating them by hand. In this paper, we present an algorithm for extracting potential entries for a category from an on-line corpus, based upon a small set of exemplars. Our algorithm finds more correct terms and fewer incorrect ones than previous work in this area. Additionally, the entries that are generated potentially provide broader coverage of the category than would occur to an individual coding them by hand. Our algorithm finds many terms not included within Wordnet (many more than previous algorithms), and could be viewed as an ``enhancer'' of existing broad-coverage resources."
            },
            "slug": "Noun-phrase-co-occurrence-statistics-for-semantic-Roark-Charniak",
            "title": {
                "fragments": [],
                "text": "Noun-phrase co-occurrence statistics for semi-automatic semantic lexicon construction"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper presents an algorithm for extracting potential entries for a category from an on-line corpus, based upon a small set of exemplars, that could be viewed as an ``enhancer'' of existing broad-coverage resources."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794100"
                        ],
                        "name": "Brian Roark",
                        "slug": "Brian-Roark",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Roark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Roark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749837"
                        ],
                        "name": "Eugene Charniak",
                        "slug": "Eugene-Charniak",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Charniak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Charniak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "However there have been recent efforts to automate the construction of domainspecific semantic lexicons as well (Riloff & Shepherd 1997;  Roark & Charniak 1998 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "To put our results in perspective, other researchers have generated a semantic lexicon for the terrorism weapon category and achieved accuracy rates of 34/200 (17%) (Riloff & Shepherd 1997) and 93/257 (36%) ( Roark & Charniak 1998 )."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 137
                            }
                        ],
                        "text": "However there have been recent efforts to automate the construction of domainspecific semantic lexicons as well (Riloff & Shepherd 1997; Roark & Charniak 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 1
                            }
                        ],
                        "text": "(Roark & Charniak 1998)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6972699,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3e15b6a849a41d18ee7773a95d61dc698f30ca52",
            "isKey": true,
            "numCitedBy": 132,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Generating semantic lexicons semi-automatically could be a great time saver, relative to creating them by hand. In this paper, we present an algorithm for extracting potential entries for a category from an on-line corpus, based upon a small set of exemplars. Our algorithm finds more correct terms and fewer incorrect ones than previous work in this area. Additionally, the entries that are generated potentially provide broader coverage of the category than would occur to an individual coding them by hand. Our algorithm finds many terms not included within Wordnet (many more than previous algorithms), and could be viewed as an \"enhancer\" of existing broad-coverage resources."
            },
            "slug": "Noun-Phrase-Co-Occurence-Statistics-for-Semantic-Roark-Charniak",
            "title": {
                "fragments": [],
                "text": "Noun-Phrase Co-Occurence Statistics for Semi-Automatic Semantic Lexicon Construction"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper presents an algorithm for extracting potential entries for a category from an on-line corpus, based upon a small set of exemplars, that could be viewed as an \"enhancer\" of existing broad-coverage resources."
            },
            "venue": {
                "fragments": [],
                "text": "COLING-ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144096985"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Miller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66511259"
                        ],
                        "name": "R. Beckwith",
                        "slug": "R.-Beckwith",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Beckwith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Beckwith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721801"
                        ],
                        "name": "C. Fellbaum",
                        "slug": "C.-Fellbaum",
                        "structuredName": {
                            "firstName": "Christiane",
                            "lastName": "Fellbaum",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Fellbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145386345"
                        ],
                        "name": "Derek Gross",
                        "slug": "Derek-Gross",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Gross",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Derek Gross"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113623689"
                        ],
                        "name": "K. Miller",
                        "slug": "K.-Miller",
                        "structuredName": {
                            "firstName": "Katherine",
                            "lastName": "Miller",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 137
                            }
                        ],
                        "text": "Semantic lexicons 1 for information extraction are almost always constructed by hand because general-purpose resources, such as WordNet (Miller 1990), do not contain the necessary domain-specific vocabulary."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2146137,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "4bd970a37c59c97804ff93cbb2c108e081de3a37",
            "isKey": false,
            "numCitedBy": 5334,
            "numCiting": 133,
            "paperAbstract": {
                "fragments": [],
                "text": "Standard alphabetical procedures for organizing lexical information put together words that are spelled alike and scatter words with similar or related meanings haphazardly through the list. Unfortunately, there is no obvious alternative, no other simple way for lexicographers to keep track of what has been done or for readers to find the word they are looking for. But a frequent objection to this solution is that finding things on an alphabetical list can be tedious and time-consuming. Many people who would like to refer to a dictionary decide not to bother with it because finding the information would interrupt their work and break their train of thought."
            },
            "slug": "Introduction-to-WordNet:-An-On-line-Lexical-Miller-Beckwith",
            "title": {
                "fragments": [],
                "text": "Introduction to WordNet: An On-line Lexical Database"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Standard alphabetical procedures for organizing lexical information put together words that are spelled alike and scatter words with similar or related meanings haphazardly through the list."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144135485"
                        ],
                        "name": "Tom. Mitchell",
                        "slug": "Tom.-Mitchell",
                        "structuredName": {
                            "firstName": "Tom.",
                            "lastName": "Mitchell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom. Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207228399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "278841ab0cb24c1abcb75e363aeed1fa741c8cc4",
            "isKey": false,
            "numCitedBy": 5471,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of using a large unlabeled sample to boost performance of a learning algorit,hrn when only a small set of labeled examples is available. In particular, we consider a problem setting motivated by the task of learning to classify web pages, in which the description of each example can be partitioned into two distinct views. For example, the description of a web page can be partitioned into the words occurring on that page, and the words occurring in hyperlinks t,hat point to that page. We assume that either view of the example would be sufficient for learning if we had enough labeled data, but our goal is to use both views together to allow inexpensive unlabeled data to augment, a much smaller set of labeled examples. Specifically, the presence of two distinct views of each example suggests strategies in which two learning algorithms are trained separately on each view, and then each algorithm\u2019s predictions on new unlabeled examples are used to enlarge the training set of the other. Our goal in this paper is to provide a PAC-style analysis for this setting, and, more broadly, a PAC-style framework for the general problem of learning from both labeled and unlabeled data. We also provide empirical results on real web-page data indicating that this use of unlabeled examples can lead to significant improvement of hypotheses in practice. *This research was supported in part by the DARPA HPKB program under contract F30602-97-1-0215 and by NSF National Young investigator grant CCR-9357793. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. TO copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. COLT 98 Madison WI USA Copyright ACM 1998 l-58113-057--0/98/ 7...%5.00 92 Tom Mitchell School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-3891 mitchell+@cs.cmu.edu"
            },
            "slug": "Combining-labeled-and-unlabeled-data-with-Blum-Mitchell",
            "title": {
                "fragments": [],
                "text": "Combining labeled and unlabeled data with co-training"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A PAC-style analysis is provided for a problem setting motivated by the task of learning to classify web pages, in which the description of each example can be partitioned into two distinct views, to allow inexpensive unlabeled data to augment, a much smaller set of labeled examples."
            },
            "venue": {
                "fragments": [],
                "text": "COLT' 98"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 151
                            }
                        ],
                        "text": "To evaluate the meta-bootstrapping algorithm, we performed experiments with two text collections: corporate web pages collected for the WebKB project (Craven et al. 1998) and terrorism news articles from the MUC-4 corpus (MUC-4 Proceedings 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learn - ing to Extract Symbolic Knowledge from the World Wide Web"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fifteenth National Conference on Artificial Intelligence"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "MUC-4 Proceedings. 1992. Proceedings of the Fourth Message Understanding Conference"
            },
            "venue": {
                "fragments": [],
                "text": "MUC-4 Proceedings. 1992. Proceedings of the Fourth Message Understanding Conference"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 7,
            "methodology": 4,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 16,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Learning-Dictionaries-for-Information-Extraction-by-Riloff-Jones/41e936981f5a2d55bfec0143e9a15e23ad96436b?sort=total-citations"
}