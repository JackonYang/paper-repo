{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759839"
                        ],
                        "name": "S. Solla",
                        "slug": "S.-Solla",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Solla",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Solla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14313855,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04641b38cdab7b5b6c7d0d09193d3220ef40efc5",
            "isKey": false,
            "numCitedBy": 122,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The method of Structural Risk Minimization refers to tuning the capacity of the classifier to the available amount of training data. This capacity is influenced by several factors, including: (1) properties of the input space, (2) nature and structure of the classifier, and (3) learning algorithm. Actions based on these three factors are combined here to control the capacity of linear classifiers and improve generalization on the problem of handwritten digit recognition."
            },
            "slug": "Structural-Risk-Minimization-for-Character-Guyon-Vapnik",
            "title": {
                "fragments": [],
                "text": "Structural Risk Minimization for Character Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "The method of Structural Risk Minimization is used to control the capacity of linear classifiers and improve generalization on the problem of handwritten digit recognition."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3065689"
                        ],
                        "name": "N. Matic",
                        "slug": "N.-Matic",
                        "structuredName": {
                            "firstName": "Nada",
                            "lastName": "Matic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Matic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62032796,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "229bd4528fc48d4ab79f67268038f1fcb06e9e0e",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for computer-aided cleaning of undesirable patterns in large training databases has been developed. The method uses the trainable classifier itself, to point out patterns that are suspicious, and should be checked by the human supervisor. While suspicious patterns that are meaningless or mislabeled are considered garbage, and removed from the database, the remaining patterns, like ambiguous or atypical, represent valid patterns that are hard to learn and should be kept in the database. By using the method of pattern cleaning, combined with an emphasizing scheme applied on the patterns that are hard to learn, the error rate on the test set has been reduced by half, in the case of the database of handwritten lowercase characters entered on a touch terminal. The classifier is based on a time delay neural network (TDNN).<<ETX>>"
            },
            "slug": "Computer-aided-cleaning-of-large-databases-for-Matic-Guyon",
            "title": {
                "fragments": [],
                "text": "Computer aided cleaning of large databases for character recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "By using the method of pattern cleaning, combined with an emphasizing scheme applied on the patterns that are hard to learn, the error rate on the test set has been reduced by half, in the case of the database of handwritten lowercase characters entered on a touch terminal."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings., 11th IAPR International Conference on Pattern Recognition. Vol.II. Conference B: Pattern Recognition Methodology and Systems"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114709889"
                        ],
                        "name": "I. Poujaud",
                        "slug": "I.-Poujaud",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Poujaud",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Poujaud"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3078169"
                        ],
                        "name": "L. Personnaz",
                        "slug": "L.-Personnaz",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Personnaz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Personnaz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144097910"
                        ],
                        "name": "G. Dreyfus",
                        "slug": "G.-Dreyfus",
                        "structuredName": {
                            "firstName": "G\u00e9rard",
                            "lastName": "Dreyfus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Dreyfus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "116525682"
                        ],
                        "name": "Y. Le Cun",
                        "slug": "Y.-Le-Cun",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Le Cun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Le Cun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2093637,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "590bd948e06e9d07e305fe175c2a86d751ccac2d",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "An evaluation is made of several neural network classifiers, comparing their performance on a typical problem, namely handwritten digit recognition. For this purpose, the authors use a database of handwritten digits, with relatively uniform handwriting styles. The authors propose a novel way of organizing the network architectures by training several small networks so as to deal separately with subsets of the problem, and then combining the results. This approach works in conjunction with various techniques including: layered networks with one or several layers of adaptive connections, fully connected recursive networks, ad hoc networks with no adaptive connections, and architectures with second-degree polynomial decision surfaces.<<ETX>>"
            },
            "slug": "Comparing-different-neural-network-architectures-Guyon-Poujaud",
            "title": {
                "fragments": [],
                "text": "Comparing different neural network architectures for classifying handwritten digits"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The authors propose a novel way of organizing the network architectures by training several small networks so as to deal separately with subsets of the problem, and then combining the results."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6635519,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4abd4e51705e74f1739bd3a1e47ac10e45f6468b",
            "isKey": false,
            "numCitedBy": 1170,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning an input-output mapping from a set of examples, of the type that many neural networks have been constructed to perform, can be regarded as synthesizing an approximation of a multidimensional function (that is, solving the problem of hypersurface reconstruction). From this point of view, this form of learning is closely related to classical approximation techniques, such as generalized splines and regularization theory. A theory is reported that shows the equivalence between regularization and a class of three-layer networks called regularization networks or hyper basis functions. These networks are not only equivalent to generalized splines but are also closely related to the classical radial basis functions used for interpolation tasks and to several pattern recognition and neural network algorithms. They also have an interesting interpretation in terms of prototypes that are synthesized and optimally combined during the learning stage."
            },
            "slug": "Regularization-Algorithms-for-Learning-That-Are-to-Poggio-Girosi",
            "title": {
                "fragments": [],
                "text": "Regularization Algorithms for Learning That Are Equivalent to Multilayer Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A theory is reported that shows the equivalence between regularization and a class of three-layer networks called regularization networks or hyper basis functions."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777660"
                        ],
                        "name": "Naftali Tishby",
                        "slug": "Naftali-Tishby",
                        "structuredName": {
                            "firstName": "Naftali",
                            "lastName": "Tishby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naftali Tishby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8992604"
                        ],
                        "name": "E. Levin",
                        "slug": "E.-Levin",
                        "structuredName": {
                            "firstName": "Esther",
                            "lastName": "Levin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Levin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759839"
                        ],
                        "name": "S. Solla",
                        "slug": "S.-Solla",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Solla",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Solla"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15012839,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b10440620da8a43a1b97e3da4b1ff13746306475",
            "isKey": false,
            "numCitedBy": 172,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of learning a general input-output relation using a layered neural network is discussed in a statistical framework. By imposing the consistency condition that the error minimization be equivalent to a likelihood maximization for training the network, the authors arrive at a Gibbs distribution on a canonical ensemble of networks with the same architecture. This statistical description enables them to evaluate the probability of a correct prediction of an independent example, after training the network on a given training set. The prediction probability is highly correlated with the generalization ability of the network, as measured outside the training set. This suggests a general and practical criterion for training layered networks by minimizing prediction errors. The authors demonstrate the utility of this criterion for selecting the optimal architecture in the continuity problem. As a theoretical application of the statistical formalism, they discuss the question of learning curves and estimate the sufficient training size needed for correct generalization, in a simple example.<<ETX>>"
            },
            "slug": "Consistent-inference-of-probabilities-in-layered-Tishby-Levin",
            "title": {
                "fragments": [],
                "text": "Consistent inference of probabilities in layered networks: predictions and generalizations"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The problem of learning a general input-output relation using a layered neural network is discussed in a statistical framework and the authors arrive at a Gibbs distribution on a canonical ensemble of networks with the same architecture."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080632378"
                        ],
                        "name": "B. Victorri",
                        "slug": "B.-Victorri",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Victorri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Victorri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2184474,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff32cebbdb8a436ccd8ae797647428615ae32d74",
            "isKey": false,
            "numCitedBy": 286,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "In many machine learning applications, one has access, not only to training data, but also to some high-level a priori knowledge about the desired behavior of the system. For example, it is known in advance that the output of a character recognizer should be invariant with respect to small spatial distortions of the input images (translations, rotations, scale changes, etcetera). \n \nWe have implemented a scheme that allows a network to learn the derivative of its outputs with respect to distortion operators of our choosing. This not only reduces the learning time and the amount of training data, but also provides a powerful language for specifying what generalizations we wish the network to perform."
            },
            "slug": "Tangent-Prop-A-Formalism-for-Specifying-Selected-in-Simard-Victorri",
            "title": {
                "fragments": [],
                "text": "Tangent Prop - A Formalism for Specifying Selected Invariances in an Adaptive Network"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A scheme is implemented that allows a network to learn the derivative of its outputs with respect to distortion operators of their choosing, which not only reduces the learning time and the amount of training data, but also provides a powerful language for specifying what generalizations the authors wish the network to perform."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403454"
                        ],
                        "name": "E. Baum",
                        "slug": "E.-Baum",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Baum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Baum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15659829,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "25406e6733a698bfc4ac836f8e74f458e75dad4f",
            "isKey": false,
            "numCitedBy": 1696,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the question of when a network can be expected to generalize from m random training examples chosen from some arbitrary probability distribution, assuming that future test examples are drawn from the same distribution. Among our results are the following bounds on appropriate sample vs. network size. Assume 0 < \u220a 1/8. We show that if m O(W/\u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 1 \u220a of future test examples drawn from the same distribution. Conversely, for fully-connected feedforward nets with one hidden layer, any learning algorithm using fewer than (W/\u220a) random training examples will, for some distributions of examples consistent with an appropriate weight choice, fail at least some fixed fraction of the time to find a weight choice that will correctly classify more than a 1 \u220a fraction of the future test examples."
            },
            "slug": "What-Size-Net-Gives-Valid-Generalization-Baum-Haussler",
            "title": {
                "fragments": [],
                "text": "What Size Net Gives Valid Generalization?"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that if m O(W/ \u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 2 \u220a of future test examples drawn from the same distribution."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2961287"
                        ],
                        "name": "N. Littlestone",
                        "slug": "N.-Littlestone",
                        "structuredName": {
                            "firstName": "Nick",
                            "lastName": "Littlestone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Littlestone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 5119145,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c5609ee7a8c7432c0f502b2a6dcfe9c0039206ab",
            "isKey": false,
            "numCitedBy": 226,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract We consider the problem of predicting {0, 1}-valued functions on Rn and smaller domains, based on their values on randomly drawn points. Our model is related to Valiant\u2032s PAC learning model, but does not require the hypotheses used for prediction to be represented in any specified form. In our main result we show how to construct prediction strategies that are optimal to within a constant factor for any reasonable class F of target functions. This result is based on new combinatorial results about classes of functions of finite VC dimension. We also discuss more computationally efficient algorithms for predicting indicator functions of axis-parallel rectangles, more general intersection closed concept classes, and halfspaces in Rn. These are also optimal to within a constant factor. Finally, we compare the general performance of prediction strategies derived by our method to that of those derived from methods in PAC learning theory."
            },
            "slug": "Predicting-{0,1}-functions-on-randomly-drawn-points-Haussler-Littlestone",
            "title": {
                "fragments": [],
                "text": "Predicting {0,1}-functions on randomly drawn points"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This model is related to Valiant\u2032s PAC learning model, but does not require the hypotheses used for prediction to be represented in any specified form and shows how to construct prediction strategies that are optimal to within a constant factor for any reasonable class F of target functions."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '88"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2246319"
                        ],
                        "name": "E. Bienenstock",
                        "slug": "E.-Bienenstock",
                        "structuredName": {
                            "firstName": "Elie",
                            "lastName": "Bienenstock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bienenstock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2330895"
                        ],
                        "name": "R. Doursat",
                        "slug": "R.-Doursat",
                        "structuredName": {
                            "firstName": "Ren\u00e9",
                            "lastName": "Doursat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Doursat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14215320,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "a34e35dbbc6911fa7b94894dffdc0076a261b6f0",
            "isKey": false,
            "numCitedBy": 3532,
            "numCiting": 151,
            "paperAbstract": {
                "fragments": [],
                "text": "Feedforward neural networks trained by error backpropagation are examples of nonparametric regression estimators. We present a tutorial on nonparametric inference and its relation to neural networks, and we use the statistical viewpoint to highlight strengths and weaknesses of neural models. We illustrate the main points with some recognition experiments involving artificial data as well as handwritten numerals. In way of conclusion, we suggest that current-generation feedforward neural networks are largely inadequate for difficult problems in machine perception and machine learning, regardless of parallel-versus-serial hardware or other implementation issues. Furthermore, we suggest that the fundamental challenges in neural modeling are about representation rather than learning per se. This last point is supported by additional experiments with handwritten numerals."
            },
            "slug": "Neural-Networks-and-the-Bias/Variance-Dilemma-Geman-Bienenstock",
            "title": {
                "fragments": [],
                "text": "Neural Networks and the Bias/Variance Dilemma"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is suggested that current-generation feedforward neural networks are largely inadequate for difficult problems in machine perception and machine learning, regardless of parallel-versus-serial hardware or other implementation issues."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145016534"
                        ],
                        "name": "J. Moody",
                        "slug": "J.-Moody",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Moody",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Moody"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2319258"
                        ],
                        "name": "C. Darken",
                        "slug": "C.-Darken",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Darken",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Darken"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 31251383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e7c4f513f24c3b82a1138b9f22ed87ed00cbe76",
            "isKey": false,
            "numCitedBy": 4527,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a network architecture which uses a single internal layer of locally-tuned processing units to learn both classification tasks and real-valued function approximations (Moody and Darken 1988). We consider training such networks in a completely supervised manner, but abandon this approach in favor of a more computationally efficient hybrid learning method which combines self-organized and supervised learning. Our networks learn faster than backpropagation for two reasons: the local representations ensure that only a few units respond to any given input, thus reducing computational overhead, and the hybrid learning rules are linear rather than nonlinear, thus leading to faster convergence. Unlike many existing methods for data analysis, our network architecture and learning rules are truly adaptive and are thus appropriate for real-time use."
            },
            "slug": "Fast-Learning-in-Networks-of-Locally-Tuned-Units-Moody-Darken",
            "title": {
                "fragments": [],
                "text": "Fast Learning in Networks of Locally-Tuned Processing Units"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This work proposes a network architecture which uses a single internal layer of locally-tuned processing units to learn both classification tasks and real-valued function approximations (Moody and Darken 1988)."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808760"
                        ],
                        "name": "S. Omohundro",
                        "slug": "S.-Omohundro",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Omohundro",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Omohundro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16048456,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "af7ea76ecaa3436db2efe30d44dc39a2d68a6ef9",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "A new class of data structures called \"bumptrees\" is described. These structures are useful for efficiently implementing a number of neural network related operations. An empirical comparison with radial basis functions is presented on a robot arm mapping learning task. Applications to density estimation, classification, and constraint representation and learning are also outlined."
            },
            "slug": "Bumptrees-for-Efficient-Function,-Constraint-and-Omohundro",
            "title": {
                "fragments": [],
                "text": "Bumptrees for Efficient Function, Constraint and Classification Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A new class of data structures called \"bumptrees\" is described, useful for efficiently implementing a number of neural network related operations and applications to density estimation, classification, and constraint representation and learning are outlined."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37274089"
                        ],
                        "name": "D. Henderson",
                        "slug": "D.-Henderson",
                        "structuredName": {
                            "firstName": "Donnie",
                            "lastName": "Henderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Henderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2799635"
                        ],
                        "name": "R. Howard",
                        "slug": "R.-Howard",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Howard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34859193"
                        ],
                        "name": "W. Hubbard",
                        "slug": "W.-Hubbard",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Hubbard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Hubbard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041866"
                        ],
                        "name": "L. Jackel",
                        "slug": "L.-Jackel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Jackel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jackel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2542741,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "86ab4cae682fbd49c5a5bedb630e5a40fa7529f6",
            "isKey": false,
            "numCitedBy": 2928,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an application of back-propagation networks to handwritten digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1% error rate and about a 9% reject rate on zipcode digits provided by the U.S. Postal Service."
            },
            "slug": "Handwritten-Digit-Recognition-with-a-Network-LeCun-Boser",
            "title": {
                "fragments": [],
                "text": "Handwritten Digit Recognition with a Back-Propagation Network"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task, and has 1% error rate and about a 9% reject rate on zipcode digits provided by the U.S. Postal Service."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15883988,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0f2433c088591d265891231f1c22424047f1bc1",
            "isKey": false,
            "numCitedBy": 254,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible: (1) objective comparisons between solutions using alternative network architectures; (2) objective stopping rules for deletion of weights; (3) objective choice of magnitude and type of weight decay terms or additive regularisers (for penalising large weights, etc.); (4) a measure of the e ective number of well{determined parameters in a model; (5) quanti ed estimates of the error bars on network parameters and on network output; (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian `evidence' automatically embodies `Occam's razor,' penalising over{ exible and over{complex architectures. The Bayesian approach helps detect poor underlying assumptions in learning models. For learning models well{ matched to a problem, a good correlation between generalisation ability and the Bayesian evidence is obtained."
            },
            "slug": "A-Practical-Bayesian-Framework-for-Backprop-Mackay",
            "title": {
                "fragments": [],
                "text": "A Practical Bayesian Framework for Backprop Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks and a good correlation between generalisation ability and the Bayesian evidence is obtained."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12946615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b07ce649d6f6eb636872527104b0209d3edc8188",
            "isKey": false,
            "numCitedBy": 16925,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "slug": "Pattern-classification-and-scene-analysis-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Pattern classification and scene analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "venue": {
                "fragments": [],
                "text": "A Wiley-Interscience publication"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1920907"
                        ],
                        "name": "W. Krauth",
                        "slug": "W.-Krauth",
                        "structuredName": {
                            "firstName": "Werner",
                            "lastName": "Krauth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Krauth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2338183"
                        ],
                        "name": "M. M\u00e9zard",
                        "slug": "M.-M\u00e9zard",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "M\u00e9zard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. M\u00e9zard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121866666,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4402eb69536144313321f446802b0971b07ff589",
            "isKey": false,
            "numCitedBy": 395,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "To ensure large basins of attraction in spin-glass-like neural networks of two-state elements xi imu =+or-1. The authors propose to study learning rules with optimal stability Delta , where delta is the largest number satisfying Delta <or=( Sigma j Jij xi jmu ) xi imu ; mu =1. . . . .p: i=1. . . . .N (where N is the number of neurons and p is the number of patterns). They motivate this proposal and provide optimal stability learning rules for two different choices of normalisation for the synaptic matrix (Jij). In addition, numerical work is presented which gives the value of the optimal stability for random uncorrelated patterns."
            },
            "slug": "Learning-algorithms-with-optimal-stability-in-Krauth-M\u00e9zard",
            "title": {
                "fragments": [],
                "text": "Learning algorithms with optimal stability in neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The authors motivate this proposal and provide optimal stability learning rules for two different choices of normalisation for the synaptic matrix (Jij) and numerical work is presented which gives the value of the optimal stability for random uncorrelated patterns."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16729445"
                        ],
                        "name": "R. Courant",
                        "slug": "R.-Courant",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Courant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Courant"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6234493"
                        ],
                        "name": "D. Hilbert",
                        "slug": "D.-Hilbert",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hilbert",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hilbert"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120525241,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "2317eb16a0997d6557034358969d63022b0b4f1d",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Partial table of contents: THE ALGEBRA OF LINEAR TRANSFORMATIONS AND QUADRATIC FORMS. Transformation to Principal Axes of Quadratic and Hermitian Forms. Minimum-Maximum Property of Eigenvalues. SERIES EXPANSION OF ARBITRARY FUNCTIONS. Orthogonal Systems of Functions. Measure of Independence and Dimension Number. Fourier Series. Legendre Polynomials. LINEAR INTEGRAL EQUATIONS. The Expansion Theorem and Its Applications. Neumann Series and the Reciprocal Kernel. The Fredholm Formulas. THE CALCULUS OF VARIATIONS. Direct Solutions. The Euler Equations. VIBRATION AND EIGENVALUE PROBLEMS. Systems of a Finite Number of Degrees of Freedom. The Vibrating String. The Vibrating Membrane. Green's Function (Influence Function) and Reduction of Differential Equations to Integral Equations. APPLICATION OF THE CALCULUS OF VARIATIONS TO EIGENVALUE PROBLEMS. Completeness and Expansion Theorems. Nodes of Eigenfunctions. SPECIAL FUNCTIONS DEFINED BY EIGENVALUE PROBLEMS. Bessel Functions. Asymptotic Expansions. Additional Bibliography. Index."
            },
            "slug": "Methods-of-Mathematical-Physics-Courant-Hilbert",
            "title": {
                "fragments": [],
                "text": "Methods of Mathematical Physics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1947
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2545803"
                        ],
                        "name": "M. Aizerman",
                        "slug": "M.-Aizerman",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Aizerman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Aizerman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60493317,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3caf34c1c86633b6e80dca29e3cb2b6367a0f93",
            "isKey": false,
            "numCitedBy": 1692,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theoretical-Foundations-of-the-Potential-Function-Aizerman",
            "title": {
                "fragments": [],
                "text": "Theoretical Foundations of the Potential Function Method in Pattern Recognition Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1390055667"
                        ],
                        "name": "Dr. M. G. Worster",
                        "slug": "Dr.-M.-G.-Worster",
                        "structuredName": {
                            "firstName": "Dr.",
                            "lastName": "Worster",
                            "middleNames": [
                                "M.",
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dr. M. G. Worster"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 4079036,
            "fieldsOfStudy": [
                "Education",
                "Physics"
            ],
            "id": "757dd5fbc67d8f2591eb2077180af74c1797fcd1",
            "isKey": false,
            "numCitedBy": 2427,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The following people have maintained these notes."
            },
            "slug": "Methods-of-Mathematical-Physics-Worster",
            "title": {
                "fragments": [],
                "text": "Methods of Mathematical Physics"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1947
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789628"
                        ],
                        "name": "F. Lootsma",
                        "slug": "F.-Lootsma",
                        "structuredName": {
                            "firstName": "Freerk",
                            "lastName": "Lootsma",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Lootsma"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121564690,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "0b79c5974c0a9685b35793ce172caa5a61488633",
            "isKey": false,
            "numCitedBy": 115,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Numerical-methods-for-non-linear-optimization-Lootsma",
            "title": {
                "fragments": [],
                "text": "Numerical methods for non-linear optimization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 222394623,
            "fieldsOfStudy": [],
            "id": "eabb2f4d4b9f767475cd5dc37d70c32196545a4c",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Methods of Mathematical Physics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1947
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49637194"
                        ],
                        "name": "A. Mullin",
                        "slug": "A.-Mullin",
                        "structuredName": {
                            "firstName": "Albert",
                            "lastName": "Mullin",
                            "middleNames": [
                                "A."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mullin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51083130"
                        ],
                        "name": "F. Rosenblatt",
                        "slug": "F.-Rosenblatt",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Rosenblatt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Rosenblatt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61566132,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cccc0a4817fd5f6d8758c66b4065a23897d49f1d",
            "isKey": false,
            "numCitedBy": 2372,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Principles-of-neurodynamics-Mullin-Rosenblatt",
            "title": {
                "fragments": [],
                "text": "Principles of neurodynamics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144681785"
                        ],
                        "name": "S. Vajda",
                        "slug": "S.-Vajda",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Vajda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vajda"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61914400,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0221dc67db0acc1d9495f896a3f4e4fb151abf7d",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Numerical-Methods-for-Non-Linear-Optimization-Vajda",
            "title": {
                "fragments": [],
                "text": "Numerical Methods for Non-Linear Optimization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "98045118"
                        ],
                        "name": "J. Darzentas",
                        "slug": "J.-Darzentas",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Darzentas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Darzentas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60609764,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "29f8b7112fb4660a64df8ac34a5d85c119019efe",
            "isKey": false,
            "numCitedBy": 2171,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Problem-Complexity-and-Method-Efficiency-in-Darzentas",
            "title": {
                "fragments": [],
                "text": "Problem Complexity and Method Efficiency in Optimization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145768864"
                        ],
                        "name": "D. Broomhead",
                        "slug": "D.-Broomhead",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Broomhead",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Broomhead"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144159852"
                        ],
                        "name": "D. Lowe",
                        "slug": "D.-Lowe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lowe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lowe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3686496,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "b08ba914037af6d88d16e2657a65cd9dc5cf5da1",
            "isKey": false,
            "numCitedBy": 2307,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multivariable-Functional-Interpolation-and-Adaptive-Broomhead-Lowe",
            "title": {
                "fragments": [],
                "text": "Multivariable Functional Interpolation and Adaptive Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Handwritten digit recognition with a b a c k-propagation network"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Information Processing Systems"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalization, weight decay, a n d architecture selection for nonlinear learning systems"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Information Processing Systems"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Joint Conference on Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Chervonenkis. The theory of pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Chervonenkis. The theory of pattern recognition"
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Chervonenkis. The theory of pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Chervonenkis. The theory of pattern recognition"
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "probabilities in layered networks : Predictions and generalization"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the International Joint Conference on Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multivariate functional interpolation and adaptive n e t works"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Systems"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural Compuiaiaon"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Compuiaiaon"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalization, weight decay, and architecture selection for nonlinear learning systems"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Information Processing Systems"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural Infor- mation Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Infor- mation Processing Systems"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalization, weight decay, and architecture selection for nonlinear learning systems"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Information Processing Systems"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On optimal nonlinear associativerecall"
            },
            "venue": {
                "fragments": [],
                "text": "Biol . Cybernetics"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bumptrees for ecient function, constraint and classiication learning"
            },
            "venue": {
                "fragments": [],
                "text": "Bumptrees for ecient function, constraint and classiication learning"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bumptrees for eecient function, constraint and classiication learning"
            },
            "venue": {
                "fragments": [],
                "text": "Bumptrees for eecient function, constraint and classiication learning"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "J. Phys. A: Math. gen"
            },
            "venue": {
                "fragments": [],
                "text": "J. Phys. A: Math. gen"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "IEEE"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Princzp!es of neurodynamics"
            },
            "venue": {
                "fragments": [],
                "text": "Spartan Books, New York,"
            },
            "year": 1962
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {},
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 41,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/A-training-algorithm-for-optimal-margin-classifiers-Boser-Guyon/2599131a4bc2fa957338732a37c744cfe3e17b24?sort=total-citations"
}