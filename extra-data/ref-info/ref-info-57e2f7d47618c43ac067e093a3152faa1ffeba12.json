{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065925658"
                        ],
                        "name": "Ian Chai",
                        "slug": "Ian-Chai",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Chai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian Chai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144964966"
                        ],
                        "name": "D. Dori",
                        "slug": "D.-Dori",
                        "structuredName": {
                            "firstName": "Dov",
                            "lastName": "Dori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Dori"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 76651473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8ce400487dab0d82ef62bfddd713f8fc68bba187",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Extracting bars (straight line segments) from a binary image is a first processing step in a system for understanding engineering drawings as well as other applications such as robotics [1]. A novel algorithm \u2014 the Orthogonal Zig-Zag (OZZ) has been developed and implemented. The underlying idea of OZZ is selective processing, or focus of attention. The binary input image is scanned intermittently and rather sparsely, but nevertheless all the necessary information needed to detect bars is extracted. Black pixels constitute a small fraction of the input drawing. Of these pixels, OZZ processes only a small fraction, resulting in both a dramatic reduction in both space and time compared to HT."
            },
            "slug": "Orthogonal-zig-zag:-an-efficient-method-for-lines-Chai-Dori",
            "title": {
                "fragments": [],
                "text": "Orthogonal zig-zag: an efficient method for extracting straight lines from engineering drawings"
            },
            "tldr": {
                "abstractSimilarityScore": 35,
                "text": "The Orthogonal Zig-Zag (OZZ) algorithm has been developed and implemented, resulting in both a dramatic reduction in both space and time compared to HT."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145820949"
                        ],
                        "name": "T. Pavlidis",
                        "slug": "T.-Pavlidis",
                        "structuredName": {
                            "firstName": "Theodosios",
                            "lastName": "Pavlidis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Pavlidis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Pavlidis [ 9 ] proposed a vectorization method that works on compressed line adjacency graphs (c-LAGs) derived from run length images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 120068581,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6839013b5dd6bcc30d6aeef615c5f034103af090",
            "isKey": false,
            "numCitedBy": 169,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-vectorizer-and-feature-extractor-for-document-Pavlidis",
            "title": {
                "fragments": [],
                "text": "A vectorizer and feature extractor for document recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1105637,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "2415fd60305739543105118739f7118493257af3",
            "isKey": false,
            "numCitedBy": 6426,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Hough has proposed an interesting and computationally efficient procedure for detecting lines in pictures. This paper points out that the use of angle-radius rather than slope-intercept parameters simplifies the computation further. It also shows how the method can be used for more general curve fitting, and gives alternative interpretations that explain the source of its efficiency."
            },
            "slug": "Use-of-the-Hough-transformation-to-detect-lines-and-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Use of the Hough transformation to detect lines and curves in pictures"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is pointed out that the use of angle-radius rather than slope-intercept parameters simplifies the computation further, and how the method can be used for more general curve fitting."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144905110"
                        ],
                        "name": "L. Lam",
                        "slug": "L.-Lam",
                        "structuredName": {
                            "firstName": "Louisa",
                            "lastName": "Lam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Lam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50112753"
                        ],
                        "name": "Seong-Whan Lee",
                        "slug": "Seong-Whan-Lee",
                        "structuredName": {
                            "firstName": "Seong-Whan",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seong-Whan Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713795"
                        ],
                        "name": "C. Suen",
                        "slug": "C.-Suen",
                        "structuredName": {
                            "firstName": "Ching",
                            "lastName": "Suen",
                            "middleNames": [
                                "Yee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Suen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Traditional straight line detection algorithms, such as vectorization by thinning [ 2 , 3], polygonal approximation [4, 5] and the Hough transform [6, 7, 8], do not work well on such distorted images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6746689,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3b8b8783155dba8ddec04cdb68204eae88fdb2a6",
            "isKey": false,
            "numCitedBy": 1846,
            "numCiting": 177,
            "paperAbstract": {
                "fragments": [],
                "text": "A comprehensive survey of thinning methodologies is presented. A wide range of thinning algorithms, including iterative deletion of pixels and nonpixel-based methods, is covered. Skeletonization algorithms based on medial axis and other distance transforms are not considered. An overview of the iterative thinning process and the pixel-deletion criteria needed to preserve the connectivity of the image pattern is given first. Thinning algorithms are then considered in terms of these criteria and their modes of operation. Nonpixel-based methods that usually produce a center line of the pattern directly in one pass without examining all the individual pixels are discussed. The algorithms are considered in great detail and scope, and the relationships among them are explored. >"
            },
            "slug": "Thinning-Methodologies-A-Comprehensive-Survey-Lam-Lee",
            "title": {
                "fragments": [],
                "text": "Thinning Methodologies - A Comprehensive Survey"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A comprehensive survey of thinning methodologies, including iterative deletion of pixels and nonpixel-based methods, is presented and the relationships among them are explored."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1835956"
                        ],
                        "name": "E. Turolla",
                        "slug": "E.-Turolla",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Turolla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Turolla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2448729"
                        ],
                        "name": "Y. Bela\u00efd",
                        "slug": "Y.-Bela\u00efd",
                        "structuredName": {
                            "firstName": "Yolande",
                            "lastName": "Bela\u00efd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Bela\u00efd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2128453"
                        ],
                        "name": "A. Bela\u00efd",
                        "slug": "A.-Bela\u00efd",
                        "structuredName": {
                            "firstName": "Abdel",
                            "lastName": "Bela\u00efd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bela\u00efd"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12914079,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "9e6e39892aa2e15fabf0b850645c0edb4211fd91",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an item searching method which has been applied to various kinds of forms. This approach is based on line detection through the Hough transform. After obtaining the straight lines, Hough directions are used to detect the real segments in the image. Segments can correspond either to continuous line, or to black parts of dashed or dotted lines. So, the segments are grouped together and classified between both adjacent line crossing points. Items are located by searching the minimum cycles of the graph constructed from the line intersection points. The last step consists of verifying the line classes based on the homogeneity hypothesis of item sides."
            },
            "slug": "Form-Item-Extraction-Based-on-Line-Searching-Turolla-Bela\u00efd",
            "title": {
                "fragments": [],
                "text": "Form Item Extraction Based on Line Searching"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "This paper presents an item searching method which has been applied to various kinds of forms based on line detection through the Hough transform, and verifying the line classes based on the homogeneity hypothesis of item sides."
            },
            "venue": {
                "fragments": [],
                "text": "GREC"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2222618"
                        ],
                        "name": "A. Filipski",
                        "slug": "A.-Filipski",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Filipski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Filipski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "67169730"
                        ],
                        "name": "R. Flandrena",
                        "slug": "R.-Flandrena",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Flandrena",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Flandrena"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Traditional straight line detection algorithms, such as vectorization by thinning [2,  3 ], polygonal approximation [4, 5] and the Hough transform [6, 7, 8], do not work well on such distorted images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 58849163,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "81a389eeab9c0c3cae19f70637eaea05258c8b2c",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present a discussion of the different aspects and requirements of the CAD conversion problem and describe the general architecture and algorithms of one commercially available CAD conversion system, the GTX 5000. Scanning, vectorization, text recognition, symbol recognition, context processing, and cleanup editing subsystems of the GTX 5000 are described. Several possible alternative approaches and algorithms are also compared. EPRI (Electric Power Research Institute)-funded enhancements are discussed, including neural networks for character and symbol recognition, touching and broken character processing, and text/symbol associativity. >"
            },
            "slug": "Automated-conversion-of-engineering-drawings-to-CAD-Filipski-Flandrena",
            "title": {
                "fragments": [],
                "text": "Automated conversion of engineering drawings to CAD form"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The authors present a discussion of the different aspects and requirements of the CAD conversion problem and describe the general architecture and algorithms of one commercially available CAD conversion system, the GTX 5000."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145700931"
                        ],
                        "name": "David H. Douglas",
                        "slug": "David-H.-Douglas",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Douglas",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David H. Douglas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2531388"
                        ],
                        "name": "T. Peucker",
                        "slug": "T.-Peucker",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Peucker",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Peucker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Traditional straight line detection algorithms, such as vectorization by thinning [2, 3], polygonal approximation [4,  5 ] and the Hough transform [6, 7, 8], do not work well on such distorted images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 119823700,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e46ac802d7207e0e51b5333456a3f46519c2f92d",
            "isKey": false,
            "numCitedBy": 3705,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "All digitizing methods, as a general rule, record lines with far more data than is necessary for accurate graphic reproduction or for computer analysis. Two algorithms to reduce the number of points required to represent the line and, if desired, produce caricatures, are presented and compared with the most promising methods so far suggested. Line reduction will form a major part of automated generalization. Regle generale, les methodes numeriques enregistrent des lignes avec beaucoup plus de donnees qu'il n'est necessaire a la reproduction graphique precise ou a la recherche par ordinateur. L'auteur presente deux algorithmes pour reduire le nombre de points necessaires pour representer la ligne et produire des caricatures si desire, et les compare aux methodes les plus prometteuses suggerees jusqu'ici. La reduction de la ligne constituera une partie importante de la generalisation automatique."
            },
            "slug": "ALGORITHMS-FOR-THE-REDUCTION-OF-THE-NUMBER-OF-TO-A-Douglas-Peucker",
            "title": {
                "fragments": [],
                "text": "ALGORITHMS FOR THE REDUCTION OF THE NUMBER OF POINTS REQUIRED TO REPRESENT A DIGITIZED LINE OR ITS CARICATURE"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16742366"
                        ],
                        "name": "Urs Ramer",
                        "slug": "Urs-Ramer",
                        "structuredName": {
                            "firstName": "Urs",
                            "lastName": "Ramer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Urs Ramer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Traditional straight line detection algorithms, such as vectorization by thinning [2, 3], polygonal approximation [ 4 , 5] and the Hough transform [6, 7, 8], do not work well on such distorted images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1213794,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "bcd48475fed672a14fb7c308bb15b4c3220d8b57",
            "isKey": false,
            "numCitedBy": 1254,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-iterative-procedure-for-the-polygonal-of-plane-Ramer",
            "title": {
                "fragments": [],
                "text": "An iterative procedure for the polygonal approximation of plane curves"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Graph. Image Process."
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055506722"
                        ],
                        "name": "D. Janssen",
                        "slug": "D.-Janssen",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Janssen",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Janssen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58839577,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e7162d87229d2b29f1ceb5aa0baaf70b130453e4",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-application-of-model-based-image-processing-to-Janssen",
            "title": {
                "fragments": [],
                "text": "The application of model-based image processing to the interpretation of maps"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "methodology": 5
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 9,
        "totalPages": 1
    },
    "page_url": "https://www.semanticscholar.org/paper/Detection-of-Horizontal-Lines-in-Noisy-Run-Length-Chhabra-Misra/57e2f7d47618c43ac067e093a3152faa1ffeba12?sort=total-citations"
}