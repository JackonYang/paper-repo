{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34317896"
                        ],
                        "name": "Minesh Mathew",
                        "slug": "Minesh-Mathew",
                        "structuredName": {
                            "firstName": "Minesh",
                            "lastName": "Mathew",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minesh Mathew"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694974"
                        ],
                        "name": "Dimosthenis Karatzas",
                        "slug": "Dimosthenis-Karatzas",
                        "structuredName": {
                            "firstName": "Dimosthenis",
                            "lastName": "Karatzas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dimosthenis Karatzas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398834003"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694502"
                        ],
                        "name": "C. Jawahar",
                        "slug": "C.-Jawahar",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Jawahar",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jawahar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 4
                            }
                        ],
                        "text": "The DocVQA was used as the most representative and challenging for Document Intelligence since its leaderboard reveals a large gap to human performance."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 4
                            }
                        ],
                        "text": "The DocVQA dataset [38] is focused on the visual question answering task."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 71
                            }
                        ],
                        "text": "The problem occurs more frequently in formated documents (FUNSD, CORD, DocVQA), where the casing is an important visual aspect."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 80
                            }
                        ],
                        "text": "Consequently, we were able to achieve state-of-the-art results on two datasets (DocVQA, CORD) and performed on par with the previous best scores on SROIE and RVL-CDIP, albeit having a much simpler workflow."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "9 \u2014 DocVQA [39] industry documents + 12."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "The DocVQA dataset [39] is focused on the visual question answering task."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "DocVQA."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 79
                            }
                        ],
                        "text": "Although encoder-decoder models are commonly applied to generative tasks, both DocVQA, SROIE, and CORD we considered are extractive."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 4
                            }
                        ],
                        "text": "For DocVQA, we relied on Amazon Textract OCR; for RVL-CDIP, we used Microsoft Azure OCR, for SROIE and CORD, we depended on the original OCR."
                    },
                    "intents": []
                }
            ],
            "corpusId": 220280200,
            "fieldsOfStudy": [
                "Computer Science",
                "Environmental Science"
            ],
            "id": "b40bfcf339de3f0dba08fabb2b58b9368ff4c51a",
            "isKey": true,
            "numCitedBy": 42,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new dataset for Visual Question Answering (VQA) on document images called DocVQA. The dataset consists of 50,000 questions defined on 12,000+ document images. Detailed analysis of the dataset in comparison with similar datasets for VQA and reading comprehension is presented. We report several baseline results by adopting existing VQA and reading comprehension models. Although the existing models perform reasonably well on certain types of questions, there is large performance gap compared to human performance (94.36% accuracy). The models need to improve specifically on questions where understanding structure of the document is crucial. The dataset, code and leaderboard are available at docvqa.org"
            },
            "slug": "DocVQA:-A-Dataset-for-VQA-on-Document-Images-Mathew-Karatzas",
            "title": {
                "fragments": [],
                "text": "DocVQA: A Dataset for VQA on Document Images"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Although the existing models perform reasonably well on certain types of questions, there is large performance gap compared to human performance (94.36% accuracy)."
            },
            "venue": {
                "fragments": [],
                "text": "2021 IEEE Winter Conference on Applications of Computer Vision (WACV)"
            },
            "year": 2021
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2402716"
                        ],
                        "name": "Colin Raffel",
                        "slug": "Colin-Raffel",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Raffel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Colin Raffel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846258"
                        ],
                        "name": "Noam M. Shazeer",
                        "slug": "Noam-M.-Shazeer",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Shazeer",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noam M. Shazeer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145625142"
                        ],
                        "name": "Adam Roberts",
                        "slug": "Adam-Roberts",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Roberts",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Roberts"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3844009"
                        ],
                        "name": "Katherine Lee",
                        "slug": "Katherine-Lee",
                        "structuredName": {
                            "firstName": "Katherine",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katherine Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46617804"
                        ],
                        "name": "Sharan Narang",
                        "slug": "Sharan-Narang",
                        "structuredName": {
                            "firstName": "Sharan",
                            "lastName": "Narang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sharan Narang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1380243217"
                        ],
                        "name": "Michael Matena",
                        "slug": "Michael-Matena",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Matena",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Matena"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2389316"
                        ],
                        "name": "Yanqi Zhou",
                        "slug": "Yanqi-Zhou",
                        "structuredName": {
                            "firstName": "Yanqi",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yanqi Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157338362"
                        ],
                        "name": "Wei Li",
                        "slug": "Wei-Li",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35025299"
                        ],
                        "name": "Peter J. Liu",
                        "slug": "Peter-J.-Liu",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Liu",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter J. Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 57
                            }
                        ],
                        "text": ", named entities are preferred rather than random tokens [44, 12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 248
                            }
                        ],
                        "text": "To address this issue, we focus on the applicability of the encoder-decoder architecture since it can generate values not included in the input text explicitly [16] and performs reasonably well on all text-based problems involving natural language [44]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 242,
                                "start": 238
                            }
                        ],
                        "text": "Since authors of the T5 argued that pretraining on a mixture of unsupervised and supervised tasks perform equally good with higher parameter count, this gap may vanish with larger variants of TILT we did not consider in the present paper [44]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 118
                            }
                        ],
                        "text": "The T5 is a prominent example of large-scale Transformers achieving state-of-the-art results on varied NLP benchmarks [44]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 65
                            }
                        ],
                        "text": "Authors of the T5 architecture disregarded positional embeddings [44], by setting X = S."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 154
                            }
                        ],
                        "text": "Although this does not necessarily lead to the use of encoder-decoder models, several successful solutions relied on variants of Transformer architecture [54, 34, 9, 44]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 204838007,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3cfb319689f06bf04c2e28399361f414ca32c4b3",
            "isKey": true,
            "numCitedBy": 3764,
            "numCiting": 139,
            "paperAbstract": {
                "fragments": [],
                "text": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."
            },
            "slug": "Exploring-the-Limits-of-Transfer-Learning-with-a-Raffel-Shazeer",
            "title": {
                "fragments": [],
                "text": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks and achieves state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40348417"
                        ],
                        "name": "Ashish Vaswani",
                        "slug": "Ashish-Vaswani",
                        "structuredName": {
                            "firstName": "Ashish",
                            "lastName": "Vaswani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashish Vaswani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846258"
                        ],
                        "name": "Noam M. Shazeer",
                        "slug": "Noam-M.-Shazeer",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Shazeer",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noam M. Shazeer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3877127"
                        ],
                        "name": "Niki Parmar",
                        "slug": "Niki-Parmar",
                        "structuredName": {
                            "firstName": "Niki",
                            "lastName": "Parmar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Niki Parmar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39328010"
                        ],
                        "name": "Jakob Uszkoreit",
                        "slug": "Jakob-Uszkoreit",
                        "structuredName": {
                            "firstName": "Jakob",
                            "lastName": "Uszkoreit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jakob Uszkoreit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145024664"
                        ],
                        "name": "Llion Jones",
                        "slug": "Llion-Jones",
                        "structuredName": {
                            "firstName": "Llion",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Llion Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "19177000"
                        ],
                        "name": "Aidan N. Gomez",
                        "slug": "Aidan-N.-Gomez",
                        "structuredName": {
                            "firstName": "Aidan",
                            "lastName": "Gomez",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aidan N. Gomez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40527594"
                        ],
                        "name": "Lukasz Kaiser",
                        "slug": "Lukasz-Kaiser",
                        "structuredName": {
                            "firstName": "Lukasz",
                            "lastName": "Kaiser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lukasz Kaiser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3443442"
                        ],
                        "name": "Illia Polosukhin",
                        "slug": "Illia-Polosukhin",
                        "structuredName": {
                            "firstName": "Illia",
                            "lastName": "Polosukhin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Illia Polosukhin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 121
                            }
                        ],
                        "text": "In the sequence labeling scenario, each document leads to multiple training instances (token classification), whereas in Transformer sequence-to-sequence models, the same document results in one training instance with feature space of higher dimension (decoding from multiple tokens)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 109
                            }
                        ],
                        "text": "Keywords: Natural Language Processing \u00b7 Transfer learning \u00b7 Document understanding \u00b7 Layout analysis \u00b7 Deep learning \u00b7 Transformer."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 154
                            }
                        ],
                        "text": "Although this does not necessarily lead to the use of encoder-decoder models, several successful solutions relied on variants of Transformer architecture [54, 35, 9, 45]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "We restrict ourselves to approaches rooted in the architecture of Transformer [54] and focus on the inclusion of spatial information or different modalities in text-processing systems, as well as on the applicability of encoder-decoder models to Information Extraction and Question Answering."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 45
                            }
                        ],
                        "text": "In order to inject visual information to the Transformer, a matrix of contextualized image-region embeddings U is added to semantic embeddings, i.e. we define\nX = S + U\nin line with the convention from Section 3 (see Figure 3)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 74
                            }
                        ],
                        "text": "Let us begin from the general view on attention in the first layer of the Transformer."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 45
                            }
                        ],
                        "text": "The T5 is a prominent example of large-scale Transformers achieving state-of-the-art results on varied NLP benchmarks [44]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 11
                            }
                        ],
                        "text": "Multimodal Transformers."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 129
                            }
                        ],
                        "text": "Although this does not necessarily lead to the use of encoder-decoder models, several successful solutions relied on variants of Transformer architecture [54, 34, 9, 44]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 121
                            }
                        ],
                        "text": "where S and P are respectively the semantic embeddings of tokens and positional embedding resulting from their positions [54]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 14
                            }
                        ],
                        "text": "Spatial-aware Transformers."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 38
                            }
                        ],
                        "text": "There is no B term in the\n(A) Vanilla Transformer (B) T5 Architecture\noriginal Transformer, and information about the order of tokens is provided explicitly to the model, that is:\nX = S + P B = 0n\u00d7d\nwhere S and P are respectively the semantic embeddings of tokens and positional embedding resulting from their positions [54]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 202
                            }
                        ],
                        "text": "Our starting point is the architecture of the Transformer, initially proposed for Neural Machine Translation, which has proven to be a solid baseline for all generative tasks involving natural language [54]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 36
                            }
                        ],
                        "text": "Spatial and image enrichment of the Transformer model allowed the TILT to combine information from text, layout, and image modalities."
                    },
                    "intents": []
                }
            ],
            "corpusId": 13756489,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "isKey": true,
            "numCitedBy": 35148,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
            },
            "slug": "Attention-is-All-you-Need-Vaswani-Shazeer",
            "title": {
                "fragments": [],
                "text": "Attention is All you Need"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely is proposed, which generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34939798"
                        ],
                        "name": "Adam W. Harley",
                        "slug": "Adam-W.-Harley",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Harley",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam W. Harley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2079687415"
                        ],
                        "name": "Alex Ufkes",
                        "slug": "Alex-Ufkes",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Ufkes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Ufkes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3150825"
                        ],
                        "name": "K. Derpanis",
                        "slug": "K.-Derpanis",
                        "structuredName": {
                            "firstName": "Konstantinos",
                            "lastName": "Derpanis",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Derpanis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "0 RVL-CDIP [14] industry documents + 400."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 67
                            }
                        ],
                        "text": "We constructed a corpus of documents with rich structure, based on RVL-CDIP (275k docs), UCSF Industry Documents Library\n(480k),\u2217 and PDF files from Common Crawl (350k)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 158
                            }
                        ],
                        "text": "Consequently, we were able to achieve state-of-the-art results on two datasets (DocVQA, CORD) and performed on par with the previous best scores on SROIE and RVL-CDIP, albeit having a much simpler workflow."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 50
                            }
                        ],
                        "text": "For DocVQA, we relied on Amazon Textract OCR; for RVL-CDIP, we used Microsoft Azure OCR, for SROIE and CORD, we depended on the original OCR."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "RVL-CDIP."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "The RVL-CDIP dataset [14] contains gray-scale images and assumes classification into 16 categories such as letter, form, invoice, news article, and scientific publication."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2760893,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd86b4b551b9d3fb498f62008b037e7599365018",
            "isKey": true,
            "numCitedBy": 174,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new state-of-the-art for document image classification and retrieval, using features learned by deep convolutional neural networks (CNNs). In object and scene analysis, deep neural nets are capable of learning a hierarchical chain of abstraction from pixel inputs to concise and descriptive representations. The current work explores this capacity in the realm of document analysis, and confirms that this representation strategy is superior to a variety of popular handcrafted alternatives. Extensive experiments show that (i) features extracted from CNNs are robust to compression, (ii) CNNs trained on non-document images transfer well to document analysis tasks, and (iii) enforcing region-specific feature-learning is unnecessary given sufficient training data. This work also makes available a new labelled subset of the IIT-CDIP collection, containing 400,000 document images across 16 categories."
            },
            "slug": "Evaluation-of-deep-convolutional-nets-for-document-Harley-Ufkes",
            "title": {
                "fragments": [],
                "text": "Evaluation of deep convolutional nets for document image classification and retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A new state-of-the-art for document image classification and retrieval, using features learned by deep convolutional neural networks (CNNs), and makes available a new labelled subset of the IIT-CDIP collection, containing 400,000 document images across 16 categories."
            },
            "venue": {
                "fragments": [],
                "text": "2015 13th International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1390799037"
                        ],
                        "name": "Zheng Huang",
                        "slug": "Zheng-Huang",
                        "structuredName": {
                            "firstName": "Zheng",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zheng Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153819461"
                        ],
                        "name": "Kai Chen",
                        "slug": "Kai-Chen",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "73730984"
                        ],
                        "name": "Jianhua He",
                        "slug": "Jianhua-He",
                        "structuredName": {
                            "firstName": "Jianhua",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianhua He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145905113"
                        ],
                        "name": "X. Bai",
                        "slug": "X.-Bai",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Bai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694974"
                        ],
                        "name": "Dimosthenis Karatzas",
                        "slug": "Dimosthenis-Karatzas",
                        "structuredName": {
                            "firstName": "Dimosthenis",
                            "lastName": "Karatzas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dimosthenis Karatzas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771189"
                        ],
                        "name": "Shijian Lu",
                        "slug": "Shijian-Lu",
                        "structuredName": {
                            "firstName": "Shijian",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shijian Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694502"
                        ],
                        "name": "C. Jawahar",
                        "slug": "C.-Jawahar",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Jawahar",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jawahar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 138
                            }
                        ],
                        "text": "Limitations of this approach are strikingly visible on tasks framed in either key information extraction or property extraction paradigms [20, 9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "The main goal of the SROIE dataset [20] is to extract values for four categories (company, date, address, total) from scanned receipts."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 79
                            }
                        ],
                        "text": "Take, for example, the total amount assigned to a receipt in the SROIE dataset [20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 211026630,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d00cbb0c05c1dc922126fe72c1078b773d01c688",
            "isKey": true,
            "numCitedBy": 51,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The ICDAR 2019 Challenge on \"Scanned receipts OCR and key information extraction\" (SROIE) covers important aspects related to the automated analysis of scanned receipts. The SROIE tasks play a key role in many document analysis systems and hold significant commercial potential. Although a lot of work has been published over the years on administrative document analysis, the community has advanced relatively slowly, as most datasets have been kept private. One of the key contributions of SROIE to the document analysis community is to offer a first, standardized dataset of 1000 whole scanned receipt images and annotations, as well as an evaluation procedure for such tasks. The Challenge is structured around three tasks, namely Scanned Receipt Text Localization (Task 1), Scanned Receipt OCR (Task 2) and Key Information Extraction from Scanned Receipts (Task 3). The competition opened on 10th February, 2019 and closed on 5th May, 2019. We received 29, 24 and 18 valid submissions received for the three competition tasks, respectively. This report presents the competition datasets, define the tasks and the evaluation protocols, offer detailed submission statistics, as well as an analysis of the submitted performance. While the tasks of text localization and recognition seem to be relatively easy to tackle, it is interesting to observe the variety of ideas and approaches proposed for the information extraction task. According to the submissions' performance we believe there is still margin for improving information extraction performance, although the current dataset would have to grow substantially in following editions. Given the success of the SROIE competition evidenced by the wide interest generated and the healthy number of submissions from academic, research institutes and industry over different countries, we consider that the SROIE competition can evolve into a useful resource for the community, drawing further attention and promoting research and development efforts in this field."
            },
            "slug": "ICDAR2019-Competition-on-Scanned-Receipt-OCR-and-Huang-Chen",
            "title": {
                "fragments": [],
                "text": "ICDAR2019 Competition on Scanned Receipt OCR and Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The ICDAR 2019 Challenge on \"Scanned receipts OCR and key information extraction\" (SROIE) covers important aspects related to the automated analysis of scanned receipts, and is considered to evolve into a useful resource for the community, drawing further attention and promoting research and development efforts in this field."
            },
            "venue": {
                "fragments": [],
                "text": "2019 International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2844898"
                        ],
                        "name": "N. Keskar",
                        "slug": "N.-Keskar",
                        "structuredName": {
                            "firstName": "Nitish",
                            "lastName": "Keskar",
                            "middleNames": [
                                "Shirish"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Keskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143775536"
                        ],
                        "name": "Bryan McCann",
                        "slug": "Bryan-McCann",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "McCann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan McCann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2228109"
                        ],
                        "name": "Caiming Xiong",
                        "slug": "Caiming-Xiong",
                        "structuredName": {
                            "firstName": "Caiming",
                            "lastName": "Xiong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Caiming Xiong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 64
                            }
                        ],
                        "text": "It can handle various tasks such as Key Information Extraction, Question Answering or Document Classification, while the need for complicated preprocessing and postprocessing steps is eliminated."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 152
                            }
                        ],
                        "text": "The relationships between text and other media have been previously studied in Visual Commonsense Reasoning, VideoGrounded Dialogue, Speech, and Visual Question Answering [13, 32, 3]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 273
                            }
                        ],
                        "text": "We restrict ourselves to approaches rooted in the architecture of Transformer [54] and focus on the inclusion of spatial information or different modalities in text-processing systems, as well as on the applicability of encoder-decoder models to Information Extraction and Question Answering."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 152
                            }
                        ],
                        "text": "Most Natural Language Processing tasks can be unified under one framework by casting them as Language Modeling, Sequence Labeling or Question Answering [46, 28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 109
                            }
                        ],
                        "text": "Most NLP tasks can be unified under one framework by casting them as Language Modeling, Sequence Labeling or Question Answering [43, 25]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 94
                            }
                        ],
                        "text": "Our model was validated on series of experiments involving Key Information Extraction, Visual Question Answering, classification of rich documents, and Question Answering from layout-rich texts."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 137
                            }
                        ],
                        "text": "Furthermore, this approach potentially solves all identified problems of sequence labeling architectures and ties various tasks, such as Question Answering or Text Classification, into the same framework."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 89
                            }
                        ],
                        "text": "We consider such unification of Document Classification, Key Information Extraction, and Question Answering in a demanding scenario where context extends beyond the text layer."
                    },
                    "intents": []
                }
            ],
            "corpusId": 125952287,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e10e2cae05b2906330eb7dde2f27042966413b1",
            "isKey": true,
            "numCitedBy": 16,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Even as pre-trained language encoders such as BERT are shared across many tasks, the output layers of question answering and text classification models are significantly different. Span decoders are frequently used for question answering and fixed-class, classification layers for text classification. We show that this distinction is not necessary, and that both can be unified as span extraction. A unified, span-extraction approach leads to superior or comparable performance in multi-task learning, low-data and supplementary supervised pretraining experiments on several text classification and question answering benchmarks."
            },
            "slug": "Unifying-Question-Answering-and-Text-Classification-Keskar-McCann",
            "title": {
                "fragments": [],
                "text": "Unifying Question Answering and Text Classification via Span Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A unified, span-extraction approach leads to superior or comparable performance in multi-task learning, low-data and supplementary supervised pretraining experiments on several text classification and question answering benchmarks."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38909097"
                        ],
                        "name": "Alec Radford",
                        "slug": "Alec-Radford",
                        "structuredName": {
                            "firstName": "Alec",
                            "lastName": "Radford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alec Radford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49387725"
                        ],
                        "name": "Jeff Wu",
                        "slug": "Jeff-Wu",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48422824"
                        ],
                        "name": "Rewon Child",
                        "slug": "Rewon-Child",
                        "structuredName": {
                            "firstName": "Rewon",
                            "lastName": "Child",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rewon Child"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150970919"
                        ],
                        "name": "D. Luan",
                        "slug": "D.-Luan",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Luan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Luan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2698777"
                        ],
                        "name": "Dario Amodei",
                        "slug": "Dario-Amodei",
                        "structuredName": {
                            "firstName": "Dario",
                            "lastName": "Amodei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dario Amodei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 64
                            }
                        ],
                        "text": "It can handle various tasks such as Key Information Extraction, Question Answering or Document Classification, while the need for complicated preprocessing and postprocessing steps is eliminated."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 152
                            }
                        ],
                        "text": "The relationships between text and other media have been previously studied in Visual Commonsense Reasoning, VideoGrounded Dialogue, Speech, and Visual Question Answering [13, 32, 3]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 273
                            }
                        ],
                        "text": "We restrict ourselves to approaches rooted in the architecture of Transformer [54] and focus on the inclusion of spatial information or different modalities in text-processing systems, as well as on the applicability of encoder-decoder models to Information Extraction and Question Answering."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 152
                            }
                        ],
                        "text": "Most Natural Language Processing tasks can be unified under one framework by casting them as Language Modeling, Sequence Labeling or Question Answering [46, 28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 109
                            }
                        ],
                        "text": "Most NLP tasks can be unified under one framework by casting them as Language Modeling, Sequence Labeling or Question Answering [43, 25]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 94
                            }
                        ],
                        "text": "Our model was validated on series of experiments involving Key Information Extraction, Visual Question Answering, classification of rich documents, and Question Answering from layout-rich texts."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 137
                            }
                        ],
                        "text": "Furthermore, this approach potentially solves all identified problems of sequence labeling architectures and ties various tasks, such as Question Answering or Text Classification, into the same framework."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 89
                            }
                        ],
                        "text": "We consider such unification of Document Classification, Key Information Extraction, and Question Answering in a demanding scenario where context extends beyond the text layer."
                    },
                    "intents": []
                }
            ],
            "corpusId": 160025533,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9405cc0d6169988371b2755e573cc28650d14dfe",
            "isKey": true,
            "numCitedBy": 6284,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations."
            },
            "slug": "Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu",
            "title": {
                "fragments": [],
                "text": "Language Models are Unsupervised Multitask Learners"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is demonstrated that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText, suggesting a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "79665491"
                        ],
                        "name": "Lukasz Garncarek",
                        "slug": "Lukasz-Garncarek",
                        "structuredName": {
                            "firstName": "Lukasz",
                            "lastName": "Garncarek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lukasz Garncarek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1498640514"
                        ],
                        "name": "Rafal Powalski",
                        "slug": "Rafal-Powalski",
                        "structuredName": {
                            "firstName": "Rafal",
                            "lastName": "Powalski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rafal Powalski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1822665"
                        ],
                        "name": "Tomasz Stanislawek",
                        "slug": "Tomasz-Stanislawek",
                        "structuredName": {
                            "firstName": "Tomasz",
                            "lastName": "Stanislawek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomasz Stanislawek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11016367"
                        ],
                        "name": "Bartosz Topolski",
                        "slug": "Bartosz-Topolski",
                        "structuredName": {
                            "firstName": "Bartosz",
                            "lastName": "Topolski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bartosz Topolski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "108821285"
                        ],
                        "name": "Piotr Halama",
                        "slug": "Piotr-Halama",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Halama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Halama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064873282"
                        ],
                        "name": "Filip Grali'nski",
                        "slug": "Filip-Grali'nski",
                        "structuredName": {
                            "firstName": "Filip",
                            "lastName": "Grali'nski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Filip Grali'nski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 33
                            }
                        ],
                        "text": "64 TaBERT [59] \u2014 \u2014 \u2014 \u2014 \u2014 LAMBERT [12] 96."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 159
                            }
                        ],
                        "text": "Authors dealing with property extraction rely on either manual annotation or the heuristic-based tagging procedure that impacts the overall end-to-end results [58, 13, 12, 21, 57, 39]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 196
                            }
                        ],
                        "text": "Several authors have shown that, when tasks involving 2D documents are considered, sequential models can be outperformed by considering layout information either directly as positional embeddings [20, 12, 58] or indirectly by allowing them to be contextualized on their spatial neighborhood [7, 59, 18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 65
                            }
                        ],
                        "text": "It is consistent with the previous works on layout understanding [57, 12]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 188
                            }
                        ],
                        "text": "Though we report the number of parameters near the name of the model size variant, note it is impossible to compare the TILT encoder-decoder model to language models such as LayoutLMs and LAMBERT under this criterion."
                    },
                    "intents": []
                }
            ],
            "corpusId": 211171875,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e551546e51c570fe796b67ae23d97297b0717921",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we introduce a novel approach to the problem of understanding documents where the local semantics is influenced by non-trivial layout. Namely, we modify the Transformer architecture in a way that allows it to use the graphical features defined by the layout, without the need to re-learn the language semantics from scratch, thanks to starting the training process from a model pretrained on classical language modeling tasks."
            },
            "slug": "LAMBERT:-Layout-Aware-language-Modeling-using-BERT-Garncarek-Powalski",
            "title": {
                "fragments": [],
                "text": "LAMBERT: Layout-Aware language Modeling using BERT for information extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The Transformer architecture is modified in a way that allows it to use the graphical features defined by the layout, without the need to re-learn the language semantics from scratch, thanks to starting the training process from a model pretrained on classical language modeling tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2020
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 191
                            }
                        ],
                        "text": "Further improvements focused on the training and inference aspects by the inclusion of the area masking loss function or achieving independence from sequential order in decoding respectively [21, 24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 159
                            }
                        ],
                        "text": "Authors dealing with property extraction rely on either manual annotation or the heuristic-based tagging procedure that impacts the overall end-to-end results [58, 13, 12, 21, 57, 39]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 236923196,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aa111c8920e963195968360f59c9de271ae470c2",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Understanding document from their visual snapshots is an emerging and challenging problem that requires both advanced computer vision and NLP methods. Although the recent advance in OCR enables the accurate extraction of text segments, it is still challenging to extract key information from documents due to the diversity of layouts. To compensate for the difficulties, this paper introduces a pre-trained language model, BERT Relying On Spatiality (BROS), that represents and understands the semantics of spatially distributed texts. Different from previous pre-training methods on 1D text, BROS is pre-trained on large-scale semistructured documents with a novel area-masking strategy while efficiently including the spatial layout information of input documents. Also, to generate structured outputs in various document understanding tasks, BROS utilizes a powerful graphbased decoder that can capture the relation between text segments. BROS achieves state-of-the-art results on four benchmark tasks: FUNSD, SROIE*, CORD, and SciTSR. Our experimental settings and implementation codes will be publicly available."
            },
            "slug": "BROS:-A-PRE-TRAINED-LANGUAGE-MODEL",
            "title": {
                "fragments": [],
                "text": "BROS: A PRE-TRAINED LANGUAGE MODEL"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A pre-trained language model, BERT Relying On Spatiality (BROS), that represents and understands the semantics of spatially distributed texts and achieves state-of-the-art results on four benchmark tasks: FUNSD, SROIE*, CORD, and SciTSR."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1822665"
                        ],
                        "name": "Tomasz Stanislawek",
                        "slug": "Tomasz-Stanislawek",
                        "structuredName": {
                            "firstName": "Tomasz",
                            "lastName": "Stanislawek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomasz Stanislawek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064873282"
                        ],
                        "name": "Filip Grali'nski",
                        "slug": "Filip-Grali'nski",
                        "structuredName": {
                            "firstName": "Filip",
                            "lastName": "Grali'nski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Filip Grali'nski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065175441"
                        ],
                        "name": "Anna Wr'oblewska",
                        "slug": "Anna-Wr'oblewska",
                        "structuredName": {
                            "firstName": "Anna",
                            "lastName": "Wr'oblewska",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anna Wr'oblewska"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055968344"
                        ],
                        "name": "Dawid Lipi'nski",
                        "slug": "Dawid-Lipi'nski",
                        "structuredName": {
                            "firstName": "Dawid",
                            "lastName": "Lipi'nski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dawid Lipi'nski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064187520"
                        ],
                        "name": "Agnieszka Kaliska",
                        "slug": "Agnieszka-Kaliska",
                        "structuredName": {
                            "firstName": "Agnieszka",
                            "lastName": "Kaliska",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Agnieszka Kaliska"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066303178"
                        ],
                        "name": "Paulina Rosalska",
                        "slug": "Paulina-Rosalska",
                        "structuredName": {
                            "firstName": "Paulina",
                            "lastName": "Rosalska",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paulina Rosalska"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11016367"
                        ],
                        "name": "Bartosz Topolski",
                        "slug": "Bartosz-Topolski",
                        "structuredName": {
                            "firstName": "Bartosz",
                            "lastName": "Topolski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bartosz Topolski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144356943"
                        ],
                        "name": "P. Biecek",
                        "slug": "P.-Biecek",
                        "structuredName": {
                            "firstName": "Przemys\u0142aw",
                            "lastName": "Biecek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Biecek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "Consequently, the authors of Kleister proposed a set of handcrafted rules for the final selection of the entity values [52]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 159
                            }
                        ],
                        "text": "Authors dealing with property extraction rely on either manual annotation or the heuristic-based tagging procedure that impacts the overall end-to-end results [56, 52, 11, 18, 55, 36]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 234470170,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8828cece68a097041d49177cd4e28ec6ee8db139",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "The relevance of the Key Information Extraction (KIE) task is increasingly important in natural language processing problems. But there are still only a few well-defined problems that serve as benchmarks for solutions in this area. To bridge this gap, we introduce two new datasets (Kleister NDA and Kleister Charity). They involve a mix of scanned and born-digital long formal English-language documents. In these datasets, an NLP system is expected to find or infer various types of entities by employing both textual and structural layout features. The Kleister Charity dataset consists of 2,788 annual financial reports of charity organizations, with 61,643 unique pages and 21,612 entities to extract. The Kleister NDA dataset has 540 Non-disclosure Agreements, with 3,229 unique pages and 2,160 entities to extract. We provide several state-of-the-art baseline systems from the KIE domain (Flair, BERT, RoBERTa, LayoutLM, LAMBERT), which show that our datasets pose a strong challenge to existing models. The best model achieved an 81.77 % and an 83.57 % F1-score on respectively the Kleister NDA and the Kleister Charity datasets. We share the datasets to encourage progress on more in-depth and complex information extraction tasks."
            },
            "slug": "Kleister:-Key-Information-Extraction-Datasets-Long-Stanislawek-Grali'nski",
            "title": {
                "fragments": [],
                "text": "Kleister: Key Information Extraction Datasets Involving Long Documents with Complex Layouts"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work introduces two new datasets (Kleister NDA and Kleister Charity) that involve a mix of scanned and born-digital long formal English-language documents and provides several state-of-the-art baseline systems from the KIE domain, which show that these datasets pose a strong challenge to existing models."
            },
            "venue": {
                "fragments": [],
                "text": "ICDAR"
            },
            "year": 2021
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115683276"
                        ],
                        "name": "Yang Xu",
                        "slug": "Yang-Xu",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3032611"
                        ],
                        "name": "Yiheng Xu",
                        "slug": "Yiheng-Xu",
                        "structuredName": {
                            "firstName": "Yiheng",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiheng Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1379581011"
                        ],
                        "name": "Tengchao Lv",
                        "slug": "Tengchao-Lv",
                        "structuredName": {
                            "firstName": "Tengchao",
                            "lastName": "Lv",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tengchao Lv"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114843952"
                        ],
                        "name": "Lei Cui",
                        "slug": "Lei-Cui",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Cui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Cui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49807919"
                        ],
                        "name": "Furu Wei",
                        "slug": "Furu-Wei",
                        "structuredName": {
                            "firstName": "Furu",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Furu Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41211249"
                        ],
                        "name": "Guoxin Wang",
                        "slug": "Guoxin-Wang",
                        "structuredName": {
                            "firstName": "Guoxin",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guoxin Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38534822"
                        ],
                        "name": "Yijuan Lu",
                        "slug": "Yijuan-Lu",
                        "structuredName": {
                            "firstName": "Yijuan",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yijuan Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1882479"
                        ],
                        "name": "D. Flor\u00eancio",
                        "slug": "D.-Flor\u00eancio",
                        "structuredName": {
                            "firstName": "Dinei",
                            "lastName": "Flor\u00eancio",
                            "middleNames": [
                                "A.",
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Flor\u00eancio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109292017"
                        ],
                        "name": "Cha Zhang",
                        "slug": "Cha-Zhang",
                        "structuredName": {
                            "firstName": "Cha",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cha Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256319"
                        ],
                        "name": "Wanxiang Che",
                        "slug": "Wanxiang-Che",
                        "structuredName": {
                            "firstName": "Wanxiang",
                            "lastName": "Che",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wanxiang Che"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2156053262"
                        ],
                        "name": "Min Zhang",
                        "slug": "Min-Zhang",
                        "structuredName": {
                            "firstName": "Min",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Min Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2143359114"
                        ],
                        "name": "Lidong Zhou",
                        "slug": "Lidong-Zhou",
                        "structuredName": {
                            "firstName": "Lidong",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lidong Zhou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 65
                            }
                        ],
                        "text": "It is consistent with the previous works on layout understanding [57, 12]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 159
                            }
                        ],
                        "text": "Authors dealing with property extraction rely on either manual annotation or the heuristic-based tagging procedure that impacts the overall end-to-end results [58, 13, 12, 21, 57, 39]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 229923949,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0197abda042e6de87b5f716caa708a6a459f078c",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to its effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents. We propose LayoutLMv2 architecture with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework. Specifically, with a two-stream multi-modal Transformer encoder, LayoutLMv2 uses not only the existing masked visual-language modeling task but also the new text-image alignment and text-image matching tasks, which make it better capture the cross-modality interaction in the pre-training stage. Meanwhile, it also integrates a spatial-aware self-attention mechanism into the Transformer architecture so that the model can fully understand the relative positional relationship among different text blocks. Experiment results show that LayoutLMv2 outperforms LayoutLM by a large margin and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks, including FUNSD (0.7895 to 0.8420), CORD (0.9493 to 0.9601), SROIE (0.9524 to 0.9781), Kleister-NDA (0.8340 to 0.8520), RVL-CDIP (0.9443 to 0.9564), and DocVQA (0.7295 to 0.8672)."
            },
            "slug": "LayoutLMv2:-Multi-modal-Pre-training-for-Document-Xu-Xu",
            "title": {
                "fragments": [],
                "text": "LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "LayoutLMv2 architecture with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2021
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3826388"
                        ],
                        "name": "Kai Han",
                        "slug": "Kai-Han",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108702980"
                        ],
                        "name": "Yunhe Wang",
                        "slug": "Yunhe-Wang",
                        "structuredName": {
                            "firstName": "Yunhe",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yunhe Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118023932"
                        ],
                        "name": "Hanting Chen",
                        "slug": "Hanting-Chen",
                        "structuredName": {
                            "firstName": "Hanting",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hanting Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736061"
                        ],
                        "name": "Xinghao Chen",
                        "slug": "Xinghao-Chen",
                        "structuredName": {
                            "firstName": "Xinghao",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinghao Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148899357"
                        ],
                        "name": "Jianyuan Guo",
                        "slug": "Jianyuan-Guo",
                        "structuredName": {
                            "firstName": "Jianyuan",
                            "lastName": "Guo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianyuan Guo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2125024057"
                        ],
                        "name": "Zhenhua Liu",
                        "slug": "Zhenhua-Liu",
                        "structuredName": {
                            "firstName": "Zhenhua",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhenhua Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103603255"
                        ],
                        "name": "Yehui Tang",
                        "slug": "Yehui-Tang",
                        "structuredName": {
                            "firstName": "Yehui",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yehui Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1569696821"
                        ],
                        "name": "An Xiao",
                        "slug": "An-Xiao",
                        "structuredName": {
                            "firstName": "An",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "An Xiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691522"
                        ],
                        "name": "Chunjing Xu",
                        "slug": "Chunjing-Xu",
                        "structuredName": {
                            "firstName": "Chunjing",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunjing Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6898202"
                        ],
                        "name": "Yixing Xu",
                        "slug": "Yixing-Xu",
                        "structuredName": {
                            "firstName": "Yixing",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yixing Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116369043"
                        ],
                        "name": "Zhaohui Yang",
                        "slug": "Zhaohui-Yang",
                        "structuredName": {
                            "firstName": "Zhaohui",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhaohui Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108440680"
                        ],
                        "name": "Yiman Zhang",
                        "slug": "Yiman-Zhang",
                        "structuredName": {
                            "firstName": "Yiman",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiman Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143719920"
                        ],
                        "name": "D. Tao",
                        "slug": "D.-Tao",
                        "structuredName": {
                            "firstName": "Dacheng",
                            "lastName": "Tao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 87
                            }
                        ],
                        "text": "Our model was validated on series of experiments involving Key Information Extraction, Visual Question Answering, classification of rich documents, and Question Answering from layout-rich texts."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 172
                            }
                        ],
                        "text": "The relationships between text and other media have been previously studied in Visual Commonsense Reasoning, Video-Grounded Dialogue, Speech, and Visual Question Answering [16, 35, 4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 145
                            }
                        ],
                        "text": "The relationships between text and other media have been previously studied in Visual Commonsense Reasoning, VideoGrounded Dialogue, Speech, and Visual Question Answering [13, 32, 3]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 229363540,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49e17ad5bf10eb17f4c35a93a1588a6f0f8760db",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 197,
            "paperAbstract": {
                "fragments": [],
                "text": "Transformer is a type of deep neural network mainly based on self-attention mechanism which is originally applied in natural language processing field. Inspired by the strong representation ability of transformer, researchers propose to extend transformer for computer vision tasks. Transformer-based models show competitive and even better performance on various visual benchmarks compared to other network types such as convolutional networks and recurrent networks. In this paper we provide a literature review of these visual transformer models by categorizing them in different tasks and analyze the advantages and disadvantages of these methods. In particular, the main categories include the basic image classification, high-level vision, low-level vision and video processing. Self-attention in computer vision is also briefly revisited as self-attention is the base component in transformer. Efficient transformer methods are included for pushing transformer into real applications. Finally, we give a discussion about the further research directions for visual transformer."
            },
            "slug": "A-Survey-on-Visual-Transformer-Han-Wang",
            "title": {
                "fragments": [],
                "text": "A Survey on Visual Transformer"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A literature review of these visual transformer models by categorizing them in different tasks and analyzing the advantages and disadvantages of these methods is provided."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3407177"
                        ],
                        "name": "Tomasz Dwojak",
                        "slug": "Tomasz-Dwojak",
                        "structuredName": {
                            "firstName": "Tomasz",
                            "lastName": "Dwojak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomasz Dwojak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749652564"
                        ],
                        "name": "Michal Pietruszka",
                        "slug": "Michal-Pietruszka",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Pietruszka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michal Pietruszka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3448729"
                        ],
                        "name": "\u0141ukasz Borchmann",
                        "slug": "\u0141ukasz-Borchmann",
                        "structuredName": {
                            "firstName": "\u0141ukasz",
                            "lastName": "Borchmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u0141ukasz Borchmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749652558"
                        ],
                        "name": "Jakub Chlkedowski",
                        "slug": "Jakub-Chlkedowski",
                        "structuredName": {
                            "firstName": "Jakub",
                            "lastName": "Chlkedowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jakub Chlkedowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064873282"
                        ],
                        "name": "Filip Grali'nski",
                        "slug": "Filip-Grali'nski",
                        "structuredName": {
                            "firstName": "Filip",
                            "lastName": "Grali'nski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Filip Grali'nski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 138
                            }
                        ],
                        "text": "Limitations of this approach are strikingly visible on tasks framed in either key information extraction or property extraction paradigms [19, 9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 154
                            }
                        ],
                        "text": "Although this does not necessarily lead to the use of encoder-decoder models, several successful solutions relied on variants of Transformer architecture [54, 34, 9, 44]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 248,
                                "start": 245
                            }
                        ],
                        "text": "Finally, the property extraction paradigm does not assume the requested value appeared in the article in any form since it is sufficient for it to be inferable from the content, as in document classification or non-extractive question answering [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 226278354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a3cba1244c1971c47798113f075f708e049db0cf",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper investigates various Transformer architectures on the WikiReading Information Extraction and Machine Reading Comprehension dataset. The proposed dual-source model outperforms the current state-of-the-art by a large margin. Next, we introduce WikiReading Recycled - a newly developed public dataset, and the task of multiple-property extraction. It uses the same data as WikiReading but does not inherit its predecessor\u2019s identified disadvantages. In addition, we provide a human-annotated test set with diagnostic subsets for a detailed analysis of model performance."
            },
            "slug": "From-Dataset-Recycling-to-Multi-Property-Extraction-Dwojak-Pietruszka",
            "title": {
                "fragments": [],
                "text": "From Dataset Recycling to Multi-Property Extraction and Beyond"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The proposed dual-source model outperforms the current state-of-the-art by a large margin, and a human-annotated test set with diagnostic subsets is provided for a detailed analysis of model performance."
            },
            "venue": {
                "fragments": [],
                "text": "CONLL"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2091768"
                        ],
                        "name": "Kelvin Guu",
                        "slug": "Kelvin-Guu",
                        "structuredName": {
                            "firstName": "Kelvin",
                            "lastName": "Guu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kelvin Guu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2544107"
                        ],
                        "name": "Kenton Lee",
                        "slug": "Kenton-Lee",
                        "structuredName": {
                            "firstName": "Kenton",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenton Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9941702"
                        ],
                        "name": "Z. Tung",
                        "slug": "Z.-Tung",
                        "structuredName": {
                            "firstName": "Zora",
                            "lastName": "Tung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Tung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2616463"
                        ],
                        "name": "Panupong Pasupat",
                        "slug": "Panupong-Pasupat",
                        "structuredName": {
                            "firstName": "Panupong",
                            "lastName": "Pasupat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Panupong Pasupat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744179"
                        ],
                        "name": "Ming-Wei Chang",
                        "slug": "Ming-Wei-Chang",
                        "structuredName": {
                            "firstName": "Ming-Wei",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Wei Chang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 57
                            }
                        ],
                        "text": ", named entities are preferred rather than random tokens [44, 12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 225626501,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ea1f95989f808f409a3cd29b128000c04036c224",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Retrieval-Augmented-Language-Model-Pre-Training-Guu-Lee",
            "title": {
                "fragments": [],
                "text": "Retrieval Augmented Language Model Pre-Training"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1498640514"
                        ],
                        "name": "Rafal Powalski",
                        "slug": "Rafal-Powalski",
                        "structuredName": {
                            "firstName": "Rafal",
                            "lastName": "Powalski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rafal Powalski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1822665"
                        ],
                        "name": "Tomasz Stanislawek",
                        "slug": "Tomasz-Stanislawek",
                        "structuredName": {
                            "firstName": "Tomasz",
                            "lastName": "Stanislawek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomasz Stanislawek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 151
                            }
                        ],
                        "text": "Although the algorithm proved its efficiency in many NLP fields, the recent work showed that it performs poorly in a case of an unusual casing of text [45], for instance, when all words are uppercased."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 225041314,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f1c8f1731f5b1949487d12a78c6fb5cb9955b8a8",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we introduce a new approach to dealing with the problem of case-sensitiveness in Language Modelling (LM). We propose simple architecture modification to the RoBERTa language model, accompanied by a new tokenization strategy, which we named Unified Case LM (UniCase). We tested our solution on the GLUE benchmark, which led to increased performance by 0.42 points. Moreover, we prove that the UniCase model works much better when we have to deal with text data, where all tokens are uppercased (+5.88 point)."
            },
            "slug": "UniCase-Rethinking-Casing-in-Language-Models-Powalski-Stanislawek",
            "title": {
                "fragments": [],
                "text": "UniCase - Rethinking Casing in Language Models"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "This paper proposes simple architecture modification to the RoBERTa language model, accompanied by a new tokenization strategy, which is named Unified Case LM (UniCase), and proves that the UniCase model works much better when the authors have to deal with text data, where all tokens are uppercased."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11348687"
                        ],
                        "name": "Jungo Kasai",
                        "slug": "Jungo-Kasai",
                        "structuredName": {
                            "firstName": "Jungo",
                            "lastName": "Kasai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jungo Kasai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143958923"
                        ],
                        "name": "Nikolaos Pappas",
                        "slug": "Nikolaos-Pappas",
                        "structuredName": {
                            "firstName": "Nikolaos",
                            "lastName": "Pappas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nikolaos Pappas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1818378366"
                        ],
                        "name": "Hao Peng",
                        "slug": "Hao-Peng",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Peng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Peng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059363961"
                        ],
                        "name": "James Cross",
                        "slug": "James-Cross",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Cross",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Cross"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685669"
                        ],
                        "name": "Noah A. Smith",
                        "slug": "Noah-A.-Smith",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Smith",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah A. Smith"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 120
                            }
                        ],
                        "text": ", in the Neural Machine Translation context, and can be alleviated by methods such as lowering the depth of the decoder [50, 27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 219792490,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8c5a394654822a5de53ac2e4a355c1c6ead4750c",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "State-of-the-art neural machine translation models generate outputs autoregressively, where every step conditions on the previously generated tokens. This sequential nature causes inherent decoding latency. Non-autoregressive translation techniques, on the other hand, parallelize generation across positions and speed up inference at the expense of translation quality. Much recent effort has been devoted to non-autoregressive methods, aiming for a better balance between speed and quality. In this work, we re-examine the trade-off and argue that transformer-based autoregressive models can be substantially sped up without loss in accuracy. Specifically, we study autoregressive models with encoders and decoders of varied depths. Our extensive experiments show that given a sufficiently deep encoder, a one-layer autoregressive decoder yields state-of-the-art accuracy with comparable latency to strong non-autoregressive models. Our findings suggest that the latency disadvantage for autoregressive translation has been overestimated due to a suboptimal choice of layer allocation, and we provide a new speed-quality baseline for future research toward fast, accurate translation."
            },
            "slug": "Deep-Encoder,-Shallow-Decoder:-Reevaluating-the-in-Kasai-Pappas",
            "title": {
                "fragments": [],
                "text": "Deep Encoder, Shallow Decoder: Reevaluating the Speed-Quality Tradeoff in Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The findings suggest that the latency disadvantage for autoregressive translation has been overestimated due to a suboptimal choice of layer allocation, and a new speed-quality baseline for future research toward fast, accurate translation is provided."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2475831"
                        ],
                        "name": "Yung-Sung Chuang",
                        "slug": "Yung-Sung-Chuang",
                        "structuredName": {
                            "firstName": "Yung-Sung",
                            "lastName": "Chuang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yung-Sung Chuang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117660020"
                        ],
                        "name": "Chi-Liang Liu",
                        "slug": "Chi-Liang-Liu",
                        "structuredName": {
                            "firstName": "Chi-Liang",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chi-Liang Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706104"
                        ],
                        "name": "Hung-yi Lee",
                        "slug": "Hung-yi-Lee",
                        "structuredName": {
                            "firstName": "Hung-yi",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hung-yi Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145535692"
                        ],
                        "name": "Lin-Shan Lee",
                        "slug": "Lin-Shan-Lee",
                        "structuredName": {
                            "firstName": "Lin-Shan",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lin-Shan Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 87
                            }
                        ],
                        "text": "Our model was validated on series of experiments involving Key Information Extraction, Visual Question Answering, classification of rich documents, and Question Answering from layout-rich texts."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 79
                            }
                        ],
                        "text": "The relationships between text and other media have been previously studied in Visual Commonsense Reasoning, VideoGrounded Dialogue, Speech, and Visual Question Answering [13, 32, 3]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 171
                            }
                        ],
                        "text": "The relationships between text and other media have been previously studied in Visual Commonsense Reasoning, VideoGrounded Dialogue, Speech, and Visual Question Answering [14, 33, 3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 219473826,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "3eae8fbef73aa5bd57490c1c8e3209a5995c6092",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "While various end-to-end models for spoken language understanding tasks have been explored recently, this paper is probably the first known attempt to challenge the very difficult task of end-to-end spoken question answering (SQA). Learning from the very successful BERT model for various text processing tasks, here we proposed an audio-and-text jointly learned SpeechBERT model. This model outperformed the conventional approach of cascading ASR with the following text question answering (TQA) model on datasets including ASR errors in answer spans, because the end-to-end model was shown to be able to extract information out of audio data before ASR produced errors. When ensembling the proposed end-to-end model with the cascade architecture, even better performance was achieved. In addition to the potential of end-to-end SQA, the SpeechBERT can also be considered for many other spoken language understanding tasks just as BERT for many text processing tasks."
            },
            "slug": "SpeechBERT:-An-Audio-and-Text-Jointly-Learned-Model-Chuang-Liu",
            "title": {
                "fragments": [],
                "text": "SpeechBERT: An Audio-and-Text Jointly Learned Language Model for End-to-End Spoken Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This model outperformed the conventional approach of cascading ASR with the following text question answering (TQA) model on datasets including ASR errors in answer spans, because the end-to-end model was shown to be able to extract information out of audio data before ASR produced errors."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38253388"
                        ],
                        "name": "Pengcheng Yin",
                        "slug": "Pengcheng-Yin",
                        "structuredName": {
                            "firstName": "Pengcheng",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pengcheng Yin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700325"
                        ],
                        "name": "Graham Neubig",
                        "slug": "Graham-Neubig",
                        "structuredName": {
                            "firstName": "Graham",
                            "lastName": "Neubig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Graham Neubig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144105277"
                        ],
                        "name": "Wen-tau Yih",
                        "slug": "Wen-tau-Yih",
                        "structuredName": {
                            "firstName": "Wen-tau",
                            "lastName": "Yih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wen-tau Yih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48662861"
                        ],
                        "name": "Sebastian Riedel",
                        "slug": "Sebastian-Riedel",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Riedel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Riedel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 302,
                                "start": 291
                            }
                        ],
                        "text": "Several authors have shown that, when tasks involving 2D documents are considered, sequential models can be outperformed by considering layout information either directly as positional embeddings [17, 11, 56] or indirectly by allowing them to be contextualized on their spatial neighborhood [6, 57, 15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 218674345,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5b1d1cab073cb746a990b37d42dc7b67763f881",
            "isKey": false,
            "numCitedBy": 141,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent years have witnessed the burgeoning of pretrained language models (LMs) for text-based natural language (NL) understanding tasks. Such models are typically trained on free-form NL text, hence may not be suitable for tasks like semantic parsing over structured data, which require reasoning over both free-form NL questions and structured tabular data (e.g., database tables). In this paper we present TaBERT, a pretrained LM that jointly learns representations for NL sentences and (semi-)structured tables. TaBERT is trained on a large corpus of 26 million tables and their English contexts. In experiments, neural semantic parsers using TaBERT as feature representation layers achieve new best results on the challenging weakly-supervised semantic parsing benchmark WikiTableQuestions, while performing competitively on the text-to-SQL dataset Spider."
            },
            "slug": "TaBERT:-Pretraining-for-Joint-Understanding-of-and-Yin-Neubig",
            "title": {
                "fragments": [],
                "text": "TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "TaBERT is a pretrained LM that jointly learns representations for NL sentences and (semi-)structured tables that achieves new best results on the challenging weakly-supervised semantic parsing benchmark WikiTableQuestions, while performing competitively on the text-to-SQL dataset Spider."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783281"
                        ],
                        "name": "Daniel Khashabi",
                        "slug": "Daniel-Khashabi",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Khashabi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Khashabi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48872685"
                        ],
                        "name": "Sewon Min",
                        "slug": "Sewon-Min",
                        "structuredName": {
                            "firstName": "Sewon",
                            "lastName": "Min",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sewon Min"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2236429"
                        ],
                        "name": "Tushar Khot",
                        "slug": "Tushar-Khot",
                        "structuredName": {
                            "firstName": "Tushar",
                            "lastName": "Khot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tushar Khot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48229640"
                        ],
                        "name": "Ashish Sabharwal",
                        "slug": "Ashish-Sabharwal",
                        "structuredName": {
                            "firstName": "Ashish",
                            "lastName": "Sabharwal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashish Sabharwal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3385516"
                        ],
                        "name": "Oyvind Tafjord",
                        "slug": "Oyvind-Tafjord",
                        "structuredName": {
                            "firstName": "Oyvind",
                            "lastName": "Tafjord",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oyvind Tafjord"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48323507"
                        ],
                        "name": "Peter Clark",
                        "slug": "Peter-Clark",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2548384"
                        ],
                        "name": "Hannaneh Hajishirzi",
                        "slug": "Hannaneh-Hajishirzi",
                        "structuredName": {
                            "firstName": "Hannaneh",
                            "lastName": "Hajishirzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hannaneh Hajishirzi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 148
                            }
                        ],
                        "text": "Most tasks in Natural Language Processing (NLP) can be unified under one framework by casting them as triplets of the question, context, and answer [29, 39, 26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 99
                            }
                        ],
                        "text": "The QA program of unifying NLP frames all the problems as triplets of question, context and answer [29, 39, 26] or item, property name and answer [16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 218487109,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ad5970584754cc7a1d91c95ab84a1e210258183a",
            "isKey": false,
            "numCitedBy": 244,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "Question answering (QA) tasks have been posed using a variety of formats, such as extractive span selection, multiple choice, etc. This has led to format-specialized models, and even to an implicit division in the QA community. We argue that such boundaries are artificial and perhaps unnecessary, given the reasoning abilities we seek to teach are not governed by the format. As evidence, we use the latest advances in language modeling to build a single pre-trained QA model, UNIFIEDQA, that performs well across 19 QA datasets spanning 4 diverse formats. UNIFIEDQA performs on par with 8 different models that were trained on individual datasets themselves. Even when faced with 12 unseen datasets of observed formats, UNIFIEDQA performs surprisingly well, showing strong generalization from its outof-format training data. Finally, simply finetuning this pre trained QA model into specialized models results in a new state of the art on 10 factoid and commonsense question answering datasets, establishing UNIFIEDQA as a strong starting point for building QA systems."
            },
            "slug": "UnifiedQA:-Crossing-Format-Boundaries-With-a-Single-Khashabi-Min",
            "title": {
                "fragments": [],
                "text": "UnifiedQA: Crossing Format Boundaries With a Single QA System"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work uses the latest advances in language modeling to build a single pre-trained QA model, UNIFIEDQA, that performs well across 19 QA datasets spanning 4 diverse formats, and results in a new state of the art on 10 factoid and commonsense question answering datasets."
            },
            "venue": {
                "fragments": [],
                "text": "FINDINGS"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054135353"
                        ],
                        "name": "Wonseok Hwang",
                        "slug": "Wonseok-Hwang",
                        "structuredName": {
                            "firstName": "Wonseok",
                            "lastName": "Hwang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wonseok Hwang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49841374"
                        ],
                        "name": "Jinyeong Yim",
                        "slug": "Jinyeong-Yim",
                        "structuredName": {
                            "firstName": "Jinyeong",
                            "lastName": "Yim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jinyeong Yim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7882243"
                        ],
                        "name": "Seunghyun Park",
                        "slug": "Seunghyun-Park",
                        "structuredName": {
                            "firstName": "Seunghyun",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seunghyun Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16110760"
                        ],
                        "name": "Sohee Yang",
                        "slug": "Sohee-Yang",
                        "structuredName": {
                            "firstName": "Sohee",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sohee Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4418074"
                        ],
                        "name": "Minjoon Seo",
                        "slug": "Minjoon-Seo",
                        "structuredName": {
                            "firstName": "Minjoon",
                            "lastName": "Seo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minjoon Seo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 132
                            }
                        ],
                        "text": "aspects by the inclusion of the area masking loss function or achieving independence from sequential order in decoding respectively [18, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 222103842,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20d0564fd3fdbc24f266ca2076826a2271c3ea08",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Information Extraction (IE) for semi-structured document images is often approached as a sequence tagging problem by classifying each recognized input token into one of the IOB (Inside, Outside, and Beginning) categories. However, such problem setup has two inherent limitations that (1) it cannot easily handle complex spatial relationships and (2) it is not suitable for highly structured information, which are nevertheless frequently observed in real-world document images. To tackle these issues, we first formulate the IE task as spatial dependency parsing problem that focuses on the relationship among text segment nodes in the documents. Under this setup, we then propose SPADE (SPAtial DEpendency parser) that models highly complex spatial relationships and an arbitrary number of information layers in the documents in an end-to-end manner. We evaluate it on various kinds of documents such as receipts, name cards, forms, and invoices, and show that it achieves a similar or better performance compared to strong baselines including BERT-based IOB taggger, with up to 37.7% improvement."
            },
            "slug": "Spatial-Dependency-Parsing-for-Semi-Structured-Hwang-Yim",
            "title": {
                "fragments": [],
                "text": "Spatial Dependency Parsing for Semi-Structured Document Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "SPADE (SPAtial DEpendency parser) is proposed that models highly complex spatial relationships and an arbitrary number of information layers in the documents in an end-to-end manner and achieves a similar or better performance compared to strong baselines including BERT-based IOB taggger."
            },
            "venue": {
                "fragments": [],
                "text": "FINDINGS"
            },
            "year": 2021
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1500435161"
                        ],
                        "name": "Yi Ren",
                        "slug": "Yi-Ren",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48211720"
                        ],
                        "name": "Jinglin Liu",
                        "slug": "Jinglin-Liu",
                        "structuredName": {
                            "firstName": "Jinglin",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jinglin Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48391466"
                        ],
                        "name": "Xu Tan",
                        "slug": "Xu-Tan",
                        "structuredName": {
                            "firstName": "Xu",
                            "lastName": "Tan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xu Tan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47601191"
                        ],
                        "name": "Sheng Zhao",
                        "slug": "Sheng-Zhao",
                        "structuredName": {
                            "firstName": "Sheng",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sheng Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47122432"
                        ],
                        "name": "Zhou Zhao",
                        "slug": "Zhou-Zhao",
                        "structuredName": {
                            "firstName": "Zhou",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhou Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110264337"
                        ],
                        "name": "Tie-Yan Liu",
                        "slug": "Tie-Yan-Liu",
                        "structuredName": {
                            "firstName": "Tie-Yan",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tie-Yan Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 120
                            }
                        ],
                        "text": ", in the Neural Machine Translation context, and can be alleviated by methods such as lowering the depth of the decoder [48, 25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 216056470,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bed87e8fb3e7e9bc87e1c2ee459ae405a35d3267",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Non-autoregressive (NAR) models generate all the tokens of a sequence in parallel, resulting in faster generation speed compared to their autoregressive (AR) counterparts but at the cost of lower accuracy. Different techniques including knowledge distillation and source-target alignment have been proposed to bridge the gap between AR and NAR models in various tasks such as neural machine translation (NMT), automatic speech recognition (ASR), and text to speech (TTS). With the help of those techniques, NAR models can catch up with the accuracy of AR models in some tasks but not in some others. In this work, we conduct a study to understand the difficulty of NAR sequence generation and try to answer: (1) Why NAR models can catch up with AR models in some tasks but not all? (2) Why techniques like knowledge distillation and source-target alignment can help NAR models. Since the main difference between AR and NAR models is that NAR models do not use dependency among target tokens while AR models do, intuitively the difficulty of NAR sequence generation heavily depends on the strongness of dependency among target tokens. To quantify such dependency, we propose an analysis model called CoMMA to characterize the difficulty of different NAR sequence generation tasks. We have several interesting findings: 1) Among the NMT, ASR and TTS tasks, ASR has the most target-token dependency while TTS has the least. 2) Knowledge distillation reduces the target-token dependency in target sequence and thus improves the accuracy of NAR models. 3) Source-target alignment constraint encourages dependency of a target token on source tokens and thus eases the training of NAR models."
            },
            "slug": "A-Study-of-Non-autoregressive-Model-for-Sequence-Ren-Liu",
            "title": {
                "fragments": [],
                "text": "A Study of Non-autoregressive Model for Sequence Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An analysis model called CoMMA is proposed to characterize the difficulty of different NAR sequence generation tasks and has several interesting findings, including that among the NMT, ASR and TTS tasks,ASR has the most target-token dependency while TTS has the least."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47426264"
                        ],
                        "name": "Jonathan Herzig",
                        "slug": "Jonathan-Herzig",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Herzig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Herzig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5274550"
                        ],
                        "name": "Pawel Krzysztof Nowak",
                        "slug": "Pawel-Krzysztof-Nowak",
                        "structuredName": {
                            "firstName": "Pawel",
                            "lastName": "Nowak",
                            "middleNames": [
                                "Krzysztof"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pawel Krzysztof Nowak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40150608"
                        ],
                        "name": "Thomas M\u00fcller",
                        "slug": "Thomas-M\u00fcller",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2174596"
                        ],
                        "name": "Francesco Piccinno",
                        "slug": "Francesco-Piccinno",
                        "structuredName": {
                            "firstName": "Francesco",
                            "lastName": "Piccinno",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Francesco Piccinno"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117595858"
                        ],
                        "name": "Julian Martin Eisenschlos",
                        "slug": "Julian-Martin-Eisenschlos",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Eisenschlos",
                            "middleNames": [
                                "Martin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julian Martin Eisenschlos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 302,
                                "start": 291
                            }
                        ],
                        "text": "Several authors have shown that, when tasks involving 2D documents are considered, sequential models can be outperformed by considering layout information either directly as positional embeddings [17, 11, 56] or indirectly by allowing them to be contextualized on their spatial neighborhood [6, 57, 15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 214802901,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "52cb05d721688cb766c6e282e9d55c3b8e3dc0cf",
            "isKey": false,
            "numCitedBy": 170,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Answering natural language questions over tables is usually seen as a semantic parsing task. To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms. However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation. In this paper, we present TaPas, an approach to question answering over tables without generating logical forms. TaPas trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection. TaPas extends BERT\u2019s architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end. We experiment with three different semantic parsing datasets, and find that TaPas outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WikiSQL and WikiTQ, but with a simpler model architecture. We additionally find that transfer learning, which is trivial in our setting, from WikiSQL to WikiTQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art."
            },
            "slug": "TaPas:-Weakly-Supervised-Table-Parsing-via-Herzig-Nowak",
            "title": {
                "fragments": [],
                "text": "TaPas: Weakly Supervised Table Parsing via Pre-training"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "TaPas is presented, an approach to question answering over tables without generating logical forms that outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA and performing on par with the state of theart on WikiSQL and WikiTQ, but with a simpler model architecture."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144656873"
                        ],
                        "name": "O. Sidorov",
                        "slug": "O.-Sidorov",
                        "structuredName": {
                            "firstName": "Oleksii",
                            "lastName": "Sidorov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Sidorov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2874347"
                        ],
                        "name": "Ronghang Hu",
                        "slug": "Ronghang-Hu",
                        "structuredName": {
                            "firstName": "Ronghang",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronghang Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34849128"
                        ],
                        "name": "Marcus Rohrbach",
                        "slug": "Marcus-Rohrbach",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Rohrbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus Rohrbach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50286460"
                        ],
                        "name": "Amanpreet Singh",
                        "slug": "Amanpreet-Singh",
                        "structuredName": {
                            "firstName": "Amanpreet",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amanpreet Singh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 214693197,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "33eadd4e666a894306a22ba0839c5e0cef77280e",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Image descriptions can help visually impaired people to quickly understand the image content. While we made significant progress in automatically describing images and optical character recognition, current approaches are unable to include written text in their descriptions, although text is omnipresent in human environments and frequently critical to understand our surroundings. To study how to comprehend text in the context of an image we collect a novel dataset, TextCaps, with 145k captions for 28k images. Our dataset challenges a model to recognize text, relate it to its visual context, and decide what part of the text to copy or paraphrase, requiring spatial, semantic, and visual reasoning between multiple text tokens and visual entities, such as objects. We study baselines and adapt existing approaches to this new task, which we refer to as image captioning with reading comprehension. Our analysis with automatic and human studies shows that our new TextCaps dataset provides many new technical challenges over previous datasets."
            },
            "slug": "TextCaps:-a-Dataset-for-Image-Captioning-with-Sidorov-Hu",
            "title": {
                "fragments": [],
                "text": "TextCaps: a Dataset for Image Captioning with Reading Comprehension"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel dataset, TextCaps, with 145k captions for 28k images, challenges a model to recognize text, relate it to its visual context, and decide what part of the text to copy or paraphrase, requiring spatial, semantic, and visual reasoning between multiple text tokens and visual entities, such as objects."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1576060249"
                        ],
                        "name": "Junteng Ma",
                        "slug": "Junteng-Ma",
                        "structuredName": {
                            "firstName": "Junteng",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junteng Ma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32158173"
                        ],
                        "name": "S. Qin",
                        "slug": "S.-Qin",
                        "structuredName": {
                            "firstName": "Shihao",
                            "lastName": "Qin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Qin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069860776"
                        ],
                        "name": "Lan Su",
                        "slug": "Lan-Su",
                        "structuredName": {
                            "firstName": "Lan",
                            "lastName": "Su",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lan Su"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1502881409"
                        ],
                        "name": "Xia Li",
                        "slug": "Xia-Li",
                        "structuredName": {
                            "firstName": "Xia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "147385373"
                        ],
                        "name": "Lixian Xiao",
                        "slug": "Lixian-Xiao",
                        "structuredName": {
                            "firstName": "Lixian",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lixian Xiao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 194
                            }
                        ],
                        "text": "In the context of images, this niche was previously approached with an image-to-text cross-attention mechanism, alternatively, by adding visual features to word embeddings or concatenating them [40, 36, 38, 55, 58]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 214595346,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d23725b963ecbd071adb21a9139e57483335dc3",
            "isKey": false,
            "numCitedBy": 1,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years, multimodal machine translation has become one of the hot research topics. In this paper, a machine translation model based on self-attention mechanism is extended for multimodal machine translation. In the model, an Image-text attention layer is added in the end of encoder layer to capture the relevant semantic information between image and text words. With this layer of attention, the model can capture the different weights between the words that is relevant to the image or appear in the image, and get a better text representation that fuses these weights, so that it can be better used for decoding of the model. Experiments are carried out on the original English-German sentence pairs of the multimodal machine translation dataset, Multi30k, and the Indonesian-Chinese sentence pairs which is manually annotated by human. The results show that our model performs better than the text-only transformer-based machine translation model and is comparable to most of the existing work, proves the effectiveness of our model."
            },
            "slug": "Fusion-of-Image-text-attention-for-Multimodal-Ma-Qin",
            "title": {
                "fragments": [],
                "text": "Fusion of Image-text attention for Transformer-based Multimodal Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "A machine translation model based on self-attention mechanism is extended for multimodal machine translation and an Image-text attention layer is added in the end of encoder layer to capture the relevant semantic information between image and text words."
            },
            "venue": {
                "fragments": [],
                "text": "2019 International Conference on Asian Language Processing (IALP)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144797264"
                        ],
                        "name": "J. Clark",
                        "slug": "J.-Clark",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2890423"
                        ],
                        "name": "Eunsol Choi",
                        "slug": "Eunsol-Choi",
                        "structuredName": {
                            "firstName": "Eunsol",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eunsol Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123052390"
                        ],
                        "name": "Michael Collins",
                        "slug": "Michael-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2758616"
                        ],
                        "name": "Dan Garrette",
                        "slug": "Dan-Garrette",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Garrette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan Garrette"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15652489"
                        ],
                        "name": "T. Kwiatkowski",
                        "slug": "T.-Kwiatkowski",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Kwiatkowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kwiatkowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48942032"
                        ],
                        "name": "Vitaly Nikolaev",
                        "slug": "Vitaly-Nikolaev",
                        "structuredName": {
                            "firstName": "Vitaly",
                            "lastName": "Nikolaev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vitaly Nikolaev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52578817"
                        ],
                        "name": "Jennimaria Palomaki",
                        "slug": "Jennimaria-Palomaki",
                        "structuredName": {
                            "firstName": "Jennimaria",
                            "lastName": "Palomaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jennimaria Palomaki"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 212657414,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6",
            "isKey": false,
            "numCitedBy": 178,
            "numCiting": 159,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Confidently making progress on multilingual modeling requires challenging, trustworthy evaluations. We present TyDi QA\u2014a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. The languages of TyDi QA are diverse with regard to their typology\u2014the set of linguistic features each language expresses\u2014such that we expect models performing well on this set to generalize across a large number of the world\u2019s languages. We present a quantitative analysis of the data quality and example-level qualitative linguistic analyses of observed language phenomena that would not be found in English-only corpora. To provide a realistic information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but don\u2019t know the answer yet, and the data is collected directly in each language without the use of translation."
            },
            "slug": "TyDi-QA:-A-Benchmark-for-Information-Seeking-in-Clark-Choi",
            "title": {
                "fragments": [],
                "text": "TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A quantitative analysis of the data quality and example-level qualitative linguistic analyses of observed language phenomena that would not be found in English-only corpora are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Transactions of the Association for Computational Linguistics"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064873282"
                        ],
                        "name": "Filip Grali'nski",
                        "slug": "Filip-Grali'nski",
                        "structuredName": {
                            "firstName": "Filip",
                            "lastName": "Grali'nski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Filip Grali'nski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1822665"
                        ],
                        "name": "Tomasz Stanislawek",
                        "slug": "Tomasz-Stanislawek",
                        "structuredName": {
                            "firstName": "Tomasz",
                            "lastName": "Stanislawek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomasz Stanislawek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065175441"
                        ],
                        "name": "Anna Wr'oblewska",
                        "slug": "Anna-Wr'oblewska",
                        "structuredName": {
                            "firstName": "Anna",
                            "lastName": "Wr'oblewska",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anna Wr'oblewska"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055968344"
                        ],
                        "name": "Dawid Lipi'nski",
                        "slug": "Dawid-Lipi'nski",
                        "structuredName": {
                            "firstName": "Dawid",
                            "lastName": "Lipi'nski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dawid Lipi'nski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064187520"
                        ],
                        "name": "Agnieszka Kaliska",
                        "slug": "Agnieszka-Kaliska",
                        "structuredName": {
                            "firstName": "Agnieszka",
                            "lastName": "Kaliska",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Agnieszka Kaliska"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066303178"
                        ],
                        "name": "Paulina Rosalska",
                        "slug": "Paulina-Rosalska",
                        "structuredName": {
                            "firstName": "Paulina",
                            "lastName": "Rosalska",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paulina Rosalska"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11016367"
                        ],
                        "name": "Bartosz Topolski",
                        "slug": "Bartosz-Topolski",
                        "structuredName": {
                            "firstName": "Bartosz",
                            "lastName": "Topolski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bartosz Topolski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144356944"
                        ],
                        "name": "P. Biecek",
                        "slug": "P.-Biecek",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Biecek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Biecek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "Consequently, the authors of Kleister proposed a set of handcrafted rules for the final selection of the entity values [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 159
                            }
                        ],
                        "text": "Authors dealing with property extraction rely on either manual annotation or the heuristic-based tagging procedure that impacts the overall end-to-end results [58, 13, 12, 21, 57, 39]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 212414676,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e2d2f64b3bb200c2c3db5ddc367b06311c369341",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "State-of-the-art solutions for Natural Language Processing (NLP) are able to capture a broad range of contexts, like the sentence-level context or document-level context for short documents. But these solutions are still struggling when it comes to longer, real-world documents with the information encoded in the spatial structure of the document, such as page elements like tables, forms, headers, openings or footers; complex page layout or presence of multiple pages. \nTo encourage progress on deeper and more complex Information Extraction (IE) we introduce a new task (named Kleister) with two new datasets. Utilizing both textual and structural layout features, an NLP system must find the most important information, about various types of entities, in long formal documents. We propose Pipeline method as a text-only baseline with different Named Entity Recognition architectures (Flair, BERT, RoBERTa). Moreover, we checked the most popular PDF processing tools for text extraction (pdf2djvu, Tesseract and Textract) in order to analyze behavior of IE system in presence of errors introduced by these tools."
            },
            "slug": "Kleister:-A-novel-task-for-Information-Extraction-Grali'nski-Stanislawek",
            "title": {
                "fragments": [],
                "text": "Kleister: A novel task for Information Extraction involving Long Documents with Complex Layout"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new task is introduced (named Kleister) with two new datasets to encourage progress on deeper and more complex Information Extraction (IE) and Pipeline method is proposed as a text-only baseline with different Named Entity Recognition architectures (Flair, BERT, RoBERTa)."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2091768"
                        ],
                        "name": "Kelvin Guu",
                        "slug": "Kelvin-Guu",
                        "structuredName": {
                            "firstName": "Kelvin",
                            "lastName": "Guu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kelvin Guu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2544107"
                        ],
                        "name": "Kenton Lee",
                        "slug": "Kenton-Lee",
                        "structuredName": {
                            "firstName": "Kenton",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenton Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9941702"
                        ],
                        "name": "Z. Tung",
                        "slug": "Z.-Tung",
                        "structuredName": {
                            "firstName": "Zora",
                            "lastName": "Tung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Tung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2616463"
                        ],
                        "name": "Panupong Pasupat",
                        "slug": "Panupong-Pasupat",
                        "structuredName": {
                            "firstName": "Panupong",
                            "lastName": "Pasupat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Panupong Pasupat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744179"
                        ],
                        "name": "Ming-Wei Chang",
                        "slug": "Ming-Wei-Chang",
                        "structuredName": {
                            "firstName": "Ming-Wei",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Wei Chang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 57
                            }
                        ],
                        "text": ", named entities are preferred rather than random tokens [47, 15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 211204736,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "832fff14d2ed50eb7969c4c4b976c35776548f56",
            "isKey": false,
            "numCitedBy": 405,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity."
            },
            "slug": "REALM:-Retrieval-Augmented-Language-Model-Guu-Lee",
            "title": {
                "fragments": [],
                "text": "REALM: Retrieval-Augmented Language Model Pre-Training"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The effectiveness of Retrieval-Augmented Language Model pre-training (REALM) is demonstrated by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA) and is found to outperform all previous methods by a significant margin, while also providing qualitative benefits such as interpretability and modularity."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34176020"
                        ],
                        "name": "Jesse Dodge",
                        "slug": "Jesse-Dodge",
                        "structuredName": {
                            "firstName": "Jesse",
                            "lastName": "Dodge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jesse Dodge"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2123694087"
                        ],
                        "name": "Gabriel Ilharco",
                        "slug": "Gabriel-Ilharco",
                        "structuredName": {
                            "firstName": "Gabriel",
                            "lastName": "Ilharco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gabriel Ilharco"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4671928"
                        ],
                        "name": "Roy Schwartz",
                        "slug": "Roy-Schwartz",
                        "structuredName": {
                            "firstName": "Roy",
                            "lastName": "Schwartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roy Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143787583"
                        ],
                        "name": "Ali Farhadi",
                        "slug": "Ali-Farhadi",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Farhadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ali Farhadi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2548384"
                        ],
                        "name": "Hannaneh Hajishirzi",
                        "slug": "Hannaneh-Hajishirzi",
                        "structuredName": {
                            "firstName": "Hannaneh",
                            "lastName": "Hajishirzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hannaneh Hajishirzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144365875"
                        ],
                        "name": "Noah A. Smith",
                        "slug": "Noah-A.-Smith",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Smith",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah A. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 163
                            }
                        ],
                        "text": "We report average results over two runs of each model varying only in the initial random seed to account for the impact of different initialization and data order [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 211132951,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "baf60d13c98916b77b09bc525ede1cd610ed1db5",
            "isKey": false,
            "numCitedBy": 221,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Fine-tuning pretrained contextual word embedding models to supervised downstream tasks has become commonplace in natural language processing. This process, however, is often brittle: even with the same hyperparameter values, distinct random seeds can lead to substantially different results. To better understand this phenomenon, we experiment with four datasets from the GLUE benchmark, fine-tuning BERT hundreds of times on each while varying only the random seeds. We find substantial performance increases compared to previously reported results, and we quantify how the performance of the best-found model varies as a function of the number of fine-tuning trials. Further, we examine two factors influenced by the choice of random seed: weight initialization and training data order. We find that both contribute comparably to the variance of out-of-sample performance, and that some weight initializations perform well across all tasks explored. On small datasets, we observe that many fine-tuning trials diverge part of the way through training, and we offer best practices for practitioners to stop training less promising runs early. We publicly release all of our experimental data, including training and validation scores for 2,100 trials, to encourage further analysis of training dynamics during fine-tuning."
            },
            "slug": "Fine-Tuning-Pretrained-Language-Models:-Weight-Data-Dodge-Ilharco",
            "title": {
                "fragments": [],
                "text": "Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work investigates how the performance of the best-found model varies as a function of the number of fine-tuning trials, and examines two factors influenced by the choice of random seed: weight initialization and training data order."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3032611"
                        ],
                        "name": "Yiheng Xu",
                        "slug": "Yiheng-Xu",
                        "structuredName": {
                            "firstName": "Yiheng",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiheng Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123545597"
                        ],
                        "name": "Minghao Li",
                        "slug": "Minghao-Li",
                        "structuredName": {
                            "firstName": "Minghao",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minghao Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145500855"
                        ],
                        "name": "Lei Cui",
                        "slug": "Lei-Cui",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Cui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Cui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110003"
                        ],
                        "name": "Shaohan Huang",
                        "slug": "Shaohan-Huang",
                        "structuredName": {
                            "firstName": "Shaohan",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaohan Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49807919"
                        ],
                        "name": "Furu Wei",
                        "slug": "Furu-Wei",
                        "structuredName": {
                            "firstName": "Furu",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Furu Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92660691"
                        ],
                        "name": "Ming Zhou",
                        "slug": "Ming-Zhou",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming Zhou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 194
                            }
                        ],
                        "text": "In the context of images, this niche was previously approached with an image-to-text cross-attention mechanism, alternatively, by adding visual features to word embeddings or concatenating them [37, 33, 35, 53, 56]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 196
                            }
                        ],
                        "text": "Several authors have shown that, when tasks involving 2D documents are considered, sequential models can be outperformed by considering layout information either directly as positional embeddings [17, 11, 56] or indirectly by allowing them to be contextualized on their spatial neighborhood [6, 57, 15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 159
                            }
                        ],
                        "text": "Authors dealing with property extraction rely on either manual annotation or the heuristic-based tagging procedure that impacts the overall end-to-end results [56, 52, 11, 18, 55, 36]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 209515395,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3465c06c872d8c48d628c5fc2d484087719351b6",
            "isKey": false,
            "numCitedBy": 164,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Pre-training techniques have been verified successfully in a variety of NLP tasks in recent years. Despite the widespread use of pre-training models for NLP applications, they almost exclusively focus on text-level manipulation, while neglecting layout and style information that is vital for document image understanding. In this paper, we propose the LayoutLM to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents. Furthermore, we also leverage image features to incorporate words' visual information into LayoutLM. To the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for document-level pre-training. It achieves new state-of-the-art results in several downstream tasks, including form understanding (from 70.72 to 79.27), receipt understanding (from 94.02 to 95.24) and document image classification (from 93.07 to 94.42). The code and pre-trained LayoutLM models are publicly available at https://aka.ms/layoutlm."
            },
            "slug": "LayoutLM:-Pre-training-of-Text-and-Layout-for-Image-Xu-Li",
            "title": {
                "fragments": [],
                "text": "LayoutLM: Pre-training of Text and Layout for Document Image Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The LayoutLM is proposed to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2126278"
                        ],
                        "name": "Jonathan Ho",
                        "slug": "Jonathan-Ho",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Ho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Ho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2583391"
                        ],
                        "name": "Nal Kalchbrenner",
                        "slug": "Nal-Kalchbrenner",
                        "structuredName": {
                            "firstName": "Nal",
                            "lastName": "Kalchbrenner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nal Kalchbrenner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3319373"
                        ],
                        "name": "Dirk Weissenborn",
                        "slug": "Dirk-Weissenborn",
                        "structuredName": {
                            "firstName": "Dirk",
                            "lastName": "Weissenborn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dirk Weissenborn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2887364"
                        ],
                        "name": "Tim Salimans",
                        "slug": "Tim-Salimans",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Salimans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Salimans"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 196
                            }
                        ],
                        "text": "Several authors have shown that, when tasks involving 2D documents are considered, sequential models can be outperformed by considering layout information either directly as positional embeddings [20, 12, 58] or indirectly by allowing them to be contextualized on their spatial neighborhood [7, 59, 18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 209323787,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "366244acdd930e488ae224ab6e2a92dc24aa7e06",
            "isKey": false,
            "numCitedBy": 167,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors. Existing autoregressive models either suffer from excessively large computational resource requirements for high dimensional data, or make compromises in terms of distribution expressiveness or ease of implementation in order to decrease resource requirements. Our architecture, by contrast, maintains both full expressiveness over joint distributions over data and ease of implementation with standard deep learning frameworks, while requiring reasonable memory and computation and achieving state-of-the-art results on standard generative modeling benchmarks. Our models are based on axial attention, a simple generalization of self-attention that naturally aligns with the multiple dimensions of the tensors in both the encoding and the decoding settings. Notably the proposed structure of the layers allows for the vast majority of the context to be computed in parallel during decoding without introducing any independence assumptions. This semi-parallel structure goes a long way to making decoding from even a very large Axial Transformer broadly applicable. We demonstrate state-of-the-art results for the Axial Transformer on the ImageNet-32 and ImageNet-64 image benchmarks as well as on the BAIR Robotic Pushing video benchmark. We open source the implementation of Axial Transformers."
            },
            "slug": "Axial-Attention-in-Multidimensional-Transformers-Ho-Kalchbrenner",
            "title": {
                "fragments": [],
                "text": "Axial Attention in Multidimensional Transformers"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "Axial Transformers is proposed, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors that maintains both full expressiveness over joint distributions over data and ease of implementation with standard deep learning frameworks, while requiring reasonable memory and computation."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7882243"
                        ],
                        "name": "Seunghyun Park",
                        "slug": "Seunghyun-Park",
                        "structuredName": {
                            "firstName": "Seunghyun",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seunghyun Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111068603"
                        ],
                        "name": "Seung Shin",
                        "slug": "Seung-Shin",
                        "structuredName": {
                            "firstName": "Seung",
                            "lastName": "Shin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seung Shin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2722273"
                        ],
                        "name": "Bado Lee",
                        "slug": "Bado-Lee",
                        "structuredName": {
                            "firstName": "Bado",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bado Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39924323"
                        ],
                        "name": "Junyeop Lee",
                        "slug": "Junyeop-Lee",
                        "structuredName": {
                            "firstName": "Junyeop",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junyeop Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10787779"
                        ],
                        "name": "Jaeheung Surh",
                        "slug": "Jaeheung-Surh",
                        "structuredName": {
                            "firstName": "Jaeheung",
                            "lastName": "Surh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jaeheung Surh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4418074"
                        ],
                        "name": "Minjoon Seo",
                        "slug": "Minjoon-Seo",
                        "structuredName": {
                            "firstName": "Minjoon",
                            "lastName": "Seo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minjoon Seo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72152162"
                        ],
                        "name": "Hwalsuk Lee",
                        "slug": "Hwalsuk-Lee",
                        "structuredName": {
                            "firstName": "Hwalsuk",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hwalsuk Lee"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "The CORD dataset [44] includes images of Indonesian receipts collected from shops and restaurants."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207900784,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c69942bf1b4f75e53cb62d0c5126c1cb4a5aa7bc",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "OCR is inevitably linked to NLP since its final output is in text. Advances in document intelligence are driving the need for a unified technology that integrates OCR with various NLP tasks, especially semantic parsing. Since OCR and semantic parsing have been studied as separate tasks so far, the datasets for each task on their own are rich, while those for the integrated post-OCR parsing tasks are relatively insufficient. In this study, we publish a consolidated dataset for receipt parsing as the first step towards post-OCR parsing tasks. The dataset consists of thousands of Indonesian receipts, which contains images and box/text annotations for OCR, and multi-level semantic labels for parsing. The proposed dataset can be used to address various OCR and parsing tasks."
            },
            "slug": "CORD:-A-Consolidated-Receipt-Dataset-for-Post-OCR-Park-Shin",
            "title": {
                "fragments": [],
                "text": "CORD: A Consolidated Receipt Dataset for Post-OCR Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A consolidated dataset for receipt parsing is published, which consists of thousands of Indonesian receipts, which contains images and box/text annotations for OCR, and multi-level semantic labels for parsing."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35084211"
                        ],
                        "name": "M. Lewis",
                        "slug": "M.-Lewis",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Lewis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11323179"
                        ],
                        "name": "Yinhan Liu",
                        "slug": "Yinhan-Liu",
                        "structuredName": {
                            "firstName": "Yinhan",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yinhan Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39589154"
                        ],
                        "name": "Naman Goyal",
                        "slug": "Naman-Goyal",
                        "structuredName": {
                            "firstName": "Naman",
                            "lastName": "Goyal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naman Goyal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2320509"
                        ],
                        "name": "Marjan Ghazvininejad",
                        "slug": "Marjan-Ghazvininejad",
                        "structuredName": {
                            "firstName": "Marjan",
                            "lastName": "Ghazvininejad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marjan Ghazvininejad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113947684"
                        ],
                        "name": "Abdelrahman Mohamed",
                        "slug": "Abdelrahman-Mohamed",
                        "structuredName": {
                            "firstName": "Abdelrahman",
                            "lastName": "Mohamed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abdelrahman Mohamed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39455775"
                        ],
                        "name": "Omer Levy",
                        "slug": "Omer-Levy",
                        "structuredName": {
                            "firstName": "Omer",
                            "lastName": "Levy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omer Levy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759422"
                        ],
                        "name": "Veselin Stoyanov",
                        "slug": "Veselin-Stoyanov",
                        "structuredName": {
                            "firstName": "Veselin",
                            "lastName": "Stoyanov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Veselin Stoyanov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1982950"
                        ],
                        "name": "Luke Zettlemoyer",
                        "slug": "Luke-Zettlemoyer",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Zettlemoyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luke Zettlemoyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 154
                            }
                        ],
                        "text": "Although this does not necessarily lead to the use of encoder-decoder models, several successful solutions relied on variants of Transformer architecture [54, 34, 9, 44]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 204960716,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
            "isKey": false,
            "numCitedBy": 2422,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance."
            },
            "slug": "BART:-Denoising-Sequence-to-Sequence-Pre-training-Lewis-Liu",
            "title": {
                "fragments": [],
                "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "BART is presented, a denoising autoencoder for pretraining sequence-to-sequence models, which matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2236429"
                        ],
                        "name": "Tushar Khot",
                        "slug": "Tushar-Khot",
                        "structuredName": {
                            "firstName": "Tushar",
                            "lastName": "Khot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tushar Khot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48323507"
                        ],
                        "name": "Peter Clark",
                        "slug": "Peter-Clark",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1983252"
                        ],
                        "name": "Michal Guerquin",
                        "slug": "Michal-Guerquin",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Guerquin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michal Guerquin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144949918"
                        ],
                        "name": "Peter Alexander Jansen",
                        "slug": "Peter-Alexander-Jansen",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Jansen",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Alexander Jansen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48229640"
                        ],
                        "name": "Ashish Sabharwal",
                        "slug": "Ashish-Sabharwal",
                        "structuredName": {
                            "firstName": "Ashish",
                            "lastName": "Sabharwal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashish Sabharwal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 7
                            }
                        ],
                        "text": "7 QASC [27] school-level science \u2212 \u2014 10."
                    },
                    "intents": []
                }
            ],
            "corpusId": 204915921,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c9b051b29feda7b62a4b683b1dfc37408724d8f5",
            "isKey": false,
            "numCitedBy": 114,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Composing knowledge from multiple pieces of texts is a key challenge in multi-hop question answering. We present a multi-hop reasoning dataset, Question Answering via Sentence Composition (QASC), that requires retrieving facts from a large corpus and composing them to answer a multiple-choice question. QASC is the first dataset to offer two desirable properties: (a) the facts to be composed are annotated in a large corpus, and (b) the decomposition into these facts is not evident from the question itself. The latter makes retrieval challenging as the system must introduce new concepts or relations in order to discover potential decompositions. Further, the reasoning model must then learn to identify valid compositions of these retrieved facts using common-sense reasoning. To help address these challenges, we provide annotation for supporting facts as well as their composition. Guided by these annotations, we present a two-step approach to mitigate the retrieval challenges. We use other multiple-choice datasets as additional training data to strengthen the reasoning model. Our proposed approach improves over current state-of-the-art language models by 11% (absolute). The reasoning and retrieval problems, however, remain unsolved as this model still lags by 20% behind human performance."
            },
            "slug": "QASC:-A-Dataset-for-Question-Answering-via-Sentence-Khot-Clark",
            "title": {
                "fragments": [],
                "text": "QASC: A Dataset for Question Answering via Sentence Composition"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This work presents a multi-hop reasoning dataset, Question Answering via Sentence Composition (QASC), that requires retrieving facts from a large corpus and composing them to answer a multiple-choice question, and provides annotation for supporting facts as well as their composition."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153368526"
                        ],
                        "name": "Timo I. Denk",
                        "slug": "Timo-I.-Denk",
                        "structuredName": {
                            "firstName": "Timo",
                            "lastName": "Denk",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timo I. Denk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9992847"
                        ],
                        "name": "C. Reisswig",
                        "slug": "C.-Reisswig",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Reisswig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Reisswig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 302,
                                "start": 291
                            }
                        ],
                        "text": "Several authors have shown that, when tasks involving 2D documents are considered, sequential models can be outperformed by considering layout information either directly as positional embeddings [20, 12, 58] or indirectly by allowing them to be contextualized on their spatial neighborhood [7, 59, 18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 202558968,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2fbda89395f993040b7665730c64182ade3be195",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "For understanding generic documents, information like font sizes, column layout, and generally the positioning of words may carry semantic information that is crucial for solving a downstream document intelligence task. Our novel BERTgrid, which is based on Chargrid by Katti et al. (2018), represents a document as a grid of contextualized word piece embedding vectors, thereby making its spatial structure and semantics accessible to the processing neural network. The contextualized embedding vectors are retrieved from a BERT language model. We use BERTgrid in combination with a fully convolutional network on a semantic instance segmentation task for extracting fields from invoices. We demonstrate its performance on tabulated line item and document header field extraction."
            },
            "slug": "BERTgrid:-Contextualized-Embedding-for-2D-Document-Denk-Reisswig",
            "title": {
                "fragments": [],
                "text": "BERTgrid: Contextualized Embedding for 2D Document Representation and Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The novel BERTgrid, which is based on Chargrid, represents a document as a grid of contextualized word piece embedding vectors, thereby making its spatial structure and semantics accessible to the processing neural network."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10324691"
                        ],
                        "name": "Kawin Ethayarajh",
                        "slug": "Kawin-Ethayarajh",
                        "structuredName": {
                            "firstName": "Kawin",
                            "lastName": "Ethayarajh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kawin Ethayarajh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 157
                            }
                        ],
                        "text": "Contextualized Word Embeddings are expected to capture context-dependent semantics and return a sequence of vectors associated with an entire input sequence [11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 202120592,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "9d7902e834d5d1d35179962c7a5b9d16623b0d39",
            "isKey": false,
            "numCitedBy": 279,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Replacing static word embeddings with contextualized word representations has yielded significant improvements on many NLP tasks. However, just how contextual are the contextualized representations produced by models such as ELMo and BERT? Are there infinitely many context-specific representations for each word, or are words essentially assigned one of a finite number of word-sense representations? For one, we find that the contextualized representations of all words are not isotropic in any layer of the contextualizing model. While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of LSTMs produce more task-specific representations. In all layers of ELMo, BERT, and GPT-2, on average, less than 5% of the variance in a word\u2019s contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations."
            },
            "slug": "How-Contextual-are-Contextualized-Word-Comparing-of-Ethayarajh",
            "title": {
                "fragments": [],
                "text": "How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is found that in all layers of ELMo, BERT, and GPT-2, on average, less than 5% of the variance in a word\u2019s contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualization representations."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145499378"
                        ],
                        "name": "Weijie Su",
                        "slug": "Weijie-Su",
                        "structuredName": {
                            "firstName": "Weijie",
                            "lastName": "Su",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weijie Su"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2578924"
                        ],
                        "name": "Xizhou Zhu",
                        "slug": "Xizhou-Zhu",
                        "structuredName": {
                            "firstName": "Xizhou",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xizhou Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112823372"
                        ],
                        "name": "Yue Cao",
                        "slug": "Yue-Cao",
                        "structuredName": {
                            "firstName": "Yue",
                            "lastName": "Cao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yue Cao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48218753"
                        ],
                        "name": "B. Li",
                        "slug": "B.-Li",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152309485"
                        ],
                        "name": "Lewei Lu",
                        "slug": "Lewei-Lu",
                        "structuredName": {
                            "firstName": "Lewei",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lewei Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49807919"
                        ],
                        "name": "Furu Wei",
                        "slug": "Furu-Wei",
                        "structuredName": {
                            "firstName": "Furu",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Furu Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3304536"
                        ],
                        "name": "Jifeng Dai",
                        "slug": "Jifeng-Dai",
                        "structuredName": {
                            "firstName": "Jifeng",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jifeng Dai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 194
                            }
                        ],
                        "text": "In the context of images, this niche was previously approached with an image-to-text cross-attention mechanism, alternatively, by adding visual features to word embeddings or concatenating them [40, 36, 38, 55, 58]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 201317624,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2527626c11a84f15709e943fbfa2356e19930e3b",
            "isKey": false,
            "numCitedBy": 707,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new pre-trainable generic representation for visual-linguistic tasks, called Visual-Linguistic BERT (VL-BERT for short). VL-BERT adopts the simple yet powerful Transformer model as the backbone, and extends it to take both visual and linguistic embedded features as input. In it, each element of the input is either of a word from the input sentence, or a region-of-interest (RoI) from the input image. It is designed to fit for most of the visual-linguistic downstream tasks. To better exploit the generic representation, we pre-train VL-BERT on the massive-scale Conceptual Captions dataset, together with text-only corpus. Extensive empirical analysis demonstrates that the pre-training procedure can better align the visual-linguistic clues and benefit the downstream tasks, such as visual commonsense reasoning, visual question answering and referring expression comprehension. It is worth noting that VL-BERT achieved the first place of single model on the leaderboard of the VCR benchmark. Code is released at \\url{this https URL}."
            },
            "slug": "VL-BERT:-Pre-training-of-Generic-Visual-Linguistic-Su-Zhu",
            "title": {
                "fragments": [],
                "text": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A new pre-trainable generic representation for visual-linguistic tasks, called Visual-Linguistic BERT (VL-BERT), which adopts the simple yet powerful Transformer model as the backbone, and extends it to take both visual and linguistic embedded features as input."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32562635"
                        ],
                        "name": "Liunian Harold Li",
                        "slug": "Liunian-Harold-Li",
                        "structuredName": {
                            "firstName": "Liunian",
                            "lastName": "Li",
                            "middleNames": [
                                "Harold"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liunian Harold Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064210"
                        ],
                        "name": "Mark Yatskar",
                        "slug": "Mark-Yatskar",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Yatskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Yatskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144508458"
                        ],
                        "name": "Da Yin",
                        "slug": "Da-Yin",
                        "structuredName": {
                            "firstName": "Da",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Da Yin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793529"
                        ],
                        "name": "Cho-Jui Hsieh",
                        "slug": "Cho-Jui-Hsieh",
                        "structuredName": {
                            "firstName": "Cho-Jui",
                            "lastName": "Hsieh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cho-Jui Hsieh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2782886"
                        ],
                        "name": "Kai-Wei Chang",
                        "slug": "Kai-Wei-Chang",
                        "structuredName": {
                            "firstName": "Kai-Wei",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai-Wei Chang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 194
                            }
                        ],
                        "text": "In the context of images, this niche was previously approached with an image-to-text cross-attention mechanism, alternatively, by adding visual features to word embeddings or concatenating them [40, 36, 38, 55, 58]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 199528533,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5aec474c31a2f4b74703c6f786c0a8ff85c450da",
            "isKey": false,
            "numCitedBy": 630,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose VisualBERT, a simple and flexible framework for modeling a broad range of vision-and-language tasks. VisualBERT consists of a stack of Transformer layers that implicitly align elements of an input text and regions in an associated input image with self-attention. We further propose two visually-grounded language model objectives for pre-training VisualBERT on image caption data. Experiments on four vision-and-language tasks including VQA, VCR, NLVR2, and Flickr30K show that VisualBERT outperforms or rivals with state-of-the-art models while being significantly simpler. Further analysis demonstrates that VisualBERT can ground elements of language to image regions without any explicit supervision and is even sensitive to syntactic relationships, tracking, for example, associations between verbs and image regions corresponding to their arguments."
            },
            "slug": "VisualBERT:-A-Simple-and-Performant-Baseline-for-Li-Yatskar",
            "title": {
                "fragments": [],
                "text": "VisualBERT: A Simple and Performant Baseline for Vision and Language"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Analysis demonstrates that VisualBERT can ground elements of language to image regions without any explicit supervision and is even sensitive to syntactic relationships, tracking, for example, associations between verbs and image regions corresponding to their arguments."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064728738"
                        ],
                        "name": "Hung Le",
                        "slug": "Hung-Le",
                        "structuredName": {
                            "firstName": "Hung",
                            "lastName": "Le",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hung Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36187119"
                        ],
                        "name": "Doyen Sahoo",
                        "slug": "Doyen-Sahoo",
                        "structuredName": {
                            "firstName": "Doyen",
                            "lastName": "Sahoo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Doyen Sahoo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2185019"
                        ],
                        "name": "Nancy F. Chen",
                        "slug": "Nancy-F.-Chen",
                        "structuredName": {
                            "firstName": "Nancy",
                            "lastName": "Chen",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nancy F. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741126"
                        ],
                        "name": "S. Hoi",
                        "slug": "S.-Hoi",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Hoi",
                            "middleNames": [
                                "C.",
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hoi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 87
                            }
                        ],
                        "text": "Our model was validated on series of experiments involving Key Information Extraction, Visual Question Answering, classification of rich documents, and Question Answering from layout-rich texts."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 172
                            }
                        ],
                        "text": "The relationships between text and other media have been previously studied in Visual Commonsense Reasoning, Video-Grounded Dialogue, Speech, and Visual Question Answering [16, 35, 4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 145
                            }
                        ],
                        "text": "The relationships between text and other media have been previously studied in Visual Commonsense Reasoning, VideoGrounded Dialogue, Speech, and Visual Question Answering [13, 32, 3]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 195776147,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "594ad264d6b92afb9d13cb56ad8ffadba94a9f7a",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "Developing Video-Grounded Dialogue Systems (VGDS), where a dialogue is conducted based on visual and audio aspects of a given video, is significantly more challenging than traditional image or text-grounded dialogue systems because (1) feature space of videos span across multiple picture frames, making it difficult to obtain semantic information; and (2) a dialogue agent must perceive and process information from different modalities (audio, video, caption, etc.) to obtain a comprehensive understanding. Most existing work is based on RNNs and sequence-to-sequence architectures, which are not very effective for capturing complex long-term dependencies (like in videos). To overcome this, we propose Multimodal Transformer Networks (MTN) to encode videos and incorporate information from different modalities. We also propose query-aware attention through an auto-encoder to extract query-aware features from non-text modalities. We develop a training procedure to simulate token-level decoding to improve the quality of generated responses during inference. We get state of the art performance on Dialogue System Technology Challenge 7 (DSTC7). Our model also generalizes to another multimodal visual-grounded dialogue task, and obtains promising performance."
            },
            "slug": "Multimodal-Transformer-Networks-for-End-to-End-Le-Sahoo",
            "title": {
                "fragments": [],
                "text": "Multimodal Transformer Networks for End-to-End Video-Grounded Dialogue Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A training procedure to simulate token-level decoding to improve the quality of generated responses during inference and a proposed Multimodal Transformer Networks (MTN) to encode videos and incorporate information from different modalities."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35570245"
                        ],
                        "name": "Ali Furkan Biten",
                        "slug": "Ali-Furkan-Biten",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Biten",
                            "middleNames": [
                                "Furkan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ali Furkan Biten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "134682605"
                        ],
                        "name": "Rub\u00e8n P\u00e9rez Tito",
                        "slug": "Rub\u00e8n-P\u00e9rez-Tito",
                        "structuredName": {
                            "firstName": "Rub\u00e8n",
                            "lastName": "Tito",
                            "middleNames": [
                                "P\u00e9rez"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rub\u00e8n P\u00e9rez Tito"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51238351"
                        ],
                        "name": "Andr\u00e9s Mafla",
                        "slug": "Andr\u00e9s-Mafla",
                        "structuredName": {
                            "firstName": "Andr\u00e9s",
                            "lastName": "Mafla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andr\u00e9s Mafla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51231577"
                        ],
                        "name": "Llu\u00eds G\u00f3mez",
                        "slug": "Llu\u00eds-G\u00f3mez",
                        "structuredName": {
                            "firstName": "Llu\u00eds",
                            "lastName": "G\u00f3mez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Llu\u00eds G\u00f3mez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143823474"
                        ],
                        "name": "Mar\u00e7al Rusi\u00f1ol",
                        "slug": "Mar\u00e7al-Rusi\u00f1ol",
                        "structuredName": {
                            "firstName": "Mar\u00e7al",
                            "lastName": "Rusi\u00f1ol",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mar\u00e7al Rusi\u00f1ol"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34317896"
                        ],
                        "name": "Minesh Mathew",
                        "slug": "Minesh-Mathew",
                        "structuredName": {
                            "firstName": "Minesh",
                            "lastName": "Mathew",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minesh Mathew"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694502"
                        ],
                        "name": "C. Jawahar",
                        "slug": "C.-Jawahar",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Jawahar",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jawahar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2864362"
                        ],
                        "name": "Ernest Valveny",
                        "slug": "Ernest-Valveny",
                        "structuredName": {
                            "firstName": "Ernest",
                            "lastName": "Valveny",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ernest Valveny"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694974"
                        ],
                        "name": "Dimosthenis Karatzas",
                        "slug": "Dimosthenis-Karatzas",
                        "structuredName": {
                            "firstName": "Dimosthenis",
                            "lastName": "Karatzas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dimosthenis Karatzas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 195767166,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b3b0d8d5a24630c940049442a8d824534faf8b9",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents final results of ICDAR 2019 Scene Text Visual Question Answering competition (ST-VQA). ST-VQA introduces an important aspect that is not addressed by any Visual Question Answering system up to date, namely the incorporation of scene text to answer questions asked about an image. The competition introduces a new dataset comprising 23,038 images annotated with 31,791 question / answer pairs where the answer is always grounded on text instances present in the image. The images are taken from 7 different public computer vision datasets, covering a wide range of scenarios. The competition was structured in three tasks of increasing difficulty, that require reading the text in a scene and understanding it in the context of the scene, to correctly answer a given question. A novel evaluation metric is presented, which elegantly assesses both key capabilities expected from an optimal model: text recognition and image understanding. A detailed analysis of results from different participants is showcased, which provides insight into the current capabilities of VQA systems that can read. We firmly believe the dataset proposed in this challenge will be an important milestone to consider towards a path of more robust and general models that can exploit scene text to achieve holistic image understanding."
            },
            "slug": "ICDAR-2019-Competition-on-Scene-Text-Visual-Biten-Tito",
            "title": {
                "fragments": [],
                "text": "ICDAR 2019 Competition on Scene Text Visual Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "This paper presents final results of ICDAR 2019 Scene Text Visual Question Answering competition (ST-VQA), which introduces a new dataset comprising 23,038 images annotated with 31,791 question / answer pairs where the answer is always grounded on text instances present in the image."
            },
            "venue": {
                "fragments": [],
                "text": "2019 International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35685584"
                        ],
                        "name": "Guillaume Jaume",
                        "slug": "Guillaume-Jaume",
                        "structuredName": {
                            "firstName": "Guillaume",
                            "lastName": "Jaume",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guillaume Jaume"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3025777"
                        ],
                        "name": "H. K. Ekenel",
                        "slug": "H.-K.-Ekenel",
                        "structuredName": {
                            "firstName": "Hazim",
                            "lastName": "Ekenel",
                            "middleNames": [
                                "Kemal"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. K. Ekenel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710257"
                        ],
                        "name": "J. Thiran",
                        "slug": "J.-Thiran",
                        "structuredName": {
                            "firstName": "Jean-Philippe",
                            "lastName": "Thiran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Thiran"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 173188931,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "58c793e278cdbf669a615b2c2479cd69ff785d63",
            "isKey": false,
            "numCitedBy": 72,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new dataset for form understanding in noisy scanned documents (FUNSD) that aims at extracting and structuring the textual content of forms. The dataset comprises 199 real, fully annotated, scanned forms. The documents are noisy and vary widely in appearance, making form understanding (FoUn) a challenging task. The proposed dataset can be used for various tasks, including text detection, optical character recognition, spatial layout analysis, and entity labeling/linking. To the best of our knowledge, this is the first publicly available dataset with comprehensive annotations to address FoUn task. We also present a set of baselines and introduce metrics to evaluate performance on the FUNSD dataset, which can be downloaded at https://guillaumejaume.github.io/FUNSD."
            },
            "slug": "FUNSD:-A-Dataset-for-Form-Understanding-in-Noisy-Jaume-Ekenel",
            "title": {
                "fragments": [],
                "text": "FUNSD: A Dataset for Form Understanding in Noisy Scanned Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This work presents a new dataset for form understanding in noisy scanned documents (FUNSD) that aims at extracting and structuring the textual content of forms, and is the first publicly available dataset with comprehensive annotations to address FoUn task."
            },
            "venue": {
                "fragments": [],
                "text": "2019 International Conference on Document Analysis and Recognition Workshops (ICDARW)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15652489"
                        ],
                        "name": "T. Kwiatkowski",
                        "slug": "T.-Kwiatkowski",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Kwiatkowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kwiatkowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52578817"
                        ],
                        "name": "Jennimaria Palomaki",
                        "slug": "Jennimaria-Palomaki",
                        "structuredName": {
                            "firstName": "Jennimaria",
                            "lastName": "Palomaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jennimaria Palomaki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "90784578"
                        ],
                        "name": "Olivia Redfield",
                        "slug": "Olivia-Redfield",
                        "structuredName": {
                            "firstName": "Olivia",
                            "lastName": "Redfield",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olivia Redfield"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123052390"
                        ],
                        "name": "Michael Collins",
                        "slug": "Michael-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144729897"
                        ],
                        "name": "Ankur P. Parikh",
                        "slug": "Ankur-P.-Parikh",
                        "structuredName": {
                            "firstName": "Ankur",
                            "lastName": "Parikh",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ankur P. Parikh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114577307"
                        ],
                        "name": "Chris Alberti",
                        "slug": "Chris-Alberti",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Alberti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Alberti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153215783"
                        ],
                        "name": "D. Epstein",
                        "slug": "D.-Epstein",
                        "structuredName": {
                            "firstName": "Danielle",
                            "lastName": "Epstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Epstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3443442"
                        ],
                        "name": "Illia Polosukhin",
                        "slug": "Illia-Polosukhin",
                        "structuredName": {
                            "firstName": "Illia",
                            "lastName": "Polosukhin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Illia Polosukhin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39172707"
                        ],
                        "name": "Jacob Devlin",
                        "slug": "Jacob-Devlin",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Devlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacob Devlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2544107"
                        ],
                        "name": "Kenton Lee",
                        "slug": "Kenton-Lee",
                        "structuredName": {
                            "firstName": "Kenton",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenton Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3259253"
                        ],
                        "name": "Kristina Toutanova",
                        "slug": "Kristina-Toutanova",
                        "structuredName": {
                            "firstName": "Kristina",
                            "lastName": "Toutanova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristina Toutanova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145024664"
                        ],
                        "name": "Llion Jones",
                        "slug": "Llion-Jones",
                        "structuredName": {
                            "firstName": "Llion",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Llion Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2554321"
                        ],
                        "name": "Matthew Kelcey",
                        "slug": "Matthew-Kelcey",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Kelcey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew Kelcey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744179"
                        ],
                        "name": "Ming-Wei Chang",
                        "slug": "Ming-Wei-Chang",
                        "structuredName": {
                            "firstName": "Ming-Wei",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Wei Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2555924"
                        ],
                        "name": "Andrew M. Dai",
                        "slug": "Andrew-M.-Dai",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Dai",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew M. Dai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39328010"
                        ],
                        "name": "Jakob Uszkoreit",
                        "slug": "Jakob-Uszkoreit",
                        "structuredName": {
                            "firstName": "Jakob",
                            "lastName": "Uszkoreit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jakob Uszkoreit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754497"
                        ],
                        "name": "Slav Petrov",
                        "slug": "Slav-Petrov",
                        "structuredName": {
                            "firstName": "Slav",
                            "lastName": "Petrov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Slav Petrov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 86611921,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "17dbd7b72029181327732e4d11b52a08ed4630d0",
            "isKey": false,
            "numCitedBy": 896,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature."
            },
            "slug": "Natural-Questions:-A-Benchmark-for-Question-Kwiatkowski-Palomaki",
            "title": {
                "fragments": [],
                "text": "Natural Questions: A Benchmark for Question Answering Research"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The Natural Questions corpus, a question answering data set, is presented, introducing robust metrics for the purposes of evaluating question answering systems; demonstrating high human upper bounds on these metrics; and establishing baseline results using competitive methods drawn from related literature."
            },
            "venue": {
                "fragments": [],
                "text": "TACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110965354"
                        ],
                        "name": "Xiaojing Liu",
                        "slug": "Xiaojing-Liu",
                        "structuredName": {
                            "firstName": "Xiaojing",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojing Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112256"
                        ],
                        "name": "Feiyu Gao",
                        "slug": "Feiyu-Gao",
                        "structuredName": {
                            "firstName": "Feiyu",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Feiyu Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112207346"
                        ],
                        "name": "Qiong Zhang",
                        "slug": "Qiong-Zhang",
                        "structuredName": {
                            "firstName": "Qiong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiong Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36530509"
                        ],
                        "name": "Huasha Zhao",
                        "slug": "Huasha-Zhao",
                        "structuredName": {
                            "firstName": "Huasha",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huasha Zhao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 159
                            }
                        ],
                        "text": "Authors dealing with property extraction rely on either manual annotation or the heuristic-based tagging procedure that impacts the overall end-to-end results [58, 13, 12, 21, 57, 39]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 85528598,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04df8c70257b5280b9d303502c9d7ddf946f181b",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Visually rich documents (VRDs) are ubiquitous in daily business and life. Examples are purchase receipts, insurance policy documents, custom declaration forms and so on. In VRDs, visual and layout information is critical for document understanding, and texts in such documents cannot be serialized into the one-dimensional sequence without losing information. Classic information extraction models such as BiLSTM-CRF typically operate on text sequences and do not incorporate visual features. In this paper, we introduce a graph convolution based model to combine textual and visual information presented in VRDs. Graph embeddings are trained to summarize the context of a text segment in the document, and further combined with text embeddings for entity extraction. Extensive experiments have been conducted to show that our method outperforms BiLSTM-CRF baselines by significant margins, on two real-world datasets. Additionally, ablation studies are also performed to evaluate the effectiveness of each component of our model."
            },
            "slug": "Graph-Convolution-for-Multimodal-Information-from-Liu-Gao",
            "title": {
                "fragments": [],
                "text": "Graph Convolution for Multimodal Information Extraction from Visually Rich Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper introduces a graph convolution based model to combine textual and visual information presented in VRDs and outperforms BiLSTM-CRF baselines by significant margins, on two real-world datasets."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50286460"
                        ],
                        "name": "Amanpreet Singh",
                        "slug": "Amanpreet-Singh",
                        "structuredName": {
                            "firstName": "Amanpreet",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amanpreet Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144223091"
                        ],
                        "name": "Vivek Natarajan",
                        "slug": "Vivek-Natarajan",
                        "structuredName": {
                            "firstName": "Vivek",
                            "lastName": "Natarajan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vivek Natarajan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144826412"
                        ],
                        "name": "Meet Shah",
                        "slug": "Meet-Shah",
                        "structuredName": {
                            "firstName": "Meet",
                            "lastName": "Shah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Meet Shah"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116341314"
                        ],
                        "name": "Yu Jiang",
                        "slug": "Yu-Jiang",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39717886"
                        ],
                        "name": "Xinlei Chen",
                        "slug": "Xinlei-Chen",
                        "structuredName": {
                            "firstName": "Xinlei",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinlei Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746610"
                        ],
                        "name": "Dhruv Batra",
                        "slug": "Dhruv-Batra",
                        "structuredName": {
                            "firstName": "Dhruv",
                            "lastName": "Batra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dhruv Batra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153432684"
                        ],
                        "name": "Devi Parikh",
                        "slug": "Devi-Parikh",
                        "structuredName": {
                            "firstName": "Devi",
                            "lastName": "Parikh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Devi Parikh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34849128"
                        ],
                        "name": "Marcus Rohrbach",
                        "slug": "Marcus-Rohrbach",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Rohrbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus Rohrbach"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 85553602,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "af1f7739283bdbd2b7a94903041f6d6afd991907",
            "isKey": false,
            "numCitedBy": 180,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "Studies have shown that a dominant class of questions asked by visually impaired users on images of their surroundings involves reading text in the image. But today\u2019s VQA models can not read! Our paper takes a first step towards addressing this problem. First, we introduce a new \u201cTextVQA\u201d dataset to facilitate progress on this important problem. Existing datasets either have a small proportion of questions about text (e.g., the VQA dataset) or are too small (e.g., the VizWiz dataset). TextVQA contains 45,336 questions on 28,408 images that require reasoning about text to answer. Second, we introduce a novel model architecture that reads text in the image, reasons about it in the context of the image and the question, and predicts an answer which might be a deduction based on the text and the image or composed of the strings found in the image. Consequently, we call our approach Look, Read, Reason & Answer (LoRRA). We show that LoRRA outperforms existing state-of-the-art VQA models on our TextVQA dataset. We find that the gap between human performance and machine performance is significantly larger on TextVQA than on VQA 2.0, suggesting that TextVQA is well-suited to benchmark progress along directions complementary to VQA 2.0."
            },
            "slug": "Towards-VQA-Models-That-Can-Read-Singh-Natarajan",
            "title": {
                "fragments": [],
                "text": "Towards VQA Models That Can Read"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel model architecture is introduced that reads text in the image, reasons about it in the context of the image and the question, and predicts an answer which might be a deduction based on the text and the image or composed of the strings found in the images."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33546336"
                        ],
                        "name": "Dheeru Dua",
                        "slug": "Dheeru-Dua",
                        "structuredName": {
                            "firstName": "Dheeru",
                            "lastName": "Dua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dheeru Dua"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705260"
                        ],
                        "name": "Yizhong Wang",
                        "slug": "Yizhong-Wang",
                        "structuredName": {
                            "firstName": "Yizhong",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yizhong Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2697425"
                        ],
                        "name": "Pradeep Dasigi",
                        "slug": "Pradeep-Dasigi",
                        "structuredName": {
                            "firstName": "Pradeep",
                            "lastName": "Dasigi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pradeep Dasigi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157025"
                        ],
                        "name": "Gabriel Stanovsky",
                        "slug": "Gabriel-Stanovsky",
                        "structuredName": {
                            "firstName": "Gabriel",
                            "lastName": "Stanovsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gabriel Stanovsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34650964"
                        ],
                        "name": "Sameer Singh",
                        "slug": "Sameer-Singh",
                        "structuredName": {
                            "firstName": "Sameer",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sameer Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40642935"
                        ],
                        "name": "Matt Gardner",
                        "slug": "Matt-Gardner",
                        "structuredName": {
                            "firstName": "Matt",
                            "lastName": "Gardner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matt Gardner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 67855846,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dda6fb309f62e2557a071522354d8c2c897a2805",
            "isKey": false,
            "numCitedBy": 350,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task. However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done. We introduce a new reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 55k-question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs, as they remove the paraphrase-and-entity-typing shortcuts available in prior datasets. We apply state-of-the-art methods from both the reading comprehension and semantic parsing literatures on this dataset and show that the best systems only achieve 38.4% F1 on our generalized accuracy metric, while expert human performance is 96%. We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 51% F1."
            },
            "slug": "DROP:-A-Reading-Comprehension-Benchmark-Requiring-Dua-Wang",
            "title": {
                "fragments": [],
                "text": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A new reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs, and presents a new model that combines reading comprehension methods with simple numerical reasoning to achieve 51% F1."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "83471827"
                        ],
                        "name": "Minseok Cho",
                        "slug": "Minseok-Cho",
                        "structuredName": {
                            "firstName": "Minseok",
                            "lastName": "Cho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minseok Cho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "23181472"
                        ],
                        "name": "Reinald Kim Amplayo",
                        "slug": "Reinald-Kim-Amplayo",
                        "structuredName": {
                            "firstName": "Reinald",
                            "lastName": "Amplayo",
                            "middleNames": [
                                "Kim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Reinald Kim Amplayo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716415"
                        ],
                        "name": "Seung-won Hwang",
                        "slug": "Seung-won-Hwang",
                        "structuredName": {
                            "firstName": "Seung-won",
                            "lastName": "Hwang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seung-won Hwang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2300225"
                        ],
                        "name": "Jonghyuck Park",
                        "slug": "Jonghyuck-Park",
                        "structuredName": {
                            "firstName": "Jonghyuck",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonghyuck Park"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 53013017,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f2bb157c66870e5abf3d0873888c231445bd16a",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "The task of answering a question given a text passage has shown great developments on model performance thanks to community efforts in building useful datasets. Recently, there have been doubts whether such rapid progress has been based on truly understanding language. The same question has not been asked in the table question answering (TableQA) task, where we are tasked to answer a query given a table. We show that existing efforts, of using \"answers\" for both evaluation and supervision for TableQA, show deteriorating performances in adversarial settings of perturbations that do not affect the answer. This insight naturally motivates to develop new models that understand question and table more precisely. For this goal, we propose Neural Operator (NeOp), a multi-layer sequential network with attention supervision to answer the query given a table. NeOp uses multiple Selective Recurrent Units (SelRUs) to further help the interpretability of the answers of the model. Experiments show that the use of operand information to train the model significantly improves the performance and interpretability of TableQA models. NeOp outperforms all the previous models by a big margin."
            },
            "slug": "Adversarial-TableQA:-Attention-Supervision-for-on-Cho-Amplayo",
            "title": {
                "fragments": [],
                "text": "Adversarial TableQA: Attention Supervision for Question Answering on Tables"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "NeOp, a multi-layer sequential network with attention supervision to answer the query given a table, which outperforms all the previous models by a big margin and uses multiple Selective Recurrent Units to further help the interpretability of the answers of the model."
            },
            "venue": {
                "fragments": [],
                "text": "ACML"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2890423"
                        ],
                        "name": "Eunsol Choi",
                        "slug": "Eunsol-Choi",
                        "structuredName": {
                            "firstName": "Eunsol",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eunsol Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144533687"
                        ],
                        "name": "He He",
                        "slug": "He-He",
                        "structuredName": {
                            "firstName": "He",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "He He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2136562"
                        ],
                        "name": "Mohit Iyyer",
                        "slug": "Mohit-Iyyer",
                        "structuredName": {
                            "firstName": "Mohit",
                            "lastName": "Iyyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohit Iyyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064210"
                        ],
                        "name": "Mark Yatskar",
                        "slug": "Mark-Yatskar",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Yatskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Yatskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144105277"
                        ],
                        "name": "Wen-tau Yih",
                        "slug": "Wen-tau-Yih",
                        "structuredName": {
                            "firstName": "Wen-tau",
                            "lastName": "Yih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wen-tau Yih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699545"
                        ],
                        "name": "Yejin Choi",
                        "slug": "Yejin-Choi",
                        "structuredName": {
                            "firstName": "Yejin",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yejin Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145419642"
                        ],
                        "name": "Percy Liang",
                        "slug": "Percy-Liang",
                        "structuredName": {
                            "firstName": "Percy",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Percy Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1982950"
                        ],
                        "name": "Luke Zettlemoyer",
                        "slug": "Luke-Zettlemoyer",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Zettlemoyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luke Zettlemoyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 52057510,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39e734da43eb8c72e9549b42e96760545036f8e5",
            "isKey": false,
            "numCitedBy": 434,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We present QuAC, a dataset for Question Answering in Context that contains 14K information-seeking QA dialogs (100K questions in total). The dialogs involve two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context, as we show in a detailed qualitative evaluation. We also report results for a number of reference models, including a recently state-of-the-art reading comprehension architecture extended to model dialog context. Our best model underperforms humans by 20 F1, suggesting that there is significant room for future work on this data. Dataset, baseline, and leaderboard available at http://quac.ai."
            },
            "slug": "QuAC:-Question-Answering-in-Context-Choi-He",
            "title": {
                "fragments": [],
                "text": "QuAC: Question Answering in Context"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context, as it shows in a detailed qualitative evaluation."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145732771"
                        ],
                        "name": "Siva Reddy",
                        "slug": "Siva-Reddy",
                        "structuredName": {
                            "firstName": "Siva",
                            "lastName": "Reddy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Siva Reddy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50536468"
                        ],
                        "name": "Danqi Chen",
                        "slug": "Danqi-Chen",
                        "structuredName": {
                            "firstName": "Danqi",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danqi Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 52055325,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "990a7b4eceedb6e053e6386269481bdfc42a1094",
            "isKey": false,
            "numCitedBy": 616,
            "numCiting": 112,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans gather information through conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets (e.g., coreference and pragmatic reasoning). We evaluate strong dialogue and reading comprehension models on CoQA. The best system obtains an F1 score of 65.4%, which is 23.4 points behind human performance (88.8%), indicating that there is ample room for improvement. We present CoQA as a challenge to the community at https://stanfordnlp.github.io/coqa."
            },
            "slug": "CoQA:-A-Conversational-Question-Answering-Challenge-Reddy-Chen",
            "title": {
                "fragments": [],
                "text": "CoQA: A Conversational Question Answering Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "CoQA is introduced, a novel dataset for building Conversational Question Answering systems and it is shown that conversational questions have challenging phenomena not present in existing reading comprehension datasets (e.g., coreference and pragmatic reasoning)."
            },
            "venue": {
                "fragments": [],
                "text": "TACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143775536"
                        ],
                        "name": "Bryan McCann",
                        "slug": "Bryan-McCann",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "McCann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan McCann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2844898"
                        ],
                        "name": "N. Keskar",
                        "slug": "N.-Keskar",
                        "structuredName": {
                            "firstName": "Nitish",
                            "lastName": "Keskar",
                            "middleNames": [
                                "Shirish"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Keskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2228109"
                        ],
                        "name": "Caiming Xiong",
                        "slug": "Caiming-Xiong",
                        "structuredName": {
                            "firstName": "Caiming",
                            "lastName": "Xiong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Caiming Xiong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 99
                            }
                        ],
                        "text": "The QA program of unifying NLP frames all the problems as triplets of question, context and answer [32, 42, 29] or item, property name and answer [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 142
                            }
                        ],
                        "text": "Most tasks in Natural Language Processing can be unified under one framework by casting them as triplets of the question, context, and answer [32, 42, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 49393754,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "9784fbf77295860b2e412137b86356d70b25e3c0",
            "isKey": false,
            "numCitedBy": 410,
            "numCiting": 138,
            "paperAbstract": {
                "fragments": [],
                "text": "Presented on August 28, 2018 at 12:15 p.m. in the Pettit Microelectronics Research Center, Room 102 A/B."
            },
            "slug": "The-Natural-Language-Decathlon:-Multitask-Learning-McCann-Keskar",
            "title": {
                "fragments": [],
                "text": "The Natural Language Decathlon: Multitask Learning as Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Presented on August 28, 2018 at 12:15 p.m. in the Pettit Microelectronics Research Center, Room 102 A/B."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765329"
                        ],
                        "name": "Taku Kudo",
                        "slug": "Taku-Kudo",
                        "structuredName": {
                            "firstName": "Taku",
                            "lastName": "Kudo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Taku Kudo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 21
                            }
                        ],
                        "text": "Subword tokenization [52, 31] was proposed to solve the word sparsity problem and keep the vocabulary at a reasonable size."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13753208,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e",
            "isKey": false,
            "numCitedBy": 541,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Subword units are an effective way to alleviate the open vocabulary problems in neural machine translation (NMT). While sentences are usually converted into unique subword sequences, subword segmentation is potentially ambiguous and multiple segmentations are possible even with the same vocabulary. The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a noise to improve the robustness of NMT. We present a simple regularization method, subword regularization, which trains the model with multiple subword segmentations probabilistically sampled during training. In addition, for better subword sampling, we propose a new subword segmentation algorithm based on a unigram language model. We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings."
            },
            "slug": "Subword-Regularization:-Improving-Neural-Network-Kudo",
            "title": {
                "fragments": [],
                "text": "Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A simple regularization method is presented, subword regularization, which trains the model with multiple subword segmentations probabilistically sampled during training, and a new sub word segmentation algorithm based on a unigram language model is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1863953"
                        ],
                        "name": "Kuang-Huei Lee",
                        "slug": "Kuang-Huei-Lee",
                        "structuredName": {
                            "firstName": "Kuang-Huei",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kuang-Huei Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145307428"
                        ],
                        "name": "Xi Chen",
                        "slug": "Xi-Chen",
                        "structuredName": {
                            "firstName": "Xi",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xi Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144988571"
                        ],
                        "name": "G. Hua",
                        "slug": "G.-Hua",
                        "structuredName": {
                            "firstName": "Gang",
                            "lastName": "Hua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Hua"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35431603"
                        ],
                        "name": "Houdong Hu",
                        "slug": "Houdong-Hu",
                        "structuredName": {
                            "firstName": "Houdong",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Houdong Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722627"
                        ],
                        "name": "Xiaodong He",
                        "slug": "Xiaodong-He",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong He"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 194
                            }
                        ],
                        "text": "In the context of images, this niche was previously approached with an image-to-text cross-attention mechanism, alternatively, by adding visual features to word embeddings or concatenating them [40, 36, 38, 55, 58]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3994012,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "45dd2a3cd7c27f2e9509b023d702408f5ac11c9d",
            "isKey": false,
            "numCitedBy": 481,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we study the problem of image-text matching. Inferring the latent semantic alignment between objects or other salient stuffs (e.g. snow, sky, lawn) and the corresponding words in sentences allows to capture fine-grained interplay between vision and language, and makes image-text matching more interpretable. Prior works either simply aggregate the similarity of all possible pairs of regions and words without attending differentially to more and less important words or regions, or use a multi-step attentional process to capture limited number of semantic alignments which is less interpretable. In this paper, we present Stacked Cross Attention to discover the full latent alignments using both image regions and words in sentence as context and infer the image-text similarity. Our approach achieves the state-of-the-art results on the MS-COCO and Flickr30K datasets. On Flickr30K, our approach outperforms the current best methods by 22.1% in text retrieval from image query, and 18.2% in image retrieval with text query (based on Recall@1). On MS-COCO, our approach improves sentence retrieval by 17.8% and image retrieval by 16.6% (based on Recall@1 using the 5K test set)."
            },
            "slug": "Stacked-Cross-Attention-for-Image-Text-Matching-Lee-Chen",
            "title": {
                "fragments": [],
                "text": "Stacked Cross Attention for Image-Text Matching"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Stacked Cross Attention to discover the full latent alignments using both image regions and words in sentence as context and infer the image-text similarity achieves the state-of-the-art results on the MS-COCO and Flickr30K datasets."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33315685"
                        ],
                        "name": "Kushal Kafle",
                        "slug": "Kushal-Kafle",
                        "structuredName": {
                            "firstName": "Kushal",
                            "lastName": "Kafle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kushal Kafle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145823372"
                        ],
                        "name": "Scott D. Cohen",
                        "slug": "Scott-D.-Cohen",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Cohen",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott D. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31844147"
                        ],
                        "name": "Brian L. Price",
                        "slug": "Brian-L.-Price",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Price",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian L. Price"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3290098"
                        ],
                        "name": "Christopher Kanan",
                        "slug": "Christopher-Kanan",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Kanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Kanan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "4 \u2014 DVQA [25] synthetic bar charts + 300."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4445015,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7289a240c9425bc7cad87b3b835e5f0cac22f488",
            "isKey": false,
            "numCitedBy": 87,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Bar charts are an effective way to convey numeric information, but today's algorithms cannot parse them. Existing methods fail when faced with even minor variations in appearance. Here, we present DVQA, a dataset that tests many aspects of bar chart understanding in a question answering framework. Unlike visual question answering (VQA), DVQA requires processing words and answers that are unique to a particular bar chart. State-of-the-art VQA algorithms perform poorly on DVQA, and we propose two strong baselines that perform considerably better. Our work will enable algorithms to automatically extract numeric and semantic information from vast quantities of bar charts found in scientific publications, Internet articles, business reports, and many other areas."
            },
            "slug": "DVQA:-Understanding-Data-Visualizations-via-Kafle-Cohen",
            "title": {
                "fragments": [],
                "text": "DVQA: Understanding Data Visualizations via Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "DVQA is presented, a dataset that tests many aspects of bar chart understanding in a question answering framework and two strong baselines are proposed that perform considerably better than current VQA algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127597"
                        ],
                        "name": "S. Kahou",
                        "slug": "S.-Kahou",
                        "structuredName": {
                            "firstName": "Samira",
                            "lastName": "Kahou",
                            "middleNames": [
                                "Ebrahimi"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kahou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144179710"
                        ],
                        "name": "Adam Atkinson",
                        "slug": "Adam-Atkinson",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Atkinson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Atkinson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748421"
                        ],
                        "name": "Vincent Michalski",
                        "slug": "Vincent-Michalski",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Michalski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent Michalski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2828538"
                        ],
                        "name": "\u00c1kos K\u00e1d\u00e1r",
                        "slug": "\u00c1kos-K\u00e1d\u00e1r",
                        "structuredName": {
                            "firstName": "\u00c1kos",
                            "lastName": "K\u00e1d\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00c1kos K\u00e1d\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3382568"
                        ],
                        "name": "Adam Trischler",
                        "slug": "Adam-Trischler",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Trischler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Trischler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "2 FigureQA [26] synthetic, scientific + 140."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3535069,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55ca9fe4ae98904bfe026d22dcf1420ff9c0dd86",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce FigureQA, a visual reasoning corpus of over one million question-answer pairs grounded in over 100,000 images. The images are synthetic, scientific-style figures from five classes: line plots, dot-line plots, vertical and horizontal bar graphs, and pie charts. We formulate our reasoning task by generating questions from 15 templates; questions concern various relationships between plot elements and examine characteristics like the maximum, the minimum, area-under-the-curve, smoothness, and intersection. To resolve, such questions often require reference to multiple plot elements and synthesis of information distributed spatially throughout a figure. To facilitate the training of machine learning systems, the corpus also includes side data that can be used to formulate auxiliary objectives. In particular, we provide the numerical data used to generate each figure as well as bounding-box annotations for all plot elements. We study the proposed visual reasoning task by training several models, including the recently proposed Relation Network as a strong baseline. Preliminary results indicate that the task poses a significant machine learning challenge. We envision FigureQA as a first step towards developing models that can intuitively recognize patterns from visual representations of data."
            },
            "slug": "FigureQA:-An-Annotated-Figure-Dataset-for-Visual-Kahou-Atkinson",
            "title": {
                "fragments": [],
                "text": "FigureQA: An Annotated Figure Dataset for Visual Reasoning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "FigureQA is envisioned as a first step towards developing models that can intuitively recognize patterns from visual representations of data, and preliminary results indicate that the task poses a significant machine learning challenge."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067170682"
                        ],
                        "name": "Rasmus Berg Palm",
                        "slug": "Rasmus-Berg-Palm",
                        "structuredName": {
                            "firstName": "Rasmus",
                            "lastName": "Palm",
                            "middleNames": [
                                "Berg"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rasmus Berg Palm"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724252"
                        ],
                        "name": "O. Winther",
                        "slug": "O.-Winther",
                        "structuredName": {
                            "firstName": "Ole",
                            "lastName": "Winther",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Winther"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805808"
                        ],
                        "name": "Florian Laws",
                        "slug": "Florian-Laws",
                        "structuredName": {
                            "firstName": "Florian",
                            "lastName": "Laws",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Florian Laws"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 70
                            }
                        ],
                        "text": "These and similar rules are either labor-intensive or prone to errors [43]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30210556,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01a50b9662a59070c4ff53873453d8854c15ade1",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We present CloudScan; an invoice analysis system that requires zero configuration or upfront annotation. In contrast to previous work, CloudScan does not rely on templates of invoice layout, instead it learns a single global model of invoices that naturally generalizes to unseen invoice layouts. The model is trained using data automatically extracted from end-user provided feedback. This automatic training data extraction removes the requirement for users to annotate the data precisely. We describe a recurrent neural network model that can capture long range context and compare it to a baseline logistic regression model corresponding to the current CloudScan production system. We train and evaluate the system on 8 important fields using a dataset of 326,471 invoices. The recurrent neural network and baseline model achieve 0.891 and 0.887 average F1 scores respectively on seen invoice layouts. For the harder task of unseen invoice layouts, the recurrent neural network model outperforms the baseline with 0.840 average F1 compared to 0.788."
            },
            "slug": "CloudScan-A-Configuration-Free-Invoice-Analysis-Palm-Winther",
            "title": {
                "fragments": [],
                "text": "CloudScan - A Configuration-Free Invoice Analysis System Using Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A recurrent neural network model that can capture long range context and compare it to a baseline logistic regression model corresponding to the current CloudScan production system are described."
            },
            "venue": {
                "fragments": [],
                "text": "2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1857734"
                        ],
                        "name": "Guokun Lai",
                        "slug": "Guokun-Lai",
                        "structuredName": {
                            "firstName": "Guokun",
                            "lastName": "Lai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guokun Lai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1912046"
                        ],
                        "name": "Qizhe Xie",
                        "slug": "Qizhe-Xie",
                        "structuredName": {
                            "firstName": "Qizhe",
                            "lastName": "Xie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qizhe Xie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2391802"
                        ],
                        "name": "Hanxiao Liu",
                        "slug": "Hanxiao-Liu",
                        "structuredName": {
                            "firstName": "Hanxiao",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hanxiao Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35729970"
                        ],
                        "name": "Yiming Yang",
                        "slug": "Yiming-Yang",
                        "structuredName": {
                            "firstName": "Yiming",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiming Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144547315"
                        ],
                        "name": "E. Hovy",
                        "slug": "E.-Hovy",
                        "structuredName": {
                            "firstName": "Eduard",
                            "lastName": "Hovy",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Hovy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6826032,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "636a79420d838eabe4af7fb25d6437de45ab64e8",
            "isKey": false,
            "numCitedBy": 698,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, RACE consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students\u2019 ability in understanding and reasoning. In particular, the proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models (43%) and the ceiling human performance (95%). We hope this new dataset can serve as a valuable resource for research and evaluation in machine comprehension. The dataset is freely available at http://www.cs.cmu.edu/~glai1/data/race/ and the code is available at https://github.com/qizhex/RACE_AR_baselines."
            },
            "slug": "RACE:-Large-scale-ReAding-Comprehension-Dataset-Lai-Xie",
            "title": {
                "fragments": [],
                "text": "RACE: Large-scale ReAding Comprehension Dataset From Examinations"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models and the ceiling human performance."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34979516"
                        ],
                        "name": "D. Hewlett",
                        "slug": "D.-Hewlett",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Hewlett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hewlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8651990"
                        ],
                        "name": "Alexandre Lacoste",
                        "slug": "Alexandre-Lacoste",
                        "structuredName": {
                            "firstName": "Alexandre",
                            "lastName": "Lacoste",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexandre Lacoste"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145024664"
                        ],
                        "name": "Llion Jones",
                        "slug": "Llion-Jones",
                        "structuredName": {
                            "firstName": "Llion",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Llion Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3443442"
                        ],
                        "name": "Illia Polosukhin",
                        "slug": "Illia-Polosukhin",
                        "structuredName": {
                            "firstName": "Illia",
                            "lastName": "Polosukhin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Illia Polosukhin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3240386"
                        ],
                        "name": "Andrew Fandrianto",
                        "slug": "Andrew-Fandrianto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Fandrianto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Fandrianto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111716473"
                        ],
                        "name": "Jay Han",
                        "slug": "Jay-Han",
                        "structuredName": {
                            "firstName": "Jay",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jay Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2554321"
                        ],
                        "name": "Matthew Kelcey",
                        "slug": "Matthew-Kelcey",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Kelcey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew Kelcey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39835551"
                        ],
                        "name": "David Berthelot",
                        "slug": "David-Berthelot",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Berthelot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Berthelot"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 146
                            }
                        ],
                        "text": "The QA program of unifying NLP frames all the problems as triplets of question, context and answer [32, 42, 29] or item, property name and answer [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 160
                            }
                        ],
                        "text": "To address this issue, we focus on the applicability of the encoder-decoder architecture since it can generate values not included in the input text explicitly [19] and performs reasonably well on all text-based problems involving natural"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15870937,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "832fc9327695f7425d8759c6aaeec0fa2d7b0a90",
            "isKey": false,
            "numCitedBy": 120,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We present WIKIREADING, a large-scale natural language understanding task and publicly-available dataset with 18 million instances. The task is to predict textual values from the structured knowledge base Wikidata by reading the text of the corresponding Wikipedia articles. The task contains a rich variety of challenging classification and extraction sub-tasks, making it well-suited for end-to-end models such as deep neural networks (DNNs). We compare various state-of-the-art DNNbased architectures for document classification, information extraction, and question answering. We find that models supporting a rich answer space, such as word or character sequences, perform best. Our best-performing model, a word-level sequence to sequence model with a mechanism to copy out-of-vocabulary words, obtains an accuracy of 71.8%."
            },
            "slug": "WikiReading:-A-Novel-Large-scale-Language-Task-over-Hewlett-Lacoste",
            "title": {
                "fragments": [],
                "text": "WikiReading: A Novel Large-scale Language Understanding Task over Wikipedia"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This work presents WIKIREADING, a large-scale natural language understanding task and publicly-available dataset with 18 million instances, and compares various state-of-the-art DNNbased architectures for document classification, information extraction, and question answering."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2706258"
                        ],
                        "name": "Pranav Rajpurkar",
                        "slug": "Pranav-Rajpurkar",
                        "structuredName": {
                            "firstName": "Pranav",
                            "lastName": "Rajpurkar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pranav Rajpurkar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2151810148"
                        ],
                        "name": "Jian Zhang",
                        "slug": "Jian-Zhang",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2787620"
                        ],
                        "name": "Konstantin Lopyrev",
                        "slug": "Konstantin-Lopyrev",
                        "structuredName": {
                            "firstName": "Konstantin",
                            "lastName": "Lopyrev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Konstantin Lopyrev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145419642"
                        ],
                        "name": "Percy Liang",
                        "slug": "Percy-Liang",
                        "structuredName": {
                            "firstName": "Percy",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Percy Liang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11816014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "05dd7254b632376973f3a1b4d39485da17814df5",
            "isKey": false,
            "numCitedBy": 4263,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. \nThe dataset is freely available at this https URL"
            },
            "slug": "SQuAD:-100,000+-Questions-for-Machine-Comprehension-Rajpurkar-Zhang",
            "title": {
                "fragments": [],
                "text": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
            },
            "tldr": {
                "abstractSimilarityScore": 35,
                "text": "A strong logistic regression model is built, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%)."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3304536"
                        ],
                        "name": "Jifeng Dai",
                        "slug": "Jifeng-Dai",
                        "structuredName": {
                            "firstName": "Jifeng",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jifeng Dai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2153682629"
                        ],
                        "name": "Yi Li",
                        "slug": "Yi-Li",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 102
                            }
                        ],
                        "text": "Then, each token\u2019s bounding box is used to extract features from U-Net\u2019s feature map with ROI pooling [6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7428689,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b724c3f7ff395235b62537203ddeb710f0eb27bb",
            "isKey": false,
            "numCitedBy": 3872,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We present region-based, fully convolutional networks for accurate and efficient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classifier backbones, such as the latest Residual Networks (ResNets), for object detection. We show competitive results on the PASCAL VOC datasets (e.g., 83.6% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20 times faster than the Faster R-CNN counterpart. Code is made publicly available at: https://github.com/daijifeng001/r-fcn."
            },
            "slug": "R-FCN:-Object-Detection-via-Region-based-Fully-Dai-Li",
            "title": {
                "fragments": [],
                "text": "R-FCN: Object Detection via Region-based Fully Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "This work presents region-based, fully convolutional networks for accurate and efficient object detection, and proposes position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082372"
                        ],
                        "name": "Rico Sennrich",
                        "slug": "Rico-Sennrich",
                        "structuredName": {
                            "firstName": "Rico",
                            "lastName": "Sennrich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rico Sennrich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2259100"
                        ],
                        "name": "B. Haddow",
                        "slug": "B.-Haddow",
                        "structuredName": {
                            "firstName": "Barry",
                            "lastName": "Haddow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Haddow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2539211"
                        ],
                        "name": "Alexandra Birch",
                        "slug": "Alexandra-Birch",
                        "structuredName": {
                            "firstName": "Alexandra",
                            "lastName": "Birch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexandra Birch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 21
                            }
                        ],
                        "text": "Subword tokenization [52, 31] was proposed to solve the word sparsity problem and keep the vocabulary at a reasonable size."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1114678,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1af68821518f03568f913ab03fc02080247a27ff",
            "isKey": false,
            "numCitedBy": 4793,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively."
            },
            "slug": "Neural-Machine-Translation-of-Rare-Words-with-Units-Sennrich-Haddow",
            "title": {
                "fragments": [],
                "text": "Neural Machine Translation of Rare Words with Subword Units"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper introduces a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units, and empirically shows that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.3 BLEU."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2119320633"
                        ],
                        "name": "A. Kumar",
                        "slug": "A.-Kumar",
                        "structuredName": {
                            "firstName": "Ankith",
                            "lastName": "Kumar",
                            "middleNames": [
                                "Jain",
                                "Rakesh"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2329943"
                        ],
                        "name": "Ozan Irsoy",
                        "slug": "Ozan-Irsoy",
                        "structuredName": {
                            "firstName": "Ozan",
                            "lastName": "Irsoy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ozan Irsoy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3214791"
                        ],
                        "name": "Peter Ondruska",
                        "slug": "Peter-Ondruska",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Ondruska",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Ondruska"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2136562"
                        ],
                        "name": "Mohit Iyyer",
                        "slug": "Mohit-Iyyer",
                        "structuredName": {
                            "firstName": "Mohit",
                            "lastName": "Iyyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohit Iyyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065251344"
                        ],
                        "name": "James Bradbury",
                        "slug": "James-Bradbury",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bradbury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Bradbury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2708454"
                        ],
                        "name": "Ishaan Gulrajani",
                        "slug": "Ishaan-Gulrajani",
                        "structuredName": {
                            "firstName": "Ishaan",
                            "lastName": "Gulrajani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ishaan Gulrajani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3428769"
                        ],
                        "name": "Victor Zhong",
                        "slug": "Victor-Zhong",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Zhong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Zhong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2896063"
                        ],
                        "name": "Romain Paulus",
                        "slug": "Romain-Paulus",
                        "structuredName": {
                            "firstName": "Romain",
                            "lastName": "Paulus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Romain Paulus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 99
                            }
                        ],
                        "text": "The QA program of unifying NLP frames all the problems as triplets of question, context and answer [32, 42, 29] or item, property name and answer [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 142
                            }
                        ],
                        "text": "Most tasks in Natural Language Processing can be unified under one framework by casting them as triplets of the question, context, and answer [32, 42, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2319779,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "452059171226626718eb677358836328f884298e",
            "isKey": false,
            "numCitedBy": 998,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook's bAbI dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets."
            },
            "slug": "Ask-Me-Anything:-Dynamic-Memory-Networks-for-Kumar-Irsoy",
            "title": {
                "fragments": [],
                "text": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers, is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737326"
                        ],
                        "name": "O. Ronneberger",
                        "slug": "O.-Ronneberger",
                        "structuredName": {
                            "firstName": "Olaf",
                            "lastName": "Ronneberger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Ronneberger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152702479"
                        ],
                        "name": "P. Fischer",
                        "slug": "P.-Fischer",
                        "structuredName": {
                            "firstName": "Philipp",
                            "lastName": "Fischer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Fischer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710872"
                        ],
                        "name": "T. Brox",
                        "slug": "T.-Brox",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Brox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Brox"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "We rely on U-Net as a backbone visual encoder network [48] since this architecture provides access to not only the information in the near neighborhood of the token, such as font and style but also to more distant regions of the page, which is useful in cases where the text is related to other structures, i."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3719281,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "isKey": false,
            "numCitedBy": 33565,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net ."
            },
            "slug": "U-Net:-Convolutional-Networks-for-Biomedical-Image-Ronneberger-Fischer",
            "title": {
                "fragments": [],
                "text": "U-Net: Convolutional Networks for Biomedical Image Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks."
            },
            "venue": {
                "fragments": [],
                "text": "MICCAI"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 196
                            }
                        ],
                        "text": "Several authors have shown that, when tasks involving 2D documents are considered, sequential models can be outperformed by considering layout information either directly as positional embeddings [17, 11, 56] or indirectly by allowing them to be contextualized on their spatial neighborhood [6, 57, 15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Axial attention in multidimensional transformers (2019)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2019
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Computer Vision Foundation / IEEE"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Conference on Computer Vision and Pattern Recognition"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 87
                            }
                        ],
                        "text": "Our model was validated on series of experiments involving Key Information Extraction, Visual Question Answering, classification of rich documents, and Question Answering from layout-rich texts."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 171
                            }
                        ],
                        "text": "The relationships between text and other media have been previously studied in Visual Commonsense Reasoning, VideoGrounded Dialogue, Speech, and Visual Question Answering [13, 32, 3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A survey on visual transformer (2021)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2021
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 87
                            }
                        ],
                        "text": "Our model was validated on series of experiments involving Key Information Extraction, Visual Question Answering, classification of rich documents, and Question Answering from layout-rich texts."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 172
                            }
                        ],
                        "text": "The relationships between text and other media have been previously studied in Visual Commonsense Reasoning, Video-Grounded Dialogue, Speech, and Visual Question Answering [16, 35, 4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 145
                            }
                        ],
                        "text": "The relationships between text and other media have been previously studied in Visual Commonsense Reasoning, VideoGrounded Dialogue, Speech, and Visual Question Answering [13, 32, 3]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "SpeechBERT: An audio-andtext jointly learned language model for end-to-end spoken question answering pp"
            },
            "venue": {
                "fragments": [],
                "text": "4168\u20134172"
            },
            "year": 2020
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 34,
            "methodology": 23,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 64,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/Going-Full-TILT-Boogie-on-Document-Understanding-Powalski-Borchmann/4bd0e72a6e41e34ae4d8d01f72aee0a3ce2d646a?sort=total-citations"
}