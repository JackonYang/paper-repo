{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2438632"
                        ],
                        "name": "X. Zeng",
                        "slug": "X.-Zeng",
                        "structuredName": {
                            "firstName": "Xinchuan",
                            "lastName": "Zeng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zeng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "90716724"
                        ],
                        "name": "T. Martinez",
                        "slug": "T.-Martinez",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Martinez",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Martinez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 80
                            }
                        ],
                        "text": "Zeng and Martinez also used neural nets to approximate ensembles of classifiers [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2459024,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8ef0c1d8284a37c7fb3e9842e4ad0fe07541d5fa",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Several methods (e.g., Bagging, Boosting) of constructing and combining an ensemble of classifiers have recently been shown capable of improving accuracy of a class of commonly used classifiers (e.g., decision trees, neural networks). The accuracy gain achieved, however, is at the expense of a higher requirement for storage and computation. This storage and computation overhead can decrease the utility of these methods when applied to real-world situations. In this Letter, we propose a learning approach which allows a single neural network to approximate a given ensemble of classifiers. Experiments on a large number of real-world data sets show that this approach can substantially save storage and computation while still maintaining accuracy similar to that of the entire ensemble."
            },
            "slug": "Using-a-Neural-Network-to-Approximate-an-Ensemble-Zeng-Martinez",
            "title": {
                "fragments": [],
                "text": "Using a Neural Network to Approximate an Ensemble of Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This Letter proposes a learning approach which allows a single neural network to approximate a given ensemble of classifiers and shows that this approach can substantially save storage and computation while still maintaining accuracy similar to that of the entire ensemble."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Processing Letters"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34056376"
                        ],
                        "name": "Mark W. Craven",
                        "slug": "Mark-W.-Craven",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Craven",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark W. Craven"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734317"
                        ],
                        "name": "J. Shavlik",
                        "slug": "J.-Shavlik",
                        "structuredName": {
                            "firstName": "Jude",
                            "lastName": "Shavlik",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shavlik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 82
                            }
                        ],
                        "text": "TREPAN was used to extract tree-structured representations of trained neural nets [6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6930911,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "11ea864eca24af40f3b48ee297c55f156c1eca3c",
            "isKey": false,
            "numCitedBy": 637,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "A significant limitation of neural networks is that the representations they learn are usually incomprehensible to humans. We present a novel algorithm, TREPAN, for extracting comprehensible, symbolic representations from trained neural networks. Our algorithm uses queries to induce a decision tree that approximates the concept represented by a given network. Our experiments demonstrate that TREPAN is able to produce decision trees that maintain a high level of fidelity to their respective networks while being comprehensible and accurate. Unlike previous work in this area, our algorithm is general in its applicability and scales well to large networks and problems with high-dimensional input spaces."
            },
            "slug": "Extracting-Tree-Structured-Representations-of-Craven-Shavlik",
            "title": {
                "fragments": [],
                "text": "Extracting Tree-Structured Representations of Trained Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This work presents a novel algorithm, TREPAN, for extracting comprehensible, symbolic representations from trained neural networks, which is general in its applicability and scales well to large networks and problems with high-dimensional input spaces."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2780489"
                        ],
                        "name": "Prem Melville",
                        "slug": "Prem-Melville",
                        "structuredName": {
                            "firstName": "Prem",
                            "lastName": "Melville",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Prem Melville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "DECORATE [13] uses artificial data to increase diversity so that better ensembles can be trained."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "DECORATE [13] uses \narti.cial data to increase diversity so that better ensembles can be trained."
                    },
                    "intents": []
                }
            ],
            "corpusId": 57251235,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67049930a70b1ea731dc9a7a7e1646735d47e113",
            "isKey": false,
            "numCitedBy": 290,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Ensemble methods like bagging and boosting that combine the decisions of multiple hypotheses are some of the strongest existing machine learning methods. The diversity of the members of an ensemble is known to be an important factor in determining its generalization error. This paper presents a new method for generating ensembles that directly constructs diverse hypotheses using additional artificially-constructed training examples. The technique is a simple, general metalearner that can use any strong learner as a base classifier to build diverse committees. Experimental results using decision-tree induction as a base learner demonstrate that this approach consistently achieves higher predictive accuracy than both the base classifier and bagging (whereas boosting can occasionally decrease accuracy), and also obtains higher accuracy than boosting early in the learning curve when training data is limited."
            },
            "slug": "Constructing-Diverse-Classifier-Ensembles-using-Melville-Mooney",
            "title": {
                "fragments": [],
                "text": "Constructing Diverse Classifier Ensembles using Artificial Training Examples"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new method for generating ensembles that directly constructs diverse hypotheses using additional artificially-constructed training examples is presented, which consistently achieves higher predictive accuracy than both the base classifier and bagging, and also obtains higher accuracy than boosting early in the learning curve when training data is limited."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740213"
                        ],
                        "name": "Pedro M. Domingos",
                        "slug": "Pedro-M.-Domingos",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Domingos",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro M. Domingos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 101
                            }
                        ],
                        "text": "CMM is a meta-learner that learns one single model from a bagged ensemble of the same type of models [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2060065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a9a39da9d4fc937bc455705d508674a205620aa",
            "isKey": false,
            "numCitedBy": 106,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "If it is to qualify as knowledge, a learner's output should be accurate, stable and com-prehensible. Learning multiple models can improve signiicantly on the accuracy and stability of single models, but at the cost of losing their comprehensibility (when they possess it, as do, for example, simple decision trees and rule sets). This paper proposes and evaluates CMM, a meta-learner that seeks to retain most of the accuracy gains of multiple model approaches, while still producing a single comprehensible model. CMM is based on reapplying the base learner to recover the frontiers implicit in the multiple model ensemble. This is done by giving the base learner a new training set, composed of a large number of examples generated and classiied according to the ensemble, plus the original examples. CMM is evaluated using C4.5RULES as the base learner, and bagging as the multiple-model methodology. On 26 benchmark datasets, CMM retains on average 60% of the accuracy gains obtained by bagging relative to a single run of C4.5RULES, while producing a rule set whose complexity is typically a small multiple (2{6) of C4.5RULES's, and also improving stability."
            },
            "slug": "Knowledge-Acquisition-from-Examples-Via-Multiple-Domingos",
            "title": {
                "fragments": [],
                "text": "Knowledge Acquisition from Examples Via Multiple Models"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "CMM, a meta-learner that seeks to retain most of the accuracy gains of multiple model approaches, while still producing a single comprehensible model, is proposed and evaluated."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35199948"
                        ],
                        "name": "Anthony G. Gualtieri",
                        "slug": "Anthony-G.-Gualtieri",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Gualtieri",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anthony G. Gualtieri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3111353"
                        ],
                        "name": "S. Chettri",
                        "slug": "S.-Chettri",
                        "structuredName": {
                            "firstName": "Samir",
                            "lastName": "Chettri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chettri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2511265"
                        ],
                        "name": "R. Cromp",
                        "slug": "R.-Cromp",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Cromp",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cromp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113716764"
                        ],
                        "name": "Laurence F. Johnson",
                        "slug": "Laurence-F.-Johnson",
                        "structuredName": {
                            "firstName": "Laurence",
                            "lastName": "Johnson",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laurence F. Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "HS is the IndianPine92 data set [10] where the difficult class Soybean-mintill is the positive class."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15743704,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "83e3788925a7a78bcc89a4540c8808f8e2b7acb0",
            "isKey": false,
            "numCitedBy": 143,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditionally, classi ers model the underlying density of the various classes and then nd a separating surface. However density estimation in high-dimensional spaces su ers from the Hughes e ect (Hughes, 1968), (Landgrebe, 1999): For a xed amount of training data the classi cation accuracy as a function of number of bands reaches a maximum and then declines, because there is limited amount of training data to estimate the large number of parameters needed. Thus usually, a feature selection step is rst performed on the high-dimensional data to reduce its dimensionality."
            },
            "slug": "Support-Vector-Machine-Classifiers-as-Applied-to-Gualtieri-Chettri",
            "title": {
                "fragments": [],
                "text": "Support Vector Machine Classifiers as Applied to AVIRIS Data"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "D density estimation in high-dimensional spaces is based on the model of the underlying density of the various classes and then a separating surface and a feature selection step is performed on the high- dimensional data to reduce its dimensionality."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "Well known ensemble methods include bagging [2], boosting [14], random forests[3], Bayesian averaging [9] and stacking [17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 47328136,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1ee87290fa827f1217b8fa2bccb3485da1a300e",
            "isKey": false,
            "numCitedBy": 15183,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy."
            },
            "slug": "Bagging-predictors-Breiman",
            "title": {
                "fragments": [],
                "text": "Bagging predictors"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61116019,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7550a05bf00f7b24aed9c1ac3ef000575388d21c",
            "isKey": false,
            "numCitedBy": 5454,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Training a support vector machine SVM leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples on the shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SVM light is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SVM light V 2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains."
            },
            "slug": "Making-large-scale-SVM-learning-practical-Joachims",
            "title": {
                "fragments": [],
                "text": "Making large scale SVM learning practical"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This chapter presents algorithmic and computational results developed for SVM light V 2.0, which make large-scale SVM training more practical and give guidelines for the application of SVMs to large domains."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740213"
                        ],
                        "name": "Pedro M. Domingos",
                        "slug": "Pedro-M.-Domingos",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Domingos",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro M. Domingos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 102
                            }
                        ],
                        "text": "Well known ensemble methods include bagging [2], boosting [14], random forests[3], Bayesian averaging [9] and stacking [17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17792327,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ce5060cf6dd54857504a0563d314d6090483dfa",
            "isKey": false,
            "numCitedBy": 171,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Although Bayesian model averaging is theoretically the optimal method for combining learned models, it has seen very little use in machine learning. In this paper we study its application to combining rule sets, and compare it with bagging and partitioning, two popular but more ad hoc alternatives. Our experiments show that, surprisingly, Bayesian model averaging\u2019s error rates are consistently higher than the other methods\u2019. Further investigation shows this to be due to a marked tendency to overfit on the part of Bayesian model averaging, contradicting previous beliefs that it solves (or avoids) the overfitting problem."
            },
            "slug": "Bayesian-Averaging-of-Classifiers-and-the-Problem-Domingos",
            "title": {
                "fragments": [],
                "text": "Bayesian Averaging of Classifiers and the Overfitting Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper studies Bayesian model averaging\u2019s application to combining rule sets, and compares it with bagging and partitioning, two popular but more ad hoc alternatives, showing its error rates are consistently higher than the other methods."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145727186"
                        ],
                        "name": "R. Caruana",
                        "slug": "R.-Caruana",
                        "structuredName": {
                            "firstName": "Rich",
                            "lastName": "Caruana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Caruana"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399048849"
                        ],
                        "name": "Alexandru Niculescu-Mizil",
                        "slug": "Alexandru-Niculescu-Mizil",
                        "structuredName": {
                            "firstName": "Alexandru",
                            "lastName": "Niculescu-Mizil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexandru Niculescu-Mizil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152614535"
                        ],
                        "name": "G. Crew",
                        "slug": "G.-Crew",
                        "structuredName": {
                            "firstName": "Geoff",
                            "lastName": "Crew",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Crew"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2481551"
                        ],
                        "name": "Alex Ksikes",
                        "slug": "Alex-Ksikes",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Ksikes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Ksikes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1345193,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6fa5d3508788f1ec9973d44f65b207092f91298f",
            "isKey": false,
            "numCitedBy": 718,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for constructing ensembles from libraries of thousands of models. Model libraries are generated using different learning algorithms and parameter settings. Forward stepwise selection is used to add to the ensemble the models that maximize its performance. Ensemble selection allows ensembles to be optimized to performance metric such as accuracy, cross entropy, mean precision, or ROC Area. Experiments with seven test problems and ten metrics demonstrate the benefit of ensemble selection."
            },
            "slug": "Ensemble-selection-from-libraries-of-models-Caruana-Niculescu-Mizil",
            "title": {
                "fragments": [],
                "text": "Ensemble selection from libraries of models"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A method for constructing ensembles from libraries of thousands of models using forward stepwise selection to be optimized to performance metric such as accuracy, cross entropy, mean precision, or ROC Area is presented."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696678"
                        ],
                        "name": "D. Wolpert",
                        "slug": "D.-Wolpert",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wolpert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wolpert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "Well known ensemble methods include bagging [2], boosting [14], random forests[3], Bayesian averaging [9] and stacking [17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5895004,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e1291583873fb890e7922ec0dfefd4846df46c9",
            "isKey": false,
            "numCitedBy": 5481,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Stacked-generalization-Wolpert",
            "title": {
                "fragments": [],
                "text": "Stacked generalization"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 78
                            }
                        ],
                        "text": "Well known ensemble methods include bagging [2], boosting [14], random forests[3], Bayesian averaging [9] and stacking [17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 89141,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "13d4c2f76a7c1a4d0a71204e1d5d263a3f5a7986",
            "isKey": false,
            "numCitedBy": 65224,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148\u2013156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression."
            },
            "slug": "Random-Forests-Breiman",
            "title": {
                "fragments": [],
                "text": "Random Forests"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the forest, and are also applicable to regression."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70219052"
                        ],
                        "name": "Wray L. Buntine",
                        "slug": "Wray-L.-Buntine",
                        "structuredName": {
                            "firstName": "Wray",
                            "lastName": "Buntine",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wray L. Buntine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145727186"
                        ],
                        "name": "R. Caruana",
                        "slug": "R.-Caruana",
                        "structuredName": {
                            "firstName": "Rich",
                            "lastName": "Caruana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Caruana"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 84
                            }
                        ],
                        "text": "For example, we use the SNNS neural net package [18], the IND decision tree package [4], the SVMlight SVM package [11], WEKA [16] random forests, etc."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 123174972,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b1888a2b01fde65d30c2455db24da290f36eb1be",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "This manual describes the IND package for learning tree classifiers from data. The package is an integrated C and C shell re-implementation of tree learning routines such as CART, C4, and various MDL and Bayesian variations. The package includes routines for experiment control, interactive operation, and analysis of tree building. The manual introduces the system and its many options, gives a basic review of tree learning, contains a guide to the literature and a glossary, and lists the manual pages for the routines and instructions on installation."
            },
            "slug": "Introduction-in-IND-and-recursive-partitioning-Buntine-Caruana",
            "title": {
                "fragments": [],
                "text": "Introduction in IND and recursive partitioning"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "This manual describes the IND package for learning tree classifiers from data, an integrated C and C shell re-implementation of tree learning routines such as CART, C4, and various MDL and Bayesian variations."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3021654"
                        ],
                        "name": "Daniel Lowd",
                        "slug": "Daniel-Lowd",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Lowd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Lowd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740213"
                        ],
                        "name": "Pedro M. Domingos",
                        "slug": "Pedro-M.-Domingos",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Domingos",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro M. Domingos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 236
                            }
                        ],
                        "text": "We experiment with three methods of generating pseudo data: RANDOM, generate data for each attribute independently from its marginal distribution; NBE, estimate the joint density of attributes using the Naive Bayes Estimation algorithm [12] and then generate samples from this joint distribution; and MUNGE, a new procedure we propose that samples from a non-parametric estimate of the joint density."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 7
                            }
                        ],
                        "text": "[9] P. Domingos."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 193
                            }
                        ],
                        "text": "We used NBE to estimate the joint distribution of the attributes because it handles mixed attributes, it is simple to use, it performs as well as learning a Bayesian Network from the same data [12], and it is readily available."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 7
                            }
                        ],
                        "text": "[8] P. Domingos."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 20
                            }
                        ],
                        "text": "[12] D. Loyd and P. \nDomingos."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 155
                            }
                        ],
                        "text": "A mixture model algorithm that handles both discrete and continuous attributes, NBE (Naive Bayes Estimation), was recently introduced by Lowd and Domingos [12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 137
                            }
                        ],
                        "text": "A mixture model algorithm that handles both discrete and \ncontinuous attributes, NBE (Naive Bayes Estimation), was recently introduced by Lowd and Domingos [12]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 207158142,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2e5a7515d76675961462db97c70f4d2b592acd6",
            "isKey": true,
            "numCitedBy": 294,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Naive Bayes models have been widely used for clustering and classification. However, they are seldom used for general probabilistic learning and inference (i.e., for estimating and computing arbitrary joint, conditional and marginal distributions). In this paper we show that, for a wide range of benchmark datasets, naive Bayes models learned using EM have accuracy and learning time comparable to Bayesian networks with context-specific independence. Most significantly, naive Bayes inference is orders of magnitude faster than Bayesian network inference using Gibbs sampling and belief propagation. This makes naive Bayes models a very attractive alternative to Bayesian networks for general probability estimation, particularly in large or real-time domains."
            },
            "slug": "Naive-Bayes-models-for-probability-estimation-Lowd-Domingos",
            "title": {
                "fragments": [],
                "text": "Naive Bayes models for probability estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown that, for a wide range of benchmark datasets, naive Bayes models learned using EM have accuracy and learning time comparable to Bayesian networks with context-specific independence."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "Well known ensemble methods include bagging [2], boosting [14], random forests[3], Bayesian averaging [9] and stacking [17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 221284382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "84bb60b83f82ad847e19d96403ad0011abfc888f",
            "isKey": false,
            "numCitedBy": 1888,
            "numCiting": 126,
            "paperAbstract": {
                "fragments": [],
                "text": "Boosting is a general method for improving the accuracy of any given learning algorithm. Focusing primarily on the AdaBoost algorithm, this chapter overviews some of the recent work on boosting including analyses of AdaBoost\u2019s training error and generalization error; boosting\u2019s connection to game theory and linear programming; the relationship between boosting and logistic regression; extensions of AdaBoost for multiclass classification problems; methods of incorporating human knowledge into boosting; and experimental and applied work using boosting."
            },
            "slug": "The-Boosting-Approach-to-Machine-Learning-An-Schapire",
            "title": {
                "fragments": [],
                "text": "The Boosting Approach to Machine Learning An Overview"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "This chapter overviews some of the recent work on boosting including analyses of AdaBoost's training error and generalization error; boosting\u2019s connection to game theory and linear programming; the relationship between boosting and logistic regression; extensions of Ada boost for multiclass classification problems; methods of incorporating human knowledge into boosting; and experimental and applied work using boosting."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9419406"
                        ],
                        "name": "I. Witten",
                        "slug": "I.-Witten",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Witten",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Witten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143713826"
                        ],
                        "name": "Eibe Frank",
                        "slug": "Eibe-Frank",
                        "structuredName": {
                            "firstName": "Eibe",
                            "lastName": "Frank",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eibe Frank"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 120
                            }
                        ],
                        "text": "For example, we use the SNNS neu\u00adral net package [18], the IND decision tree \npackage [4], the SVMlight SVM package [11], WEKA [16] random forests, etc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 125
                            }
                        ],
                        "text": "For example, we use the SNNS neural net package [18], the IND decision tree package [4], the SVMlight SVM package [11], WEKA [16] random forests, etc."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61973115,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "26ae952599aa9ba5815a80356024258247fc2b10",
            "isKey": false,
            "numCitedBy": 5714,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "1. What's It All About? 2. Input: Concepts, Instances, Attributes 3. Output: Knowledge Representation 4. Algorithms: The Basic Methods 5. Credibility: Evaluating What's Been Learned 6. Implementations: Real Machine Learning Schemes 7. Moving On: Engineering The Input And Output 8. Nuts And Bolts: Machine Learning Algorithms In Java 9. Looking Forward"
            },
            "slug": "Data-mining:-practical-machine-learning-tools-and-Witten-Frank",
            "title": {
                "fragments": [],
                "text": "Data mining: practical machine learning tools and techniques with Java implementations"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This presentation discusses the design and implementation of machine learning algorithms in Java, as well as some of the techniques used to develop and implement these algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "SGMD"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145298005"
                        ],
                        "name": "Catherine Blake",
                        "slug": "Catherine-Blake",
                        "structuredName": {
                            "firstName": "Catherine",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Catherine Blake"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62622768,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e068be31ded63600aea068eacd12931efd2a1029",
            "isKey": false,
            "numCitedBy": 13446,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "UCI-Repository-of-machine-learning-databases-Blake",
            "title": {
                "fragments": [],
                "text": "UCI Repository of machine learning databases"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144625394"
                        ],
                        "name": "A. Zell",
                        "slug": "A.-Zell",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Zell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3353750"
                        ],
                        "name": "N. Mache",
                        "slug": "N.-Mache",
                        "structuredName": {
                            "firstName": "Niels",
                            "lastName": "Mache",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Mache"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060281613"
                        ],
                        "name": "Ralf H\u00fcbner",
                        "slug": "Ralf-H\u00fcbner",
                        "structuredName": {
                            "firstName": "Ralf",
                            "lastName": "H\u00fcbner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ralf H\u00fcbner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1852577"
                        ],
                        "name": "G. Mamier",
                        "slug": "G.-Mamier",
                        "structuredName": {
                            "firstName": "G\u00fcnter",
                            "lastName": "Mamier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Mamier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055420748"
                        ],
                        "name": "M. Vogt",
                        "slug": "M.-Vogt",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Vogt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Vogt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49664947"
                        ],
                        "name": "M. Schmalzl",
                        "slug": "M.-Schmalzl",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Schmalzl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schmalzl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40466466"
                        ],
                        "name": "K. Herrmann",
                        "slug": "K.-Herrmann",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Herrmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Herrmann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "For example, we use the SNNS neural net package [18], the IND decision tree package [4], the SVMlight SVM package [11], WEKA [16] random forests, etc."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61750649,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "80e1524756467c899b8d7a7fa069279df84a76c5",
            "isKey": false,
            "numCitedBy": 238,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We here describe SNNS, a neural network simulator for Unix workstations that has been developed at the University of Stuttgart, Germany. Our network simulation environment is a tool to generate, train, test, and visualize artificial neural networks. The simulator consists of three major components: a simulator kernel that operates on the internal representation of the neural networks, a graphical user interface based on X-Windows to interactively create, modify and visualize neural nets, and a compiler to generate large neural networks from a high level network description language."
            },
            "slug": "SNNS-(Stuttgart-Neural-Network-Simulator)-Zell-Mache",
            "title": {
                "fragments": [],
                "text": "SNNS (Stuttgart Neural Network Simulator)"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "SNNS, a neural network simulator for Unix workstations that has been developed at the University of Stuttgart, Germany, is described, a tool to generate, train, test, and visualize artificial neural networks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 124
                            }
                        ],
                        "text": "The most well known algorithm in this category, used in domains with only continuous attributes is the mixture of Gaussians [7], where each component consists of a Gaussian distribution with a different mean and covariance matrix."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 114
                            }
                        ],
                        "text": "The most well known algorithm in this category, used in domains with only \ncontinuous attributes is the mixture of Gaussians [7], where each component consists of a Gaussian distribution \nwith a di.erent mean and covariance matrix."
                    },
                    "intents": []
                }
            ],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": false,
            "numCitedBy": 48406,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49800308"
                        ],
                        "name": "B. Silverman",
                        "slug": "B.-Silverman",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Silverman",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Silverman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 189
                            }
                        ],
                        "text": "The continuous attributes are usually modeled using a uniform distribution, a Gaussian distribution with mean and variance estimated from the training set, or via kernel density estimation [15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "For continuous attributes, a kernel density estimator [15] was used."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 67073029,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "729cb7a620b4e81b63b281627474020cdfbadd39",
            "isKey": false,
            "numCitedBy": 7456,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction. Survey of Existing Methods. The Kernel Method for Univariate Data. The Kernel Method for Multivariate Data. Three Important Methods. Density Estimation in Action."
            },
            "slug": "Density-Estimation-for-Statistics-and-Data-Analysis-Silverman",
            "title": {
                "fragments": [],
                "text": "Density Estimation for Statistics and Data Analysis."
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "The Kernel Method for Multivariate Data: Three Important Methods and Density Estimation in Action."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740213"
                        ],
                        "name": "Pedro M. Domingos",
                        "slug": "Pedro-M.-Domingos",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Domingos",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro M. Domingos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35132687,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8261143d81e3731a64f815ba3fd85b009974db79",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Knowledge-Acquisition-form-Examples-Vis-Multiple-Domingos",
            "title": {
                "fragments": [],
                "text": "Knowledge Acquisition form Examples Vis Multiple Models"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1997
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 3,
            "methodology": 13
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 20,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Model-compression-Bucila-Caruana/30c9bb327b7f2b9f1d1e5b69b9d0c97b410948d9?sort=total-citations"
}