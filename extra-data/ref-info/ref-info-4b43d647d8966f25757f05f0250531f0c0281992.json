{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2168619284"
                        ],
                        "name": "Jingyu He",
                        "slug": "Jingyu-He",
                        "structuredName": {
                            "firstName": "Jingyu",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jingyu He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798094"
                        ],
                        "name": "A. Downton",
                        "slug": "A.-Downton",
                        "structuredName": {
                            "firstName": "Andy",
                            "lastName": "Downton",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Downton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 50
                            }
                        ],
                        "text": "Most designers of multilingual software resort to Unicode-based fonts, and software vendors provide detailed guidelines for internationalization [24]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 113
                            }
                        ],
                        "text": "Although alternate schemes have been suggested [43], they do not have the compatibility and global acceptance of Unicode."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "To measure the quality of the OCR results we calculated the Levenshtein distance [19] between the correct text (ground truth) and the resulting text."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 36
                            }
                        ],
                        "text": "In terms of character encoding, the Unicode Consortium aims at providing a reliable encoding scheme for all scripts in the world [17]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 81
                            }
                        ],
                        "text": "Alternate schemes do not have the compatibility and global acceptance of Unicode [1, 8, 19, 22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 81
                            }
                        ],
                        "text": "Even if DIA could provide \u201cperfect\u201d transcription of the textual content (as ASCII/Unicode/XML), many critical features of its original appearance may have been discarded."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 203
                            }
                        ],
                        "text": "However, there remain many ways in which information conveyed originally as ink-on-paper may not be better delivered by digital means: these need to be better elucidated (for an extended discussion, see [19])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "Our first approach was to use the SOM followed by Kmeans algorithm as done in [19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2070419,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2131c5dc71a849b30ca9c7bf23acc8b7ebe10167",
            "isKey": true,
            "numCitedBy": 29,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "A configurable archive document image analysis system for digital library construction has been designed using rapid prototyping and top-down iterative development methods. This approach has been found to be essential in order to capture the curators' expertise about existing card archive structures, content and databases. The design currently achieves about 93% correct segmentation of the required archive card fields overall, with 81.3% of all archive cards in a testset of 2000 images having all fields correctly segmented and labeled. Analysis of errors in the testset indicates that heavily-annotated cards and non-standard card formats comprise 5-10% of the overall archive, and a significant proportion of these are unlikely to be resolvable without curatorial intervention."
            },
            "slug": "User-assisted-archive-document-image-analysis-for-He-Downton",
            "title": {
                "fragments": [],
                "text": "User-assisted archive document image analysis for digital library construction"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Analysis of errors in the testset indicates that heavily-annotated cards and non-standard card formats comprise 5-10% of the overall archive, and a significant proportion of these are unlikely to be resolvable without curatorial intervention."
            },
            "venue": {
                "fragments": [],
                "text": "Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798094"
                        ],
                        "name": "A. Downton",
                        "slug": "A.-Downton",
                        "structuredName": {
                            "firstName": "Andy",
                            "lastName": "Downton",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Downton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2037941"
                        ],
                        "name": "A. Tams",
                        "slug": "A.-Tams",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Tams",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057231255"
                        ],
                        "name": "G. Wells",
                        "slug": "G.-Wells",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Wells",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wells"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057118403"
                        ],
                        "name": "A. C. Holmes",
                        "slug": "A.-C.-Holmes",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Holmes",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. C. Holmes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145815031"
                        ],
                        "name": "S. Lucas",
                        "slug": "S.-Lucas",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Lucas",
                            "middleNames": [
                                "M.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lucas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1959964"
                        ],
                        "name": "G. Beccaloni",
                        "slug": "G.-Beccaloni",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Beccaloni",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Beccaloni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2103835579"
                        ],
                        "name": "M. Scoble",
                        "slug": "M.-Scoble",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Scoble",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Scoble"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058542353"
                        ],
                        "name": "G. S. Robinson",
                        "slug": "G.-S.-Robinson",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Robinson",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. S. Robinson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 96
                            }
                        ],
                        "text": "Na\u00efve methods are based on extensions to binarization techniques for gray-scale document images [13,14,15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 12
                            }
                        ],
                        "text": "Statistical [5, 14, 17] techniques extract representative feature vectors from character images and apply pattern classification techniques on these features."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 104
                            }
                        ],
                        "text": "The classification step was performed using two well known classification algorithms, K-NN [13] and SVM [13,14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 102
                            }
                        ],
                        "text": "The system supports text-image separation, word boundaries estimation and OCR of Devanagari documents [14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "Once that is done it would be tempting to apply the techniques of Xu and Nagy [14] to improve character bitmaps by statistical techniques rather than by selection."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 153
                            }
                        ],
                        "text": "Providing data sets for certain scripts is a non-trivial task due to their large character sets and the variety of recognition units used by researchers [8, 13, 14, 38]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 111
                            }
                        ],
                        "text": "Among the most popular we can cite Kise\u2019s method [13] based on area Voronoi diagram, O\u2019Gorman\u2019s Docstrum method[14] based on neighbor clustering and Nagy\u2019s X-Y cut [15] based on the analysis of projection profiles."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 96
                            }
                        ],
                        "text": "Instead, adaptive to local information techniques for document binarization have been developed [11-14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 84
                            }
                        ],
                        "text": "Common methods for Indic script OCR use structural features to build decision trees [13, 14] or combine multiple knowledge bases to create statistical classifiers [8, 38]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "Character segmentation algorithms [14] are used to compute the list of component boundaries in each word image."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 33634219,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a489fe6a67c799a22f44a43815a1bfc2ad2fb83f",
            "isKey": true,
            "numCitedBy": 9,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Presents a progress report (after 1 year of a 3 year project) on the overall design for a flexible archive conversion system, intended eventually for widespread use as a tool to convert legacy typescript and handwritten archive card indexes into Internet-accessible and searchable databases. The VIADOCS system is being developed and evaluated on a demonstrator archive of 30,000 pyraloid moth cards at the UK Natural History Museum, and has already demonstrated a successful and efficient mechanism for image acquisition using a modified bank cheque scanner. Document image processing and analysis techniques, defined by an XML validating document type definition (DTD), are being used to correct defects in the acquired images and parse card sequences to match the hierarchical taxonomy of pyraloid moth species. Parsed data is processed by offline OCR engines augmented by field-specific subject dictionaries to produce a 'draft' online archive. This archive will then be validated interactively via a Web browser as it is used. It is hoped eventually to provide an efficient and configurable legacy archive document conversion system not only for the Natural History Museum, but also for all museums, libraries and archives where there is a need to interrogate legacy documents via computer."
            },
            "slug": "Constructing-Web-based-legacy-index-card-design-and-Downton-Tams",
            "title": {
                "fragments": [],
                "text": "Constructing Web-based legacy index card archives-architectural design issues and initial data acquisition"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The VIADOCS system is being developed and evaluated on a demonstrator archive of 30,000 pyraloid moth cards at the UK Natural History Museum, and has already demonstrated a successful and efficient mechanism for image acquisition using a modified bank cheque scanner."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Sixth International Conference on Document Analysis and Recognition"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46719733"
                        ],
                        "name": "J. Simon",
                        "slug": "J.-Simon",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Simon",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Simon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "Fourier-Mellin transform is used to correct rotation/shear, scale and translation errors [28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62739568,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "07f63a438f766d8ca48e1f338ed267f1eb20022f",
            "isKey": false,
            "numCitedBy": 193,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The state of the art in handwriting recognition, especially in cursive word recognition, is surveyed, and some basic notions are reviewed in the field of picture recognition, particularly, line image recognition. The usefulness of 'regular' versus 'singular' classes of features is stressed. These notions are applied to obtain a graph, G, representing a line image, and also to find an 'axis' as the regular part of G. The complements to G of the axis are the 'tarsi', singular parts of G, which correspond to informative features of a cursive word. A segmentation of the graph is obtained, giving a symbolic description chain (SDC). Using one or more as robust anchors, possible words in a list of words are selected. Candidate words are examined to see if the other letters fit the rest of the SDC. Good results are obtained for clean images of words written by several persons. >"
            },
            "slug": "Off-line-cursive-word-recognition-Simon",
            "title": {
                "fragments": [],
                "text": "Off-line cursive word recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The state of the art in handwriting recognition, especially in cursive word recognition, is surveyed, and some basic notions are reviewed in the field of picture recognition, particularly, line image recognition."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37526757"
                        ],
                        "name": "Urs-Viktor Marti",
                        "slug": "Urs-Viktor-Marti",
                        "structuredName": {
                            "firstName": "Urs-Viktor",
                            "lastName": "Marti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Urs-Viktor Marti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720945"
                        ],
                        "name": "H. Bunke",
                        "slug": "H.-Bunke",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bunke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bunke"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "A HMM based recognizer for large lexicons is examined for indexing historical documents in [23]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 114
                            }
                        ],
                        "text": "This is intended to circumvent the lack of perfect Optical Character Recognition (OCR) for ancient writing styles [23, 33, 66]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29622813,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "04a10e1b25f35a9ac1a4d4344bfbdb34b253cb59",
            "isKey": false,
            "numCitedBy": 1060,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract. In this paper we describe a database that consists of handwritten English sentences. It is based on the Lancaster-Oslo/Bergen (LOB) corpus. This corpus is a collection of texts that comprise about one million word instances. The database includes 1,066 forms produced by approximately 400 different writers. A total of 82,227 word instances out of a vocabulary of 10,841 words occur in the collection. The database consists of full English sentences. It can serve as a basis for a variety of handwriting recognition tasks. However, it is expected that the database would be particularly useful for recognition tasks where linguistic knowledge beyond the lexicon level is used, because this knowledge can be automatically derived from the underlying corpus. The database also includes a few image-processing procedures for extracting the handwritten text from the forms and the segmentation of the text into lines and words."
            },
            "slug": "The-IAM-database:-an-English-sentence-database-for-Marti-Bunke",
            "title": {
                "fragments": [],
                "text": "The IAM-database: an English sentence database for offline handwriting recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A database that consists of handwritten English sentences based on the Lancaster-Oslo/Bergen corpus, which is expected that the database would be particularly useful for recognition tasks where linguistic knowledge beyond the lexicon level is used."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal on Document Analysis and Recognition"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48471936"
                        ],
                        "name": "D. Doermann",
                        "slug": "D.-Doermann",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doermann",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747801"
                        ],
                        "name": "E. Rivlin",
                        "slug": "E.-Rivlin",
                        "structuredName": {
                            "firstName": "Ehud",
                            "lastName": "Rivlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Rivlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143867195"
                        ],
                        "name": "I. Weiss",
                        "slug": "I.-Weiss",
                        "structuredName": {
                            "firstName": "Isaac",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 158
                            }
                        ],
                        "text": "Most of the proposed algorithms for optimum image binarization rely on statistical methods, without taking into account the special nature of document images [8-10]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 22
                            }
                        ],
                        "text": "Structural techniques [9, 10] classify characters on the basis of geometric attributes, relying on coarse characteristics at first, and classifying sub-groups using detail features."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 82
                            }
                        ],
                        "text": "2), practical systems have been built using only the layout analysis stage of DIA [9, 26, 35]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 129
                            }
                        ],
                        "text": "Continuous scanning is followed by automatic frame cropping as an efficient and fast procedure to generate images from microfilm [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": "Zipf\u2019s Law [9] does not hold for words in the catalogue."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 96
                            }
                        ],
                        "text": "Researchers have been showing interest in developing recognition and DL tools for Indic scripts [3, 5, 6, 9, 10, 17, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 167
                            }
                        ],
                        "text": "This initial decision was made after consideration of a number of alternatives (including variants of histogram equalisation techniques [8] and Weszka and Rosenfeld\u2019s [9] approach)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 34
                            }
                        ],
                        "text": "Finally, we plan to use OntoLearn [9], a tool for ontology learning, to enrich the reference ontology with new concepts and relations extracted from a corpus of documents like the ones used for the ICDAR 2003 [10] page layout competition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 129
                            }
                        ],
                        "text": "Some approaches that do not involve any segmentation task are based on the concept and techniques of occluded object recognition [8,9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 52
                            }
                        ],
                        "text": "It is this robustness, described by Spitz and Marks [9], that led to the application of shape coding and reconstruction as a technique for handling poor quality images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14573635,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "566259863707583ad0684b06014a39b33660b808",
            "isKey": true,
            "numCitedBy": 75,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of logo recognition is of great interest in the document domain, especially for databases, because of its potential for identifying the source of the document and its generality as a recognition problem. By recognizing the logo, one obtains semantic information about the document, which may be useful in deciding whether or not to analyze the textual components. A multi-level stages approach to logo recognition which uses global invariants to prune the database and local affine invariants to obtain a more refined match is presented. An invariant signature which can be used for matching under a variety of transformations is obtained. The authors provide a method of computing Euclidean invariants and show how to extend them to capture similarity, affine, and projective invariants when necessary. They implement feature detection, feature extraction, and local invariant algorithms and successfully demonstrate the approach on a small database.<<ETX>>"
            },
            "slug": "Logo-recognition-using-geometric-invariants-Doermann-Rivlin",
            "title": {
                "fragments": [],
                "text": "Logo recognition using geometric invariants"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A method of computing Euclidean invariants is provided and shown how to extend them to capture similarity, affine, and projective invariants when necessary and an invariant signature which can be used for matching under a variety of transformations is obtained."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR '93)"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409052480"
                        ],
                        "name": "S. C. Hinds",
                        "slug": "S.-C.-Hinds",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Hinds",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. C. Hinds"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2168553590"
                        ],
                        "name": "James L. Fisher",
                        "slug": "James-L.-Fisher",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Fisher",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. Fisher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1402307078"
                        ],
                        "name": "D. D'Amato",
                        "slug": "D.-D'Amato",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "D'Amato",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. D'Amato"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 64
                            }
                        ],
                        "text": "That is why more sophisticated methods have been proposed since [17,18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 12
                            }
                        ],
                        "text": "Statistical [5, 14, 17] techniques extract representative feature vectors from character images and apply pattern classification techniques on these features."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 96
                            }
                        ],
                        "text": "Researchers have been showing interest in developing recognition and DL tools for Indic scripts [3, 5, 6, 9, 10, 17, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 129
                            }
                        ],
                        "text": "In terms of character encoding, the Unicode Consortium aims at providing a reliable encoding scheme for all scripts in the world [17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "Many non-Latin scripts have a complicated character set and need a separate encoding system [17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 155
                            }
                        ],
                        "text": "Below follows a detailed step-by-step description of the post-processing algorithm that consists of a consequent application of shrink and swell filtering [17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61919721,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b27f45bebdec8e8d8503f7314533674c51de0e84",
            "isKey": true,
            "numCitedBy": 249,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "As part of the development of a document image analysis system, a method, based on the Hough transform, was devised for the detection of document skew and interline spacing-necessary parameters for the automatic segmentation of text from graphics. Because the Hough transform is computationally expensive, the amount of data within a document image is reduced through the computation of its horizontal and vertical black runlengths. Histograms of these runlengths are used to determine whether the document is in portrait or landscape orientation. A gray scale burst image is created from the black runlengths that are perpendicular to the text lines by placing the length of the run in the run's bottom-most pixel. By creating a burst image from the original document image, the processing time of the Hough transform can be reduced by a factor of as much as 7.4 for documents with gray-scale images. Because only small runlengths are input to the Hough transform and because the accumulator array is incremented by the runlength associated with a pixel rather than by a factor of 1, the negative effects of noise, black margins, and figures are avoided. Consequently, interline spacing can be determined more accurately.<<ETX>>"
            },
            "slug": "A-document-skew-detection-method-using-run-length-Hinds-Fisher",
            "title": {
                "fragments": [],
                "text": "A document skew detection method using run-length encoding and the Hough transform"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "By creating a burst image from the original document image, the processing time of the Hough transform can be reduced by a factor of as much as 7.4 for documents with gray-scale images and interline spacing can be determined more accurately."
            },
            "venue": {
                "fragments": [],
                "text": "[1990] Proceedings. 10th International Conference on Pattern Recognition"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054206556"
                        ],
                        "name": "Matthias Zimmermann",
                        "slug": "Matthias-Zimmermann",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Zimmermann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthias Zimmermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720945"
                        ],
                        "name": "H. Bunke",
                        "slug": "H.-Bunke",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bunke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bunke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 114
                            }
                        ],
                        "text": "This is intended to circumvent the lack of perfect Optical Character Recognition (OCR) for ancient writing styles [23, 33, 66]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1528253,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "189c52bde47d04f4e4b4bfe2825c9bab9c35be22",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Presents an automatic segmentation scheme for cursive handwritten text lines using the transcriptions of the text lines and a hidden Markov model (HMM) based recognition system. The segmentation scheme has been developed and tested on the IAM database that contains offline images of cursively handwritten English text. The original version of this database contains ground truth for complete lines of text only, but not for individual words. With the method described in the paper the usability of the database is greatly improved because accurate bounding box information and ground truth for individual words (including punctuation characters) is now available as well. Applying the segmentation scheme on 417 pages of handwritten text a correct word segmentation rate of 98% has been achieved, producing correct bounding boxes for over 25,000 handwritten words."
            },
            "slug": "Automatic-segmentation-of-the-IAM-off-line-database-Zimmermann-Bunke",
            "title": {
                "fragments": [],
                "text": "Automatic segmentation of the IAM off-line database for handwritten English text"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "An automatic segmentation scheme for cursive handwritten text lines using the transcriptions of the text lines and a hidden Markov model (HMM) based recognition system is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Object recognition supported by user interaction for service robots"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37526757"
                        ],
                        "name": "Urs-Viktor Marti",
                        "slug": "Urs-Viktor-Marti",
                        "structuredName": {
                            "firstName": "Urs-Viktor",
                            "lastName": "Marti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Urs-Viktor Marti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720945"
                        ],
                        "name": "H. Bunke",
                        "slug": "H.-Bunke",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bunke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bunke"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 145
                            }
                        ],
                        "text": "Most designers of multilingual software resort to Unicode-based fonts, and software vendors provide detailed guidelines for internationalization [24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10207300,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e15725948c2ea8b190b825a0887e430dc4898428",
            "isKey": false,
            "numCitedBy": 486,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, a system for the reading of totally unconstrained handwritten text is presented. The kernel of the system is a hidden Markov model (HMM) for handwriting recognition. The HMM is enhanced by a statistical language model. Thus linguistic knowledge beyond the lexicon level is incorporated in the recognition process. Another novel feature of the system is that the HMM is applied in such a way that the difficult problem of segmenting a line of text into individual words is avoided. A number of experiments with various language models and large vocabularies have been conducted. The language models used in the system were also analytically compared based on their perplexity."
            },
            "slug": "Using-a-Statistical-Language-Model-to-Improve-the-Marti-Bunke",
            "title": {
                "fragments": [],
                "text": "Using a Statistical Language Model to Improve the Performance of an HMM-Based Cursive Handwriting Recognition System"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel feature of the system is that the HMM is applied in such a way that the difficult problem of segmenting a line of text into individual words is avoided and linguistic knowledge beyond the lexicon level is incorporated in the recognition process."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Pattern Recognit. Artif. Intell."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117470205"
                        ],
                        "name": "Christiane Schmidt",
                        "slug": "Christiane-Schmidt",
                        "structuredName": {
                            "firstName": "Christiane",
                            "lastName": "Schmidt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christiane Schmidt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713850"
                        ],
                        "name": "K. Kraiss",
                        "slug": "K.-Kraiss",
                        "structuredName": {
                            "firstName": "Karl-Friedrich",
                            "lastName": "Kraiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kraiss"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 206
                            }
                        ],
                        "text": "Three different cases are compared in terms of quality value: the direct application of the off-the-shelf package to the document, the application of the OCR package following thresholding by Otsu\u2019s method [7], and finally, the comprehensive approach of the MEMORIAL project."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 133
                            }
                        ],
                        "text": "For handwritten character recognition two main approaches can be identified: The global approach [4,5] and the segmentation approach [6,7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 257,
                                "start": 254
                            }
                        ],
                        "text": "For the second phase, all tests have been carried out using the relevance feedback process by which the user analyzes the responses of the system and indicates, for each item retrieved, a degree of relevance/non-relevance or the exactness of the ranking [7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 6
                            }
                        ],
                        "text": "ATLAS [7] outlines an annotation graph model for linguistic material (text, image and video)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 88
                            }
                        ],
                        "text": "But it is known that recognition-based Hanja segmentation methods have some problems [6][7]: (1) Out-of-class which has incomplete shape is matched into a character defined by recognition model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 66
                            }
                        ],
                        "text": "A method based on a shortest spanning tree search is presented in [7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 162
                            }
                        ],
                        "text": "Image color clustering by means of self-organizing maps has been proposed in various works to achieve different goals, for instance, segmentation and compression [5-10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 29
                            }
                        ],
                        "text": "He used Bandinel\u2019s Catalogue [7] of the Bodleian Library in Oxford as a model but made significant improvements."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 264,
                                "start": 259
                            }
                        ],
                        "text": "The global methods (global thresholding) use a single threshold value to classify image pixels into object or background classes [1-5], whereas the local schemes (adaptive thresholding) can use multiple values selected according to the local area information [6,7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 57
                            }
                        ],
                        "text": "Twenty-nine regular papers, published in the proceedings [7], established the framework for discussion, which embraced six broad topics:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8595306,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8cab7f41e35cb1535459d1bb766a35f6d8c8b03e",
            "isKey": true,
            "numCitedBy": 27,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper presents a novel method of on-line signature verification that analyzes both the shape of the signature and dynamics of the writing process. This approach automatically determines characteristic features of the written image and combines these shape features with features from the writing dynamics. For establishing a writing characteristic template for one signer the signature is separated into characteristic segments. The segmentation algorithm extracts writing points which would give a forgery the appearance of the original. For these significant points local extreme values, which identify writing segments, are calculated. Subsequently, dynamic features are computed for the segments. The developed system needs three signatures of one person for the establishment of a personalized template. A database has been collected with 544 signatures of 27 signers for evaluation. The developed system achieved a correct acceptance rate of 78% and a correct rejection rate of 100%."
            },
            "slug": "Establishment-of-personalized-templates-for-Schmidt-Kraiss",
            "title": {
                "fragments": [],
                "text": "Establishment of personalized templates for automatic signature verification"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A novel method of on-line signature verification that analyzes both the shape of the signature and dynamics of the writing process and automatically determines characteristic features of the written image and combines these shape features with features from the writing dynamics."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fourth International Conference on Document Analysis and Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145652961"
                        ],
                        "name": "L. Xu",
                        "slug": "L.-Xu",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9745969"
                        ],
                        "name": "A. Krzy\u017cak",
                        "slug": "A.-Krzy\u017cak",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Krzy\u017cak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krzy\u017cak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713795"
                        ],
                        "name": "C. Suen",
                        "slug": "C.-Suen",
                        "structuredName": {
                            "firstName": "Ching",
                            "lastName": "Suen",
                            "middleNames": [
                                "Yee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Suen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "For Hanja scripts, OCR and UI techniques [31] incorporate nonlinear shape normalization, contour direction features and recognizers based on Mahalanobis distance to generate transcripts for Hanja (Korean) documents."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206400534,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ec25da04ef7f09396ca00da3f9b5f2d9670cb6fc",
            "isKey": false,
            "numCitedBy": 2369,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Possible solutions to the problem of combining classifiers can be divided into three categories according to the levels of information available from the various classifiers. Four approaches based on different methodologies are proposed for solving this problem. One is suitable for combining individual classifiers such as Bayesian, k-nearest-neighbor, and various distance classifiers. The other three could be used for combining any kind of individual classifiers. On applying these methods to combine several classifiers for recognizing totally unconstrained handwritten numerals, the experimental results show that the performance of individual classifiers can be improved significantly. For example, on the US zipcode database, 98.9% recognition with 0.90% substitution and 0.2% rejection can be obtained, as well as high reliability with 95% recognition, 0% substitution, and 5% rejection. >"
            },
            "slug": "Methods-of-combining-multiple-classifiers-and-their-Xu-Krzy\u017cak",
            "title": {
                "fragments": [],
                "text": "Methods of combining multiple classifiers and their applications to handwriting recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "On applying these methods to combine several classifiers for recognizing totally unconstrained handwritten numerals, the experimental results show that the performance of individual classifiers can be improved significantly."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713795"
                        ],
                        "name": "C. Suen",
                        "slug": "C.-Suen",
                        "structuredName": {
                            "firstName": "Ching",
                            "lastName": "Suen",
                            "middleNames": [
                                "Yee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Suen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717110"
                        ],
                        "name": "C. Nadal",
                        "slug": "C.-Nadal",
                        "structuredName": {
                            "firstName": "Christine",
                            "lastName": "Nadal",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Nadal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49691013"
                        ],
                        "name": "R. Legault",
                        "slug": "R.-Legault",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Legault",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Legault"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47184210"
                        ],
                        "name": "T. Mai",
                        "slug": "T.-Mai",
                        "structuredName": {
                            "firstName": "Tuan",
                            "lastName": "Mai",
                            "middleNames": [
                                "Anh"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Mai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144905110"
                        ],
                        "name": "L. Lam",
                        "slug": "L.-Lam",
                        "structuredName": {
                            "firstName": "Louisa",
                            "lastName": "Lam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Lam"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "To pick only a single example, the JSTOR DL [29] includes over 12 million imaged pages from over 300 scholarly journals and allows searching on (OCRed) full text as well as on selected metadata (author, title, or abstract field)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62684858,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "08ce01a35cf8af38ee24a9a0f549f7d4d16e55bf",
            "isKey": false,
            "numCitedBy": 347,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Four independently, developed expert algorithms for recognizing unconstrained handwritten numerals are presented. All have high recognition rates. Different experimental approaches for incorporating these recognition methods into a more powerful system are also presented. The resulting multiple-expert system proves that the consensus of these methods tends to compensate for individual weaknesses, while preserving individual strengths. It is shown that it is possible to reduce the substitution rate to a desired level while maintaining a fairly high recognition rate in the classification of totally unconstrained handwritten ZIP code numerals. If reliability is of the utmost importance, substitutions can be avoided completely (reliability=100%) while retaining a recognition rate above 90%. Results are compared with those for some of the most effective numeral recognition systems found in the literature. >"
            },
            "slug": "Computer-recognition-of-unconstrained-handwritten-Suen-Nadal",
            "title": {
                "fragments": [],
                "text": "Computer recognition of unconstrained handwritten numerals"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown that it is possible to reduce the substitution rate to a desired level while maintaining a fairly high recognition rate in the classification of totally unconstrained handwritten ZIP code numerals."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2282951"
                        ],
                        "name": "P. Suda",
                        "slug": "P.-Suda",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Suda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Suda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50402126"
                        ],
                        "name": "C. Bridoux",
                        "slug": "C.-Bridoux",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Bridoux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bridoux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2062799"
                        ],
                        "name": "B. K\u00e4mmerer",
                        "slug": "B.-K\u00e4mmerer",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "K\u00e4mmerer",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. K\u00e4mmerer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3081240"
                        ],
                        "name": "G. Maderlechner",
                        "slug": "G.-Maderlechner",
                        "structuredName": {
                            "firstName": "Gerd",
                            "lastName": "Maderlechner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Maderlechner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 903,
                                "start": 898
                            }
                        ],
                        "text": "June 2004 Simone Marinai Andreas Dengel\nOrganization\nWorkshop Co-chairs\nSimone Marinai University of Florence, Italy Andreas Dengel DFKI, Germany\nProgram Committee\nApostolos Antonacopoulos University of Liverpool, UK Henry Baird Lehigh University, USA Francesca Cesarini University of Florence, Italy David Doermann University of Maryland, USA Andrew Downton University of Essex, UK Hiromichi Fujisawa Hitachi Central Research Laboratory, Japan Jianying Hu IBM T.J. Watson Research Center, USA Rolf Ingold University of Fribourg, Switzerland Ramanujan Kashi Avaya Labs Research, USA Koichi Kise Osaka Prefecture University, Japan Dan Lopresti Lehigh University, USA Donato Malerba University of Bari, Italy Udo Miletzki Siemens Dematic, Germany Yasuaki Nakano Kyushu University, Japan Lambert Schomaker Rijksuniversiteit Groningen, The Netherlands Giovanni Soda University of Florence, Italy Larry Spitz Document Recognition Technologies, New Zealand Karl Tombre LORIA-INPL, France Luc Vincent LizardTech, USA Marcel Worring University of Amsterdam, The Netherlands\nAdditional Referees\nAnnalisa Appice Margherita Berardi Alain Biem Thomas Breuel Michelangelo Ceci Philippe Dosch Stefan Jaeger\nDimosthenis Karatzas Michele Lapi Larry O\u2019Gorman Huanfeng Ma Ge\u0301rald Masini Eugene Ratzlaff Maurizio Rigamonti\nT.R. Roth-Berghofer Jane Snowdon Salvatore Tabbone Yefeng Zheng Gary Zi\nTable of Contents\nDigital Libraries\nDocument Analysis Systems for Digital Libraries: Challenges and Opportunities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\nHenry S. Baird, Venugopal Govindaraju, and Daniel P. Lopresti\nThe Trinity College Dublin 1872 Online Catalogue . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 79
                            }
                        ],
                        "text": "An enhancement of the document reconstruction technique was described by Spitz [10] and draws from all three of the previously cited processes, thus providing an integration of information from multiple sources: word shape, character bitmaps and lexical information."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "An approach is described in Nic Gerailt and Byrne [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 158
                            }
                        ],
                        "text": "Most of the proposed algorithms for optimum image binarization rely on statistical methods, without taking into account the special nature of document images [8-10]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 22
                            }
                        ],
                        "text": "Structural techniques [9, 10] classify characters on the basis of geometric attributes, relying on coarse characteristics at first, and classifying sub-groups using detail features."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 162
                            }
                        ],
                        "text": "Image color clustering by means of self-organizing maps has been proposed in various works to achieve different goals, for instance, segmentation and compression [5-10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 130
                            }
                        ],
                        "text": "There exist several hole detection algorithms mainly based on contour following and distinguishing external and internal contours [10,11]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[10] recognize Arabic and English, using wordbased HMM\u2019s with trigram character probabilities to improve recognition rates."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 96
                            }
                        ],
                        "text": "Researchers have been showing interest in developing recognition and DL tools for Indic scripts [3, 5, 6, 9, 10, 17, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 11
                            }
                        ],
                        "text": "17 John G. Byrne"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 209
                            }
                        ],
                        "text": "Finally, we plan to use OntoLearn [9], a tool for ontology learning, to enrich the reference ontology with new concepts and relations extracted from a corpus of documents like the ones used for the ICDAR 2003 [10] page layout competition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 15
                            }
                        ],
                        "text": "51\nA. Lawrence Spitz\nA Segmentation-Free Recognition Technique to Assist Old Greek Handwritten Manuscript OCR . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 80
                            }
                        ],
                        "text": "The line warp was removed in the process of character shape coding described in [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 23525902,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "59151b09371a2474095b560d142fcea49b2a7185",
            "isKey": true,
            "numCitedBy": 24,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper presents work in the field of logo and word recognition. The approach is based on a general theory for signal registration and is thus applicable to a broad variety of signal processing domains. It has been fruitfully applied to solve speech and handwriting recognition as well as tasks in the field of document analysis."
            },
            "slug": "Logo-and-word-matching-using-a-general-approach-to-Suda-Bridoux",
            "title": {
                "fragments": [],
                "text": "Logo and word matching using a general approach to signal registration"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "The approach is based on a general theory for signal registration and is thus applicable to a broad variety of signal processing domains and has been fruitfully applied to solve speech and handwriting recognition as well as tasks in the field of document analysis."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fourth International Conference on Document Analysis and Recognition"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780936"
                        ],
                        "name": "B. Wirtz",
                        "slug": "B.-Wirtz",
                        "structuredName": {
                            "firstName": "Brigitte",
                            "lastName": "Wirtz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Wirtz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 13
                            }
                        ],
                        "text": "Later, Spitz [6] described an unusual, lexically-driven word recognition engine based on the reconstruction and resolving ambiguities based on selective matching of character bitmaps."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 231
                            }
                        ],
                        "text": "Suryaprakash Kompalli, Srirangaraj Setlur, and Venugopal Govindaraju\nA Semantic-Based System for Querying Personal Digital Libraries . . . . . . . . 39 Luigi Cinque, Alessio Malizia, and Roberto Navigli\nToward Personalized Digital Library for Providing \u201cInformation JIT\u201d . . . . . 47 Hisashi Ikeda, Naohiro Furukawa, Katsumi Marukawa, and Hiromichi Fujisawa\nHistorical Documents\nTilting at Windmills: Adventures in Attempting to Reconstruct Don Quixote . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 903,
                                "start": 898
                            }
                        ],
                        "text": "June 2004 Simone Marinai Andreas Dengel\nOrganization\nWorkshop Co-chairs\nSimone Marinai University of Florence, Italy Andreas Dengel DFKI, Germany\nProgram Committee\nApostolos Antonacopoulos University of Liverpool, UK Henry Baird Lehigh University, USA Francesca Cesarini University of Florence, Italy David Doermann University of Maryland, USA Andrew Downton University of Essex, UK Hiromichi Fujisawa Hitachi Central Research Laboratory, Japan Jianying Hu IBM T.J. Watson Research Center, USA Rolf Ingold University of Fribourg, Switzerland Ramanujan Kashi Avaya Labs Research, USA Koichi Kise Osaka Prefecture University, Japan Dan Lopresti Lehigh University, USA Donato Malerba University of Bari, Italy Udo Miletzki Siemens Dematic, Germany Yasuaki Nakano Kyushu University, Japan Lambert Schomaker Rijksuniversiteit Groningen, The Netherlands Giovanni Soda University of Florence, Italy Larry Spitz Document Recognition Technologies, New Zealand Karl Tombre LORIA-INPL, France Luc Vincent LizardTech, USA Marcel Worring University of Amsterdam, The Netherlands\nAdditional Referees\nAnnalisa Appice Margherita Berardi Alain Biem Thomas Breuel Michelangelo Ceci Philippe Dosch Stefan Jaeger\nDimosthenis Karatzas Michele Lapi Larry O\u2019Gorman Huanfeng Ma Ge\u0301rald Masini Eugene Ratzlaff Maurizio Rigamonti\nT.R. Roth-Berghofer Jane Snowdon Salvatore Tabbone Yefeng Zheng Gary Zi\nTable of Contents\nDigital Libraries\nDocument Analysis Systems for Digital Libraries: Challenges and Opportunities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\nHenry S. Baird, Venugopal Govindaraju, and Daniel P. Lopresti\nThe Trinity College Dublin 1872 Online Catalogue . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 133
                            }
                        ],
                        "text": "For handwritten character recognition two main approaches can be identified: The global approach [4,5] and the segmentation approach [6,7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 93
                            }
                        ],
                        "text": "Experimentation still takes place but as an initial approach, the method proposed by Niblack [6] has been adopted."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 10
                            }
                        ],
                        "text": "See Spitz [6] for a discussion of lexical intersection and specificity."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 175
                            }
                        ],
                        "text": "A DIA oriented approach is suggested to effectively increase resolution and digitization speed, as well as to ensure document preservation during scanning and quality control [6, 35]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 435,
                                "start": 428
                            }
                        ],
                        "text": "Document Analysis Systems VI\n6th International Workshop, DAS 2004 Florence, Italy, September 8 - 10, 2004 Proceedings\n13\nVolume Editors\nSimone Marinai Universit\u00e0 di Firenze, Dipartimento di Sistemi e Informatica Via S. Marta, 3 - 50139 Firenze, Italy E-mail: marinai@dsi.unifi.it\nAndreas Dengel German Research Center for Artificial Intelligence (DFKI) P.O.Box 2080, 67608 Kaiserslautern, Germany E-mail: Andreas.Dengel@dfki.de\nLibrary of Congress Control Number: 2004111168\nCR Subject Classification (1998): I.5, H.3, I.4, I.7, J.1, J.2\nISSN 0302-9743 ISBN 3-540-23060-2 Springer Berlin Heidelberg New York\nThis work is subject to copyright."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 264,
                                "start": 259
                            }
                        ],
                        "text": "The global methods (global thresholding) use a single threshold value to classify image pixels into object or background classes [1-5], whereas the local schemes (adaptive thresholding) can use multiple values selected according to the local area information [6,7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 96
                            }
                        ],
                        "text": "Researchers have been showing interest in developing recognition and DL tools for Indic scripts [3, 5, 6, 9, 10, 17, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 46
                            }
                        ],
                        "text": "For the Italian language we used MultiWordNet [6], an Italian version of WordNet."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 56
                            }
                        ],
                        "text": "For a more complete discussion of lexical structure see [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 15
                            }
                        ],
                        "text": "51\nA. Lawrence Spitz\nA Segmentation-Free Recognition Technique to Assist Old Greek Handwritten Manuscript OCR . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "In the following the history of the Library [6] is described briefly."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 29847847,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7ef209c0a63164a11aeae4cd08abfc39786dfac3",
            "isKey": true,
            "numCitedBy": 56,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new technique for dynamic signature verification. A dynamic programming (DP) approach is used for function-based signature verification. Dynamic data such as pressure is treated as a function of positional data and therefore evaluated locally. Verification is based on strokes as the structural units of the signature. This global knowledge is fed into the verification procedure. The application of a 3D non-linear correlation of the signature signals uses the stroke index as the third DP index. In conjunction with the definition of a finite state automaton on the set of reference strokes the system can handle different stroke numbers, missing or additional strokes correctly. The correct alignment of matching strokes is determined simultaneously to the signature verification process; an additional alignment stage before the actual nonlinear correlation is obsolete."
            },
            "slug": "Stroke-based-time-warping-for-signature-Wirtz",
            "title": {
                "fragments": [],
                "text": "Stroke-based time warping for signature verification"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "A dynamic programming (DP) approach is used for function-based signature verification and the application of a 3D non-linear correlation of the signature signals uses the stroke index as the third DP index to handle different stroke numbers, missing or additional strokes correctly."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 3rd International Conference on Document Analysis and Recognition"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34953007"
                        ],
                        "name": "H. Haruki",
                        "slug": "H.-Haruki",
                        "structuredName": {
                            "firstName": "Haruki",
                            "lastName": "Haruki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Haruki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122533395"
                        ],
                        "name": "T. Horiuchi",
                        "slug": "T.-Horiuchi",
                        "structuredName": {
                            "firstName": "Takahiko",
                            "lastName": "Horiuchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Horiuchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10639741"
                        ],
                        "name": "H. Yamada",
                        "slug": "H.-Yamada",
                        "structuredName": {
                            "firstName": "Hiromitsu",
                            "lastName": "Yamada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Yamada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143686714"
                        ],
                        "name": "Kazuhiko Yamamoto",
                        "slug": "Kazuhiko-Yamamoto",
                        "structuredName": {
                            "firstName": "Kazuhiko",
                            "lastName": "Yamamoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kazuhiko Yamamoto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 36
                            }
                        ],
                        "text": "It was carried out by undergraduate [2, 3] and postgraduate students [4, 5] commencing in 1990."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 205
                            }
                        ],
                        "text": "We have also developed a specific color segmentation algorithm for digitized documents based on a unsupervised classifier which use local information computed in a sliding window to adapt color variations [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 79
                            }
                        ],
                        "text": "To realize the digitization of personal annotations, we adopted the digital pen[3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": "Our system [3] uses a split and merge technique similar to the approach that has been obtained by Nagy\u2019s X-Y cut algorithm, but instead of working topdown, we use the recognized horizontal and vertical lines to cut the image into small regions, we then try to merge from bigger regions, using a quad-tree technique and image processing algorithms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 74
                            }
                        ],
                        "text": ", historians) who will most probably also transcribe the handwritten text [3]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 90
                            }
                        ],
                        "text": "On the storage front, XML is emerging as a versatile and preferred scheme for DL projects [3, 32, 53, 63]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 109
                            }
                        ],
                        "text": "As a result, we have developed a OCR-based system to enhance the overall efficiency of digitalization process[3](Fig."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 129
                            }
                        ],
                        "text": "The global methods (global thresholding) use a single threshold value to classify image pixels into object or background classes [1-5], whereas the local schemes (adaptive thresholding) can use multiple values selected according to the local area information [6,7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 96
                            }
                        ],
                        "text": "Researchers have been showing interest in developing recognition and DL tools for Indic scripts [3, 5, 6, 9, 10, 17, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[3], that significant applications such as text retrieval and topic identification can be accomplished even in the presence of OCR errors, digital libraries require a much higher level of accuracy in terms of faithful rendition of the source text."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6130957,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "eb297f38d5d4def06e9bda90c75a8d88df21c296",
            "isKey": true,
            "numCitedBy": 15,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The most important problem on automatic seal impression verification is to absorb the various impression quality depending on the various affixing conditions. Over the past years, many studies have been made on absorbing the various impression quality using a two-dimensional image as reference and input. But it is difficult to verify each of a large number of input patterns by a sole two-dimensional reference pattern. This paper proposes a seal identification method using a three-dimensional image (range image) as reference. As results of the verification with actual seal impressions, the verification rate is improved from that by using two-dimensional reference impressions."
            },
            "slug": "Automatic-seal-verification-using-three-dimensional-Haruki-Horiuchi",
            "title": {
                "fragments": [],
                "text": "Automatic seal verification using three-dimensional reference seals"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper proposes a seal identification method using a three-dimensional image (range image) as reference and results of the verification with actual seal impressions show the verification rate is improved from that by using two-dimensional reference impressions."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 13th International Conference on Pattern Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2640070"
                        ],
                        "name": "Hiromitsu Nishimura",
                        "slug": "Hiromitsu-Nishimura",
                        "structuredName": {
                            "firstName": "Hiromitsu",
                            "lastName": "Nishimura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hiromitsu Nishimura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111044343"
                        ],
                        "name": "Makoto Kobayashi",
                        "slug": "Makoto-Kobayashi",
                        "structuredName": {
                            "firstName": "Makoto",
                            "lastName": "Kobayashi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Makoto Kobayashi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740697"
                        ],
                        "name": "M. Maruyama",
                        "slug": "M.-Maruyama",
                        "structuredName": {
                            "firstName": "Minoru",
                            "lastName": "Maruyama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Maruyama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737939"
                        ],
                        "name": "Y. Nakano",
                        "slug": "Y.-Nakano",
                        "structuredName": {
                            "firstName": "Yasuaki",
                            "lastName": "Nakano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Nakano"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 40
                            }
                        ],
                        "text": "New technologies such as E-ink [21] and Gyricon [25] promise electronic document display with more of the advantages of paper (and new advantages of electronics)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "\u201d New technologies such as E-ink [21] and Gyricon [25] promise electronic document display with more of the advantages of paper (and new advantages of electronics)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 33290391,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5b22efc6952e6532779fe1368d563892dedb51b7",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The purpose of our research is to improve the recognition rate of offline character recognition systems using the HMM (Hidden Markov Model) without increasing the number of HMM parameters too much. Some 2-dimensional HMM character recognition systems have been proposed to increase representational power. However since 2D HMM has a much more complex structure and thus requires much more parameters than 1-dimensional HMM, it becomes very hard to gather sufficient samples in order to guarantee successful generalization. To overcome the problem, we propose a method for character recognition using 1D HMMs in multiple directions with 2-dimensional feature extraction. To further improve the performance, some voting methods using a bagging algorithm are also exploited. In our experiment, the recognition rate is increased by about 1% with the multiple directional HMM character recognition system compared to the 1D HMM character recognition system. The recognition rate is further increased by about 1% with the HMM character recognition system using a bagging algorithm."
            },
            "slug": "Off-line-character-recognition-using-HMM-by-feature-Nishimura-Kobayashi",
            "title": {
                "fragments": [],
                "text": "Off-line character recognition using HMM by multiple directional feature extraction and voting with bagging algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "This research proposes a method for character recognition using 1D HMMs in multiple directions with 2-dimensional feature extraction and increases the recognition rate by about 1% compared to the 1D HMM character recognition system using a bagging algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR '99 (Cat. No.PR00318)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789709"
                        ],
                        "name": "K. Ueda",
                        "slug": "K.-Ueda",
                        "structuredName": {
                            "firstName": "Katsuhiko",
                            "lastName": "Ueda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Ueda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069229192"
                        ],
                        "name": "Takeshi Mutoh",
                        "slug": "Takeshi-Mutoh",
                        "structuredName": {
                            "firstName": "Takeshi",
                            "lastName": "Mutoh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Takeshi Mutoh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "25301001"
                        ],
                        "name": "Ken'ichi Matsuo",
                        "slug": "Ken'ichi-Matsuo",
                        "structuredName": {
                            "firstName": "Ken'ichi",
                            "lastName": "Matsuo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ken'ichi Matsuo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 97
                            }
                        ],
                        "text": "For handwritten character recognition two main approaches can be identified: The global approach [4,5] and the segmentation approach [6,7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[4] and Ho and Nagy [2] took a different approach, relying on similarity of bitmaps and assigning character identities based on statistics of occurrence as well as lexical matching."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 75
                            }
                        ],
                        "text": "Analyzing artificial degradation algorithms used in Latin script documents [4] and developing similar techniques for inflective scripts is also being considered."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 81
                            }
                        ],
                        "text": "Holistic techniques are being used for off-line and online recognition of Arabic [1, 4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 53
                            }
                        ],
                        "text": "All these tools are based on mathematical morphology [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 69
                            }
                        ],
                        "text": "It was carried out by undergraduate [2, 3] and postgraduate students [4, 5] commencing in 1990."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 283,
                                "start": 280
                            }
                        ],
                        "text": "On the other end of the spectrum, relatively modern printed documents do not suffer from significant substrate/ink degradation problems and lend themselves to a higher degree of automated processing, OCR (albeit not trivial) and more sophisticated content extraction and indexing [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 28
                            }
                        ],
                        "text": "Bazzi et al. [10] recognize Arabic and English, using wordbased HMM\u2019s with trigram character probabilities to improve recognition rates."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 169
                            }
                        ],
                        "text": "During the split phase, the whole image of the document is split according to the extracted vertical and horizontal lines as well as the boundaries of recognized images [4, 11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 129
                            }
                        ],
                        "text": "The global methods (global thresholding) use a single threshold value to classify image pixels into object or background classes [1-5], whereas the local schemes (adaptive thresholding) can use multiple values selected according to the local area information [6,7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 98
                            }
                        ],
                        "text": "2(a) it was decided to make use of this and write our own software which is described by Anderson [4] in his M."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 89
                            }
                        ],
                        "text": "Despite excellent advances in Latin script DL\u2019s, research in other scripts such as Indic (Arabic, Bengali, Devanagari, and Telugu), Chinese, Korean, etc. is only recently receiving attention."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 50
                            }
                        ],
                        "text": "He\nPhysical Layout Analysis of Complex Structured Arabic Documents Using Artificial Neural Nets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 68
                            }
                        ],
                        "text": "For instance, gaps may not be used to identify words in Chinese and Arabic."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 136
                            }
                        ],
                        "text": "Variation in the writing order of scripts, and the presence of language-specific constructs such as shirorekha (Devanagari), modifiers (Arabic and Devanagari), or non-regular word spacing (Arabic and Chinese) require different approaches to layout analysis."
                    },
                    "intents": []
                }
            ],
            "corpusId": 19311204,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "00b921a0547e0c0b5b3a0e994bebd22c512296ac",
            "isKey": true,
            "numCitedBy": 23,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an automatic seal imprint verification system composed of a seal imprint extraction stage and a verification stage. It is assumed that each seal imprint is affixed on a bankcheck which has a background pattern and a signature. The extraction stage to separately extract the seal imprint and the signature from a background consists of 3 steps: the transformation of RGB color bankcheck image into HSV color space, the extraction of seal imprint and signature by clustering method in HSV color space, and isolated noise and baseline elimination. The seal imprint verification stage following the extraction stage is based on a method using global and local features. An experiment has been performed to examine the performance of the proposed system. Forty genuine imprints and forty-eight forged imprints on real Japanese bankchecks with various background patterns and signatures were used in the experiment. The experiment achieved as low a false-rejection error rate (Type I error rate) of 7.5% and a false-acceptance error rate (Type II error rate) of 0%."
            },
            "slug": "Automatic-verification-system-for-seal-imprints-on-Ueda-Mutoh",
            "title": {
                "fragments": [],
                "text": "Automatic verification system for seal imprints on Japanese bankchecks"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "An automatic seal imprint verification system composed of a seal imprint extraction stage and a verification stage based on a method using global and local features to extract the seal imprint and the signature from a background."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143604406"
                        ],
                        "name": "B. Juang",
                        "slug": "B.-Juang",
                        "structuredName": {
                            "firstName": "Biing-Hwang",
                            "lastName": "Juang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Juang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 118
                            }
                        ],
                        "text": "individual words in a multilingual document [42] to those that determine scripts of lines [44] and entire text blocks [27, 62]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7788300,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "df50c6e1903b1e2d657f78c28ab041756baca86a",
            "isKey": false,
            "numCitedBy": 8924,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Fundamentals of Speech Recognition. 2. The Speech Signal: Production, Perception, and Acoustic-Phonetic Characterization. 3. Signal Processing and Analysis Methods for Speech Recognition. 4. Pattern Comparison Techniques. 5. Speech Recognition System Design and Implementation Issues. 6. Theory and Implementation of Hidden Markov Models. 7. Speech Recognition Based on Connected Word Models. 8. Large Vocabulary Continuous Speech Recognition. 9. Task-Oriented Applications of Automatic Speech Recognition."
            },
            "slug": "Fundamentals-of-speech-recognition-Rabiner-Juang",
            "title": {
                "fragments": [],
                "text": "Fundamentals of speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This book presents a meta-modelling framework for speech recognition that automates the very labor-intensive and therefore time-heavy and therefore expensive and expensive process of manually modeling speech."
            },
            "venue": {
                "fragments": [],
                "text": "Prentice Hall signal processing series"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50112753"
                        ],
                        "name": "Seong-Whan Lee",
                        "slug": "Seong-Whan-Lee",
                        "structuredName": {
                            "firstName": "Seong-Whan",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seong-Whan Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152672892"
                        ],
                        "name": "J. H. Kim",
                        "slug": "J.-H.-Kim",
                        "structuredName": {
                            "firstName": "Jin",
                            "lastName": "Kim",
                            "middleNames": [
                                "Hyung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H. Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 50
                            }
                        ],
                        "text": "Most designers of multilingual software resort to Unicode-based fonts, and software vendors provide detailed guidelines for internationalization [24]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[1], is raising the demand for converting paper documents into digital,"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 93
                            }
                        ],
                        "text": "The field of handwritten character recognition has made great progress during the past years [1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 81
                            }
                        ],
                        "text": "Holistic techniques are being used for off-line and online recognition of Arabic [1, 4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 122
                            }
                        ],
                        "text": "Platform Independent Representation: Indic script data is stored in XML, a versatile and preferred scheme for DL projects [1, 2, 8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 113
                            }
                        ],
                        "text": "Although alternate schemes have been suggested [43], they do not have the compatibility and global acceptance of Unicode."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 28
                            }
                        ],
                        "text": "Bazzi et al. [10] recognize Arabic and English, using wordbased HMM\u2019s with trigram character probabilities to improve recognition rates."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "The Printed Catalogue [1] is a finding-catalogue and Todd did not aim to produce a full descriptive catalogue."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 36
                            }
                        ],
                        "text": "In terms of character encoding, the Unicode Consortium aims at providing a reliable encoding scheme for all scripts in the world [17]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 81
                            }
                        ],
                        "text": "Alternate schemes do not have the compatibility and global acceptance of Unicode [1, 8, 19, 22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 81
                            }
                        ],
                        "text": "Even if DIA could provide \u201cperfect\u201d transcription of the textual content (as ASCII/Unicode/XML), many critical features of its original appearance may have been discarded."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 89
                            }
                        ],
                        "text": "Despite excellent advances in Latin script DL\u2019s, research in other scripts such as Indic (Arabic, Bengali, Devanagari, and Telugu), Chinese, Korean, etc. is only recently receiving attention."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 79
                            }
                        ],
                        "text": "were able to significantly improve OCR performance by pre-processing the image [1], after making evaluations of which image processing steps were likely to help in each particular document image."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 50
                            }
                        ],
                        "text": "He\nPhysical Layout Analysis of Complex Structured Arabic Documents Using Artificial Neural Nets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 74
                            }
                        ],
                        "text": "Ontologies play a major role in the context of the so called Semantic Web [1], Tim Berners-Lee\u2019s vision of the next-generation Web, by enabling semantic awareness for online content."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 68
                            }
                        ],
                        "text": "For instance, gaps may not be used to identify words in Chinese and Arabic."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 203
                            }
                        ],
                        "text": "Users of DL systems can retrieve documents relevant to their needs by providing keywords with DL systems, and browse retrieved documents by referring document clustering results or summaries of documents[1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 141
                            }
                        ],
                        "text": "Most of the books and periodicals in the Library of Trinity College Dublin received up to 1872 are listed in the so called Printed Catalogue [1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 136
                            }
                        ],
                        "text": "Variation in the writing order of scripts, and the presence of language-specific constructs such as shirorekha (Devanagari), modifiers (Arabic and Devanagari), or non-regular word spacing (Arabic and Chinese) require different approaches to layout analysis."
                    },
                    "intents": []
                }
            ],
            "corpusId": 35490970,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d1c45cfadff66b8003dca3427f1ef2ff0e81753",
            "isKey": true,
            "numCitedBy": 25,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Unconstrained-seal-imprint-verification-using-graph-Lee-Kim",
            "title": {
                "fragments": [],
                "text": "Unconstrained seal imprint verification using attributed stroke graph matching"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2975527"
                        ],
                        "name": "L. Zadeh",
                        "slug": "L.-Zadeh",
                        "structuredName": {
                            "firstName": "Lotfi",
                            "lastName": "Zadeh",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Zadeh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760085"
                        ],
                        "name": "J. Kacprzyk",
                        "slug": "J.-Kacprzyk",
                        "structuredName": {
                            "firstName": "Janusz",
                            "lastName": "Kacprzyk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kacprzyk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 903,
                                "start": 898
                            }
                        ],
                        "text": "June 2004 Simone Marinai Andreas Dengel\nOrganization\nWorkshop Co-chairs\nSimone Marinai University of Florence, Italy Andreas Dengel DFKI, Germany\nProgram Committee\nApostolos Antonacopoulos University of Liverpool, UK Henry Baird Lehigh University, USA Francesca Cesarini University of Florence, Italy David Doermann University of Maryland, USA Andrew Downton University of Essex, UK Hiromichi Fujisawa Hitachi Central Research Laboratory, Japan Jianying Hu IBM T.J. Watson Research Center, USA Rolf Ingold University of Fribourg, Switzerland Ramanujan Kashi Avaya Labs Research, USA Koichi Kise Osaka Prefecture University, Japan Dan Lopresti Lehigh University, USA Donato Malerba University of Bari, Italy Udo Miletzki Siemens Dematic, Germany Yasuaki Nakano Kyushu University, Japan Lambert Schomaker Rijksuniversiteit Groningen, The Netherlands Giovanni Soda University of Florence, Italy Larry Spitz Document Recognition Technologies, New Zealand Karl Tombre LORIA-INPL, France Luc Vincent LizardTech, USA Marcel Worring University of Amsterdam, The Netherlands\nAdditional Referees\nAnnalisa Appice Margherita Berardi Alain Biem Thomas Breuel Michelangelo Ceci Philippe Dosch Stefan Jaeger\nDimosthenis Karatzas Michele Lapi Larry O\u2019Gorman Huanfeng Ma Ge\u0301rald Masini Eugene Ratzlaff Maurizio Rigamonti\nT.R. Roth-Berghofer Jane Snowdon Salvatore Tabbone Yefeng Zheng Gary Zi\nTable of Contents\nDigital Libraries\nDocument Analysis Systems for Digital Libraries: Challenges and Opportunities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\nHenry S. Baird, Venugopal Govindaraju, and Daniel P. Lopresti\nThe Trinity College Dublin 1872 Online Catalogue . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 38
                            }
                        ],
                        "text": "Complete developments may be found in [11,12]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 121
                            }
                        ],
                        "text": "since the ROI algorithm performance is encoder dependent, Park & Park [11] avoid bitplane shifting for the SPIHT encoder [12], as by mixing ROI and non-ROI coefficients in the same bit-plane through shifting, some correlations, which are otherwise present, cannot be used to reduce the bit-rate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 80
                            }
                        ],
                        "text": "use an automated web-based system for collecting annotations of French archives [12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[12] proposes a method for line detection and segmentation in historical church registers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 108
                            }
                        ],
                        "text": "However, the research on this topic has not been extensively studied so far, except Burge and Monagan\u2019s work[12] which made an attempt using the Voronoi tessellation for grouping words and multi-part symbols in a map understanding system."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 72
                            }
                        ],
                        "text": "The variable skew was removed first using techniques described in Spitz [12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 88
                            }
                        ],
                        "text": "The coordinates of these pixels can be extracted by using a contour following algorithm [12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 261,
                                "start": 257
                            }
                        ],
                        "text": "They are so relevant, that some distinguished researchers claimed that document image analysis and understanding belong to a branch of artificial intelligence [16], despite the fact that most of the contributions fall within the area of pattern recognition [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "ROI\u2019s were implemented using IW44 , SPIHT [12], and JPEG2000 [11] algorithms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 96
                            }
                        ],
                        "text": "Instead, adaptive to local information techniques for document binarization have been developed [11-14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 15
                            }
                        ],
                        "text": "51\nA. Lawrence Spitz\nA Segmentation-Free Recognition Technique to Assist Old Greek Handwritten Manuscript OCR . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                }
            ],
            "corpusId": 118210936,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a897a02f1adef7802c7f8030cd4a2e19153cfc7",
            "isKey": true,
            "numCitedBy": 746,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Partial table of contents: ISSUES IN THE MANAGEMENT OF UNCERTAINTY A Survey of Uncertain and Approximate Inference (R. Neapolitan) Rough Sets: A New Approach to Vagueness (Z. Pawlak) ASPECTS OF FUZZY LOGIC: THEORY AND IMPLEMENTATIONS LT-Fuzzy Logics (H. Rasiowa & N. Cat Ho) On Fuzzy Intuitionistic Logic (E. Turunen) On Modifier Logic (J. Mattila) FUZZY LOGIC FOR APPROXIMATE REASONING Presumption, Prejudice, and Regularity in Fuzzy Material Implication (T. Whalen & B. Schott) Inference for Information Systems Containing Probabilistic and Fuzzy Uncertainties (J. Baldwin) FUZZY LOGIC FOR KNOWLEDGE REPRESENTATION AND ELICITATION Approximate Reasoning in Diagnosis, Therapy, and Prognosis (A. Rocha, et al.) Elementary Learning in a Fuzzy Expert System (J. Buckley) KNOWLEDGE-BASED SYSTEMS USING FUZZY LOGIC Structured Local Fuzzy Logics in MILORD (J. Agustm, et al.) The Validation of Fuzzy Knowledge-Based Systems (A. Chang & L. Hall) FUZZY LOGIC FOR INTELLIGENT DATABASE MANAGEMENT SYSTEMS Fuzzy Querying in Conventional Databases (P. Bosc & O. Pivert) Index."
            },
            "slug": "Fuzzy-Logic-for-the-Management-of-Uncertainty-Zadeh-Kacprzyk",
            "title": {
                "fragments": [],
                "text": "Fuzzy Logic for the Management of Uncertainty"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Partial table of contents:Issues in the MANAGEMENT of UNCERTAINty A Survey of Uncertain and Approximate Inference."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144891254"
                        ],
                        "name": "D. Partridge",
                        "slug": "D.-Partridge",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Partridge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Partridge"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8504660"
                        ],
                        "name": "W. B. Yates",
                        "slug": "W.-B.-Yates",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Yates",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. B. Yates"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 82
                            }
                        ],
                        "text": "2), practical systems have been built using only the layout analysis stage of DIA [9, 26, 35]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1107076,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b86b13ea2493bab31cb4e47ceb31f0df2710b10",
            "isKey": false,
            "numCitedBy": 240,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we address the problem of constructing reliable neural-net implementations, given the assumption that any particular implementation will not be totally correct. The approach taken in this paper is to organize the inevitable errors so as to minimize their impact in the context of a multiversion system, i.e., the system functionality is reproduced in multiple versions, which together will constitute the neural-net system. The unique characteristics of neural computing are exploited in order to engineer reliable systems in the form of diverse, multiversion systems that are used together with a \"decision strategy\" (such as majority vote). Theoretical notions of \"methodological diversity\" contributing to the improvement of system performance are implemented and tested. An important aspect of the engineering of an optimal system is to overproduce the components and then choose an optimal subset. Three general techniques for choosing final system components are implemented and evaluated. Several different approaches to the effective engineering of complex multiversion systems designs are realized and evaluated to determine overall reliability as well as reliability of the overall system in comparison to the lesser reliability of component substructures."
            },
            "slug": "Engineering-Multiversion-Neural-Net-Systems-Partridge-Yates",
            "title": {
                "fragments": [],
                "text": "Engineering Multiversion Neural-Net Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The approach taken in this paper is to organize the inevitable errors so as to minimize their impact in the context of a multiversion system, i.e., the system functionality is reproduced in multiple versions, which together will constitute the neural-net system."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750281"
                        ],
                        "name": "M. Hanmandlu",
                        "slug": "M.-Hanmandlu",
                        "structuredName": {
                            "firstName": "Madasu",
                            "lastName": "Hanmandlu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hanmandlu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145278511"
                        ],
                        "name": "K. M. Mohan",
                        "slug": "K.-M.-Mohan",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Mohan",
                            "middleNames": [
                                "R.",
                                "Murali"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M. Mohan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075404578"
                        ],
                        "name": "Sourav Chakraborty",
                        "slug": "Sourav-Chakraborty",
                        "structuredName": {
                            "firstName": "Sourav",
                            "lastName": "Chakraborty",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sourav Chakraborty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143750885"
                        ],
                        "name": "Gaurav Garg",
                        "slug": "Gaurav-Garg",
                        "structuredName": {
                            "firstName": "Gaurav",
                            "lastName": "Garg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gaurav Garg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 50
                            }
                        ],
                        "text": "Most designers of multilingual software resort to Unicode-based fonts, and software vendors provide detailed guidelines for internationalization [24]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 122
                            }
                        ],
                        "text": "Platform Independent Representation: Indic script data is stored in XML, a versatile and preferred scheme for DL projects [1, 2, 8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 113
                            }
                        ],
                        "text": "Although alternate schemes have been suggested [43], they do not have the compatibility and global acceptance of Unicode."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 158
                            }
                        ],
                        "text": "Most of the proposed algorithms for optimum image binarization rely on statistical methods, without taking into account the special nature of document images [8-10]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 78
                            }
                        ],
                        "text": "Among the most known approaches for adaptive thresholding is Niblack\u2019s method [8] and Sauvola\u2019s method [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 49
                            }
                        ],
                        "text": "The OCROC program [2] was written in the Spitbol [8] implementation of Snobol 4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 92
                            }
                        ],
                        "text": "In the next version, we plan to include an inference system based on the rules described in [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 36
                            }
                        ],
                        "text": "In terms of character encoding, the Unicode Consortium aims at providing a reliable encoding scheme for all scripts in the world [17]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 81
                            }
                        ],
                        "text": "Alternate schemes do not have the compatibility and global acceptance of Unicode [1, 8, 19, 22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 81
                            }
                        ],
                        "text": "Even if DIA could provide \u201cperfect\u201d transcription of the textual content (as ASCII/Unicode/XML), many critical features of its original appearance may have been discarded."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 153
                            }
                        ],
                        "text": "Providing data sets for certain scripts is a non-trivial task due to their large character sets and the variety of recognition units used by researchers [8, 13, 14, 38]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 136
                            }
                        ],
                        "text": "This initial decision was made after consideration of a number of alternatives (including variants of histogram equalisation techniques [8] and Weszka and Rosenfeld\u2019s [9] approach)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 129
                            }
                        ],
                        "text": "Some approaches that do not involve any segmentation task are based on the concept and techniques of occluded object recognition [8,9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 163
                            }
                        ],
                        "text": "Common methods for Indic script OCR use structural features to build decision trees [13, 14] or combine multiple knowledge bases to create statistical classifiers [8, 38]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 29215761,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b4d5e3dc3ec4de2d891e98870999b662159a8a09",
            "isKey": true,
            "numCitedBy": 18,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Presents an approach for the verification of a signature using fuzzy modeling. For feature extraction, a signature is enclosed in a box. Taking the left side bottom corner of the box as the origin, angles of all pixels are calculated and then distributions of angles are generated using fixed class intervals. By considering these distributions as fuzzy sets, a Takagi-Sugeno model is constructed by defining the output of the signature to be a fixed number. This model is then used for the twin-purpose of verification and forgery detection. The results are demonstrated on several signature samples."
            },
            "slug": "Fuzzy-modeling-based-signature-verification-system-Hanmandlu-Mohan",
            "title": {
                "fragments": [],
                "text": "Fuzzy modeling based signature verification system"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "An approach for the verification of a signature using fuzzy modeling, a Takagi-Sugeno model is constructed by defining the output of the signature to be a fixed number for the twin-purpose of verification and forgery detection."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Sixth International Conference on Document Analysis and Recognition"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4632093"
                        ],
                        "name": "G. Zipf",
                        "slug": "G.-Zipf",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Zipf",
                            "middleNames": [
                                "Kingsley"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Zipf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 31034137,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "08d4da77c489d550c3e215725551481310719893",
            "isKey": false,
            "numCitedBy": 892,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Some people may be laughing when looking at you reading in your spare time. Some may be admired of you. And some may want be like you who have reading hobby. What about your own feel? Have you felt right? Reading is a need and a hobby at once. This condition is the on that will make you feel that you must read. If you know are looking for the book enPDFd human behavior and the principle of least effort an introduction to human ecology as the choice of reading, you can find here."
            },
            "slug": "Human-Behaviour-and-the-Principle-of-Least-Effort:-Zipf",
            "title": {
                "fragments": [],
                "text": "Human Behaviour and the Principle of Least Effort: an Introduction to Human Ecology"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Reading is a need and a hobby at once, and the principle of least effort an introduction to human ecology as the choice of reading is found here."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1949
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789709"
                        ],
                        "name": "K. Ueda",
                        "slug": "K.-Ueda",
                        "structuredName": {
                            "firstName": "Katsuhiko",
                            "lastName": "Ueda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Ueda"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "A joint activity between AIIM and the Association for Suppliers of Printing, Publishing and Converting Technologies (NPES) is discussing an international standard (PDF-Archive) [45] to define the use of PDF for archiving and preserving documents."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 20
                            }
                        ],
                        "text": "[4] and Ho and Nagy [2] took a different approach, relying on similarity of bitmaps and assigning character identities based on statistics of occurrence as well as lexical matching."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[2] use a combination of OCR and meta-data extraction to generate XML representations of newspapers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 36
                            }
                        ],
                        "text": "It was carried out by undergraduate [2, 3] and postgraduate students [4, 5] commencing in 1990."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 122
                            }
                        ],
                        "text": "Platform Independent Representation: Indic script data is stored in XML, a versatile and preferred scheme for DL projects [1, 2, 8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 117
                            }
                        ],
                        "text": "A notable exception is a project to convert file cards from the archives of the Natural History Museum in London, UK [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 69
                            }
                        ],
                        "text": "Metadata from MASTER project has been extended to Arabic manuscripts [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 227
                            }
                        ],
                        "text": "Test targets for evaluating scanners include:\n\u2013 IEEE Std 167A-1987, a facsimile machine test target that is produced by continuous-tone photography, with patterns and marks for a large range of measurements of moderate accuracy;\n\u2013 AIIM Scanner Target, an ink-on-paper, halftone-printed target; and \u2013 RIT Process Ink Gamut Chart, a four-color (cyan, magenta, yellow, and\nblack), halftone-printed chart for low accuracy color sensitivity determinations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 18
                            }
                        ],
                        "text": "The OCROC program [2] was written in the Spitbol [8] implementation of Snobol 4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 7
                            }
                        ],
                        "text": ", AIIM [2] ANSI/AIIM MS-44-1988 \u201cRecommended Practice for Quality Control of Image Scanners\u201d and MS-44, allow for the manual or automatic monitoring of image quality needed for DIA processing? Do we need to design new targets for this purpose?"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": "Document Analysis Systems for Digital Libraries 7\nTo what extent do existing test targets, e.g., AIIM [2] ANSI/AIIM MS-44-1988 \u201cRecommended Practice for Quality Control of Image Scanners\u201d and MS-44, allow for the manual or automatic monitoring of image quality needed for DIA processing?"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 152
                            }
                        ],
                        "text": "In order to remember the documents more deeply, we often make paper copies to read because paper is easy to glance over and suitable for careful reading[2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 12
                            }
                        ],
                        "text": "An ontology [2] specifies a shared understanding of a domain of interest."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60313103,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9134b189882ba65c7d52b18e215fe16517bf3b54",
            "isKey": true,
            "numCitedBy": 11,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-Seal-Imprint-Verification-System-with-and-Ueda",
            "title": {
                "fragments": [],
                "text": "Automatic Seal Imprint Verification System with Imprint Quality Assessment Function and Its Performance Evaluation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 132
                            }
                        ],
                        "text": "It currently supports all commercial scripts and is accepted as a system standard by many DL researchers and software manufacturers [11, 32, 36, 40, 60, 63]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 106
                            }
                        ],
                        "text": "Hash coding is used to create the index on disk and the formula is based on that used by Smith and Devine [11] in the Queen\u2019s University Belfast Microbird System."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 169
                            }
                        ],
                        "text": "During the split phase, the whole image of the document is split according to the extracted vertical and horizontal lines as well as the boundaries of recognized images [4, 11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 167
                            }
                        ],
                        "text": "Devanagari is a syllabic-alphabetic script derived from Brahmi, and provides written form to approximately forty eight languages including Hindi, Konkani, and Marathi [11, 13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 103
                            }
                        ],
                        "text": "Among the most known approaches for adaptive thresholding is Niblack\u2019s method [8] and Sauvola\u2019s method [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 130
                            }
                        ],
                        "text": "There exist several hole detection algorithms mainly based on contour following and distinguishing external and internal contours [10,11]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 219
                            }
                        ],
                        "text": "A number of independent bodies have devised encoding schemes, with each providing a set of codes for the alphabet set, and a corresponding set of parsing rules to create character forms from sequences of alphabet codes [11, 21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 96
                            }
                        ],
                        "text": "Instead, adaptive to local information techniques for document binarization have been developed [11-14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "As Spitz has shown [11], the performance of our reconstruction technique is improves with the use of the optimum lexicon: one with a large intersection of the words in the document and with a minimal number of words not found in the document."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "Sauvola\u2019s method [11] solves this problem by adding a hypothesis on the gray values of text and background pixels (text pixels have gray values near 0 and background pixels have gray values near 255), which results in the following formula for the threshold:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A symbol recogonition system"
            },
            "venue": {
                "fragments": [],
                "text": "Proc.ICDAR\u201993 2nd Int.Conf.on Document Analysis and Recognition"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 84
                            }
                        ],
                        "text": "In order to facilitate this task, we mapped each concept of our ontology to WordNet [5], a de facto standard lexicalized ontology containing more than 120,000 concepts."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 97
                            }
                        ],
                        "text": "For handwritten character recognition two main approaches can be identified: The global approach [4,5] and the segmentation approach [6,7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 34
                            }
                        ],
                        "text": "35% was achieved on the 100 pages [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 179
                            }
                        ],
                        "text": "For this application we developed the lexicon iteratively, starting with a bootstrap list of frequently-occurring words: articles, pronouns and some proper nouns, shown in Figure [5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 115
                            }
                        ],
                        "text": "Similar approaches used to disconnect touched components and filter noise of the objects contours are described in [5]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 69
                            }
                        ],
                        "text": "It was carried out by undergraduate [2, 3] and postgraduate students [4, 5] commencing in 1990."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 12
                            }
                        ],
                        "text": "Statistical [5, 14, 17] techniques extract representative feature vectors from character images and apply pattern classification techniques on these features."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 96
                            }
                        ],
                        "text": "Researchers have been showing interest in developing recognition and DL tools for Indic scripts [3, 5, 6, 9, 10, 17, 20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 105
                            }
                        ],
                        "text": "and Saturation) image data, and connected component analysis is used to remove reconstructed paper areas [5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "In 1995, Reynar, Spitz and Sibun [5] described a process of reconstructing a document from its image without resorting to Optical Character Recognition (OCR), leaving ambiguities to be resolved by a downstream process or by a human reader."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic signature verification and writer recognition -the state of the art"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognition 22"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 250
                            }
                        ],
                        "text": "To resolve the limitations of the na\u00efve methods, sophisticated methods have been developed on the assumption that images on both sides of the paper sheet are acquired and processed so that the front side image can be compared with the backside image [16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "TrueViz [36] uses a graphical keyboard for Russian script input."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "The use of a low-pass Wiener filter [16] has proved efficient for the above goals."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 159
                            }
                        ],
                        "text": "They are so relevant, that some distinguished researchers claimed that document image analysis and understanding belong to a branch of artificial intelligence [16], despite the fact that most of the contributions fall within the area of pattern recognition [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 157
                            }
                        ],
                        "text": "To avoid these drawbacks, we propose to adapt the techniques used in the domain of structured document recognition, to formalize prior syntactical knowledge [16], and we propose a segmentation strategy based on the principles of Dynamic Programming."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "integrate page segmentation analysis [18] with a visualization toolkit in TrueViz [16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Colour map detection for archive documents"
            },
            "venue": {
                "fragments": [],
                "text": "submitted to DAS2004"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 96
                            }
                        ],
                        "text": "Na\u00efve methods are based on extensions to binarization techniques for gray-scale document images [13,14,15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 108
                            }
                        ],
                        "text": "For the details about how to constructing the Voronoi tessellation in a digital image, readers can refer to [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 167
                            }
                        ],
                        "text": "Devanagari is a syllabic-alphabetic script derived from Brahmi, and provides written form to approximately forty eight languages including Hindi, Konkani, and Marathi [11, 13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "The classification step was performed using two well known classification algorithms, K-NN [13] and SVM [13,14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 158
                            }
                        ],
                        "text": "Forming Syllabic characters from Alphabets is a representative feature of Syllabic-alphabetic scripts, which include Devanagari, Telugu, Kannada, Bengali etc [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 153
                            }
                        ],
                        "text": "Providing data sets for certain scripts is a non-trivial task due to their large character sets and the variety of recognition units used by researchers [8, 13, 14, 38]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 49
                            }
                        ],
                        "text": "Among the most popular we can cite Kise\u2019s method [13] based on area Voronoi diagram, O\u2019Gorman\u2019s Docstrum method[14] based on neighbor clustering and Nagy\u2019s X-Y cut [15] based on the analysis of projection profiles."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 96
                            }
                        ],
                        "text": "Instead, adaptive to local information techniques for document binarization have been developed [11-14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 84
                            }
                        ],
                        "text": "Common methods for Indic script OCR use structural features to build decision trees [13, 14] or combine multiple knowledge bases to create statistical classifiers [8, 38]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fuzzy Sets for Intelligent Systems"
            },
            "venue": {
                "fragments": [],
                "text": "Morgan Kaufmann Publishers, San Mateo"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 22
                            }
                        ],
                        "text": "Creation of data sets [30, 32] is a welcome development in providing training and testing resources for non-Latin script OCR."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 90
                            }
                        ],
                        "text": "On the storage front, XML is emerging as a versatile and preferred scheme for DL projects [3, 32, 53, 63]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 132
                            }
                        ],
                        "text": "It currently supports all commercial scripts and is accepted as a system standard by many DL researchers and software manufacturers [11, 32, 36, 40, 60, 63]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[32] use a transliteration scheme, where Devanagari characters are entered by phonetic equivalent strings in English."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The HTK Hidden Markov Model Toolkit Book"
            },
            "venue": {
                "fragments": [],
                "text": "Entropic Cambridge Research Laboratory, http://htk.eng.cam.ac.uk/,"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145894068"
                        ],
                        "name": "F. J. Smith",
                        "slug": "F.-J.-Smith",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Smith",
                            "middleNames": [
                                "Jack"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. J. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121041119"
                        ],
                        "name": "K. Devine",
                        "slug": "K.-Devine",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Devine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Devine"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62568000,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "661ed3c2c112da335368f3a6657ea9cbd576670d",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "BIRD,-QUILL-and-MicroBIRD:-A-Successful-Family-of-Smith-Devine",
            "title": {
                "fragments": [],
                "text": "BIRD, QUILL and MicroBIRD: A Successful Family of Text Retrieval Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Error Detection in Several Languages for an OCRGenerated Multilingual Database"
            },
            "venue": {
                "fragments": [],
                "text": "Proc . Third International Workshop on Applications of Natural Language to Information Systems"
            },
            "year": 1949
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 24,
            "methodology": 22,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 30,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Document-Analysis-Systems-VI-Marinai-Dengel/4b43d647d8966f25757f05f0250531f0c0281992?sort=total-citations"
}