{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791802"
                        ],
                        "name": "J. Jeon",
                        "slug": "J.-Jeon",
                        "structuredName": {
                            "firstName": "Jiwoon",
                            "lastName": "Jeon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Jeon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757708"
                        ],
                        "name": "V. Lavrenko",
                        "slug": "V.-Lavrenko",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Lavrenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lavrenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14303727,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "228029e7533e32a025071e31e3f4f08d2bea5f5a",
            "isKey": false,
            "numCitedBy": 1301,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Libraries have traditionally used manual image annotation for indexing and then later retrieving their image collections. However, manual image annotation is an expensive and labor intensive procedure and hence there has been great interest in coming up with automatic ways to retrieve images based on content. Here, we propose an automatic approach to annotating and retrieving images based on a training set of images. We assume that regions in an image can be described using a small vocabulary of blobs. Blobs are generated from image features using clustering. Given a training set of images with annotations, we show that probabilistic models allow us to predict the probability of generating a word given the blobs in an image. This may be used to automatically annotate and retrieve images given a word as a query. We show that relevance models allow us to derive these probabilities in a natural way. Experiments show that the annotation performance of this cross-media relevance model is almost six times as good (in terms of mean precision) than a model based on word-blob co-occurrence model and twice as good as a state of the art model derived from machine translation. Our approach shows the usefulness of using formal information retrieval models for the task of image annotation and retrieval."
            },
            "slug": "Automatic-image-annotation-and-retrieval-using-Jeon-Lavrenko",
            "title": {
                "fragments": [],
                "text": "Automatic image annotation and retrieval using cross-media relevance models"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The approach shows the usefulness of using formal information retrieval models for the task of image annotation and retrieval by assuming that regions in an image can be described using a small vocabulary of blobs."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791802"
                        ],
                        "name": "J. Jeon",
                        "slug": "J.-Jeon",
                        "structuredName": {
                            "firstName": "Jiwoon",
                            "lastName": "Jeon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Jeon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757708"
                        ],
                        "name": "V. Lavrenko",
                        "slug": "V.-Lavrenko",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Lavrenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lavrenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 132
                            }
                        ],
                        "text": "Image annotation has been a topic of on-going research for more than a decade and several interesting techniques have been proposed [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 36
                            }
                        ],
                        "text": "Cross Media Relevance Models (CMRM) [5], Continuous Relevance Model (CRM) [7], and Multiple Bernoulli Relevance Model (MBRM) [9] assume different, nonparametric density representations of the joint word-image space."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61867173,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "149ba98b4573e39308b93b6fe3dc288a194cf9d2",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Libraries have traditionally used manual image annotation for indexing and then later retrieving their image collections. However, manual image annotation is an expensive and labor intensive procedure and hence there has been great interest in coming up with automatic ways to retrieve images based on content. Here, we propose an automatic approach to annotating and retrieving images based on a training set of images. We assume that regions in an image can be described using a small vocabulary of blobs. Blobs are generated from image features using clustering. Given a training set of images with annotations, we show that probabilistic models allow us to predict the probability of generating a word given the blobs in an image. This may be used to automatically annotate and retrieve images given a word as a query. We show that relevance models. allow us to derive these probabilities in a natural way. Experiments show that the annotation performance of this cross-media relevance model is almost six times as good (in terms of mean precision) than a model based on word-blob co-occurrence model and twice as good as a state of the art model derived from machine translation. Our approach shows the usefulness of using formal information retrieval models for the task of image annotation and retrieval."
            },
            "slug": "Automatic-Image-Annotation-and-Retrieval-using-Jeon-Lavrenko",
            "title": {
                "fragments": [],
                "text": "Automatic Image Annotation and Retrieval using CrossMedia Relevance Models"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The approach shows the usefulness of using formal information retrieval models for the task of image annotation and retrieval by assuming that regions in an image can be described using a small vocabulary of blobs."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1824057"
                        ],
                        "name": "Florent Monay",
                        "slug": "Florent-Monay",
                        "structuredName": {
                            "firstName": "Florent",
                            "lastName": "Monay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Florent Monay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403029865"
                        ],
                        "name": "D. G\u00e1tica-P\u00e9rez",
                        "slug": "D.-G\u00e1tica-P\u00e9rez",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "G\u00e1tica-P\u00e9rez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. G\u00e1tica-P\u00e9rez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 138
                            }
                        ],
                        "text": "The translation approach of [3] was extended to models that ascertain associations indirectly, through latent topic/aspect/context spaces [4, 8] ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 132
                            }
                        ],
                        "text": "Image annotation has been a topic of on-going research for more than a decade and several interesting techniques have been proposed [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1007967,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c253729d6170b31972ded6bfec1ea502f3ff86e",
            "isKey": false,
            "numCitedBy": 281,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Image auto-annotation, i.e., the association of words to whole images, has attracted considerable attention. In particular, unsupervised, probabilistic latent variable models of text and image features have shown encouraging results, but their performance with respect to other approaches remains unknown. In this paper, we apply and compare two simple latent space models commonly used in text analysis, namely Latent Semantic Analysis (LSA) and Probabilistic LSA (PLSA). Annotation strategies for each model are discussed. Remarkably, we found that, on a 8000-image dataset, a classic LSA model defined on keywords and a very basic image representation performed as well as much more complex, state-of-the-art methods. Furthermore, non-probabilistic methods (LSA and direct image matching) outperformed PLSA on the same dataset."
            },
            "slug": "On-image-auto-annotation-with-latent-space-models-Monay-G\u00e1tica-P\u00e9rez",
            "title": {
                "fragments": [],
                "text": "On image auto-annotation with latent space models"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper applies and compares two simple latent space models commonly used in text analysis, namely Latent Semantic Analysis (LSA) and Probabilistic LSA (PLSA), and found that, on a 8000-image dataset, a classic LSA model defined on keywords and a very basic image representation performed as well as much more complex, state-of-the-art methods."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '03"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144723884"
                        ],
                        "name": "Rong Jin",
                        "slug": "Rong-Jin",
                        "structuredName": {
                            "firstName": "Rong",
                            "lastName": "Jin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rong Jin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707259"
                        ],
                        "name": "J. Chai",
                        "slug": "J.-Chai",
                        "structuredName": {
                            "firstName": "Joyce",
                            "lastName": "Chai",
                            "middleNames": [
                                "Yue"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Chai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145388187"
                        ],
                        "name": "Luo Si",
                        "slug": "Luo-Si",
                        "structuredName": {
                            "firstName": "Luo",
                            "lastName": "Si",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luo Si"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "For instance, [17] utilizes a coherent language model, eliminating independence between keywords."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9904485,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c678c5626f9bc264b8d7626bb08630c76f77fa08",
            "isKey": false,
            "numCitedBy": 149,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Image annotations allow users to access a large image database with textual queries. There have been several studies on automatic image annotation utilizing machine learning techniques, which automatically learn statistical models from annotated images and apply them to generate annotations for unseen images. One common problem shared by most previous learning approaches for automatic image annotation is that each annotated word is predicated for an image independently from other annotated words. In this paper, we proposed a coherent language model for automatic image annotation that takes into account the word-to-word correlation by estimating a coherent language model for an image. This new approach has two important advantages: 1) it is able to automatically determine the annotation length to improve the accuracy of retrieval results, and 2) it can be used with active learning to significantly reduce the required number of annotated image examples. Empirical studies with Corel dataset are presented to show the effectiveness of the coherent language model for automatic image annotation."
            },
            "slug": "Effective-automatic-image-annotation-via-a-coherent-Jin-Chai",
            "title": {
                "fragments": [],
                "text": "Effective automatic image annotation via a coherent language model and active learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A coherent language model for automatic image annotation is proposed that takes into account the word-to-word correlation by estimating a coherent language models for an image to significantly reduce the required number of annotated image examples."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '04"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145131936"
                        ],
                        "name": "Lei Wang",
                        "slug": "Lei-Wang",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150978083"
                        ],
                        "name": "Li Liu",
                        "slug": "Li-Liu",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145155297"
                        ],
                        "name": "L. Khan",
                        "slug": "L.-Khan",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Khan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Khan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 132
                            }
                        ],
                        "text": "Image annotation has been a topic of on-going research for more than a decade and several interesting techniques have been proposed [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14052879,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d9166cc6a675090b57886586b69581d24fc44d4e",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The development of technology generates huge amounts of non-textual information, such as images. An efficient image annotation and retrieval system is highly desired. Clustering algorithms make it possible to represent visual features of images with finite symbols. Based on this, many statistical models, which analyze correspondence between visual features and words and discover hidden semantics, have been published. These models improve the annotation and retrieval of large image databases. However, image data usually have a large number of dimensions. Traditional clustering algorithms assign equal weights to these dimensions, and become confounded in the process of dealing with these dimensions. In this paper, we propose a top-down, subspace clustering algorithm as a solution to this problem. For a given cluster, we determine relevant features based on histogram analysis and assign greater weight to relevant features as compared to less relevant features. We have implemented four different models to link visual tokens with keywords based on the clustering results of our clustering algorithm and K-means algorithm, and evaluated performance using precision, recall and correspondence accuracy using benchmark dataset. The results show that our algorithm is better than traditional ones for automatic image annotation and retrieval."
            },
            "slug": "Automatic-image-annotation-and-retrieval-using-Wang-Liu",
            "title": {
                "fragments": [],
                "text": "Automatic image annotation and retrieval using subspace clustering algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper proposes a top-down, subspace clustering algorithm, which determines relevant features based on histogram analysis and assign greater weight to relevant features as compared to less relevant features in a given cluster."
            },
            "venue": {
                "fragments": [],
                "text": "MMDB '04"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145575177"
                        ],
                        "name": "G. Carneiro",
                        "slug": "G.-Carneiro",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Carneiro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Carneiro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3651407"
                        ],
                        "name": "Antoni B. Chan",
                        "slug": "Antoni-B.-Chan",
                        "structuredName": {
                            "firstName": "Antoni",
                            "lastName": "Chan",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antoni B. Chan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47690405"
                        ],
                        "name": "P. Moreno",
                        "slug": "P.-Moreno",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Moreno",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Moreno"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699559"
                        ],
                        "name": "N. Vasconcelos",
                        "slug": "N.-Vasconcelos",
                        "structuredName": {
                            "firstName": "Nuno",
                            "lastName": "Vasconcelos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Vasconcelos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 166
                            }
                        ],
                        "text": "For comparisons on Corel5K, we summarized published results of several approaches, including the most popular topic model (i.e. CorrLDA [4]), as well as MBRM [9] and SML [2], which have shown state-of-the-art performance on Corel5K."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 132
                            }
                        ],
                        "text": "Image annotation has been a topic of on-going research for more than a decade and several interesting techniques have been proposed [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 207
                            }
                        ],
                        "text": "Additionally, we report two retrieval performance measures based on top 10 images retrieved for each keyword: mean retrieval precision (rP%) and mean retrieval precision for only the recalled keywords (rP%) [2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "SML [2] 23 29 137 31 49 CorrLDA[4](1) 6 9 59 27 37"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 163
                            }
                        ],
                        "text": "Recently, techniques have emerged to circumvent the correspondence problem under a discriminative multiple instance learning paradigm [1] or a generative paradigm [2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 27
                            }
                        ],
                        "text": "6 No implementation of SML [2] was publicly available."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 94
                            }
                        ],
                        "text": "Performance of all models was evaluated using five measures following the methodology used in [2, 9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 160
                            }
                        ],
                        "text": "methods that approach the annotation problem from different perspectives, using different image representations: CRM [7], InfNet [11], NPDE [21], MBRM [9], SML [2], and CorrLDA [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 247
                            }
                        ],
                        "text": "On the IAPR TC12 and ESP datasets, where no published results of annotation methods are available, we compared the performance of our baseline methods against MBRM [9] which was relatively easier to implement and had comparable performance to SML [2]6."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 122
                            }
                        ],
                        "text": "The top portion of the table displays published results of a number of standard and top-performing\n6 No implementation of SML [2] was publicly available.\nmethods that approach the annotation problem from different perspectives, using different image representations: CRM [7], InfNet [11], NPDE [21], MBRM [9], SML [2], and CorrLDA [4]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 28
                            }
                        ],
                        "text": "In an alternative approach, [2] relies on a hierarchical mixture representation of keyword classes, leading to a method that demonstrates both computational efficiency and stateof-the-art performance on several complex annotation tasks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 42
                            }
                        ],
                        "text": "CorrLDA [4]), as well as MBRM [9] and SML [2], which have shown state-of-the-art performance on Corel5K."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2717049,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e638e2c7f7bbc788eb4adb5b5c67bde5ffc11bc5",
            "isKey": true,
            "numCitedBy": 955,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "A probabilistic formulation for semantic image annotation and retrieval is proposed. Annotation and retrieval are posed as classification problems where each class is defined as the group of database images labeled with a common semantic label. It is shown that, by establishing this one-to-one correspondence between semantic labels and semantic classes, a minimum probability of error annotation and retrieval are feasible with algorithms that are 1) conceptually simple, 2) computationally efficient, and 3) do not require prior semantic segmentation of training images. In particular, images are represented as bags of localized feature vectors, a mixture density estimated for each image, and the mixtures associated with all images annotated with a common semantic label pooled into a density estimate for the corresponding semantic class. This pooling is justified by a multiple instance learning argument and performed efficiently with a hierarchical extension of expectation-maximization. The benefits of the supervised formulation over the more complex, and currently popular, joint modeling of semantic label and visual feature distributions are illustrated through theoretical arguments and extensive experiments. The supervised formulation is shown to achieve higher accuracy than various previously published methods at a fraction of their computational cost. Finally, the proposed method is shown to be fairly robust to parameter tuning"
            },
            "slug": "Supervised-Learning-of-Semantic-Classes-for-Image-Carneiro-Chan",
            "title": {
                "fragments": [],
                "text": "Supervised Learning of Semantic Classes for Image Annotation and Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The supervised formulation is shown to achieve higher accuracy than various previously published methods at a fraction of their computational cost and to be fairly robust to parameter tuning."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757708"
                        ],
                        "name": "V. Lavrenko",
                        "slug": "V.-Lavrenko",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Lavrenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lavrenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791802"
                        ],
                        "name": "J. Jeon",
                        "slug": "J.-Jeon",
                        "structuredName": {
                            "firstName": "Jiwoon",
                            "lastName": "Jeon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Jeon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 132
                            }
                        ],
                        "text": "Image annotation has been a topic of on-going research for more than a decade and several interesting techniques have been proposed [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 74
                            }
                        ],
                        "text": "Cross Media Relevance Models (CMRM) [5], Continuous Relevance Model (CRM) [7], and Multiple Bernoulli Relevance Model (MBRM) [9] assume different, nonparametric density representations of the joint word-image space."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 119
                            }
                        ],
                        "text": "It should be noted that most top-performing methods in literature rely on instancebased representations (such as MBRM, CRM, InfNet, and NPDE) which are closely related to our baseline approach."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 117
                            }
                        ],
                        "text": "methods that approach the annotation problem from different perspectives, using different image representations: CRM [7], InfNet [11], NPDE [21], MBRM [9], SML [2], and CorrLDA [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 267
                            }
                        ],
                        "text": "The top portion of the table displays published results of a number of standard and top-performing\n6 No implementation of SML [2] was publicly available.\nmethods that approach the annotation problem from different perspectives, using different image representations: CRM [7], InfNet [11], NPDE [21], MBRM [9], SML [2], and CorrLDA [4]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 575890,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "18f8820e2a5ca6273a39123c27c0745870cda057",
            "isKey": true,
            "numCitedBy": 798,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an approach to learning the semantics of images which allows us to automatically annotate an image with keywords and to retrieve images based on text queries. We do this using a formalism that models the generation of annotated images. We assume that every image is divided into regions, each described by a continuous-valued feature vector. Given a training set of images with annotations, we compute a joint probabilistic model of image features and words which allow us to predict the probability of generating a word given the image regions. This may be used to automatically annotate and retrieve images given a word as a query. Experiments show that our model significantly outperforms the best of the previously reported results on the tasks of automatic image annotation and retrieval."
            },
            "slug": "A-Model-for-Learning-the-Semantics-of-Pictures-Lavrenko-Manmatha",
            "title": {
                "fragments": [],
                "text": "A Model for Learning the Semantics of Pictures"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "An approach to learning the semantics of images which allows us to automatically annotate an image with keywords and to retrieve images based on text queries using a formalism that models the generation of annotated images."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1857558"
                        ],
                        "name": "Shaolei Feng",
                        "slug": "Shaolei-Feng",
                        "structuredName": {
                            "firstName": "Shaolei",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaolei Feng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757708"
                        ],
                        "name": "V. Lavrenko",
                        "slug": "V.-Lavrenko",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Lavrenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lavrenko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 153
                            }
                        ],
                        "text": "For comparisons on Corel5K, we summarized published results of several approaches, including the most popular topic model (i.e. CorrLDA [4]), as well as MBRM [9] and SML [2], which have shown state-of-the-art performance on Corel5K."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 132
                            }
                        ],
                        "text": "Image annotation has been a topic of on-going research for more than a decade and several interesting techniques have been proposed [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 125
                            }
                        ],
                        "text": "Cross Media Relevance Models (CMRM) [5], Continuous Relevance Model (CRM) [7], and Multiple Bernoulli Relevance Model (MBRM) [9] assume different, nonparametric density representations of the joint word-image space."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 60
                            }
                        ],
                        "text": "The baseline also shows performance superior to that of the MBRM."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 30
                            }
                        ],
                        "text": "Table 1(b) depicts\nresults of MBRM and our baseline methods on this set."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 94
                            }
                        ],
                        "text": "Performance of all models was evaluated using five measures following the methodology used in [2, 9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 113
                            }
                        ],
                        "text": "It should be noted that most top-performing methods in literature rely on instancebased representations (such as MBRM, CRM, InfNet, and NPDE) which are closely related to our baseline approach."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 151
                            }
                        ],
                        "text": "methods that approach the annotation problem from different perspectives, using different image representations: CRM [7], InfNet [11], NPDE [21], MBRM [9], SML [2], and CorrLDA [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 164
                            }
                        ],
                        "text": "On the IAPR TC12 and ESP datasets, where no published results of annotation methods are available, we compared the performance of our baseline methods against MBRM [9] which was relatively easier to implement and had comparable performance to SML [2]6."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "In particular, MBRM achieves robust annotation performance using simple image and text representations: a mixture density model of image appearance that relies on regions extracted from a regular grid, thus avoiding potentially noisy segmentation, and the ability to naturally incorporate complex word annotations using multiple Bernoulli models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "However, the complexity of the kernel density representations may hinder MBRM\u2019s applicability to large data sets."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 304,
                                "start": 300
                            }
                        ],
                        "text": "The top portion of the table displays published results of a number of standard and top-performing\n6 No implementation of SML [2] was publicly available.\nmethods that approach the annotation problem from different perspectives, using different image representations: CRM [7], InfNet [11], NPDE [21], MBRM [9], SML [2], and CorrLDA [4]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 30
                            }
                        ],
                        "text": "CorrLDA [4]), as well as MBRM [9] and SML [2], which have shown state-of-the-art performance on Corel5K."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3829888,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba4e1089e2c5a1c12e9f6c2686e9c8d1870c718e",
            "isKey": true,
            "numCitedBy": 912,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Retrieving images in response to textual queries requires some knowledge of the semantics of the picture. Here, we show how we can do both automatic image annotation and retrieval (using one word queries) from images and videos using a multiple Bernoulli relevance model. The model assumes that a training set of images or videos along with keyword annotations is provided. Multiple keywords are provided for an image and the specific correspondence between a keyword and an image is not provided. Each image is partitioned into a set of rectangular regions and a real-valued feature vector is computed over these regions. The relevance model is a joint probability distribution of the word annotations and the image feature vectors and is computed using the training set. The word probabilities are estimated using a multiple Bernoulli model and the image feature probabilities using a non-parametric kernel density estimate. The model is then used to annotate images in a test set. We show experiments on both images from a standard Corel data set and a set of video key frames from NIST's video tree. Comparative experiments show that the model performs better than a model based on estimating word probabilities using the popular multinomial distribution. The results also show that our model significantly outperforms previously reported results on the task of image and video annotation."
            },
            "slug": "Multiple-Bernoulli-relevance-models-for-image-and-Feng-Manmatha",
            "title": {
                "fragments": [],
                "text": "Multiple Bernoulli relevance models for image and video annotation"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This work shows how it can do both automatic image annotation and retrieval (using one word queries) from images and videos using a multiple Bernoulli relevance model, which significantly outperforms previously reported results on the task of image and video annotation."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40490812"
                        ],
                        "name": "R. Datta",
                        "slug": "R.-Datta",
                        "structuredName": {
                            "firstName": "Ritendra",
                            "lastName": "Datta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Datta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5113463"
                        ],
                        "name": "D. Joshi",
                        "slug": "D.-Joshi",
                        "structuredName": {
                            "firstName": "Dhiraj",
                            "lastName": "Joshi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Joshi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40116905"
                        ],
                        "name": "Jia Li",
                        "slug": "Jia-Li",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48094094"
                        ],
                        "name": "James Ze Wang",
                        "slug": "James-Ze-Wang",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Wang",
                            "middleNames": [
                                "Ze"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Ze Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "A large number of techniques have been proposed in the last decade [15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7060187,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0dfa5679a15d0125ecec8539b79e8ba0babb8f73",
            "isKey": false,
            "numCitedBy": 3618,
            "numCiting": 325,
            "paperAbstract": {
                "fragments": [],
                "text": "We have witnessed great interest and a wealth of promise in content-based image retrieval as an emerging technology. While the last decade laid foundation to such promise, it also paved the way for a large number of new techniques and systems, got many new people involved, and triggered stronger association of weakly related fields. In this article, we survey almost 300 key theoretical and empirical contributions in the current decade related to image retrieval and automatic image annotation, and in the process discuss the spawning of related subfields. We also discuss significant challenges involved in the adaptation of existing image retrieval techniques to build systems that can be useful in the real world. In retrospect of what has been achieved so far, we also conjecture what the future may hold for image retrieval research."
            },
            "slug": "Image-retrieval:-Ideas,-influences,-and-trends-of-Datta-Joshi",
            "title": {
                "fragments": [],
                "text": "Image retrieval: Ideas, influences, and trends of the new age"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Almost 300 key theoretical and empirical contributions in the current decade related to image retrieval and automatic image annotation are surveyed, and the spawning of related subfields are discussed, to discuss the adaptation of existing image retrieval techniques to build systems that can be useful in the real world."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680617"
                        ],
                        "name": "Donald Metzler",
                        "slug": "Donald-Metzler",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Metzler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald Metzler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758550"
                        ],
                        "name": "R. Manmatha",
                        "slug": "R.-Manmatha",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Manmatha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manmatha"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 132
                            }
                        ],
                        "text": "Image annotation has been a topic of on-going research for more than a decade and several interesting techniques have been proposed [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 70
                            }
                        ],
                        "text": "Alternative approaches based on graph representation of joint queries [11], and cross-language LSI [12], offer means for linking the word-image occurrences, but still do not perform as well as the non-parametric models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 124
                            }
                        ],
                        "text": "It should be noted that most top-performing methods in literature rely on instancebased representations (such as MBRM, CRM, InfNet, and NPDE) which are closely related to our baseline approach."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 129
                            }
                        ],
                        "text": "methods that approach the annotation problem from different perspectives, using different image representations: CRM [7], InfNet [11], NPDE [21], MBRM [9], SML [2], and CorrLDA [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 282,
                                "start": 276
                            }
                        ],
                        "text": "The top portion of the table displays published results of a number of standard and top-performing\n6 No implementation of SML [2] was publicly available.\nmethods that approach the annotation problem from different perspectives, using different image representations: CRM [7], InfNet [11], NPDE [21], MBRM [9], SML [2], and CorrLDA [4]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2658075,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d2d96b57554816d545515804512d43879ed7fcf",
            "isKey": true,
            "numCitedBy": 123,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Most image retrieval systems only allow a fragment of text or an example image as a query. Most users have more complex information needs that are not easily expressed in either of these forms. This paper proposes a model based on the Inference Network framework from information retrieval that employs a powerful query language that allows structured query operators, term weighting, and the combination of text and images within a query. The model uses non-parametric methods to estimate probabilities within the inference network. Image annotation and retrieval results are reported and compared against other published systems and illustrative structured and weighted query results are given to show the power of the query language. The resulting system both performs well and is robust compared to existing approaches."
            },
            "slug": "An-Inference-Network-Approach-to-Image-Retrieval-Metzler-Manmatha",
            "title": {
                "fragments": [],
                "text": "An Inference Network Approach to Image Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A model based on the Inference Network framework from information retrieval that employs a powerful query language that allows structured query operators, term weighting, and the combination of text and images within a query is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "CIVR"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2279670"
                        ],
                        "name": "Andrea Frome",
                        "slug": "Andrea-Frome",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Frome",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrea Frome"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145757665"
                        ],
                        "name": "Fei Sha",
                        "slug": "Fei-Sha",
                        "structuredName": {
                            "firstName": "Fei",
                            "lastName": "Sha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fei Sha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "Recently, combining different distances or kernels has been shown to yield good performance in object recognition task [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1555909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "861839d67bacd1d2e5a956a172dddc8644e634fb",
            "isKey": false,
            "numCitedBy": 391,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of visual category recognition by learning an image-to-image distance function that attempts to satisfy the following property: the distance between images from the same category should be less than the distance between images from different categories. We use patch-based feature vectors common in object recognition work as a basis for our image-to-image distance functions. Our large-margin formulation for learning the distance functions is similar to formulations used in the machine learning literature on distance metric learning, however we differ in that we learn local distance functions\u00bfa different parameterized function for every image of our training set\u00bfwhereas typically a single global distance function is learned. This was a novel approach first introduced in Frome, Singer, & Malik, NIPS 2006. In that work we learned the local distance functions independently, and the outputs of these functions could not be compared at test time without the use of additional heuristics or training. Here we introduce a different approach that has the advantage that it learns distance functions that are globally consistent in that they can be directly compared for purposes of retrieval and classification. The output of the learning algorithm are weights assigned to the image features, which is intuitively appealing in the computer vision setting: some features are more salient than others, and which are more salient depends on the category, or image, being considered. We train and test using the Caltech 101 object recognition benchmark."
            },
            "slug": "Learning-Globally-Consistent-Local-Distance-for-and-Frome-Singer",
            "title": {
                "fragments": [],
                "text": "Learning Globally-Consistent Local Distance Functions for Shape-Based Image Retrieval and Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This work addresses the problem of visual category recognition by learning an image-to-image distance function that attempts to satisfy the following property: the distance between images from the same category should be less than the distanceBetween images from different categories."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2026151"
                        ],
                        "name": "Yuli Gao",
                        "slug": "Yuli-Gao",
                        "structuredName": {
                            "firstName": "Yuli",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuli Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152732685"
                        ],
                        "name": "Jianping Fan",
                        "slug": "Jianping-Fan",
                        "structuredName": {
                            "firstName": "Jianping",
                            "lastName": "Fan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianping Fan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "Hierarchical annotations in [18] aim not only to identify specific objects in an image, but also explicitly incorporate concept ontologies."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10736594,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fc9b7abd222518ba1cedfd62f0bf56540e6958c8",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "To enable automatic multi-level image annotation, we have addressed two inter-related important issues:(1)more effective framework for image content representation and feature extraction to characterize the middle-level semantics of image contents;(2)new framework for hierarchical probabilistic image concept reasoning and detection. To address the first issue salient objects are used as the semantic building blocks to characterize the middle-level semantics of image contents effectively while reducing the image analysis cost significantly. We have proposed three approaches to designing the detection functions for automatic salient object detection,and automatic function selection is also supported to find the \"right \"assumptions of the principal visual properties for the corresponding salient object classes. To address the second issue wehaveproposed a novel framework to incorporate the concept ontology to achieve hierarchical probabilistic image concept reasoning for multi-level image annotation. The concept ontology for a large-scale public image database called Label Me is semi-automatically derived from the available image labels by using WordNet The image concepts at the first level of the concept ontology are used to characterize the most specific semantics of image contents with the smallest variations, and their correspondences with the semantic building blocks (i.e.,salient objects)are well-de fined and can be modeled accurately by using Bayesian networks. In addition,the predictions of the appearances of the higher-level image concepts with large variations are adopted by the underlying concept ontology or by combining the available predictions of the appearances of their children concepts through hierarchical Bayesian networks.Our experiments on a large public dataset have shown that our framework for hierarchical probabilistic image concept reasoning is scalable to diverse image contents (i.e.,large amount of salient object classes)with large within-category variations."
            },
            "slug": "Incorporating-concept-ontology-to-enable-concept-Gao-Fan",
            "title": {
                "fragments": [],
                "text": "Incorporating concept ontology to enable probabilistic concept reasoning for multi-level image annotation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel framework to incorporate the concept ontology to achieve hierarchical probabilistic image concept reasoning for multi-level image annotation is proposed and shown that it is scalable to diverse image contents with large within-category variations."
            },
            "venue": {
                "fragments": [],
                "text": "MIR '06"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2446509"
                        ],
                        "name": "P. D. Sahin",
                        "slug": "P.-D.-Sahin",
                        "structuredName": {
                            "firstName": "Pinar",
                            "lastName": "Sahin",
                            "middleNames": [
                                "Duygulu"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. D. Sahin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145602732"
                        ],
                        "name": "Kobus Barnard",
                        "slug": "Kobus-Barnard",
                        "structuredName": {
                            "firstName": "Kobus",
                            "lastName": "Barnard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kobus Barnard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059257793"
                        ],
                        "name": "Jo\u00e3o Freitas",
                        "slug": "Jo\u00e3o-Freitas",
                        "structuredName": {
                            "firstName": "Jo\u00e3o",
                            "lastName": "Freitas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jo\u00e3o Freitas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 28
                            }
                        ],
                        "text": "The translation approach of [3] was extended to models that ascertain associations indirectly, through latent topic/aspect/context spaces [4, 8] ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 132
                            }
                        ],
                        "text": "Image annotation has been a topic of on-going research for more than a decade and several interesting techniques have been proposed [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 91
                            }
                        ],
                        "text": "The translation paradigm is typically based on some model of image and text co-occurrences [3, 16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 10
                            }
                        ],
                        "text": "\u2013 Corel5K [3] has become a de-facto evaluation benchmark in the image annotation community."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12561212,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d9f55b445f36578802e7eef4393cfa914b11620",
            "isKey": true,
            "numCitedBy": 1765,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a model of object recognition as machine translation. In this model, recognition is a process of annotating image regions with words. Firstly, images are segmented into regions, which are classified into region types using a variety of features. A mapping between region types and keywords supplied with the images, is then learned, using a method based around EM. This process is analogous with learning a lexicon from an aligned bitext. For the implementation we describe, these words are nouns taken from a large vocabulary. On a large test set, the method can predict numerous words with high accuracy. Simple methods identify words that cannot be predicted well. We show how to cluster words that individually are difficult to predict into clusters that can be predicted well -- for example, we cannot predict the distinction between train and locomotive using the current set of features, but we can predict the underlying concept. The method is trained on a substantial collection of images. Extensive experimental results illustrate the strengths and weaknesses of the approach."
            },
            "slug": "Object-Recognition-as-Machine-Translation:-Learning-Sahin-Barnard",
            "title": {
                "fragments": [],
                "text": "Object Recognition as Machine Translation: Learning a Lexicon for a Fixed Image Vocabulary"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work shows how to cluster words that individually are difficult to predict into clusters that can be predicted well, and cannot predict the distinction between train and locomotive using the current set of features, but can predict the underlying concept."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723700"
                        ],
                        "name": "Changbo Yang",
                        "slug": "Changbo-Yang",
                        "structuredName": {
                            "firstName": "Changbo",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Changbo Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144964053"
                        ],
                        "name": "Ming Dong",
                        "slug": "Ming-Dong",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Dong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming Dong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48840512"
                        ],
                        "name": "Jing Hua",
                        "slug": "Jing-Hua",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Hua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Hua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 132
                            }
                        ],
                        "text": "Image annotation has been a topic of on-going research for more than a decade and several interesting techniques have been proposed [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 134
                            }
                        ],
                        "text": "Recently, techniques have emerged to circumvent the correspondence problem under a discriminative multiple instance learning paradigm [1] or a generative paradigm [2]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15968240,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "35bcf0efc70975eff7dd4bb95856cdee66c6a159",
            "isKey": false,
            "numCitedBy": 174,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "In region-based image annotation, keywords are usually associated with images instead of individual regions in the training data set. This poses a major challenge for any learning strategy. In this paper, we formulate image annotation as a supervised learning problem under Multiple-Instance Learning (MIL) framework. We present a novel Asymmetrical Support Vector Machine-based MIL algorithm (ASVM-MIL), which extends the conventional Support Vector Machine (SVM) to the MIL setting by introducing asymmetrical loss functions for false positives and false negatives. The proposed ASVM-MIL algorithm is evaluated on both image annotation data sets and the benchmark MUSK data sets."
            },
            "slug": "Region-based-Image-Annotation-using-Asymmetrical-Yang-Dong",
            "title": {
                "fragments": [],
                "text": "Region-based Image Annotation using Asymmetrical Support Vector Machine-based Multiple-Instance Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel Asymmetrical Support Vector Machine-based MIL algorithm (ASVM-MIL) is presented, which extends the conventional Support vector Machine to the MIL setting by introducing asymmetrical loss functions for false positives and false negatives."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3328108"
                        ],
                        "name": "Luis von Ahn",
                        "slug": "Luis-von-Ahn",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Ahn",
                            "middleNames": [
                                "von"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luis von Ahn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784365"
                        ],
                        "name": "Laura A. Dabbish",
                        "slug": "Laura-A.-Dabbish",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Dabbish",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laura A. Dabbish"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 101
                            }
                        ],
                        "text": "\u2013 ESP Game consists of a set of 21,844 images collected in the ESP collaborative image labeling task [20]3."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 111
                            }
                        ],
                        "text": "ESP game set has arisen from an experiment in collaborative human computing\u2014 annotation of images in this case [20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 338469,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c2d4a6e4900ec0f096c87bb2b1272eeceaa584a6",
            "isKey": false,
            "numCitedBy": 2386,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new interactive system: a game that is fun and can be used to create valuable output. When people play the game they help determine the contents of images by providing meaningful labels for them. If the game is played as much as popular online games, we estimate that most images on the Web can be labeled in a few months. Having proper labels associated with each image on the Web would allow for more accurate image search, improve the accessibility of sites (by providing descriptions of images to visually impaired individuals), and help users block inappropriate images. Our system makes a significant contribution because of its valuable output and because of the way it addresses the image-labeling problem. Rather than using computer vision techniques, which don't work well enough, we encourage people to do the work by taking advantage of their desire to be entertained."
            },
            "slug": "Labeling-images-with-a-computer-game-Ahn-Dabbish",
            "title": {
                "fragments": [],
                "text": "Labeling images with a computer game"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A new interactive system: a game that is fun and can be used to create valuable output that addresses the image-labeling problem and encourages people to do the work by taking advantage of their desire to be entertained."
            },
            "venue": {
                "fragments": [],
                "text": "CHI"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3297491"
                        ],
                        "name": "A. Yavlinsky",
                        "slug": "A.-Yavlinsky",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Yavlinsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yavlinsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21438741"
                        ],
                        "name": "E. Schofield",
                        "slug": "E.-Schofield",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Schofield",
                            "middleNames": [
                                "James"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Schofield"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2375038"
                        ],
                        "name": "S. R\u00fcger",
                        "slug": "S.-R\u00fcger",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "R\u00fcger",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. R\u00fcger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 140
                            }
                        ],
                        "text": "methods that approach the annotation problem from different perspectives, using different image representations: CRM [7], InfNet [11], NPDE [21], MBRM [9], SML [2], and CorrLDA [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 136
                            }
                        ],
                        "text": "It should be noted that most top-performing methods in literature rely on instancebased representations (such as MBRM, CRM, InfNet, and NPDE) which are closely related to our baseline approach."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 289
                            }
                        ],
                        "text": "The top portion of the table displays published results of a number of standard and top-performing\n6 No implementation of SML [2] was publicly available.\nmethods that approach the annotation problem from different perspectives, using different image representations: CRM [7], InfNet [11], NPDE [21], MBRM [9], SML [2], and CorrLDA [4]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14275133,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "afae0b355b876cfce497d96ec1d04a2c6d801595",
            "isKey": false,
            "numCitedBy": 177,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a simple framework for automatically annotating images using non-parametric models of distributions of image features. We show that under this framework quite simple image properties such as global colour and texture distributions provide a strong basis for reliably annotating images. We report results on subsets of two photographic libraries, the Corel Photo Archive and the Getty Image Archive. We also show how the popular Earth Mover\u2019s Distance measure can be effectively incorporated within this framework."
            },
            "slug": "Automated-Image-Annotation-Using-Global-Features-Yavlinsky-Schofield",
            "title": {
                "fragments": [],
                "text": "Automated Image Annotation Using Global Features and Robust Nonparametric Density Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is shown that under this framework quite simple image properties such as global colour and texture distributions provide a strong basis for reliably annotating images."
            },
            "venue": {
                "fragments": [],
                "text": "CIVR"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3724810"
                        ],
                        "name": "Jonathon S. Hare",
                        "slug": "Jonathon-S.-Hare",
                        "structuredName": {
                            "firstName": "Jonathon",
                            "lastName": "Hare",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathon S. Hare"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1773066"
                        ],
                        "name": "P. Lewis",
                        "slug": "P.-Lewis",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Lewis",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2137152"
                        ],
                        "name": "P. Enser",
                        "slug": "P.-Enser",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Enser",
                            "middleNames": [
                                "G.",
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Enser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3736701"
                        ],
                        "name": "C. Sandom",
                        "slug": "C.-Sandom",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Sandom",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sandom"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "Alternative approaches based on graph representation of joint queries [11], and cross-language LSI [12], offer means for linking the word-image occurrences, but still do not perform as well as the non-parametric models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 132
                            }
                        ],
                        "text": "Image annotation has been a topic of on-going research for more than a decade and several interesting techniques have been proposed [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17848539,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc3a9540f8804148e493b36829f74bd75abcef37",
            "isKey": false,
            "numCitedBy": 182,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper attempts to review and characterise the problem of the semantic gap in image retrieval and the attempts being made to bridge it. In particular, we draw from our own experience in user queries, automatic annotation and ontological techniques. The first section of the paper describes a characterisation of the semantic gap as a hierarchy between the raw media and full semantic understanding of the media's content. The second section discusses real users' queries with respect to the semantic gap. The final sections of the paper describe our own experience in attempting to bridge the semantic gap. In particular we discuss our work on auto-annotation and semantic-space models of image retrieval in order to bridge the gap from the bottom up, and the use of ontologies, which capture more semantics than keyword object labels alone, as a technique for bridging the gap from the top down."
            },
            "slug": "Mind-the-gap:-another-look-at-the-problem-of-the-in-Hare-Lewis",
            "title": {
                "fragments": [],
                "text": "Mind the gap: another look at the problem of the semantic gap in image retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This paper describes a characterisation of the semantic gap as a hierarchy between the raw media and full semantic understanding of the media's content and the use of ontologies, which capture more semantics than keyword object labels alone, as a technique for bridging the gap from the top down."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40116905"
                        ],
                        "name": "Jia Li",
                        "slug": "Jia-Li",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48094094"
                        ],
                        "name": "James Ze Wang",
                        "slug": "James-Ze-Wang",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Wang",
                            "middleNames": [
                                "Ze"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Ze Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "To address this problem, [19] developed a real-time ALIPR image search engine which uses multiresolution 2D Hidden Markov Models to model concepts determined by a training set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3028284,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f39d3e88cce063ccd3ca01100efd44dcabc9d3b4",
            "isKey": false,
            "numCitedBy": 1187,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic linguistic indexing of pictures is an important but highly challenging problem for researchers in computer vision and content-based image retrieval. In this paper, we introduce a statistical modeling approach to this problem. Categorized images are used to train a dictionary of hundreds of statistical models each representing a concept. Images of any given concept are regarded as instances of a stochastic process that characterizes the concept. To measure the extent of association between an image and the textual description of a concept, the likelihood of the occurrence of the image based on the characterizing stochastic process is computed. A high likelihood indicates a strong association. In our experimental implementation, we focus on a particular group of stochastic processes, that is, the two-dimensional multiresolution hidden Markov models (2D MHMMs). We implemented and tested our ALIP (Automatic Linguistic Indexing of Pictures) system on a photographic image database of 600 different concepts, each with about 40 training images. The system is evaluated quantitatively using more than 4,600 images outside the training database and compared with a random annotation scheme. Experiments have demonstrated the good accuracy of the system and its high potential in linguistic indexing of photographic images."
            },
            "slug": "Automatic-Linguistic-Indexing-of-Pictures-by-a-Li-Wang",
            "title": {
                "fragments": [],
                "text": "Automatic Linguistic Indexing of Pictures by a Statistical Modeling Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper implemented and tested the ALIP (Automatic Linguistic Indexing of Pictures) system on a photographic image database of 600 different concepts, each with about 40 training images and demonstrated the good accuracy of the system and its high potential in linguistic indexing of photographic images."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796335"
                        ],
                        "name": "D. Blei",
                        "slug": "D.-Blei",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Blei",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Blei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 128
                            }
                        ],
                        "text": "For comparisons on Corel5K, we summarized published results of several approaches, including the most popular topic model (i.e. CorrLDA [4]), as well as MBRM [9] and SML [2], which have shown state-of-the-art performance on Corel5K."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 132
                            }
                        ],
                        "text": "Image annotation has been a topic of on-going research for more than a decade and several interesting techniques have been proposed [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 92
                            }
                        ],
                        "text": "CorrLDA(1) and JEC(1) correspond to models built on a reduced 168 keyword dictionary, as in [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 31
                            }
                        ],
                        "text": "SML [2] 23 29 137 31 49 CorrLDA[4](1) 6 9 59 27 37"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 138
                            }
                        ],
                        "text": "The translation approach of [3] was extended to models that ascertain associations indirectly, through latent topic/aspect/context spaces [4, 8] ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 51
                            }
                        ],
                        "text": "While generative parametric models such as CorrLDA [4] have significant modeling appeal due to the interpretability of the learned models, they fail to stack up to the nonparametric representations on this difficult task."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 177
                            }
                        ],
                        "text": "methods that approach the annotation problem from different perspectives, using different image representations: CRM [7], InfNet [11], NPDE [21], MBRM [9], SML [2], and CorrLDA [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 330,
                                "start": 323
                            }
                        ],
                        "text": "The top portion of the table displays published results of a number of standard and top-performing\n6 No implementation of SML [2] was publicly available.\nmethods that approach the annotation problem from different perspectives, using different image representations: CRM [7], InfNet [11], NPDE [21], MBRM [9], SML [2], and CorrLDA [4]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 8
                            }
                        ],
                        "text": "CorrLDA [4]), as well as MBRM [9] and SML [2], which have shown state-of-the-art performance on Corel5K."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 73
                            }
                        ],
                        "text": "One such model, the Correspondence Latent Dirichlet Allocation (CorrLDA) [4], considers associations through a latent topic space in a generatively learned model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207561477,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "473f4b7f8ae2b03dda2593f54b316ff7d55db26b",
            "isKey": true,
            "numCitedBy": 1214,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of modeling annotated data---data with multiple types where the instance of one type (such as a caption) serves as a description of the other type (such as an image). We describe three hierarchical probabilistic mixture models which aim to describe such data, culminating in correspondence latent Dirichlet allocation, a latent variable model that is effective at modeling the joint distribution of both types and the conditional distribution of the annotation given the primary type. We conduct experiments on the Corel database of images and captions, assessing performance in terms of held-out likelihood, automatic annotation, and text-based image retrieval."
            },
            "slug": "Modeling-annotated-data-Blei-Jordan",
            "title": {
                "fragments": [],
                "text": "Modeling annotated data"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Three hierarchical probabilistic mixture models which aim to describe annotated data with multiple types, culminating in correspondence latent Dirichlet allocation, a latent variable model that is effective at modeling the joint distribution of both types and the conditional distribution of the annotation given the primary type."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2333082"
                        ],
                        "name": "Y. Mori",
                        "slug": "Y.-Mori",
                        "structuredName": {
                            "firstName": "Yasuhide",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Mori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2079483"
                        ],
                        "name": "Hironobu Takahashi",
                        "slug": "Hironobu-Takahashi",
                        "structuredName": {
                            "firstName": "Hironobu",
                            "lastName": "Takahashi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hironobu Takahashi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1776022"
                        ],
                        "name": "R. Oka",
                        "slug": "R.-Oka",
                        "structuredName": {
                            "firstName": "Ryu-ichi",
                            "lastName": "Oka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Oka"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 91
                            }
                        ],
                        "text": "The translation paradigm is typically based on some model of image and text co-occurrences [3, 16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18574318,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b29ffb4207435540ddecf4b14a8a32106b33830",
            "isKey": false,
            "numCitedBy": 448,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a method to make a relationship between images and words. We adopt two processes in the method, one is a process to uniformly divide each image into sub-images with key words, and the other is a process to carry out vector quantization of the sub-images. These processes lead to results which show that each sub-image can be correlated to a set of words each of which is selected from words assigned to whole images. Original aspects of the method are, (1) all words assigned to a whole image are inherited to each divided sub-image, (2) the voting probability of each word for a set of divided images is estimated by the result of a vector quantization of the feature vector of sub-images. Some experiments show the e ectiveness of the proposed method."
            },
            "slug": "Image-to-word-transformation-based-on-dividing-Mori-Takahashi",
            "title": {
                "fragments": [],
                "text": "Image-to-word transformation based on dividing"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "All words assigned to a whole image are inherited to each divided sub-image and the voting probability of each word for a set of divided images is estimated by the result of a vector quantization of the feature vector of sub-images."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787377"
                        ],
                        "name": "W. Leow",
                        "slug": "W.-Leow",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Leow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Leow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731570"
                        ],
                        "name": "M. Lew",
                        "slug": "M.-Lew",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lew",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lew"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144078686"
                        ],
                        "name": "Tat-Seng Chua",
                        "slug": "Tat-Seng-Chua",
                        "structuredName": {
                            "firstName": "Tat-Seng",
                            "lastName": "Chua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tat-Seng Chua"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712167"
                        ],
                        "name": "Wei-Ying Ma",
                        "slug": "Wei-Ying-Ma",
                        "structuredName": {
                            "firstName": "Wei-Ying",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Ying Ma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1986874"
                        ],
                        "name": "L. Chaisorn",
                        "slug": "L.-Chaisorn",
                        "structuredName": {
                            "firstName": "Lekha",
                            "lastName": "Chaisorn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Chaisorn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143866184"
                        ],
                        "name": "E. Bakker",
                        "slug": "E.-Bakker",
                        "structuredName": {
                            "firstName": "Erwin",
                            "lastName": "Bakker",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bakker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 236700,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9467048bd79722b4367fb91585694dee157d8af6",
            "isKey": false,
            "numCitedBy": 341,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We have witnessed a decade of exploding research interest in multimedia content analysis. The goal of content analysis has been to derive automatic methods for high-level description and annotation. In this paper we will summarize the main research topics in this area and state some assumptions that we have been using all along. We will also postulate the main future trends including usage of long term memory, context, dynamic processing, evolvable generalized detectors and user aspects."
            },
            "slug": "Image-and-Video-Retrieval-Leow-Lew",
            "title": {
                "fragments": [],
                "text": "Image and Video Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The main research topics in multimedia content analysis are summarized, some assumptions are state and the main future trends including usage of long term memory, context, dynamic processing, evolvable generalized detectors and user aspects are postulated."
            },
            "venue": {
                "fragments": [],
                "text": "Lecture Notes in Computer Science"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 248,
                                "start": 244
                            }
                        ],
                        "text": "Since we are using different color (and texture) features that are not completely independent, it is an obvious question to ask: Which of these color (or texture) features are redundant? Logistic regression with L1 penalty, also known as Lasso [14], provides a simple way to answer this question."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 105
                            }
                        ],
                        "text": "The second one is based on selecting relevant distances using a sparse logistic regression method, Lasso [14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "L1-Penalized Logistic Regression (Lasso [14])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16162039,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b365b8e45b7d81f081de44ac8f9eadf9144f3ca5",
            "isKey": true,
            "numCitedBy": 36493,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMMARY We propose a new method for estimation in linear models. The 'lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described."
            },
            "slug": "Regression-Shrinkage-and-Selection-via-the-Lasso-Tibshirani",
            "title": {
                "fragments": [],
                "text": "Regression Shrinkage and Selection via the Lasso"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A new method for estimation in linear models called the lasso, which minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant, is proposed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 124
                            }
                        ],
                        "text": "It should be noted that most top-performing methods in literature rely on instancebased representations (such as MBRM, CRM, InfNet, and NPDE) which are closely related to our baseline approach."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 231
                            }
                        ],
                        "text": "The top portion of the table displays published results of a number of st andard and top-performing methods that approach the annotation problem from differen t p rspectives, using different image representations: CRM [ 7], InfNet [11], NPDE [21], MBRM [ 9], SML [2], and CorrLDA [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 282,
                                "start": 276
                            }
                        ],
                        "text": "The top portion of the table displays published results of a number of standard and top-performing\n6 No implementation of SML [2] was publicly available.\nmethods that approach the annotation problem from different perspectives, using different image representations: CRM [7], InfNet [11], NPDE [21], MBRM [9], SML [2], and CorrLDA [4]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "Alternative approaches based on graph repr esentation of joint queries [11], and cross-language LSI [ 12], offer means for linking the word-image occurrences, but still do not perform as well as the non-parametric models ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An inference network approac  h to image retrieval"
            },
            "venue": {
                "fragments": [],
                "text": "Image and Video Retrieval, Springer"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Image-to-word transfo rmation based on dividing and vector quantizing images with words"
            },
            "venue": {
                "fragments": [],
                "text": "First International Work  shop on Multimedia Intel ligent Storage and Retrieval Management (MISRM)."
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A model for learningthe semantics of pictures"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Region - based image annotation using asymmetrical support vector machinebased multipleinstance learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Word sense disambiguation wit  h pictures"
            },
            "venue": {
                "fragments": [],
                "text": "Artificial Intelligence"
            },
            "year": 2005
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 17,
            "methodology": 15,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 27,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/A-New-Baseline-for-Image-Annotation-Makadia-Pavlovic/e9a6bc1bcaf78a8667221c63847de4dcbd4bfcb3?sort=total-citations"
}