{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2365155"
                        ],
                        "name": "S. Deerwester",
                        "slug": "S.-Deerwester",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Deerwester",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Deerwester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737579"
                        ],
                        "name": "G. Furnas",
                        "slug": "G.-Furnas",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Furnas",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Furnas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3154682"
                        ],
                        "name": "R. Harshman",
                        "slug": "R.-Harshman",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Harshman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Harshman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3252915,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20a80a7356859daa4170fb4da6b87b84adbb547f",
            "isKey": false,
            "numCitedBy": 7018,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (\u201csemantic structure\u201d) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. initial tests find this completely automatic method for retrieval to be promising."
            },
            "slug": "Indexing-by-Latent-Semantic-Analysis-Deerwester-Dumais",
            "title": {
                "fragments": [],
                "text": "Indexing by Latent Semantic Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A new method for automatic indexing and retrieval to take advantage of implicit higher-order structure in the association of terms with documents (\u201csemantic structure\u201d) in order to improve the detection of relevant documents on the basis of terms found in queries."
            },
            "venue": {
                "fragments": [],
                "text": "J. Am. Soc. Inf. Sci."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153379696"
                        ],
                        "name": "T. Hofmann",
                        "slug": "T.-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hofmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2121009"
                        ],
                        "name": "J. Puzicha",
                        "slug": "J.-Puzicha",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Puzicha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Puzicha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 121
                            }
                        ],
                        "text": "The combination of deterministic annealing with the EM algorithm has been investigated before in Ueda and Nakano (1998), Hofmann, Puzicha, and Jordan (1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18598130,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3e9dfc4f5ced83a745249e409223ddab85381c88",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "Dyadic data refers to a domain with two nite sets of objects in which observations are made for dyads, i.e., pairs with one element from either set. This includes event co-occurrences, histogram data, and single stimulus preference data as special cases. Dyadic data arises naturally in many applications ranging from computational linguistics and information retrieval to preference analysis and computer vision. In this paper, we present a systematic, domain-independent framework for unsupervised learning from dyadic data by statistical mixture models. Our approach covers diier-ent models with at and hierarchical latent class structures and uniies probabilistic modeling and structure discovery. Mixture models provide both, a parsimonious yet exible parameterization of probability distributions with good generalization performance on sparse data, as well as structural information about data-inherent grouping structure. We propose an annealed version of the standard Expectation Maximization algorithm for model tting which is empirically evaluated on a variety of data sets from diierent domains."
            },
            "slug": "Unsupervised-Learning-from-Dyadic-Data-Hofmann-Puzicha",
            "title": {
                "fragments": [],
                "text": "Unsupervised Learning from Dyadic Data"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A systematic, domain-independent framework for unsupervised learning from dyadic data by statistical mixture models is presented and an annealed version of the standard Expectation Maximization algorithm is proposed which is empirically evaluated on a variety of data sets from diierent domains."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145632116"
                        ],
                        "name": "N. Coccaro",
                        "slug": "N.-Coccaro",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Coccaro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Coccaro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746807"
                        ],
                        "name": "Dan Jurafsky",
                        "slug": "Dan-Jurafsky",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Jurafsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan Jurafsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 38
                            }
                        ],
                        "text": "We have thus followed the approach of Coccaro and Jurafsky (1998) to derive LSA probabilities."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13185450,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b888cae7e6e288b108f9d119fc23b84b4d447029",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a number of techniques designed to help integrate semantic knowledge with N-gram language models for automatic speech recognition. Our techniques allow us to integrate Latent Semantic Analysis (LSA), a word-similarity algorithm based on word co-occurrence information, with N-gram models. While LSA is good at predicting content words which are coherent with the rest of a text, it is a bad predictor of frequent words, has a low dynamic range, and is inaccurate when combined linearly with N-grams. We show that modifying the dynamic range, applying a per-word con \ufb01 dence metric, and using geometric rather than linear combinations with N-grams produces a more robust language model which has a lower perplexity on a Wall Street Journal test-set than a baseline N-gram model."
            },
            "slug": "Towards-better-integration-of-semantic-predictors-Coccaro-Jurafsky",
            "title": {
                "fragments": [],
                "text": "Towards better integration of semantic predictors in statistical language modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that modifying the dynamic range, applying a per-word con \ufb01 dence metric, and using geometric rather than linear combinations with N-grams produces a more robust language model which has a lower perplexity on a Wall Street Journal test-set than a baseline N- gram model."
            },
            "venue": {
                "fragments": [],
                "text": "ICSLP"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1380069640"
                        ],
                        "name": "L. Baker",
                        "slug": "L.-Baker",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Baker",
                            "middleNames": [
                                "Douglas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Baker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 105
                            }
                        ],
                        "text": "Most closely related to our approach is thedistributional clustering model(Pereira, Tishby, & Lee, 1993; Baker & McCallum, 1998) and the multinomial (maximum likelihood) version of Autoclass clustering (Cheeseman & Stutz, 1996), an unsupervised version of a naive Bayes\u2019 classifier."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Most closely related to our approach is the distributional clustering model (Pereira, Tishby, & Lee, 1993;  Baker & McCallum, 1998 ) and the multinomial (maximum likelihood) version of Autoclass clustering (Cheeseman & Stutz, 1996), an unsupervised version of a naive Bayes\u2019 classifier."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6146974,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e733226b881f11f25c87e8bac8d602ba3d9c220e",
            "isKey": false,
            "numCitedBy": 843,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the application of Distributional Clustering [20] to document classification. This approach clusters words into groups based on the distribution of class labels associated with each word. Thus, unlike some other unsupervised dimensionalityreduction techniques, such as Latent Semantic Indexing, we are able to compress the feature space much more aggressively, while still maintaining high document classification accuracy. Experimental results obtained on three real-world data sets show that we can reduce the feature dimensional&y by three orders of magnitude and lose only 2% accuracy-significantly better than Latent Semantic Indexing [6], class-based clustering [l], feature selection by mutual information [23], or Markov-blanket-based feature selection [13]. We also show that less aggressive clustering sometimes results in improved classification accuracy over classification without clustering."
            },
            "slug": "Distributional-clustering-of-words-for-text-Baker-McCallum",
            "title": {
                "fragments": [],
                "text": "Distributional clustering of words for text classification"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "This paper describes the application of Distributional Clustering to document classification and shows that it can reduce the feature dimensional&y by three orders of magnitude and lose only 2% accuracy-significantly better than Latent Semantic Indexing, class-based clustering, feature selection by mutual information, or Markov-blanket-based feature selection."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2084425923"
                        ],
                        "name": "G. W. O'BrienComputer",
                        "slug": "G.-W.-O'BrienComputer",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "O'BrienComputer",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. W. O'BrienComputer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15460579,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a5aeac29e719c3017c9d5cbaa47a7885913aa1d",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Currently, most approaches to retrieving textual materials from scientiic databases depend on a lexical match between words in users' requests and those in or assigned to documents in a database. Because of the tremendous diversity in the words people use to describe the same document, lexical methods are necessarily incomplete and imprecise. Using the singular value decomposition (SVD), one can take advantage of the implicit higher-order structure in the association of terms with documents by determining the SVD of large sparse term by document matrices. Terms and documents represented by 200-300 of the largest singular vectors are then matched against user queries. We call this retrieval method Latent Semantic Indexing (LSI) because the subspace represents important associative relationships between terms and documents that are not evident in individual documents. LSI is a completely automatic yet intelligent indexing method, widely applicable, and a promising way to improve users' access to many kinds of textual materials, or to documents and services for which textual descriptions are available. A survey of the computational requirements for managing LSI-encoded databases as well as current and future applications of LSI is presented. with those of a query. However, lexical matching methods can be inaccurate when they are used to match a user's query. Since there are usually many ways to express a given concept (synonymy), the literal terms in a user's query may not match those of a relevant document. In addition, most words have multiple meanings (polysemy), so terms in a user's query will literally match terms in irrelevant documents. A better approach would allow users to retrieve information on the basis of a conceptual topic or meaning of a document. Latent Semantic Indexing (LSI) 4] tries to overcome the problems of lexical matching by using statistically derived conceptual indices instead of individual words for retrieval. LSI assumes that there is some underlying or latent structure in word usage that is partially obscured by variability in word choice. A truncated singular value decomposition (SVD) 14] is used to estimate the structure in word usage across documents. Retrieval is then performed using the database of singular values and vectors obtained from the truncated SVD. Performance data shows that these statistically derived vectors are more robust indicators of meaning than individual terms. A number of software tools have been developed to perform operations such as parsing document texts, creating a term by document matrix, computing the truncated \u2026"
            },
            "slug": "Using-Linear-Algebra-for-Intelligent-Information-Dumais-O'BrienComputer",
            "title": {
                "fragments": [],
                "text": "Using Linear Algebra for Intelligent Information Retrieval Using Linear Algebra for Intelligent Information Retrieval Using Linear Algebra for Intelligent Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "LSI tries to overcome the problems of lexical matching by using statistically derived conceptual indices instead of individual words for retrieval by determining the SVD of large sparse term by document matrices of singular value decomposition."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39952106"
                        ],
                        "name": "Michael B. W. Wolfe",
                        "slug": "Michael-B.-W.-Wolfe",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Wolfe",
                            "middleNames": [
                                "B.",
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael B. W. Wolfe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144029655"
                        ],
                        "name": "M. E. Schreiner",
                        "slug": "M.-E.-Schreiner",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Schreiner",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. E. Schreiner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2197285"
                        ],
                        "name": "B. Rehder",
                        "slug": "B.-Rehder",
                        "structuredName": {
                            "firstName": "Bob",
                            "lastName": "Rehder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Rehder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2444937"
                        ],
                        "name": "Darrell Laham",
                        "slug": "Darrell-Laham",
                        "structuredName": {
                            "firstName": "Darrell",
                            "lastName": "Laham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Darrell Laham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2036336"
                        ],
                        "name": "P. Foltz",
                        "slug": "P.-Foltz",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Foltz",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Foltz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3025325"
                        ],
                        "name": "W. Kintsch",
                        "slug": "W.-Kintsch",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Kintsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Kintsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 257,
                                "start": 239
                            }
                        ],
                        "text": "Due to its generality, LSA has proven to be a valuable analysis tool for many different problems in practice and thus has a wide range of possible applications (e.g., Deerwester et al., 1990; Foltz & Dumais, 1992; Landauer & Dumais, 1997; Wolfe et al., 1998; Bellegarda, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62526953,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0cb6eb8776d372f76be62c95252162086b46eff",
            "isKey": false,
            "numCitedBy": 249,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This study examines the hypothesis that the ability of a reader to learn from text depends on the match between the background knowledge of the reader and the difficulty of the text information. Latent Semantic Analysis (LSA), a statistical technique that represents the content of a document as a vector in high\u2010dimensional semantic space based on a large text corpus, is used to predict how much readers will learn from texts based on the estimated conceptual match between their topic knowledge and the text information. Participants completed tests to assess their knowledge of the human heart and circulatory system, then read one of four texts that ranged in difficulty from elementary to medical school level, then completed the tests again. Results show a nonmonotonic relation in which learning was greatest for texts that were neither too easy nor too difficult. LSA proved as effective at predicting learning from these texts as traditional knowledge assessment measures. For these texts, optimal assignment o..."
            },
            "slug": "Learning-from-text:-Matching-readers-and-texts-by-Wolfe-Schreiner",
            "title": {
                "fragments": [],
                "text": "Learning from text: Matching readers and texts by latent semantic analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Results show a nonmonotonic relation in which learning was greatest for texts that were neither too easy nor too difficult, and LSA proved as effective at predicting learning from these texts as traditional knowledge assessment measures."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836606"
                        ],
                        "name": "T. Landauer",
                        "slug": "T.-Landauer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Landauer",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Landauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Keywords: unsupervised learning, latent class models, mixture models, dimension reduction, EM algorithm, information retrieval, natural language processing, language modeling"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 214
                            }
                        ],
                        "text": "Due to its generality, LSA has proven to be a valuable analysis tool for many different problems in practice and thus has a wide range of possible applications (e.g., Deerwester et al., 1990; Foltz & Dumais, 1992; Landauer & Dumais, 1997; Wolfe et al., 1998; Bellegarda, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1144461,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68dd4b89ce1407372a29d05ca9e4e1a2e0513617",
            "isKey": false,
            "numCitedBy": 5786,
            "numCiting": 210,
            "paperAbstract": {
                "fragments": [],
                "text": "How do people know as much as they do with as little information as they get? The problem takes many forms; learning vocabulary from text is an especially dramatic and convenient case for research. A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena. By inducing global knowledge indirectly from local co-occurrence data in a large body of representative text, LSA acquired knowledge about the full vocabulary of English at a comparable rate to schoolchildren. LSA uses no prior linguistic or perceptual similarity knowledge; it is based solely on a general mathematical learning method that achieves powerful inductive effects by extracting the right number of dimensions (e.g., 300) to represent objects and contexts. Relations to other theories, phenomena, and problems are sketched."
            },
            "slug": "A-Solution-to-Plato's-Problem:-The-Latent-Semantic-Landauer-Dumais",
            "title": {
                "fragments": [],
                "text": "A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge."
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 38
                            }
                        ],
                        "text": "Suppose therefore that we have given a collection of text documentsD = {d1, . . . ,dN} with terms from a vocabularyW = {w1, . . . , wM}."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 279
                            }
                        ],
                        "text": "\u2026in the introduction, the key idea of LSA is to map documents\u2014and by symmetry terms\u2014to a vector space of reduced dimensionality, thela ent semantic space, which in a typical application in document indexing is chosen to have of the order\u2248100\u2212300 dimensions (Deerwester et al., 1990; Dumais, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17050166,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "21496686f28b5b7b978cc9155bbb164424b55e7b",
            "isKey": false,
            "numCitedBy": 184,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper reports on recent developments of the Latent Semantic Indexing (LSI) retrieval method for TREC-3. LSI uses a reduced-dimension vector space to represent words and documents. An important aspect of this representation is that the association between terms is automatically captured, explicitly represented, and used to improve retrieval. We used LSI for both TREC-3 routing and adhoc tasks. For the routing tasks an LSI space was constructed using the training documents. We compared profiles constructed using just the topic words (no training) with profiles constructed using the average of relevant documents (no use of the topic words). Not surprisingly, the centroid of the relevant documents was 30% better than the topic words. This simple feedback method was quite good compared to the routing performance of other systems. Various combinations of information from the topic words and relevant documents provide small additional improvements in performance. For the adhoc task we compared LSI to keyword vector matching (i.e. using no dimension reduction). Small advantages were obtained for LSI even with the long TREC topic statements."
            },
            "slug": "Latent-Semantic-Indexing-(LSI):-TREC-3-Report-Dumais",
            "title": {
                "fragments": [],
                "text": "Latent Semantic Indexing (LSI): TREC-3 Report"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This paper reports on recent developments of the Latent Semantic Indexing (LSI) retrieval method for TREC-3 and compares LSI to keyword vector matching (i.e. using no dimension reduction)."
            },
            "venue": {
                "fragments": [],
                "text": "TREC"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777660"
                        ],
                        "name": "Naftali Tishby",
                        "slug": "Naftali-Tishby",
                        "structuredName": {
                            "firstName": "Naftali",
                            "lastName": "Tishby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naftali Tishby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145810617"
                        ],
                        "name": "Lillian Lee",
                        "slug": "Lillian-Lee",
                        "structuredName": {
                            "firstName": "Lillian",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lillian Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 201
                            }
                        ],
                        "text": "In our experiments, we have actually considered linear combinations of the original similarity score (18) (weight\u03bb) and the one derived from the latent space representation (weight 1\u2212 \u03bb), as suggested in Pereira et al. (1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6713452,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5eb328cf7e94995199e4c82a1f4d0696430a80b5",
            "isKey": false,
            "numCitedBy": 1193,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical \"soft\" clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data."
            },
            "slug": "Distributional-Clustering-of-English-Words-Pereira-Tishby",
            "title": {
                "fragments": [],
                "text": "Distributional Clustering of English Words"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Deterministic annealing is used to find lowest distortion sets of clusters: as the annealed parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical \"soft\" clustering of the data."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 97
                            }
                        ],
                        "text": "The combination of deterministic annealing with the EM algorithm has been investigated before in Ueda and Nakano (1998), Hofmann, Puzicha, and Jordan (1999). The starting point of TEM is a derivation of the E-step based on an optimization principle."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 97
                            }
                        ],
                        "text": "The combination of deterministic annealing with the EM algorithm has been investigated before in Ueda and Nakano (1998), Hofmann, Puzicha, and Jordan (1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3204901,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f7463aad3b5182820995101602788ea4c9bb4d9f",
            "isKey": false,
            "numCitedBy": 199,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the use of language models whose size and accuracy are intermediate between different order n-gram models. Two types of models are studied in particular. Aggregate Markov models are classbased bigram models in which the mapping from words to classes is probabilistic. Mixed-order Markov models combine bigram models whose predictions are conditioned on different words. Both types of models are trained by ExpectationMaximization (EM) algorithms for maximum likelihood estimation. We examine smoothing procedures in which these models are interposed between different order n-grams. This is found to significantly reduce the perplexity of unseen word combinations."
            },
            "slug": "Aggregate-and-mixed-order-Markov-models-for-Saul-Pereira",
            "title": {
                "fragments": [],
                "text": "Aggregate and mixed-order Markov models for statistical language processing"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This work considers the use of language models whose size and accuracy are intermediate between different order n-gram models and examines smoothing procedures in which these models are interposed between different orders."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "82932230"
                        ],
                        "name": "Z. Gilula",
                        "slug": "Z.-Gilula",
                        "structuredName": {
                            "firstName": "Zvi",
                            "lastName": "Gilula",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Gilula"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4710675"
                        ],
                        "name": "S. Haberman",
                        "slug": "S.-Haberman",
                        "structuredName": {
                            "firstName": "Shelby",
                            "lastName": "Haberman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Haberman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 109
                            }
                        ],
                        "text": "In the statistical literature similar models have been discussed for the analysis of contingency tables (cf. Gilula & Haberman, 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 120514447,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4d49ccfd716417853e308fddc161f78567725ffe",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Canonical analysis has often been employed instead of log-linear models to analyze the relationship of two polytomous random variables; however, until the last few years, analysis has been informal. In this article, models are examined that place nontrivial restrictions on the values of the canonical parameters so that a parsimonious description of association is obtained. Maximum likelihood is used to obtain parameter estimates for these restricted models. Approximate confidence intervals are derived for parameters, and chi-squared tests are used to check adequacy of models. The resulting models may be used to determine the appropriateness of latent-class analysis or to determine whether a set of canonical scores has specified patterns. Results are illustrated through analysis of two tables previously analyzed in the statistical literature. Comparisons are made with alternate methods of analysis based on a log-linear parameterization of cell probabilities. It is shown that canonical analysis, wh..."
            },
            "slug": "Canonical-Analysis-of-Contingency-Tables-by-Maximum-Gilula-Haberman",
            "title": {
                "fragments": [],
                "text": "Canonical Analysis of Contingency Tables by Maximum Likelihood"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35229948"
                        ],
                        "name": "S. Katz",
                        "slug": "S.-Katz",
                        "structuredName": {
                            "firstName": "Slava",
                            "lastName": "Katz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Katz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The co-occurrence table representation immediately reveals the problem of data sparseness ( Katz, 1987 ), also known as the zero-frequency problem (Witten & Bell, 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 89
                            }
                        ],
                        "text": "The co-occurrence table representation immediately reveals the problem ofdata sparseness(Katz, 1987), also known as thezero-frequency problem(Witten & Bell, 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6555412,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0130277677e5b915d5cd86b3afafd77fd08eb2e",
            "isKey": false,
            "numCitedBy": 1907,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The description of a novel type of m-gram language model is given. The model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data. This solution compares favorably to other proposed methods. While the method has been developed for and successfully implemented in the IBM Real Time Speech Recognizers, its generality makes it applicable in other areas where the problem of estimating probabilities from sparse data arises."
            },
            "slug": "Estimation-of-probabilities-from-sparse-data-for-of-Katz",
            "title": {
                "fragments": [],
                "text": "Estimation of probabilities from sparse data for the language model component of a speech recognizer"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data, and compares favorably to other proposed methods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9419406"
                        ],
                        "name": "I. Witten",
                        "slug": "I.-Witten",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Witten",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Witten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49014513"
                        ],
                        "name": "T. Bell",
                        "slug": "T.-Bell",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Bell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Bell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The co-occurrence table representation immediately reveals the problem of data sparseness (Katz, 1987), also known as the zero-frequency problem ( Witten & Bell, 1991 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 142
                            }
                        ],
                        "text": "The co-occurrence table representation immediately reveals the problem ofdata sparseness(Katz, 1987), also known as thezero-frequency problem(Witten & Bell, 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10314497,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f5d21625f8264f455591b3c7cbdac18b983b3c0",
            "isKey": false,
            "numCitedBy": 848,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Approaches to the zero-frequency problem in adaptive text compression are discussed. This problem relates to the estimation of the likelihood of a novel event occurring. Although several methods have been used, their suitability has been on empirical evaluation rather than a well-founded model. The authors propose the application of a Poisson process model of novelty. Its ability to predict novel tokens is evaluated, and it consistently outperforms existing methods. It is applied to a practical statistical coding scheme, where a slight modification is required to avoid divergence. The result is a well-founded zero-frequency model that explains observed differences in the performance of existing methods, and offers a small improvement in the coding efficiency of text compression over the best method previously known. >"
            },
            "slug": "The-zero-frequency-problem:-Estimating-the-of-novel-Witten-Bell",
            "title": {
                "fragments": [],
                "text": "The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The authors propose the application of a Poisson process model of novelty, which ability to predict novel tokens is evaluated, and it consistently outperforms existing methods and offers a small improvement in the coding efficiency of text compression over the best method previously known."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748407"
                        ],
                        "name": "J. Bellegarda",
                        "slug": "J.-Bellegarda",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Bellegarda",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bellegarda"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 275,
                                "start": 259
                            }
                        ],
                        "text": "Due to its generality, LSA has proven to be a valuable analysis tool for many different problems in practice and thus has a wide range of possible applications (e.g., Deerwester et al., 1990; Foltz & Dumais, 1992; Landauer & Dumais, 1997; Wolfe et al., 1998; Bellegarda, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10513624,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6612de2c77458ca746325272a99e22c156263383",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "A new framework is proposed to integrate the various constraints, both local and global, that are present in the language. Local constraints are captured via n-gram language modeling, while global constraints are taken into account through the use of latent semantic analysis. An integrative formulation is derived for the combination of these two paradigms, resulting in several families of multi-span language models for large vocabulary speech recognition. Because of the inherent complementarity in the two types of constraints, the performance of the integrated language models, as measured by the perplexity, compares favorably with the corresponding n-gram performance."
            },
            "slug": "Exploiting-both-local-and-global-constraints-for-Bellegarda",
            "title": {
                "fragments": [],
                "text": "Exploiting both local and global constraints for multi-span statistical language modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A new framework is proposed to integrate the various constraints, both local and global, that are present in the language, resulting in several families of multi-span language models for large vocabulary speech recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP '98 (Cat. No.98CH36181)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115651440"
                        ],
                        "name": "Daniel D. Lee",
                        "slug": "Daniel-D.-Lee",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Lee",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel D. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144924970"
                        ],
                        "name": "H. Seung",
                        "slug": "H.-Seung",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Seung",
                            "middleNames": [
                                "Sebastian"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Seung"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Another closely related technique called non-negative matrix decomposition has been investigated in  Lee and Seung (1999) ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 100
                            }
                        ],
                        "text": "Another closely related technique called non-negative matrix decomposition has been investigated in Lee and Seung (1999)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4428232,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "29bae9472203546847ec1352a604566d0f602728",
            "isKey": false,
            "numCitedBy": 11310,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Is perception of the whole based on perception of its parts? There is psychological and physiological evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign."
            },
            "slug": "Learning-the-parts-of-objects-by-non-negative-Lee-Seung",
            "title": {
                "fragments": [],
                "text": "Learning the parts of objects by non-negative matrix factorization"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An algorithm for non-negative matrix factorization is demonstrated that is able to learn parts of faces and semantic features of text and is in contrast to other methods that learn holistic, not parts-based, representations."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40484982"
                        ],
                        "name": "P. Cheeseman",
                        "slug": "P.-Cheeseman",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Cheeseman",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Cheeseman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87277825"
                        ],
                        "name": "J. Stutz",
                        "slug": "J.-Stutz",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Stutz",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Stutz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 15
                            }
                        ],
                        "text": "The perplexity is defined to be the log-averaged inverse probability on unseen data, i.e.,\nP = exp [ \u2212 \u2211 i, j n \u2032(di , w j ) log P(w j | di )\u2211\ni, j n \u2032(di , w j )\n] , (15)\nwheren\u2032(di , w j ) denotes counts on hold-out or test data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 203
                            }
                        ],
                        "text": "Most closely related to our approach is thedistributional clustering model(Pereira, Tishby, & Lee, 1993; Baker & McCallum, 1998) and the multinomial (maximum likelihood) version of Autoclass clustering (Cheeseman & Stutz, 1996), an unsupervised version of a naive Bayes\u2019 classifier."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6176762,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42f75b297aed474599c8e598dd211a1999804138",
            "isKey": false,
            "numCitedBy": 1298,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe AutoClass an approach to unsupervised classi cation based upon the classical mixture model supplemented by a Bayesian method for determining the optimal classes We include a moderately detailed exposition of the mathematics behind the AutoClass system We emphasize that no current unsupervised classi cation system can produce maximally useful results when operated alone It is the interaction between domain experts and the machine searching over the model space that generates new knowledge Both bring unique information and abilities to the database analysis task and each enhances the others e ectiveness We illustrate this point with several applications of AutoClass to complex real world databases and describe the resulting successes and failures"
            },
            "slug": "Bayesian-Classification-(AutoClass):-Theory-and-Cheeseman-Stutz",
            "title": {
                "fragments": [],
                "text": "Bayesian Classification (AutoClass): Theory and Results"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is emphasized that no current unsupervised classi cation system can produce maximally useful results when operated alone and that it is the interaction between domain experts and the machine searching over the model space that generates new knowledge."
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Knowledge Discovery and Data Mining"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2121009"
                        ],
                        "name": "J. Puzicha",
                        "slug": "J.-Puzicha",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Puzicha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Puzicha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1312504,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7ad58ea541efaec494ea1ec6a5414ea01e236b04",
            "isKey": false,
            "numCitedBy": 159,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Dyadzc data refers to a domain with two finite sets of objects in which observations are made for dyads, i.e., pairs with one element from either set. This type of data arises naturally in many application ranging from computational linguistics and information retrieval to preference analysis and computer vision. In this paper, we present a systematic, domain-independent framework of learning from dyadic data by statistical mixture models. Our approach covers different models with fiat and hierarchical latent class structures. We propose an annealed version of the standard EM algorithm for model fitting which is empirically evaluated on a variety of data sets from different domains."
            },
            "slug": "Learning-from-Dyadic-Data-Hofmann-Puzicha",
            "title": {
                "fragments": [],
                "text": "Learning from Dyadic Data"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper proposes an annealed version of the standard EM algorithm for model fitting which is empirically evaluated on a variety of data sets from different domains."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735221"
                        ],
                        "name": "N. Ueda",
                        "slug": "N.-Ueda",
                        "structuredName": {
                            "firstName": "Naonori",
                            "lastName": "Ueda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ueda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763517"
                        ],
                        "name": "R. Nakano",
                        "slug": "R.-Nakano",
                        "structuredName": {
                            "firstName": "Ryohei",
                            "lastName": "Nakano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Nakano"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1890148,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0f3bbe7b5654427246bee62650f8f89ec3ba26c",
            "isKey": false,
            "numCitedBy": 510,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Deterministic-annealing-EM-algorithm-Ueda-Nakano",
            "title": {
                "fragments": [],
                "text": "Deterministic annealing EM algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143629177"
                        ],
                        "name": "K. Rose",
                        "slug": "K.-Rose",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Rose",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Rose"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3043610"
                        ],
                        "name": "E. Gurewitz",
                        "slug": "E.-Gurewitz",
                        "structuredName": {
                            "firstName": "Eitan",
                            "lastName": "Gurewitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Gurewitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153840193"
                        ],
                        "name": "G. Fox",
                        "slug": "G.-Fox",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Fox",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Fox"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The resulting method is called Tempered Expectation Maximization(TEM) and is closely related to deterministic annealing( Rose, Gurewitz, & Fox, 1990 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 36027870,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7984546a154eae54375fcda9bc9bcde08c096d0",
            "isKey": false,
            "numCitedBy": 409,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-deterministic-annealing-approach-to-clustering-Rose-Gurewitz",
            "title": {
                "fragments": [],
                "text": "A deterministic annealing approach to clustering"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2271549"
                        ],
                        "name": "M. Berry",
                        "slug": "M.-Berry",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Berry",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Berry"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401512392"
                        ],
                        "name": "G. O'Brien",
                        "slug": "G.-O'Brien",
                        "structuredName": {
                            "firstName": "Gavin",
                            "lastName": "O'Brien",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. O'Brien"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7580761,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "06871028d4e71ceefe879853e9dc2183ea81bb32",
            "isKey": false,
            "numCitedBy": 1755,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Currently, most approaches to retrieving textual materials from scientific databases depend on a lexical match between words in users\u2019 requests and those in or assigned to documents in a database. ..."
            },
            "slug": "Using-Linear-Algebra-for-Intelligent-Information-Berry-Dumais",
            "title": {
                "fragments": [],
                "text": "Using Linear Algebra for Intelligent Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A lexical match between words in users\u2019 requests and those in or assigned to documents in a database helps retrieve textual materials from scientific databases."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM Rev."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2036336"
                        ],
                        "name": "P. Foltz",
                        "slug": "P.-Foltz",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Foltz",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Foltz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728602"
                        ],
                        "name": "S. Dumais",
                        "slug": "S.-Dumais",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Dumais",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dumais"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 192
                            }
                        ],
                        "text": "Due to its generality, LSA has proven to be a valuable analysis tool for many different problems in practice and thus has a wide range of possible applications (e.g., Deerwester et al., 1990; Foltz & Dumais, 1992; Landauer & Dumais, 1997; Wolfe et al., 1998; Bellegarda, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8331866,
            "fieldsOfStudy": [
                "Chemistry"
            ],
            "id": "fc5e48abb78199d3a964a5284c38750a514d82a1",
            "isKey": false,
            "numCitedBy": 675,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "10345 Abstract 10346 Abstract 10347 Abstract 10348 :iltered Abstracts Abstract 10349 Abstract 1040210349 Abstract 10402 Highly rated abstracts added to document profile each month T h e score for e ach new T M was t he cos ine b e t w e e n t he T M vec tor a n d the nearest i n t e r e s t vector . T h e new T M s were t h e n r a n k e d based o n the i r m a x i m u m cosine score. T h u s , T M s o c c u r r i n g close to any p o i n t o f i n t e r e s t fo r a p a r t i c u l a r e m p l o y e e would be r a n k e d t he h ighes t . T h e same type o f c o m p a r i s o n was d o n e fo r the d o c u m e n t prof i les . Each docu m e n t in t he d o c u m e n t p rof i l e was r e p r e s e n t e d as a s epa ra t e vec tor a n d c o m p a r e d to all new T M s . T h e new T M s were t h e n r a n k e d based o n the m a x i m u m cosine for e ach T M to any d o c u m e n t in t he d o c u m e n t prof i le . T h e s e c o m p a r i s o n s r e su l t ed in f o u r r a n k o r d e r e d lists o f abstracts , o n e for e ach f i l t e r ing m e t h o d . T h e top abs t rac t s f r o m each m e t h o d were t h e n sen t to each emp loyee , w h o r a t e d t h e m fo r r e l evance to his o r h e r t echn ica l in teres ts . Fo r each m e t h o d , the t op 25% o f t he abs t rac t s r a t e d 4 or h i g h e r were t h e n i n c o r p o r a t e d in to t he e m p l o y e e s ' d o c u m e n t p ro files. 3 I n the f i rs t m o n t h o f s tudy, on ly the w o r d prof i l e was u sed s ince e m p l o y e e s h a d no t p rev ious ly indica ted which abs t rac t s to i nc lude in t h e i r d o c u m e n t prof i le . I n the subseFlOure 11. The f i l t e r i n g p r o c e s s Table 1. The four f i l ter ing methods"
            },
            "slug": "Personalized-information-delivery:-an-analysis-of-Foltz-Dumais",
            "title": {
                "fragments": [],
                "text": "Personalized information delivery: an analysis of information filtering methods"
            },
            "tldr": {
                "abstractSimilarityScore": 31,
                "text": "T h e score for e ach new T M was t he cos ine b e t w e e n t he T M vec tor a n d the nearest i n t e r e s t vector ."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144321599"
                        ],
                        "name": "M. McGill",
                        "slug": "M.-McGill",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "McGill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. McGill"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The key idea in LSA is to map high-dimensional count vectors, such as termfrequency (tf) vectors arising in the vector space representation of text documents ( Salton & McGill, 1983 ), to a lower dimensional representation in a so-called latent semantic space."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 87
                            }
                        ],
                        "text": "The key assumption is that the simplified \u2018bag-of-words\u2019 or vector-space representation (Salton & McGill, 1983) of documents will in many cases preserve most of the relevant information, e.g., for tasks like text retrieval based on keywords."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The most popular family of information retrieval techniques is based on the Vector\u2010Space Model (VSM) for documents ( Salton & McGill, 1983 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 159
                            }
                        ],
                        "text": "The key idea in LSA is to map high-dimensional count vectors, such as termfrequency (tf) vectors arising in the vector space representation of text documents (Salton & McGill, 1983), to a lower dimensional representation in a so-calledatent semantic space."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The key assumption is that the simplified \u2018bag-of-words\u2019 or vector-space representation ( Salton & McGill, 1983 ) of documents will in many cases preserve most of the relevant information, e.g., for tasks like text retrieval based on keywords."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 113
                            }
                        ],
                        "text": "The most popular family of information retrieval techniques is based on theVector\u2013Space Model(VSM) for documents (Salton & McGill, 1983)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 43685115,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "49af3e80343eb80c61e727ae0c27541628c7c5e2",
            "isKey": true,
            "numCitedBy": 12605,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Some people may be laughing when looking at you reading in your spare time. Some may be admired of you. And some may want be like you who have reading hobby. What about your own feel? Have you felt right? Reading is a need and a hobby at once. This condition is the on that will make you feel that you must read. If you know are looking for the book enPDFd introduction to modern information retrieval as the choice of reading, you can find here."
            },
            "slug": "Introduction-to-Modern-Information-Retrieval-Salton-McGill",
            "title": {
                "fragments": [],
                "text": "Introduction to Modern Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Reading is a need and a hobby at once and this condition is the on that will make you feel that you must read."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The standard procedure for maximum likelihood estimation in latent variable models is the Expectation Maximization (EM) algorithm ( Dempster, Laird, & Rubin, 1977 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": false,
            "numCitedBy": 48403,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 116
                            }
                        ],
                        "text": "As the name PLSA indicates, our approach has been largely inspired and influenced by Latent Semantic Analysis (LSA) (Deerwester et al., 1990), although there are also notable differences."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 116
                            }
                        ],
                        "text": "As the name PLSA indicates, our approach has been largely inspired and influenced by Latent Semantic Analysis(LSA) (Deerwester et al., 1990), although there are also notable differences."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 277,
                                "start": 254
                            }
                        ],
                        "text": "\u2026in the introduction, the key idea of LSA is to map documents\u2014and by symmetry terms\u2014to a vector space of reduced dimensionality, thela ent semantic space, which in a typical application in document indexing is chosen to have of the order\u2248100\u2212300 dimensions (Deerwester et al., 1990; Dumais, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 311,
                                "start": 272
                            }
                        ],
                        "text": "As mentioned in the introduction, the key idea of LSA is to map documents\u2014and by symmetry terms\u2014to a vector space of reduced dimensionality, the la ent semantic space , which in a typical application in document indexing is chosen to have of the order \u2248100\u2212300 dimensions (Deerwester et al., 1990; Dumais, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 151
                            }
                        ],
                        "text": "In particular, PLSI works well even in cases where LSI fails completely (these problems of LSI are in accordance with the original results reported in Deerwester et al. (1990))."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 1
                            }
                        ],
                        "text": "(Deerwester et al., 1990) for details). In our experiments, we have actually considered linear combinations of the original similarity score (18) (weight \u03bb) and the one derived from the latent space representation (weight 1 \u2212 \u03bb), as suggested in Pereira et al. (1993). The same ideas have been applied in Probabilistic Latent Semantic Indexing (PLSI) in conjunction with the PLSA model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 167
                            }
                        ],
                        "text": "Due to its generality, LSA has proven to be a valuable analysis tool for many different problems in practice and thus has a wide range of possible applications (e.g., Deerwester et al., 1990; Foltz & Dumais, 1992; Landauer & Dumais, 1997; Wolfe et al., 1998; Bellegarda, 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 123
                            }
                        ],
                        "text": "Queries or documents which were not part of the original collection can befolded inby a simple matrix multiplication (cf. (Deerwester et al., 1990) for details)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 328,
                                "start": 303
                            }
                        ],
                        "text": "Notice that the relative precision gain compared to the baseline method is typically around 100% in the most interesting intermediate regime of recall! In particular, PLSI works well even in cases where LSI fails completely (these problems of LSI are in accordance with the original results reported in Deerwester et al. (1990))."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Indexing by latent semantic analysis.Journal of the American Society for Information Science"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712211"
                        ],
                        "name": "G. Golub",
                        "slug": "G.-Golub",
                        "structuredName": {
                            "firstName": "Gene",
                            "lastName": "Golub",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Golub"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 126299280,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9efffc63f81bf3f6cb6357ddc15e9cd9da75d16",
            "isKey": false,
            "numCitedBy": 27000,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Matrix-computations-Golub",
            "title": {
                "fragments": [],
                "text": "Matrix computations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764325"
                        ],
                        "name": "Radford M. Neal",
                        "slug": "Radford-M.-Neal",
                        "structuredName": {
                            "firstName": "Radford",
                            "lastName": "Neal",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radford M. Neal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "As has been pointed out in  Neal and Hinton (1998) , the EM procedure in latent variable models can be obtained by minimizing a common objective function\u2014the (Helmholtz) free energy\u2014which for the aspect model is given by"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62562212,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "418cc44768ff9d0ed8cf4cef79869f90ab672f7b",
            "isKey": false,
            "numCitedBy": 215,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-new-view-of-the-EM-algorithm-that-justifies-and-Neal",
            "title": {
                "fragments": [],
                "text": "A new view of the EM algorithm that justifies incremental and other variants"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Received February"
            },
            "venue": {
                "fragments": [],
                "text": "Received February"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Linguistic Data Consortium: TDT pilot study corpus documentation"
            },
            "venue": {
                "fragments": [],
                "text": "Linguistic Data Consortium: TDT pilot study corpus documentation"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 89
                            }
                        ],
                        "text": "The co-occurrence table representation immediately reveals the problem of data sparseness(Katz, 1987), also known as the zero-frequency problem(Witten & Bell, 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 89
                            }
                        ],
                        "text": "The co-occurrence table representation immediately reveals the problem ofdata sparseness(Katz, 1987), also known as thezero-frequency problem(Witten & Bell, 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimation of probabilities for sparse data for the language model component of a speech"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Deterministic annealing EM"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM algorithm.J"
            },
            "venue": {
                "fragments": [],
                "text": "Royal Statist. Soc. B"
            },
            "year": 1977
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 9,
            "methodology": 15,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 31,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Unsupervised-Learning-by-Probabilistic-Latent-Hofmann/e6dd83b2aa34c806596fc619ff3fbccf5f9830ab?sort=total-citations"
}