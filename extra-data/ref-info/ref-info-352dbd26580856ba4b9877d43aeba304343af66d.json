{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32326549"
                        ],
                        "name": "J. Kupiec",
                        "slug": "J.-Kupiec",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Kupiec",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kupiec"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2094488,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14bb02f0a0555b3d9aaf8f3f130516dd70bf19d9",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper describes refinements that are currently being investigated in a model for part-of-speech assignment to words in unrestricted text. The model has the advantage that a pre-tagged training corpus is not required. Words are represented by equivalence classes to reduce the number of parameters required and provide an essentially vocabulary-independent model. State chains are used to model selective higher-order conditioning in the model, which obviates the proliferation of parameters attendant in uniformly higher-order models. The structure of the state chains is based on both an analysis of errors and linguistic knowledge. Examples show how word dependency across phrases can be modeled."
            },
            "slug": "Augmenting-a-Hidden-Markov-Model-for-Word-Tagging-Kupiec",
            "title": {
                "fragments": [],
                "text": "Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging"
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686820"
                        ],
                        "name": "B. M\u00e9rialdo",
                        "slug": "B.-M\u00e9rialdo",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "M\u00e9rialdo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. M\u00e9rialdo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61014458,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8a9b6828c5e4339025bb78af6b025d21b4830800",
            "isKey": false,
            "numCitedBy": 99,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Experiments on the use of a probabilistic model to tag English text, that is, to assign to each word the correct tag (part of speech) in the context of the sentence, are presented. A simple triclass Markov model is used, and the best way to estimate the parameters of this model, depending on the kind and amount of training data that is provided, is found. Two approaches are compared: the use of text that has been tagged by hand and comparing relative frequency counts; and use text without tags and training the model as a hidden Markov process, according to a maximum likelihood principle. Experiments show that the best training is obtained by using as much tagged text as is available, a maximum likelihood training may improve the accuracy of the tagging.<<ETX>>"
            },
            "slug": "Tagging-text-with-a-probabilistic-model-M\u00e9rialdo",
            "title": {
                "fragments": [],
                "text": "Tagging text with a probabilistic model"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Experiments show that the best training is obtained by using as much tagged text as is available, and a maximum likelihood training may improve the accuracy of the tagging."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings] ICASSP 91: 1991 International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143937779"
                        ],
                        "name": "R. Kuhn",
                        "slug": "R.-Kuhn",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Kuhn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kuhn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714393"
                        ],
                        "name": "R. Mori",
                        "slug": "R.-Mori",
                        "structuredName": {
                            "firstName": "Renato",
                            "lastName": "Mori",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mori"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 31924166,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be1fed9544830df1137e72b1d2396c40d3e18365",
            "isKey": false,
            "numCitedBy": 580,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Speech-recognition systems must often decide between competing ways of breaking up the acoustic input into strings of words. Since the possible strings may be acoustically similar, a language model is required; given a word string, the model returns its linguistic probability. Several Markov language models are discussed. A novel kind of language model which reflects short-term patterns of word use by means of a cache component (analogous to cache memory in hardware terminology) is presented. The model also contains a 3g-gram component of the traditional type. The combined model and a pure 3g-gram model were tested on samples drawn from the Lancaster-Oslo/Bergen (LOB) corpus of English text. The relative performance of the two models is examined, and suggestions for the future improvements are made. >"
            },
            "slug": "A-Cache-Based-Natural-Language-Model-for-Speech-Kuhn-Mori",
            "title": {
                "fragments": [],
                "text": "A Cache-Based Natural Language Model for Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A novel kind of language model which reflects short-term patterns of word use by means of a cache component (analogous to cache memory in hardware terminology) is presented and contains a 3g-gram component of the traditional type."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144712062"
                        ],
                        "name": "P. Dumouchel",
                        "slug": "P.-Dumouchel",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Dumouchel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dumouchel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1387789571"
                        ],
                        "name": "Vishwa Gupta",
                        "slug": "Vishwa-Gupta",
                        "structuredName": {
                            "firstName": "Vishwa",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vishwa Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2959613"
                        ],
                        "name": "Matthew Lennig",
                        "slug": "Matthew-Lennig",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Lennig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew Lennig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143791674"
                        ],
                        "name": "P. Mermelstein",
                        "slug": "P.-Mermelstein",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Mermelstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Mermelstein"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60979195,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bf65e2343b11559b1f309a421b8fba57a8f9c7ac",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Relative performance is compared for three different language models applied to the linguistic decoding part of a 75000-word speech recognizer. These models are the trigram model, the tri-POS model (POS stands for parts of speech), and a smoothed trigram model with tied distributions for words three or more syllables long. The full trigram model gives the best performance but is most expensive in terms of data and storage requirements. The smoothed trigram and tri-POS models yield equivalent performance. For general text entry tasks, use of the tri-POS model is suggested since it is less sensitive to variations in the discourse domains. For applications specific to individual discourse domains, trigram models trained on data obtained from the target domain are recommended.<<ETX>>"
            },
            "slug": "Three-probabilistic-language-models-for-a-speech-Dumouchel-Gupta",
            "title": {
                "fragments": [],
                "text": "Three probabilistic language models for a large-vocabulary speech recognizer"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "Relative performance is compared for three different language models applied to the linguistic decoding part of a 75000-word speech recognizer, and a smoothed trigram model with tied distributions for words three or more syllables long is recommended."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP-88., International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2803620"
                        ],
                        "name": "J. R. Rohlicek",
                        "slug": "J.-R.-Rohlicek",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Rohlicek",
                            "middleNames": [
                                "Robin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Rohlicek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2727234"
                        ],
                        "name": "Y. Chow",
                        "slug": "Y.-Chow",
                        "structuredName": {
                            "firstName": "Yen-lu",
                            "lastName": "Chow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Chow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46924970"
                        ],
                        "name": "Salim Roukos",
                        "slug": "Salim-Roukos",
                        "structuredName": {
                            "firstName": "Salim",
                            "lastName": "Roukos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Salim Roukos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60903164,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "265e506c9bfde064424b36d84424d2333ec1ff13",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical language models have been successfully used to improve the performance of continuous speech recognition algorithms. Application of such techniques is difficult when only a small training corpus is available. The authors present an approach for dealing with limited training available from the DARPA resource management domain. An initial training corpus of sentences was abstracted by replacing sentence fragments or phrases with variables. This training corpus of phrase sequences was used to derive parameters of a Markov model. The probability of a word sequence is then decomposed into the probability of possible phrase sequences within each of the phrases. Initial results obtained on 150 utterances from six speakers in the DARPA database indicate that this language modeling technique has potential for improved recognition performance. Furthermore, this approach provides a framework for incorporating linguistic knowledge into statistical language models.<<ETX>>"
            },
            "slug": "Statistical-language-modeling-using-a-small-corpus-Rohlicek-Chow",
            "title": {
                "fragments": [],
                "text": "Statistical language modeling using a small corpus from an application domain"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Initial results obtained on 150 utterances from six speakers in the DARPA database indicate that this language modeling technique has potential for improved recognition performance and provides a framework for incorporating linguistic knowledge into statistical language models."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP-88., International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108943475"
                        ],
                        "name": "Masami Nakamura",
                        "slug": "Masami-Nakamura",
                        "structuredName": {
                            "firstName": "Masami",
                            "lastName": "Nakamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Masami Nakamura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9243990"
                        ],
                        "name": "K. Shikano",
                        "slug": "K.-Shikano",
                        "structuredName": {
                            "firstName": "Kiyohiro",
                            "lastName": "Shikano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Shikano"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 51825049,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a044d798082c6dda101932a13d6c324e2edd551b",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Using traditional statistical approaches, it is difficult to develop an N-gram word prediction model for constructing an accurate word recognition system because of the increased demand for sample data and parameters to memorize probabilities. To solve this problem, NETgrams, which are neural networks for N-gram word category prediction in text are proposed. NETgrams can easily be expanded from bigram to N-gram networks without exponentially increasing the number of free parameters. Training results show that the NETgrams are comparable to the statistical model and compress information. Results of analyzing the hidden layer (microfeatures) show that the word categories are classified into some linguistically significant groups. It is confirmed that NETgrams perform effectively for unknown data, i.e NETgrams interpolate sparse training data naturally just like the deleted interpolation. A method for speeding up the back-propagation algorithm, which can automatically determine better parameters and achieve a shorter training time is proposed.<<ETX>>"
            },
            "slug": "A-study-of-English-word-category-prediction-based-Nakamura-Shikano",
            "title": {
                "fragments": [],
                "text": "A study of English word category prediction based on neutral networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Training results show that the NETgrams are comparable to the statistical model and compress information, and a method for speeding up the back-propagation algorithm, which can automatically determine better parameters and achieve a shorter training time is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "International Conference on Acoustics, Speech, and Signal Processing,"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7510969"
                        ],
                        "name": "S. DeRose",
                        "slug": "S.-DeRose",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "DeRose",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. DeRose"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1275545,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3a8de92b304729f15d9bd6c3d22a56ab9b31e212",
            "isKey": false,
            "numCitedBy": 430,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Several algorithms have been developed in the past that attempt to resolve categorial ambiguities in natural language text without recourse to syntactic or semantic level information. An innovative method (called \"CLAWS\") was recently developed by those working with the Lancaster-Oslo/Bergen Corpus of British English. This algorithm uses a systematic calculation based upon the probabilities of co-occurrence of particular tags. Its accuracy is high, but it is very slow, and it has been manually augmented in a number of ways. The effects upon accuracy of this manual augmentation are not individually known.The current paper presents an algorithm for disambiguation that is similar to CLAWS but that operates in linear rather than in exponential time and space, and which minimizes the unsystematic augments. Tests of the algorithm using the million words of the Brown Standard Corpus of English are reported; the overall accuracy is 96%. This algorithm can provide a fast and accurate front end to any parsing or natural language processing system for English."
            },
            "slug": "Grammatical-Category-Disambiguation-by-Statistical-DeRose",
            "title": {
                "fragments": [],
                "text": "Grammatical Category Disambiguation by Statistical Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An algorithm for disambiguation that is similar to CLAWS but that operates in linear rather than in exponential time and space, and which minimizes the unsystematic augments is presented."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759195"
                        ],
                        "name": "S. Levinson",
                        "slug": "S.-Levinson",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Levinson",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Levinson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34830449"
                        ],
                        "name": "M. Sondhi",
                        "slug": "M.-Sondhi",
                        "structuredName": {
                            "firstName": "Man",
                            "lastName": "Sondhi",
                            "middleNames": [
                                "Mohan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sondhi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 46254718,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "090f3ea5bc188bbb03aec02aba9ed9c7b38ff870",
            "isKey": false,
            "numCitedBy": 1082,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present several of the salient theoretical and practical issues associated with modeling a speech signal as a probabilistic function of a (hidden) Markov chain. First we give a concise review of the literature with emphasis on the Baum-Welch algorithm. This is followed by a detailed discussion of three issues not treated in the literature: alternatives to the Baum-Welch algorithm; critical facets of the implementation of the algorithms, with emphasis on their numerical properties; and behavior of Markov models on certain artificial but realistic problems. Special attention is given to a particular class of Markov models, which we call \u201cleft-to-right\u201d models. This class of models is especially appropriate for isolated word recognition. The results of the application of these methods to an isolated word, speaker-independent speech recognition experiment are given in a companion paper."
            },
            "slug": "An-introduction-to-the-application-of-the-theory-of-Levinson-Rabiner",
            "title": {
                "fragments": [],
                "text": "An introduction to the application of the theory of probabilistic functions of a Markov process to automatic speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This paper presents several of the salient theoretical and practical issues associated with modeling a speech signal as a probabilistic function of a (hidden) Markov chain, and focuses on a particular class of Markov models, which are especially appropriate for isolated word recognition."
            },
            "venue": {
                "fragments": [],
                "text": "The Bell System Technical Journal"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403810216"
                        ],
                        "name": "H. Cerf-Danon",
                        "slug": "H.-Cerf-Danon",
                        "structuredName": {
                            "firstName": "Helene",
                            "lastName": "Cerf-Danon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Cerf-Danon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388702976"
                        ],
                        "name": "M. El-B\u00e8ze",
                        "slug": "M.-El-B\u00e8ze",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "El-B\u00e8ze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. El-B\u00e8ze"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62627735,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5abe02230f91f5858d0ca202095573b67736f269",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors outline the different problems that arise when using a statistical language model for speech recognition, especially for inflected languages such as French, Italian or German. After a brief review of two classical models (TriPOS and Trigram), the authors present a refinement of the morphological language model (Trilemma). They give the different methods used to evaluate performances. They discuss combination experiments between two of these three building blocks and present a model which takes advantage of all three models through a backing-off strategy. Assuming the same vocabulary (20000 forms), experiments show equivalent results using either a classical trigram language model or a trilemma model. The second model can be extended to a full dictionary containing all the inflected forms of each lemma, whereas the first needs a large amount of data to perform such a task.<<ETX>>"
            },
            "slug": "Three-different-probabilistic-language-models:-and-Cerf-Danon-El-B\u00e8ze",
            "title": {
                "fragments": [],
                "text": "Three different probabilistic language models: comparison and combination"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The authors present a refinement of the morphological language model (Trilemma) which takes advantage of all three models through a backing-off strategy and gives the different methods used to evaluate performances."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings] ICASSP 91: 1991 International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3227843"
                        ],
                        "name": "M. Meteer",
                        "slug": "M.-Meteer",
                        "structuredName": {
                            "firstName": "Marie",
                            "lastName": "Meteer",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Meteer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35442155"
                        ],
                        "name": "R. Schwartz",
                        "slug": "R.-Schwartz",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732071"
                        ],
                        "name": "R. Weischedel",
                        "slug": "R.-Weischedel",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Weischedel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Weischedel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14999150,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "493b63168e2778d64934f9027a3dc74f8f9f746f",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We report here on our experiments with POST (Part of Speech Tagger) to address problems of ambiguity and of understanding unknown words. Part of speech tagging, perse, is a well understood problem. Our paper reports experiments in three important areas: handling unknown words, limiting the size of the training set, and returning a set of the most likely tags for each word rather than a single tag. We describe the algorithms that we used and the specific results of our experiments on Wall Street Journal articles and on MUC terrorist messages."
            },
            "slug": "POST:-Using-Probabilities-in-Language-Processing-Meteer-Schwartz",
            "title": {
                "fragments": [],
                "text": "POST: Using Probabilities in Language Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper reports experiments in three important areas: handling unknown words, limiting the size of the training set, and returning a set of the most likely tags for each word rather than a single tag."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72994026"
                        ],
                        "name": "Julian Benello",
                        "slug": "Julian-Benello",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Benello",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julian Benello"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053677731"
                        ],
                        "name": "Andrew Mackie",
                        "slug": "Andrew-Mackie",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Mackie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Mackie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47022813"
                        ],
                        "name": "J.A. Anderson",
                        "slug": "J.A.-Anderson",
                        "structuredName": {
                            "firstName": "J.A.",
                            "lastName": "Anderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J.A. Anderson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62575584,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "13ad44461eb281434bc91e4a2318b9e29ed8716d",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Syntactic-category-disambiguation-with-neural-Benello-Mackie",
            "title": {
                "fragments": [],
                "text": "Syntactic category disambiguation with neural networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2696176"
                        ],
                        "name": "L. Bahl",
                        "slug": "L.-Bahl",
                        "structuredName": {
                            "firstName": "Lalit",
                            "lastName": "Bahl",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 38697325,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "22b6737a38179c01444d69443e327850c9956c15",
            "isKey": false,
            "numCitedBy": 314,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Most current attempts at automatic speech recognition are formulated in an artificial intelligence framework. In this paper we approach the problem from an information-theoretic point of view. We describe the overall structure of a linguistic statistical decoder (LSD) for the recognition of continuous speech. The input to the decoder is a string of phonetic symbols estimated by an acoustic processor (AP). For each phonetic string, the decoder finds the most likely input sentence. The decoder consists of four major subparts: 1) a statistical model of the language being recognized; 2) a phonemic dictionary and statistical phonological rules characterizing the speaker; 3) a phonetic matching algorithm that computes the similarity between phonetic strings, using the performance characteristics of the AP; 4) a word level search control. The details of each of the subparts and their interaction during the decoding process are discussed."
            },
            "slug": "Design-of-a-linguistic-statistical-decoder-for-the-Jelinek-Bahl",
            "title": {
                "fragments": [],
                "text": "Design of a linguistic statistical decoder for the recognition of continuous speech"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper describes the overall structure of a linguistic statistical decoder (LSD) for the recognition of continuous speech and describes a phonetic matching algorithm that computes the similarity between phonetic strings, using the performance characteristics of the AP."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48903026"
                        ],
                        "name": "Fred Karlsson",
                        "slug": "Fred-Karlsson",
                        "structuredName": {
                            "firstName": "Fred",
                            "lastName": "Karlsson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fred Karlsson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9809022,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "2b91a3954cf5c0d44344eb5b87111e95cfa7a6c5",
            "isKey": false,
            "numCitedBy": 256,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Outline Grammars which are used in parsers are often directly imported from autonomous grammar theory and descriptive practice that were not exercised for the explicit purpose of parsing. Parsers have been designed for English based on e.g. Government and Binding Theory, Generalized Phrase Structure Grammar, and LexicaI-Functional Grammar. We present a formalism to be used for parsing where the grammar statements are closer to real text sentences and more directly address some notorious parsing problems, especially ambiguity. The formalism is a linguistic one. It relies on transitional probabilities in an indirect way. The probabilities are not part of the description. The descriptive statements, constraints, do not have the ordinary task of defining the notion 'correct sentence in L'. They are less categorical in nature, more closely tied to morphological features, and more directly geared towards the basic task of parsing. We see this task as one of inferring surface structure from a stream of concrete tokens in a basically bottom-up mode. Constraints are formulated on the basis of extensive corpus studies. They may reflect absolute, ruleqike facts, or probabilistic tendencies where a certain risk is judged to be proper to take. Constraints of the former rule-like type are of course preferable. The ensemble of constraints for language L constitute a Constraint Grammar (CG) for L. A CG is intended to be used by the Constraint Grammar Parser CGP, implemented as a Lisp interpreter. Our input tokens to CGP are morphologically analyzed word-forms. One central idea is to maximize the use of morphological information for parsing purposes. All relevant structure is assigned directly via lexicon, morphology, and simple mappings from morphology to syntax. ]he task of the constraints is basically to discard as many alternatives as possible, the optimum being a fully disambiguated sentence with one syntactic reading only. The second central idea is to treat morphological disambiguation and syntactic labelling by the same mechanism of discarding improper alternatives."
            },
            "slug": "Constraint-Grammar-as-a-Framework-for-Parsing-Text-Karlsson",
            "title": {
                "fragments": [],
                "text": "Constraint Grammar As A Framework For Parsing Running Text"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work presents a formalism to be used for parsing where the grammar statements are closer to real text sentences and more directly address some notorious parsing problems, especially ambiguity."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32326549"
                        ],
                        "name": "J. Kupiec",
                        "slug": "J.-Kupiec",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Kupiec",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kupiec"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14679951,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "343c8af478f7703459b0e390e888efe723f15e31",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This article describes two complementary models that represent dependencies between words in local and non-local contexts. The type of local dependencies considered are sequences of part of speech categories for words. The non-local context of word dependency considered here is that of word recurrence, which is typical in a text. Both are models of phenomena that are to a reasonable extent domain independent, and thus are useful for doing prediction in systems using large vocabularies."
            },
            "slug": "Probabilistic-Models-of-Short-and-Long-Distance-in-Kupiec",
            "title": {
                "fragments": [],
                "text": "Probabilistic Models of Short and Long Distance Word Dependencies in Running Text"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "Two complementary models that represent dependencies between words in local and non-local contexts are described, which are useful for doing prediction in systems using large vocabularies."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3166885,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10",
            "isKey": false,
            "numCitedBy": 1058,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A program that tags each word in an input sentence with the most likely part of speech has been written. The program uses a linear-time dynamic programming algorithm to find an assignment of parts of speech to words that optimizes the product of (a) lexical probabilities (probability of observing part of speech i given word i) and (b) contextual probabilities (probability of observing part of speech i given n following parts of speech). Program performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct.<<ETX>>"
            },
            "slug": "A-Stochastic-Parts-Program-and-Noun-Phrase-Parser-Church",
            "title": {
                "fragments": [],
                "text": "A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A program that tags each word in an input sentence with the most likely part of speech has been written and performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2687013"
                        ],
                        "name": "A. Derouault",
                        "slug": "A.-Derouault",
                        "structuredName": {
                            "firstName": "Anne-Marie",
                            "lastName": "Derouault",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Derouault"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686820"
                        ],
                        "name": "B. M\u00e9rialdo",
                        "slug": "B.-M\u00e9rialdo",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "M\u00e9rialdo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. M\u00e9rialdo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 3173459,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "85da46e840ab8ea85759f4308aa195bde2aaadab",
            "isKey": false,
            "numCitedBy": 126,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper relates different kinds of language modeling methods that can be applied to the linguistic decoding part of a speech recognition system with a very large vocabulary. These models are studied experimentally on a pseudophonetic input arising from French stenotypy. We propose a model which combines the advantages of a statistical modeling with information theoretic tools, and those of a grammatical approach."
            },
            "slug": "Natural-Language-Modeling-for-Phoneme-to-Text-Derouault-M\u00e9rialdo",
            "title": {
                "fragments": [],
                "text": "Natural Language Modeling for Phoneme-to-Text Transcription"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This paper relates different kinds of language modeling methods that can be applied to the linguistic decoding part of a speech recognition system with a very large vocabulary and proposes a model which combines the advantages of a statistical modeling with information theoretic tools, and those of a grammatical approach."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2466657"
                        ],
                        "name": "K. Koskenniemi",
                        "slug": "K.-Koskenniemi",
                        "structuredName": {
                            "firstName": "Kimmo",
                            "lastName": "Koskenniemi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Koskenniemi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 19487388,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "1ef69ff7760477d26d6c8d06f07dd021d60f9413",
            "isKey": false,
            "numCitedBy": 119,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "A language-independent method of finite-state surface syntactic parsing and word-disambiguation is discussed. Input sentences are represented as finite-state networks already containing all possible roles and interpretations of its units. Also syntactic constraint rules are represented as finite-state machines where each constraint excludes certain types of ungrammatical readings. The whole grammar is an intersection of its constraint rules and excludes all ungrammatical possibilities leaving the correct interpretation(s) of the sentence. The method is being tested for Finnish, Swedish and English."
            },
            "slug": "Finite-State-Parsing-and-Disambiguation-Koskenniemi",
            "title": {
                "fragments": [],
                "text": "Finite-State Parsing And Disambiguation"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A language-independent method of finite-state surface syntactic parsing and word-disambiguation is discussed, which excludes all ungrammatical possibilities leaving the correct interpretation of the sentence."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145500689"
                        ],
                        "name": "A. Viterbi",
                        "slug": "A.-Viterbi",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Viterbi",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Viterbi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15843983,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "145c0b53514b02bdc3dadfb2e1cea124f2abd99b",
            "isKey": false,
            "numCitedBy": 5209,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates above R_{0} , the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R_{0} and whose performance bears certain similarities to that of sequential decoding algorithms."
            },
            "slug": "Error-bounds-for-convolutional-codes-and-an-optimum-Viterbi",
            "title": {
                "fragments": [],
                "text": "Error bounds for convolutional codes and an asymptotically optimum decoding algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R_{0} and whose performance bears certain similarities to that of sequential decoding algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61012010,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a923c9f89ed53b6e835b3807c0c1bd8d532687b",
            "isKey": false,
            "numCitedBy": 1037,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Interpolated-estimation-of-Markov-source-parameters-Jelinek",
            "title": {
                "fragments": [],
                "text": "Interpolated estimation of Markov source parameters from sparse data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39361090"
                        ],
                        "name": "L. Baum",
                        "slug": "L.-Baum",
                        "structuredName": {
                            "firstName": "Leonard",
                            "lastName": "Baum",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Baum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60804212,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "539036ab9e8f038c8a948596e77cc0dfcfa91fb3",
            "isKey": false,
            "numCitedBy": 1785,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-inequality-and-associated-maximization-technique-Baum",
            "title": {
                "fragments": [],
                "text": "An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {},
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 20,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Robust-part-of-speech-tagging-using-a-hidden-Markov-Kupiec/352dbd26580856ba4b9877d43aeba304343af66d?sort=total-citations"
}