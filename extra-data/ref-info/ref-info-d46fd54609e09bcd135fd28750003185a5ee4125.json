{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30017846"
                        ],
                        "name": "N. Pinto",
                        "slug": "N.-Pinto",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Pinto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Pinto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3250342"
                        ],
                        "name": "D. Doukhan",
                        "slug": "D.-Doukhan",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doukhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doukhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1865831"
                        ],
                        "name": "J. DiCarlo",
                        "slug": "J.-DiCarlo",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "DiCarlo",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. DiCarlo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2042941"
                        ],
                        "name": "D. Cox",
                        "slug": "D.-Cox",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Cox",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cox"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59453979,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "068b9abedd8d287034554d11d9ee6c3db8cdc920",
            "isKey": false,
            "numCitedBy": 1,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "While many models of biological object recognition share a common set of \u2018\u2018broad-stroke\u2019\u2019 properties, the performance of any one model depends strongly on the choice of parameters in a particular instantiation of that model\u2014e.g., the number of units per layer, the size of pooling kernels, exponents in normalization operations, etc. Since the number of such parameters (explicit or implicit) is typically large and the computational cost of evaluating one particular parameter set is high, the space of possible model instantiations goes largely unexplored. Thus, when a model fails to approach the abilities of biological visual systems, we are left uncertain whether this failure is because we are missing a fundamental idea or because the correct \u2018\u2018parts\u2019\u2019 have not been tuned correctly, assembled at sufficient scale, or provided with enough training. Here, we present a high-throughput approach to the exploration of such parameter sets, leveraging recent advances in stream processing hardware (high-end NVIDIA graphic cards and the PlayStation 3\u2019s IBM Cell Processor). In analogy to highthroughput screening approaches in molecular biology and genetics, we explored thousands of potential network architectures and parameter instantiations, screening those that show promising object recognition performance for further analysis. We show that this approach can yield significant, reproducible gains in performance across an array of basic object recognition tasks, consistently outperforming a variety of state-of-the-art purpose-built vision systems from the literature. As the scale of available computational power continues to expand, we argue that this approach has the potential to greatly accelerate progress in both artificial vision and our understanding of the computational underpinning of biological vision. Citation: Pinto N, Doukhan D, DiCarlo JJ, Cox DD (2009) A High-Throughput Screening Approach to Discovering Good Forms of Biologically Inspired Visual Representation. PLoS Comput Biol 5(11): e1000579. doi:10.1371/journal.pcbi.1000579 Editor: Karl J. Friston, University College London, United Kingdom Received June 11, 2009; Accepted October 26, 2009; Published November 26, 2009 Copyright: 2009 Pinto et al. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. Funding: This study was funded in part by The National Institutes of Health (NEI R01EY014970), The McKnight Endowment for Neuroscience, Dr. Gerald Burnett and Marjorie Burnett, and The Rowland Institute of Harvard. Hardware support was generously provided by the NVIDIA Corporation. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. Competing Interests: The authors have declared that no competing interests exist. * E-mail: pinto@mit.edu (NP); cox@rowland.harvard.edu (DDC)"
            },
            "slug": "A-high-throughput-screening-approach-to-discovering-Pinto-Doukhan",
            "title": {
                "fragments": [],
                "text": "A high-throughput screening approach to discovering good forms of inspired visual representation"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is argued that this approach has the potential to greatly accelerate progress in both artificial vision and the authors' understanding of the computational underpinning of biological vision, and can yield significant, reproducible gains in performance across an array of basic object recognition tasks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2042941"
                        ],
                        "name": "D. Cox",
                        "slug": "D.-Cox",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Cox",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30017846"
                        ],
                        "name": "N. Pinto",
                        "slug": "N.-Pinto",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Pinto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Pinto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3250342"
                        ],
                        "name": "D. Doukhan",
                        "slug": "D.-Doukhan",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Doukhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Doukhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2079955555"
                        ],
                        "name": "B. Corda",
                        "slug": "B.-Corda",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Corda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Corda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1865831"
                        ],
                        "name": "J. DiCarlo",
                        "slug": "J.-DiCarlo",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "DiCarlo",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. DiCarlo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 30996407,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "55fb23d8e3280298df8e275c9f1ea57869047739",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The experimental study of biological vision and the creation of artificial vision systems are naturally intertwined \u2013 while exploration of the neuronal substrates of visual processing provides clues and inspiration for artificial systems, artificial systems can in turn serve as important generators of new ideas and working hypotheses. However, while systems neuroscience has so far provided inspiration and constraints for some of the \"broad-stroke\" properties of the visual system (e.g. hierarchical organization, synaptic integration of inputs and threshold, normalization, plasticity, etc.), much is still unknown. Even for those qualitative properties that most biological-inspired models hold in common, experimental data currently provide little constraint on their key parameters. Consequently, it is difficult to truly evaluate a set of computational ideas, since the performance of any one model depends strongly on its particular instantiation \u2013 e.g. the size of the pooling kernels, the number of units per layer, exponents in normalization operations, etc. Since the number of such parameters (explicit or implicit) is large, and the typical computational cost of evaluating one particular model is high, the space of possible model instantiations usually goes largely unexplored. Compounding the problem, even if a set of computational ideas are on the right track, the instantiated \"scale\" of those ideas is typically small (e.g. in terms of dimensionality and amount of training experience provided). Thus, when a model fails to approach the abilities of the visual system, we are left uncertain whether this failure is because we are missing a fundamental idea, or because the correct \"parts\" have not been tuned correctly, assembled at sufficient scale, or provided with sufficient experience."
            },
            "slug": "A-High-Throughput-Screening-Approach-to-Discovering-Cox-Pinto",
            "title": {
                "fragments": [],
                "text": "A High-Throughput Screening Approach to Discovering Good Forms of Visual Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The experimental study of biological vision and the creation of artificial vision systems are naturally intertwined \u2013 while exploration of the neuronal substrates of visual processing provides clues and inspiration for artificial systems, artificial systems can in turn serve as important generators of new ideas and working hypotheses."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1981539"
                        ],
                        "name": "Thomas Serre",
                        "slug": "Thomas-Serre",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Serre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Serre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145128145"
                        ],
                        "name": "Lior Wolf",
                        "slug": "Lior-Wolf",
                        "structuredName": {
                            "firstName": "Lior",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lior Wolf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747918"
                        ],
                        "name": "S. Bileschi",
                        "slug": "S.-Bileschi",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Bileschi",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bileschi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1996960"
                        ],
                        "name": "M. Riesenhuber",
                        "slug": "M.-Riesenhuber",
                        "structuredName": {
                            "firstName": "Maximilian",
                            "lastName": "Riesenhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Riesenhuber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 109
                            }
                        ],
                        "text": "Of note, the nearest contender from the set of state-of-the-art models is another biologicallyinspired model [7,8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2179592,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71e3d9fc53ba14c2feeb7390f0dc99076553b05a",
            "isKey": false,
            "numCitedBy": 1714,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new general framework for the recognition of complex visual scenes, which is motivated by biology: We describe a hierarchical system that closely follows the organization of visual cortex and builds an increasingly complex and invariant feature representation by alternating between a template matching and a maximum pooling operation. We demonstrate the strength of the approach on a range of recognition tasks: From invariant single object recognition in clutter to multiclass categorization problems and complex scene understanding tasks that rely on the recognition of both shape-based as well as texture-based objects. Given the biological constraints that the system had to satisfy, the approach performs surprisingly well: It has the capability of learning from only a few training examples and competes with state-of-the-art systems. We also discuss the existence of a universal, redundant dictionary of features that could handle the recognition of most object categories. In addition to its relevance for computer vision, the success of this approach suggests a plausibility proof for a class of feedforward models of object recognition in cortex"
            },
            "slug": "Robust-Object-Recognition-with-Cortex-Like-Serre-Wolf",
            "title": {
                "fragments": [],
                "text": "Robust Object Recognition with Cortex-Like Mechanisms"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A hierarchical system that closely follows the organization of visual cortex and builds an increasingly complex and invariant feature representation by alternating between a template matching and a maximum pooling operation is described."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30017846"
                        ],
                        "name": "N. Pinto",
                        "slug": "N.-Pinto",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Pinto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Pinto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2042941"
                        ],
                        "name": "D. Cox",
                        "slug": "D.-Cox",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Cox",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1865831"
                        ],
                        "name": "J. DiCarlo",
                        "slug": "J.-DiCarlo",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "DiCarlo",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. DiCarlo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 291
                            }
                        ],
                        "text": "This recognition test has the benefit of being relatively quick to evaluate (because it only contains two classes), while at the same time having previous empirical grounding as a challenging object recognition test due to the large amount of position, scale, view, and background variation [11] (see Figure 5A)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "These same 3D models were also used in a previous study [11]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "Planes\u2019\u2019 object discrimination task [11] to assess the performance of each model, and the most promising five models from each set of 2,500 models was submitted to further analysis."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "Planes\u2019\u2019 synthetic object recognition test as a screening task (see [11] for details)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5955557,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "688b6fbc3c5c06e254961f70de9d855d3d008d09",
            "isKey": false,
            "numCitedBy": 583,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Progress in understanding the brain mechanisms underlying vision requires the construction of computational models that not only emulate the brain's anatomy and physiology, but ultimately match its performance on visual tasks. In recent years, \u201cnatural\u201d images have become popular in the study of vision and have been used to show apparently impressive progress in building such models. Here, we challenge the use of uncontrolled \u201cnatural\u201d images in guiding that progress. In particular, we show that a simple V1-like model\u2014a neuroscientist's \u201cnull\u201d model, which should perform poorly at real-world visual object recognition tasks\u2014outperforms state-of-the-art object recognition systems (biologically inspired and otherwise) on a standard, ostensibly natural image recognition test. As a counterpoint, we designed a \u201csimpler\u201d recognition test to better span the real-world variation in object pose, position, and scale, and we show that this test correctly exposes the inadequacy of the V1-like model. Taken together, these results demonstrate that tests based on uncontrolled natural images can be seriously misleading, potentially guiding progress in the wrong direction. Instead, we reexamine what it means for images to be natural and argue for a renewed focus on the core problem of object recognition\u2014real-world image variation."
            },
            "slug": "Why-is-Real-World-Visual-Object-Recognition-Hard-Pinto-Cox",
            "title": {
                "fragments": [],
                "text": "Why is Real-World Visual Object Recognition Hard?"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that a simple V1-like model\u2014a neuroscientist's \u201cnull\u201d model\u2014outperforms state-of-the-art object recognition systems (biologically inspired and otherwise) on a standard, ostensibly natural image recognition test."
            },
            "venue": {
                "fragments": [],
                "text": "PLoS Comput. Biol."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1917767"
                        ],
                        "name": "W. Einh\u00e4user",
                        "slug": "W.-Einh\u00e4user",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Einh\u00e4user",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Einh\u00e4user"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7618724"
                        ],
                        "name": "J. Hipp",
                        "slug": "J.-Hipp",
                        "structuredName": {
                            "firstName": "J\u00f6rg",
                            "lastName": "Hipp",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hipp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145203312"
                        ],
                        "name": "J. Eggert",
                        "slug": "J.-Eggert",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Eggert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Eggert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145115105"
                        ],
                        "name": "E. K\u00f6rner",
                        "slug": "E.-K\u00f6rner",
                        "structuredName": {
                            "firstName": "Edgar",
                            "lastName": "K\u00f6rner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. K\u00f6rner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40089171"
                        ],
                        "name": "P. K\u00f6nig",
                        "slug": "P.-K\u00f6nig",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "K\u00f6nig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. K\u00f6nig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1079265,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8d94f68fb8f15eaca70003d26d88c326ca0407af",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Invariant object recognition is arguably one of the major challenges for contemporary machine vision systems. In contrast, the mammalian visual system performs this task virtually effortlessly. How can we exploit our knowledge on the biological system to improve artificial systems? Our understanding of the mammalian early visual system has been augmented by the discovery that general coding principles could explain many aspects of neuronal response properties. How can such schemes be transferred to system level performance? In the present study we train cells on a particular variant of the general principle of temporal coherence, the \u201cstability\u201d objective. These cells are trained on unlabeled real-world images without a teaching signal. We show that after training, the cells form a representation that is largely independent of the viewpoint from which the stimulus is looked at. This finding includes generalization to previously unseen viewpoints. The achieved representation is better suited for view-point invariant object classification than the cells\u2019 input patterns. This property to facilitate view-point invariant classification is maintained even if training and classification take place in the presence of an \u2013 also unlabeled \u2013 distractor object. In summary, here we show that unsupervised learning using a general coding principle facilitates the classification of real-world objects, that are not segmented from the background and undergo complex, non-isomorphic, transformations."
            },
            "slug": "Learning-viewpoint-invariant-object-representations-Einh\u00e4user-Hipp",
            "title": {
                "fragments": [],
                "text": "Learning viewpoint invariant object representations using a temporal coherence principle"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is shown that unsupervised learning using a general coding principle facilitates the classification of real-world objects, that are not segmented from the background and undergo complex, non-isomorphic, transformations."
            },
            "venue": {
                "fragments": [],
                "text": "Biological Cybernetics"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2243801"
                        ],
                        "name": "Jim Mutch",
                        "slug": "Jim-Mutch",
                        "structuredName": {
                            "firstName": "Jim",
                            "lastName": "Mutch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jim Mutch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35238678"
                        ],
                        "name": "D. Lowe",
                        "slug": "D.-Lowe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lowe",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lowe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 578,
                                "start": 575
                            }
                        ],
                        "text": "For each validation set, the performance (averaged over 10 random splits; error bars represent standard error of the mean) is first plotted for V1-like and V1-like+ baseline models (see [10\u201312] for a detailed description of these two variants) (gray bars), and for five state-of-the-art vision systems (green bars): Scale Invariant Feature Transform (SIFT, [40]), Geometric Blur Descriptor (GB, [39]), Pyramidal Histogram of Gradients (PHOG, [37]), Pyramidal Histogram of Words (PHOW, [38]), and a biologically-inspired hierarchical model (\u2018\u2018Sparse Localized Features\u2019\u2019 SLF, [8])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 9
                            }
                        ],
                        "text": "[38] and [8]), we remained faithful to the authors\u2019 published descriptions and allowed this optimization, resulting in a different individually tailored model for each validation set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 109
                            }
                        ],
                        "text": "Of note, the nearest contender from the set of state-of-the-art models is another biologicallyinspired model [7,8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 525,
                                "start": 522
                            }
                        ],
                        "text": "To facilitate comparison with other models in the literature, we obtained code for, or re-implemented five \u2018\u2018state of the art\u2019\u2019 object recognition algorithms from the extant literature: \u2018\u2018Pyramid Histogram of Oriented Gradients\u2019\u2019 (PHOG) [37], \u2018\u2018Pyramid Histogram of Words\u2019\u2019 (PHOW) (also known as the Spatial Pyramid [38]), the \u2018\u2018Geometric Blur\u2019\u2019 shape descriptors [39], the descriptors from the \u2018\u2018Scale Invariant Feature Transformation\u2019\u2019 (SIFT) [40], and the \u2018\u2018Sparse Localized Features\u2019\u2019 (SLF) features of Mutch and Lowe [8] (a sparse extension of the C2 features from the Serre et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 497,
                                "start": 483
                            }
                        ],
                        "text": "To facilitate comparison with other models in the literature, we obtained code for, or re-implemented five \u2018\u2018state of the art\u2019\u2019 object recognition algorithms from the extant literature: \u2018\u2018Pyramid Histogram of Oriented Gradients\u2019\u2019 (PHOG) [37], \u2018\u2018Pyramid Histogram of Words\u2019\u2019 (PHOW) (also known as the Spatial Pyramid [38]), the \u2018\u2018Geometric Blur\u2019\u2019 shape descriptors [39], the descriptors from the \u2018\u2018Scale Invariant Feature Transformation\u2019\u2019 (SIFT) [40], and the \u2018\u2018Sparse Localized Features\u2019\u2019 (SLF) features of Mutch and Lowe [8] (a sparse extension of the C2 features from the Serre et al. HMAX model [7])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2640801,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2355a243662cb2379cc3e2949d7455c2d6510ac3",
            "isKey": true,
            "numCitedBy": 408,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the role of sparsity and localized features in a biologically-inspired model of visual object classification. As in the model of Serre, Wolf, and Poggio, we first apply Gabor filters at all positions and scales; feature complexity and position/scale invariance are then built up by alternating template matching and max pooling operations. We refine the approach in several biologically plausible ways. Sparsity is increased by constraining the number of feature inputs, lateral inhibition, and feature selection. We also demonstrate the value of retaining some position and scale information above the intermediate feature level. Our final model is competitive with current computer vision algorithms on several standard datasets, including the Caltech 101 object categories and the UIUC car localization task. The results further the case for biologically-motivated approaches to object classification."
            },
            "slug": "Object-Class-Recognition-and-Localization-Using-Mutch-Lowe",
            "title": {
                "fragments": [],
                "text": "Object Class Recognition and Localization Using Sparse Features with Limited Receptive Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This work investigates the role of sparsity and localized features in a biologically-inspired model of visual object classification and demonstrates the value of retaining some position and scale information above the intermediate feature level."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30017846"
                        ],
                        "name": "N. Pinto",
                        "slug": "N.-Pinto",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Pinto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Pinto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1865831"
                        ],
                        "name": "J. DiCarlo",
                        "slug": "J.-DiCarlo",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "DiCarlo",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. DiCarlo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2042941"
                        ],
                        "name": "D. Cox",
                        "slug": "D.-Cox",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Cox",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cox"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 101
                            }
                        ],
                        "text": "rendered boats and animals 3) a set of rendered images of two synthetic faces (one male, one female, [10,12]), and 4) a modified"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5711600,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "f5a79fd404d3417f6e10ed683460d51d391f02d4",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The Rowland Institute at Harvard, Harvard University, 100 Edwin Land Blvd.,Cambridge, MA 02142, USA,cox@rowland.harvard.eduAbstract. Progress in face recognition relies critically on the creationof test sets against which the performance of various approaches can beevaluated. A good set must capture the essential elements of what makesthe problem hard, while conforming to practical scale limitations. How-ever, these goals are often deceptively di\u000ecult to achieve. In the relatedarea of object recognition, Pinto et al. [2] demonstrated the potentialdangers of using a large, uncontrolled natural image set, showing thatan extremely rudimentary vision system (inspired by the early stages ofvisual processing in the brain) was able to perform on par with manystate-of-the-art vision systems on the popular Caltech101 object set [3].At the same time, this same rudimentary system was easily defeatedby an ostensibly \\simpler\" synthetic recognition test designed to betterspan the range of real world variation in object pose, position, scale, etc.These results suggested that image sets that look \\natural\" to human ob-servers may nonetheless fail to properly embody the problem of interest,and that care must be taken to establish baselines against which perfor-mance can be judged. Here, we repeat this approach for the \\LabeledFaces in the Wild\" (LFW) dataset [1], and for a collection of standardface recognition tests. The goal of the present work is not to competein the LFW challenge, per se, but to provide a baseline against whichthe performance of other systems can be judged. In particular, we foundthat our rudimentary \\baseline\" vision system was able to achieve \u0018 68%correct performance on the LFW challenge, substantially higher than apure \\chance\" baseline. We argue that this value might serve as a moreuseful baseline against which to evaluate absolute performance and arguethat the LFW set, while perhaps not perfect, represents an improvementover other standard face sets."
            },
            "slug": "Establishing-Good-Benchmarks-and-Baselines-for-Face-Pinto-DiCarlo",
            "title": {
                "fragments": [],
                "text": "Establishing Good Benchmarks and Baselines for Face Recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1865831"
                        ],
                        "name": "J. DiCarlo",
                        "slug": "J.-DiCarlo",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "DiCarlo",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. DiCarlo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2042941"
                        ],
                        "name": "D. Cox",
                        "slug": "D.-Cox",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Cox",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cox"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "the first place [11,19]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In analogy to high-throughput screening approaches in molecular biology and genetics, we generated and trained thousands of potential network architectures and parameter instantiations, and we \u2018\u2018screened\u2019\u2019 the visual representations produced by these models using tasks that engage the core problem of object recognition\u2013tolerance to image variation [10\u201312,18,19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11527344,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "a7886ec9d38ff28020e5e7e280ac930759a64483",
            "isKey": false,
            "numCitedBy": 784,
            "numCiting": 71,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Untangling-invariant-object-recognition-DiCarlo-Cox",
            "title": {
                "fragments": [],
                "text": "Untangling invariant object recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Trends in Cognitive Sciences"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144663088"
                        ],
                        "name": "E. Rolls",
                        "slug": "E.-Rolls",
                        "structuredName": {
                            "firstName": "Edmund",
                            "lastName": "Rolls",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Rolls"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144539170"
                        ],
                        "name": "T. Milward",
                        "slug": "T.-Milward",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Milward",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Milward"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 45
                            }
                        ],
                        "text": "\u2022 \u201cTemporal Advantage\u201d (or \u201ctrace\u201d, see also [3,4,5,6] for variants): the output score of the last-winning filter is multiplied by {1, 2 or 4} (3 choices) prior to determining which filter \u201cwins."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1845969,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb373d57b2dd98c97cc7e30d29bd050de12e54d4",
            "isKey": false,
            "numCitedBy": 255,
            "numCiting": 99,
            "paperAbstract": {
                "fragments": [],
                "text": "VisNet2 is a model to investigate some aspects of invariant visual object recognition in the primate visual system. It is a four-layer feedforward network with convergence to each part of a layer from a small region of the preceding layer, with competition between the neurons within a layer and with a trace learning rule to help it learn transform invariance. The trace rule is a modified Hebbian rule, which modifies synaptic weights according to both the current firing rates and the firing rates to recently seen stimuli. This enables neurons to learn to respond similarly to the gradually transforming inputs it receives, which over the short term are likely to be about the same object, given the statistics of normal visual inputs. First, we introduce for VisNet2 both single-neuron and multiple-neuron information-theoretic measures of its ability to respond to transformed stimuli. Second, using these measures, we show that quantitatively resetting the trace between stimuli is not necessary for good performance. Third, it is shown that the sigmoid activation functions used in VisNet2, which allow the sparseness of the representation to be controlled, allow good performance when using sparse distributed representations. Fourth, it is shown that VisNet2 operates well with medium-range lateral inhibition with a radius in the same order of size as the region of the preceding layer from which neurons receive inputs. Fifth, in an investigation of different learning rules for learning transform invariance, it is shown that VisNet2 operates better with a trace rule that incorporates in the trace only activity from the preceding presentations of a given stimulus, with no contribution to the trace from the current presentation, and that this is related to temporal difference learning."
            },
            "slug": "A-Model-of-Invariant-Object-Recognition-in-the-and-Rolls-Milward",
            "title": {
                "fragments": [],
                "text": "A Model of Invariant Object Recognition in the Visual System: Learning Rules, Activation Functions, Lateral Inhibition, and Information-Based Performance Measures"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that the sigmoid activation functions used in VisNet2, which allow the sparseness of the representation to be controlled, allow good performance when using sparse distributed representations and that this is related to temporal difference learning."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1996960"
                        ],
                        "name": "M. Riesenhuber",
                        "slug": "M.-Riesenhuber",
                        "structuredName": {
                            "firstName": "Maximilian",
                            "lastName": "Riesenhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Riesenhuber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[1\u20136]), biologically-inspired models are routinely among the highest-performing artificial vision systems in practical tests of object and face recognition [7\u201312]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8920227,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "85abadb689897997f1e37baa7b5fc6f7d497518b",
            "isKey": false,
            "numCitedBy": 3318,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual processing in cortex is classically modeled as a hierarchy of increasingly sophisticated representations, naturally extending the model of simple to complex cells of Hubel and Wiesel. Surprisingly, little quantitative modeling has been done to explore the biological feasibility of this class of models to explain aspects of higher-level visual processing such as object recognition. We describe a new hierarchical model consistent with physiological data from inferotemporal cortex that accounts for this complex visual task and makes testable predictions. The model is based on a MAX-like operation applied to inputs to certain cortical neurons that may have a general role in cortical function."
            },
            "slug": "Hierarchical-models-of-object-recognition-in-cortex-Riesenhuber-Poggio",
            "title": {
                "fragments": [],
                "text": "Hierarchical models of object recognition in cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A new hierarchical model consistent with physiological data from inferotemporal cortex that accounts for this complex visual task and makes testable predictions is described."
            },
            "venue": {
                "fragments": [],
                "text": "Nature Neuroscience"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30017846"
                        ],
                        "name": "N. Pinto",
                        "slug": "N.-Pinto",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Pinto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Pinto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1865831"
                        ],
                        "name": "J. DiCarlo",
                        "slug": "J.-DiCarlo",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "DiCarlo",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. DiCarlo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2042941"
                        ],
                        "name": "D. Cox",
                        "slug": "D.-Cox",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Cox",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cox"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 101
                            }
                        ],
                        "text": "rendered boats and animals 3) a set of rendered images of two synthetic faces (one male, one female, [10,12]), and 4) a modified"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1741129,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e9e94c3c576e63e62873cfb58ac4c772f77625ac",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years, large databases of natural images have become increasingly popular in the evaluation of face and object recognition algorithms. However, Pinto et al. previously illustrated an inherent danger in using such sets, showing that an extremely basic recognition system, built on a trivial feature set, was able to take advantage of low-level regularities in popular object and face recognition sets, performing on par with many state-of-the-art systems. Recently, several groups have raised the performance \u201cbar\u201d for these sets, using more advanced classification tools. However, it is difficult to know whether these improvements are due to progress towards solving the core computational problem, or are due to further improvements in the exploitation of low-level regularities. Here, we show that even modest optimization of the simple model introduced by Pinto et al. using modern multiple kernel learning (MKL) techniques once again yields \u201cstate-of-the-art\u201d performance levels on a standard face recognition set (\u201clabeled faces in the wild\u201d). However, at the same time, even with the inclusion of MKL techniques, systems based on these simple features still fail on a synthetic face recognition test that includes more \u201crealistic\u201d view variation by design. These results underscore the importance of building test sets focussed on capturing the central computational challenges of real-world face recognition."
            },
            "slug": "How-far-can-you-get-with-a-modern-face-recognition-Pinto-DiCarlo",
            "title": {
                "fragments": [],
                "text": "How far can you get with a modern face recognition test set using only simple features?"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown that even modest optimization of the simple model introduced by Pinto et al. using modern multiple kernel learning (MKL) techniques once again yields \u201cstate-of-the-art\u201d performance levels on a standard face recognition set (\u201clabeled faces in the wild\u201d)."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "recognition test sets exist [28\u201334], we made a deliberate choice not to use these sets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2156851,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aedb8df8f953429ec5a6df99fda5c5d71dbee4ff",
            "isKey": false,
            "numCitedBy": 2326,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-Generative-Visual-Models-from-Few-Training-Fei-Fei-Fergus",
            "title": {
                "fragments": [],
                "text": "Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories"
            },
            "venue": {
                "fragments": [],
                "text": "2004 Conference on Computer Vision and Pattern Recognition Workshop"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3160228"
                        ],
                        "name": "K. Fukushima",
                        "slug": "K.-Fukushima",
                        "structuredName": {
                            "firstName": "Kunihiko",
                            "lastName": "Fukushima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukushima"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[1\u20136]), biologically-inspired models are routinely among the highest-performing artificial vision systems in practical tests of object and face recognition [7\u201312]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206775608,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "69e68bfaadf2dccff800158749f5a50fe82d173b",
            "isKey": false,
            "numCitedBy": 3718,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by \u201clearning without a teacher\u201d, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname \u201cneocognitron\u201d. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of \u201cS-cells\u201d, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of \u201cC-cells\u201d similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any \u201cteacher\u201d during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern."
            },
            "slug": "Neocognitron:-A-self-organizing-neural-network-for-Fukushima",
            "title": {
                "fragments": [],
                "text": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A neural network model for a mechanism of visual pattern recognition that is self-organized by \u201clearning without a teacher\u201d, and acquires an ability to recognize stimulus patterns based on the geometrical similarity of their shapes without affected by their positions."
            },
            "venue": {
                "fragments": [],
                "text": "Biological Cybernetics"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47655614"
                        ],
                        "name": "G. Griffin",
                        "slug": "G.-Griffin",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Griffin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Griffin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144160673"
                        ],
                        "name": "Alex Holub",
                        "slug": "Alex-Holub",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Holub",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Holub"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118828957,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a5effa909cdeafaddbbb7855037e02f8e25d632",
            "isKey": false,
            "numCitedBy": 2545,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a challenging set of 256 object categories containing a total of 30607 images. The original Caltech-101 [1] was collected by choosing a set of object categories, downloading examples from Google Images and then manually screening out all images that did not fit the category. Caltech-256 is collected in a similar manner with several improvements: a) the number of categories is more than doubled, b) the minimum number of images in any category is increased from 31 to 80, c) artifacts due to image rotation are avoided and d) a new and larger clutter category is introduced for testing background rejection. We suggest several testing paradigms to measure classification performance, then benchmark the dataset using two simple metrics as well as a state-of-the-art spatial pyramid matching [2] algorithm. Finally we use the clutter category to train an interest detector which rejects uninformative background regions."
            },
            "slug": "Caltech-256-Object-Category-Dataset-Griffin-Holub",
            "title": {
                "fragments": [],
                "text": "Caltech-256 Object Category Dataset"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A challenging set of 256 object categories containing a total of 30607 images is introduced and the clutter category is used to train an interest detector which rejects uninformative background regions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3149655"
                        ],
                        "name": "M. Franzius",
                        "slug": "M.-Franzius",
                        "structuredName": {
                            "firstName": "Mathias",
                            "lastName": "Franzius",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Franzius"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1871065"
                        ],
                        "name": "Niko Wilbert",
                        "slug": "Niko-Wilbert",
                        "structuredName": {
                            "firstName": "Niko",
                            "lastName": "Wilbert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Niko Wilbert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736245"
                        ],
                        "name": "Laurenz Wiskott",
                        "slug": "Laurenz-Wiskott",
                        "structuredName": {
                            "firstName": "Laurenz",
                            "lastName": "Wiskott",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laurenz Wiskott"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 45
                            }
                        ],
                        "text": "\u2022 \u201cTemporal Advantage\u201d (or \u201ctrace\u201d, see also [3,4,5,6] for variants): the output score of the last-winning filter is multiplied by {1, 2 or 4} (3 choices) prior to determining which filter \u201cwins."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10412866,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ccfb40ccc269c37d32e213a80957e8a49af3e5f7",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Primates are very good at recognizing objects independently of viewing angle or retinal position and outperform existing computer vision systems by far. But invariant object recognition is only one prerequisite for successful interaction with the environment. An animal also needs to assess an object's position and relative rotational angle. We propose here a model that is able to extract object identity, position, and rotation angles, where each code is independent of all others. We demonstrate the model behavior on complex three-dimensional objects under translation and in-depth rotation on homogeneous backgrounds. A similar model has previously been shown to extract hippocampal spatial codes from quasi-natural videos. The rigorous mathematical analysis of this earlier application carries over to the scenario of invariant object recognition."
            },
            "slug": "Invariant-Object-Recognition-with-Slow-Feature-Franzius-Wilbert",
            "title": {
                "fragments": [],
                "text": "Invariant Object Recognition with Slow Feature Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A model that is able to extract object identity, position, and rotation angles, where each code is independent of all others is proposed, which demonstrates the model behavior on complex three-dimensional objects under translation and in-depth rotation on homogeneous backgrounds."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49820761"
                        ],
                        "name": "S. Gerber",
                        "slug": "S.-Gerber",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Gerber",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gerber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108781466"
                        ],
                        "name": "Xinran Liu",
                        "slug": "Xinran-Liu",
                        "structuredName": {
                            "firstName": "Xinran",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinran Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5357354"
                        ],
                        "name": "I. Dulubova",
                        "slug": "I.-Dulubova",
                        "structuredName": {
                            "firstName": "Irina",
                            "lastName": "Dulubova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Dulubova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113694808"
                        ],
                        "name": "Alexander C. Meyer",
                        "slug": "Alexander-C.-Meyer",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Meyer",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander C. Meyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4164608"
                        ],
                        "name": "J. Rizo",
                        "slug": "J.-Rizo",
                        "structuredName": {
                            "firstName": "Josep",
                            "lastName": "Rizo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rizo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6257439"
                        ],
                        "name": "Marife Arancillo",
                        "slug": "Marife-Arancillo",
                        "structuredName": {
                            "firstName": "Marife",
                            "lastName": "Arancillo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marife Arancillo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "90422041"
                        ],
                        "name": "R. Hammer",
                        "slug": "R.-Hammer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Hammer",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hammer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3955061"
                        ],
                        "name": "Christian Rosenmund",
                        "slug": "Christian-Rosenmund",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Rosenmund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Rosenmund"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 55
                            }
                        ],
                        "text": "Building upon recent findings from visual neuroscience [18,23,24], unsupervised learning could also be biased by temporal factors, such that filters that \u2018\u2018won\u2019\u2019 in previous frames were biased to win again (see Supplemental Text S1 for details)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9993868,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "944a221e5cdc27501ed865bb002fc53d47a91a5d",
            "isKey": false,
            "numCitedBy": 201,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "cells with constructs encoding both FBXW7 and HA-ubiquitin, and found that ubiquitination of mTORwas restored by exogenousFBXW7 expression (Fig. 2E). Thus, ubiquitination of mTOR is largely, if not exclusively, mediated by binding to FBXW7. As FBXW7 and PTEN both affect signaling through mTOR, we examined the genetic status of both genes in a panel of 53 breast cancer cell lines (11). Quantitative TaqMan real-time polymerase chain reaction (PCR) assays of the number of copies of FBXW7 and PTEN genes in each of the cell lines were in good concordance with data found by bacterial artificial chromosome (BAC) comparative genomic hybridization (CGH) microarray (see table S1). Most of the breast cancer cell lines that exhibited loss of a single copy of FBXW7 (23 out of 53, Fig. 3A) did not show corresponding loss of PTEN. In contrast, of the 14 lines that showed loss of a single copy of PTEN (Fig. 3A), only one had also lost a copy ofFBXW7, which suggested that FBXW7 and PTEN show some functional redundancy in tumor development. Similar results were obtained by examination of the copy number status of genomic regions containing FBXW7 and PTEN genes in three independent human primary breast cancer sets for which BACCGHmicroarray data were available (12\u201314). From a total of 450 tumor and cell line DNA samples shown in Fig. 3, A to D, only 4 had lost a copies of the regions containing both genes, a result that is unlikely to be a consequence of random genetic alterations (P = 4.9 \u00d7 10). We also considered the possibility that other somatic changes such as point mutations or genesilencing events could affect the results. TheFBXW7 gene continued to be expressed in all 25 breast cancer cell lines examined (fig. S8), which indicated that no gene silencing had occurred, although very low levels were found in five cell lines [lanes 10, 13, 14, 16, and 20 (fig. S8)]. All of these lines had lost one copy of the FBXW7 gene except one (SUM149PT, lane 16), in which a point mutation was detected (table S1). The PTEN gene was found to be silent in two cell lines (fig. S8, lanes 11 and 12), and both had lost one copy of the PTEN gene. Three mutations in PTEN were found (fig. S8 and table S1). Thus, gene silencing (for example, by promoter methylation) or point mutations in FBXW7 and PTEN are relatively rare mechanisms of inactivation of these genes, in comparison with single-copy deletions. These data are further compatible with the identification of both genes as haplo-insufficient tumor suppressors (3, 15, 16). Because deletion or mutation of FBXW7 in human breast cancer cells leads to increased levels of mTOR, we tested the possibility that cells harboring these deletions may show increased sensitivity to themTOR inhibitor rapamycin.We treated twobreast cancer cell lines,SUM149PTcells (homozygous FBXW7 mutations) and MDA-MB453 cells (wild-type FBXW7) with rapamycin and counted numbers of viable cells. SUM149PTcells proved to be very sensitive to this treatment [median inhibitory concentration (IC50) < 200 nM], whereas MDA-MB453 cells were relatively resistant (IC50 > 2 mM) (Fig. 4A). In nude mouse xenografts, groups of fivemice were injected with both cell lines, one on each flank, and were treated by intraperitoneal injection with rapamycin over an 11-day period. The SUM149PT cells showed a relative decrease in size followed by stable tumor growth, whereas the MDA-MB453 cells were relatively unaffected by treatment (Fig. 4B). An additional set of 10 breast cancer cell lines was treated with rapamycin at concentrations of 200 and 400 nM. Cells with deletion or mutation of FBXW7 (HBL100, 600MPE, SUM149PT, HCC3153, and HCC1143) or PTEN (HCC1937 and HCC3153) showed significant sensitivity to killing by rapamycin, although the magnitude of the effect varied (17) (Fig. 4C). To establish a direct link between loss of FBXW7 and rapamycin sensitivity, we down-regulated expression levels of FBXW7 using short hairpin RNA (shRNA) (18) in the rapamycin-resistantMDA-MB453 cells, which resulted in an increase in sensitivity to this drug [IC50< 0.8mM (Fig. 4D)]. Our findings implicate FBXW7 in an evolutionarily conserved pathway that controls regulation of mTOR protein levels. Because FBXW7 is a haploinsufficient tumor suppressor that undergoes heterozygous loss in a substantial proportion of human tumors, the data suggest new approaches to reducemTOR levels in cancers by the use of drugs thatmay reactivate the remaining copy of FBXW7 in a similarway that nutlins (small-moleculeMDM2antagonists) have been shown to activate wild-type copiesof p53 inhuman tumors (19).LossofFBXW7 may also be a useful biomarker for sensitivity of human tumors to inhibitors of themTORpathway. References and Notes 1. M. Welcker, B. E. Clurman, Nat. Rev. Cancer 8, 83 (2008). 2. S. Akhoondi et al., Cancer Res. 67, 9006 (2007). 3. J. H. Mao et al., Nature 432, 775 (2004). 4. H. Rajagopalan et al., Nature 428, 77 (2004). 5. M. Yada et al., EMBO J. 23, 2116 (2004). 6. M. Welcker et al., Proc. Natl. Acad. Sci. U.S.A. 101, 9085 (2004). 7. Z. Kemp et al., Cancer Res. 65, 11361 (2005). 8. Single-letter abbreviations for the amino acid residues are as follows: A, Ala; C, Cys; D, Asp; E, Glu; F, Phe; G, Gly; H, His; I, Ile; K, Lys; L, Leu; M, Met; N, Asn; P, Pro; Q, Gln; R, Arg; S, Ser; T, Thr; V, Val; W, Trp; Y, Tyr; and X, any amino acid. 9. G. Wu et al., Mol. Cell. Biol. 21, 7403 (2001). 10. H. Strohmaier et al., Nature 413, 316 (2001). 11. R. M. Neve et al., Cancer Cell 10, 515 (2006). 12. J. Climent et al., Cancer Res. 67, 818 (2007). 13. K. Chin et al., Cancer Cell 10, 529 (2006). 14. J. Fridlyand et al., BMC Cancer 6, 96 (2006). 15. J. H. Mao et al., Oncogene 22, 8379 (2003). 16. A. Di Cristofano, B. Pesce, C. Cordon-Cardo, P. P. Pandolfi, Nat. Genet. 19, 348 (1998). 17. L. S. Steelman et al., Oncogene 27, 4086 (2008). 18. M. Welcker, A. Orian, J. E. Grim, R. N. Eisenman, B. E. Clurman, Curr. Biol. 14, 1852 (2004). 19. J. K. Buolamwini et al., Curr. Cancer Drug Targets 5, 57 (2005). 20. We thank B. Vogelstein for providing us with the HCT116 WT, HCT116 FBXW7, DLD1 wild-type, and DLD1 FBXW7 cell lines; K.I. Nakayama for providing Fbxw7 knockout mice and vectors (HA-FBXW7 and HA-FBXW7DF); and O. Tetsu for vector encoding HA-ubiquitin. These studies were supported by NCI grant U01 CA84244 and the U.S. Department of Energy (DE-FG02-03ER63630) to A.B., the University of California at San Francisco Research-Evaluation Allocation Committee (REAC) to J.-H.M. A.B. acknowledges support from the Barbara Bass Bakar Chair of Cancer Genetics."
            },
            "slug": "Unsupervised-Natural-Experience-Rapidly-Alters-in-Gerber-Liu",
            "title": {
                "fragments": [],
                "text": "Unsupervised Natural Experience Rapidly Alters Invariant Object Representation in Visual Cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "It was found that ubiquitination of mTOR was restored by exogenousFBXW7 expression, and the identification of both genes as haplo-insufficient tumor suppressors as well as by examination of the copy number status of genomic regions containing FBXW 7 and PTEN genes."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13919023"
                        ],
                        "name": "F. Huang",
                        "slug": "F.-Huang",
                        "structuredName": {
                            "firstName": "Fu",
                            "lastName": "Huang",
                            "middleNames": [
                                "Jie"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[1\u20134,7,9])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[1\u20136]), biologically-inspired models are routinely among the highest-performing artificial vision systems in practical tests of object and face recognition [7\u201312]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 712708,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f354310098e09c1e1dc88758fca36767fd9d084d",
            "isKey": false,
            "numCitedBy": 1306,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We assess the applicability of several popular learning methods for the problem of recognizing generic visual categories with invariance to pose, lighting, and surrounding clutter. A large dataset comprising stereo image pairs of 50 uniform-colored toys under 36 azimuths, 9 elevations, and 6 lighting conditions was collected (for a total of 194,400 individual images). The objects were 10 instances of 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. Five instances of each category were used for training, and the other five for testing. Low-resolution grayscale images of the objects with various amounts of variability and surrounding clutter were used for training and testing. Nearest neighbor methods, support vector machines, and convolutional networks, operating on raw pixels or on PCA-derived features were tested. Test error rates for unseen object instances placed on uniform backgrounds were around 13% for SVM and 7% for convolutional nets. On a segmentation/recognition task with highly cluttered images, SVM proved impractical, while convolutional nets yielded 16/7% error. A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second."
            },
            "slug": "Learning-methods-for-generic-object-recognition-to-LeCun-Huang",
            "title": {
                "fragments": [],
                "text": "Learning methods for generic object recognition with invariance to pose and lighting"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second and proved impractical, while convolutional nets yielded 16/7% error."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144423856"
                        ],
                        "name": "Anna Bosch",
                        "slug": "Anna-Bosch",
                        "structuredName": {
                            "firstName": "Anna",
                            "lastName": "Bosch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anna Bosch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2062941326"
                        ],
                        "name": "X. Mu\u00f1oz",
                        "slug": "X.-Mu\u00f1oz",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Mu\u00f1oz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Mu\u00f1oz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7211257,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b98c68f01d84ac07dc7fc51af782018070da748f",
            "isKey": false,
            "numCitedBy": 1496,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "The objective of this paper is classifying images by the object categories they contain, for example motorbikes or dolphins. There are three areas of novelty. First, we introduce a descriptor that represents local image shape and its spatial layout, together with a spatial pyramid kernel. These are designed so that the shape correspondence between two images can be measured by the distance between their descriptors using the kernel. Second, we generalize the spatial pyramid kernel, and learn its level weighting parameters (on a validation set). This significantly improves classification performance. Third, we show that shape and appearance kernels may be combined (again by learning parameters on a validation set).\n Results are reported for classification on Caltech-101 and retrieval on the TRECVID 2006 data sets. For Caltech-101 it is shown that the class specific optimization that we introduce exceeds the state of the art performance by more than 10%."
            },
            "slug": "Representing-shape-with-a-spatial-pyramid-kernel-Bosch-Zisserman",
            "title": {
                "fragments": [],
                "text": "Representing shape with a spatial pyramid kernel"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work introduces a descriptor that represents local image shape and its spatial layout, together with a spatial pyramid kernel that is designed so that the shape correspondence between two images can be measured by the distance between their descriptors using the kernel."
            },
            "venue": {
                "fragments": [],
                "text": "CIVR '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1858054"
                        ],
                        "name": "P. F\u00f6ldi\u00e1k",
                        "slug": "P.-F\u00f6ldi\u00e1k",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "F\u00f6ldi\u00e1k",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. F\u00f6ldi\u00e1k"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2175819,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2da4e9984a75ffe28c5364662807996ac5bb2662",
            "isKey": false,
            "numCitedBy": 698,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The visual system can reliably identify objects even when the retinal image is transformed considerably by commonly occurring changes in the environment. A local learning rule is proposed, which allows a network to learn to generalize across such transformations. During the learning phase, the network is exposed to temporal sequences of patterns undergoing the transformation. An application of the algorithm is presented in which the network learns invariance to shift in retinal position. Such a principle may be involved in the development of the characteristic shift invariance property of complex cells in the primary visual cortex, and also in the development of more complicated invariance properties of neurons in higher visual areas."
            },
            "slug": "Learning-Invariance-from-Transformation-Sequences-F\u00f6ldi\u00e1k",
            "title": {
                "fragments": [],
                "text": "Learning Invariance from Transformation Sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An application of the algorithm is presented in which the network learns invariance to shift in retinal position, which may be involved in the development of the characteristic shift invariance property of complex cells in the primary visual cortex and also in theDevelopment of more complicated invariance properties of neurons in higher visual areas."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Comput."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749609"
                        ],
                        "name": "S. Lazebnik",
                        "slug": "S.-Lazebnik",
                        "structuredName": {
                            "firstName": "Svetlana",
                            "lastName": "Lazebnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lazebnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189388"
                        ],
                        "name": "J. Ponce",
                        "slug": "J.-Ponce",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Ponce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ponce"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 489,
                                "start": 485
                            }
                        ],
                        "text": "For each validation set, the performance (averaged over 10 random splits; error bars represent standard error of the mean) is first plotted for V1-like and V1-like+ baseline models (see [10\u201312] for a detailed description of these two variants) (gray bars), and for five state-of-the-art vision systems (green bars): Scale Invariant Feature Transform (SIFT, [40]), Geometric Blur Descriptor (GB, [39]), Pyramidal Histogram of Gradients (PHOG, [37]), Pyramidal Histogram of Words (PHOW, [38]), and a biologically-inspired hierarchical model (\u2018\u2018Sparse Localized Features\u2019\u2019 SLF, [8])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 320,
                                "start": 316
                            }
                        ],
                        "text": "To facilitate comparison with other models in the literature, we obtained code for, or re-implemented five \u2018\u2018state of the art\u2019\u2019 object recognition algorithms from the extant literature: \u2018\u2018Pyramid Histogram of Oriented Gradients\u2019\u2019 (PHOG) [37], \u2018\u2018Pyramid Histogram of Words\u2019\u2019 (PHOW) (also known as the Spatial Pyramid [38]), the \u2018\u2018Geometric Blur\u2019\u2019 shape descriptors [39], the descriptors from the \u2018\u2018Scale Invariant Feature Transformation\u2019\u2019 (SIFT) [40], and the \u2018\u2018Sparse Localized Features\u2019\u2019 (SLF) features of Mutch and Lowe [8] (a sparse extension of the C2 features from the Serre et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 303,
                                "start": 288
                            }
                        ],
                        "text": "To facilitate comparison with other models in the literature, we obtained code for, or re-implemented five \u2018\u2018state of the art\u2019\u2019 object recognition algorithms from the extant literature: \u2018\u2018Pyramid Histogram of Oriented Gradients\u2019\u2019 (PHOG) [37], \u2018\u2018Pyramid Histogram of Words\u2019\u2019 (PHOW) (also known as the Spatial Pyramid [38]), the \u2018\u2018Geometric Blur\u2019\u2019 shape descriptors [39], the descriptors from the \u2018\u2018Scale Invariant Feature Transformation\u2019\u2019 (SIFT) [40], and the \u2018\u2018Sparse Localized Features\u2019\u2019 (SLF) features of Mutch and Lowe [8] (a sparse extension of the C2 features from the Serre et al. HMAX model [7])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[38] and [8]), we remained faithful to the authors\u2019 published descriptions and allowed this optimization, resulting in a different individually tailored model for each validation set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2421251,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6dbaff29d3898cf60f63f5a34cb9610ebb75220c",
            "isKey": true,
            "numCitedBy": 8328,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method for recognizing scene categories based on approximate global geometric correspondence. This technique works by partitioning the image into increasingly fine sub-regions and computing histograms of local features found inside each sub-region. The resulting \"spatial pyramid\" is a simple and computationally efficient extension of an orderless bag-of-features image representation, and it shows significantly improved performance on challenging scene categorization tasks. Specifically, our proposed method exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories. The spatial pyramid framework also offers insights into the success of several recently proposed image descriptions, including Torralba\u2019s \"gist\" and Lowe\u2019s SIFT descriptors."
            },
            "slug": "Beyond-Bags-of-Features:-Spatial-Pyramid-Matching-Lazebnik-Schmid",
            "title": {
                "fragments": [],
                "text": "Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This paper presents a method for recognizing scene categories based on approximate global geometric correspondence that exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219900"
                        ],
                        "name": "Gary B. Huang",
                        "slug": "Gary-B.-Huang",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Huang",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gary B. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2985062"
                        ],
                        "name": "Marwan A. Mattar",
                        "slug": "Marwan-A.-Mattar",
                        "structuredName": {
                            "firstName": "Marwan",
                            "lastName": "Mattar",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marwan A. Mattar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685538"
                        ],
                        "name": "Tamara L. Berg",
                        "slug": "Tamara-L.-Berg",
                        "structuredName": {
                            "firstName": "Tamara",
                            "lastName": "Berg",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tamara L. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404579703"
                        ],
                        "name": "Eric Learned-Miller",
                        "slug": "Eric-Learned-Miller",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Learned-Miller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Learned-Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "recognition test sets exist [28\u201334], we made a deliberate choice not to use these sets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 88166,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6b3ca4f939e36a9679a70e14ce8b1bbbc5618f3",
            "isKey": false,
            "numCitedBy": 4899,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Most face databases have been created under controlled conditions to facilitate the study of specific parameters on the face recognition problem. These parameters include such variables as position, pose, lighting, background, camera quality, and gender. While there are many applications for face recognition technology in which one can control the parameters of image acquisition, there are also many applications in which the practitioner has little or no control over such parameters. This database, Labeled Faces in the Wild, is provided as an aid in studying the latter, unconstrained, recognition problem. The database contains labeled face photographs spanning the range of conditions typically encountered in everyday life. The database exhibits \u201cnatural\u201d variability in factors such as pose, lighting, race, accessories, occlusions, and background. In addition to describing the details of the database, we provide specific experimental paradigms for which the database is suitable. This is done in an effort to make research performed with the database as consistent and comparable as possible. We provide baseline results, including results of a state of the art face recognition system combined with a face alignment system. To facilitate experimentation on the database, we provide several parallel databases, including an aligned version."
            },
            "slug": "Labeled-Faces-in-the-Wild:-A-Database-forStudying-Huang-Mattar",
            "title": {
                "fragments": [],
                "text": "Labeled Faces in the Wild: A Database forStudying Face Recognition in Unconstrained Environments"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The database contains labeled face photographs spanning the range of conditions typically encountered in everyday life, and exhibits \u201cnatural\u201d variability in factors such as pose, lighting, race, accessories, occlusions, and background."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2042941"
                        ],
                        "name": "D. Cox",
                        "slug": "D.-Cox",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Cox",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144291690"
                        ],
                        "name": "Philip Meier",
                        "slug": "Philip-Meier",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Meier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip Meier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4692478"
                        ],
                        "name": "Nadja Oertelt",
                        "slug": "Nadja-Oertelt",
                        "structuredName": {
                            "firstName": "Nadja",
                            "lastName": "Oertelt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nadja Oertelt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1865831"
                        ],
                        "name": "J. DiCarlo",
                        "slug": "J.-DiCarlo",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "DiCarlo",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. DiCarlo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 55
                            }
                        ],
                        "text": "Building upon recent findings from visual neuroscience [18,23,24], unsupervised learning could also be biased by temporal factors, such that filters that \u2018\u2018won\u2019\u2019 in previous frames were biased to win again (see Supplemental Text S1 for details)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1291179,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "57a1857f4a066fd433e4790c5aca411769f0b401",
            "isKey": false,
            "numCitedBy": 159,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "While it is often assumed that objects can be recognized irrespective of where they fall on the retina, little is known about the mechanisms underlying this ability. By exposing human subjects to an altered world where some objects systematically changed identity during the transient blindness that accompanies eye movements, we induced predictable object confusions across retinal positions, effectively 'breaking' position invariance. Thus, position invariance is not a rigid property of vision but is constantly adapting to the statistics of the environment."
            },
            "slug": "'Breaking'-position-invariant-object-recognition-Cox-Meier",
            "title": {
                "fragments": [],
                "text": "'Breaking' position-invariant object recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "By exposing human subjects to an altered world where some objects systematically changed identity during the transient blindness that accompanies eye movements, this work induced predictable object confusions across retinal positions, effectively 'breaking' position invariance."
            },
            "venue": {
                "fragments": [],
                "text": "Nature Neuroscience"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145833095"
                        ],
                        "name": "S. Kothari",
                        "slug": "S.-Kothari",
                        "structuredName": {
                            "firstName": "Suresh",
                            "lastName": "Kothari",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kothari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681982"
                        ],
                        "name": "H. Oh",
                        "slug": "H.-Oh",
                        "structuredName": {
                            "firstName": "Heekuck",
                            "lastName": "Oh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Oh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[1\u20136]), biologically-inspired models are routinely among the highest-performing artificial vision systems in practical tests of object and face recognition [7\u201312]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 177751,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbc0a468ab103ae29717703d4aa9f682f6a2b664",
            "isKey": false,
            "numCitedBy": 15339,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-Networks-for-Pattern-Recognition-Kothari-Oh",
            "title": {
                "fragments": [],
                "text": "Neural Networks for Pattern Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Adv. Comput."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189388"
                        ],
                        "name": "J. Ponce",
                        "slug": "J.-Ponce",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Ponce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ponce"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685538"
                        ],
                        "name": "Tamara L. Berg",
                        "slug": "Tamara-L.-Berg",
                        "structuredName": {
                            "firstName": "Tamara",
                            "lastName": "Berg",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tamara L. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144016256"
                        ],
                        "name": "D. Forsyth",
                        "slug": "D.-Forsyth",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Forsyth",
                            "middleNames": [
                                "Alexander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Forsyth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749609"
                        ],
                        "name": "S. Lazebnik",
                        "slug": "S.-Lazebnik",
                        "structuredName": {
                            "firstName": "Svetlana",
                            "lastName": "Lazebnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lazebnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3502855"
                        ],
                        "name": "Marcin Marszalek",
                        "slug": "Marcin-Marszalek",
                        "structuredName": {
                            "firstName": "Marcin",
                            "lastName": "Marszalek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcin Marszalek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145160921"
                        ],
                        "name": "Bryan C. Russell",
                        "slug": "Bryan-C.-Russell",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Russell",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan C. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2875887"
                        ],
                        "name": "Jianguo Zhang",
                        "slug": "Jianguo-Zhang",
                        "structuredName": {
                            "firstName": "Jianguo",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianguo Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Previous investigations [10\u201312,35,36] have raised"
                    },
                    "intents": []
                }
            ],
            "corpusId": 2451202,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e1eca4ee67c65624a2949f33cfc7ccb4c14b0a15",
            "isKey": false,
            "numCitedBy": 240,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Appropriate datasets are required at all stages of object recognition research, including learning visual models of object and scene categories, detecting and localizing instances of these models in images, and evaluating the performance of recognition algorithms. Current datasets are lacking in several respects, and this paper discusses some of the lessons learned from existing efforts, as well as innovative ways to obtain very large and diverse annotated datasets. It also suggests a few criteria for gathering future datasets."
            },
            "slug": "Dataset-Issues-in-Object-Recognition-Ponce-Berg",
            "title": {
                "fragments": [],
                "text": "Dataset Issues in Object Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Current datasets are lacking in several respects, and this paper discusses some of the lessons learned from existing efforts, as well as innovative ways to obtain very large and diverse annotated datasets."
            },
            "venue": {
                "fragments": [],
                "text": "Toward Category-Level Object Recognition"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2966196"
                        ],
                        "name": "W. Geisler",
                        "slug": "W.-Geisler",
                        "structuredName": {
                            "firstName": "Wilson",
                            "lastName": "Geisler",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Geisler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2129244"
                        ],
                        "name": "D. G. Albrecht",
                        "slug": "D.-G.-Albrecht",
                        "structuredName": {
                            "firstName": "Duane",
                            "lastName": "Albrecht",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. G. Albrecht"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 68
                            }
                        ],
                        "text": "contrast gain control mechanisms in cortical area V1, and elsewhere [1, 2])"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18857682,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "83e50d7a7b395d471173e4b2b13a3bbdb2373256",
            "isKey": false,
            "numCitedBy": 230,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Cortical-neurons:-Isolation-of-contrast-gain-Geisler-Albrecht",
            "title": {
                "fragments": [],
                "text": "Cortical neurons: Isolation of contrast gain control"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144663088"
                        ],
                        "name": "E. Rolls",
                        "slug": "E.-Rolls",
                        "structuredName": {
                            "firstName": "Edmund",
                            "lastName": "Rolls",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Rolls"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145333301"
                        ],
                        "name": "G. Deco",
                        "slug": "G.-Deco",
                        "structuredName": {
                            "firstName": "Gustavo",
                            "lastName": "Deco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Deco"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 68
                            }
                        ],
                        "text": "contrast gain control mechanisms in cortical area V1, and elsewhere [1, 2])"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 47074,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "3a2b6d523914aba26a46f6f5d5a1f976304793c9",
            "isKey": false,
            "numCitedBy": 538,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "PREFACE 1. Introduction 2. The primary visual cortex 3. Extrastriate visual areas 4. The parietal cortex 5. Inferior temporal cortical visual areas 6. Visual attentional mechanisms 7. Neural network models 8. Models of invariant object recognition 9. The cortical neurodynamics of visual attention - a model 10. Visual search: Attentional neurodynamics at work 11. A computational approach to the neuropsychology of visual attention 12. Outputs of visual processing 13. Principles and conclusions INTRODUCTION TO LINEAR ALGEBRA FOR NEURAL NETWORKS INFORMATION THEORY References Index"
            },
            "slug": "Computational-neuroscience-of-vision-Rolls-Deco",
            "title": {
                "fragments": [],
                "text": "Computational neuroscience of vision"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A computational approach to the neuropsychology of visual attention using neural network models and models of invariant object recognition as a model for this purpose is suggested."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758404"
                        ],
                        "name": "John Douglas Owens",
                        "slug": "John-Douglas-Owens",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Owens",
                            "middleNames": [
                                "Douglas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Douglas Owens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10127169"
                        ],
                        "name": "D. Luebke",
                        "slug": "D.-Luebke",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Luebke",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Luebke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3050319"
                        ],
                        "name": "N. Govindaraju",
                        "slug": "N.-Govindaraju",
                        "structuredName": {
                            "firstName": "Naga",
                            "lastName": "Govindaraju",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Govindaraju"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143937132"
                        ],
                        "name": "Mark J. Harris",
                        "slug": "Mark-J.-Harris",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Harris",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark J. Harris"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33238637"
                        ],
                        "name": "J. Kr\u00fcger",
                        "slug": "J.-Kr\u00fcger",
                        "structuredName": {
                            "firstName": "Jens",
                            "lastName": "Kr\u00fcger",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kr\u00fcger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3329508"
                        ],
                        "name": "A. Lefohn",
                        "slug": "A.-Lefohn",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Lefohn",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lefohn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34748729"
                        ],
                        "name": "Timothy J. Purcell",
                        "slug": "Timothy-J.-Purcell",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Purcell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timothy J. Purcell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 96
                            }
                        ],
                        "text": "In the present work, we take advantage of these recent advances in graphics processing hardware [16,17] to more expansively explore the range of biologically-inspired models\u2013including models of larger, more realistic scale."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15634829,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2bbf413f36f366fa73da4dc028a32131b5d205d6",
            "isKey": false,
            "numCitedBy": 1945,
            "numCiting": 396,
            "paperAbstract": {
                "fragments": [],
                "text": "The rapid increase in the performance of graphics hardware, coupled with recent improvements in its programmability, have made graphics hardware a compelling platform for computationally demanding tasks in a wide variety of application domains. In this report, we describe, summarize, and analyze the latest research in mapping general-purpose computation to graphics hardware. We begin with the technical motivations that underlie general-purpose computation on graphics processors (GPGPU) and describe the hardware and software developments that have led to the recent interest in this \ufffdeld. We then aim the main body of this report at two separate audiences. First, we describe the techniques used in mapping general-purpose computation to graphics hardware. We believe these techniques will be generally useful for researchers who plan to develop the next generation of GPGPU algorithms and techniques. Second, we survey and categorize the latest developments in general-purpose application development on graphics hardware."
            },
            "slug": "A-Survey-of-General-Purpose-Computation-on-Graphics-Owens-Luebke",
            "title": {
                "fragments": [],
                "text": "A Survey of General-Purpose Computation on Graphics Hardware"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The techniques used in mapping general-purpose computation to graphics hardware will be generally useful for researchers who plan to develop the next generation of GPGPU algorithms and techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Eurographics"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758404"
                        ],
                        "name": "John Douglas Owens",
                        "slug": "John-Douglas-Owens",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Owens",
                            "middleNames": [
                                "Douglas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Douglas Owens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21387495"
                        ],
                        "name": "M. Houston",
                        "slug": "M.-Houston",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Houston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Houston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10127169"
                        ],
                        "name": "D. Luebke",
                        "slug": "D.-Luebke",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Luebke",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Luebke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2652072"
                        ],
                        "name": "Simon Green",
                        "slug": "Simon-Green",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Green",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Green"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3241210"
                        ],
                        "name": "J. Stone",
                        "slug": "J.-Stone",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Stone",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Stone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145837401"
                        ],
                        "name": "James C. Phillips",
                        "slug": "James-C.-Phillips",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Phillips",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James C. Phillips"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17091128,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ab565c3da1b4246589b802cfd1d6764d7a54840",
            "isKey": false,
            "numCitedBy": 1597,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "The graphics processing unit (GPU) has become an integral part of today's mainstream computing systems. Over the past six years, there has been a marked increase in the performance and capabilities of GPUs. The modern GPU is not only a powerful graphics engine but also a highly parallel programmable processor featuring peak arithmetic and memory bandwidth that substantially outpaces its CPU counterpart. The GPU's rapid increase in both programmability and capability has spawned a research community that has successfully mapped a broad range of computationally demanding, complex problems to the GPU. This effort in general-purpose computing on the GPU, also known as GPU computing, has positioned the GPU as a compelling alternative to traditional microprocessors in high-performance computer systems of the future. We describe the background, hardware, and programming model for GPU computing, summarize the state of the art in tools and techniques, and present four GPU computing successes in game physics and computational biophysics that deliver order-of-magnitude performance gains over optimized CPU applications."
            },
            "slug": "GPU-Computing-Owens-Houston",
            "title": {
                "fragments": [],
                "text": "GPU Computing"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The background, hardware, and programming model for GPU computing is described, the state of the art in tools and techniques are summarized, and four GPU computing successes in game physics and computational biophysics that deliver order-of-magnitude performance gains over optimized CPU applications are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2542217"
                        ],
                        "name": "H. Yao",
                        "slug": "H.-Yao",
                        "structuredName": {
                            "firstName": "Haishan",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1909068"
                        ],
                        "name": "Y. Dan",
                        "slug": "Y.-Dan",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Dan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Dan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 55
                            }
                        ],
                        "text": "Building upon recent findings from visual neuroscience [18,23,24], unsupervised learning could also be biased by temporal factors, such that filters that \u2018\u2018won\u2019\u2019 in previous frames were biased to win again (see Supplemental Text S1 for details)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6163092,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "1ff82813c757172d0ae3604e1bc2d82ea4d38744",
            "isKey": false,
            "numCitedBy": 251,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Stimulus-Timing-Dependent-Plasticity-in-Cortical-of-Yao-Dan",
            "title": {
                "fragments": [],
                "text": "Stimulus Timing-Dependent Plasticity in Cortical Processing of Orientation"
            },
            "venue": {
                "fragments": [],
                "text": "Neuron"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789860"
                        ],
                        "name": "L. Shamir",
                        "slug": "L.-Shamir",
                        "structuredName": {
                            "firstName": "Lior",
                            "lastName": "Shamir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Shamir"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Previous investigations [10\u201312,35,36] have raised"
                    },
                    "intents": []
                }
            ],
            "corpusId": 6960275,
            "fieldsOfStudy": [
                "Computer Science",
                "Environmental Science"
            ],
            "id": "b313751548018e4ecd5ae2ce6b3b94fbd9cae33e",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Face datasets are considered a primary tool for evaluating the efficacy of face recognition methods. Here we show that in many of the commonly used face datasets, face images can be recognized accurately at a rate significantly higher than random even when no face, hair or clothes features appear in the image. The experiments were done by cutting a small background area from each face image, so that each face dataset provided a new image dataset which included only seemingly blank images. Then, an image classification method was used in order to check the classification accuracy. Experimental results show that the classification accuracy ranged between 13.5% (color FERET) to 99% (YaleB). These results indicate that the performance of face recognition methods measured using face image datasets may be biased. Compilable source code used for this experiment is freely available for download via the Internet."
            },
            "slug": "Evaluation-of-Face-Datasets-as-Tools-for-Assessing-Shamir",
            "title": {
                "fragments": [],
                "text": "Evaluation of Face Datasets as Tools for Assessing the\u00a0Performance of Face Recognition Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "It is shown that in many of the commonly used face datasets, face images can be recognized accurately at a rate significantly higher than random even when no face, hair or clothes features appear in the image."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12650942,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "af220e04f193ed2a44e89e2cfd45a4e28ab35a52",
            "isKey": false,
            "numCitedBy": 348,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of finding point correspondences in images by way of an approach to template matching that is robust under affine distortions. This is achieved by applying \"geometric blur\" to both the template and the image, resulting in a fall-off in similarity that is close to linear in the norm of the distortion between the template and the image. Results in wide baseline stereo correspondence, face detection, and feature correspondence are included."
            },
            "slug": "Geometric-blur-for-template-matching-Berg-Malik",
            "title": {
                "fragments": [],
                "text": "Geometric blur for template matching"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "This work addresses the problem of finding point correspondences in images by way of an approach to template matching that is robust under affine distortions by applying \"geometric blur\" to both the template and the image, resulting in a fall-off in similarity."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38958903"
                        ],
                        "name": "Ruigang Yang",
                        "slug": "Ruigang-Yang",
                        "structuredName": {
                            "firstName": "Ruigang",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ruigang Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8472871,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "72e273e18ca863f3a7d13fd7a76d660113e16f5d",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Driven by the need for interactive entertainment, modern PCs are equipped with specialized graphics processors (GPUs) for creation and display of images. These GPUs have become increasingly programmable, to the point that they now are capable of efficiently executing a significant number of computational kernels from non-graphical applications. In this introductory paper we first present a high-level overview of modern graphics hardware's architecture, then introduce several applications in scientific computing that can be efficiently accelerated by GPUs. Finally we list programming tools available for application development on GPUs."
            },
            "slug": "Scientific-Computing-on-Commodity-Graphics-Hardware-Yang",
            "title": {
                "fragments": [],
                "text": "Scientific Computing on Commodity Graphics Hardware"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This introductory paper presents a high-level overview of modern graphics hardware's architecture, then introduces several applications in scientific computing that can be efficiently accelerated by GPUs, and lists programming tools available for application development on GPUs."
            },
            "venue": {
                "fragments": [],
                "text": "CIS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2048154"
                        ],
                        "name": "R. K. Ursem",
                        "slug": "R.-K.-Ursem",
                        "structuredName": {
                            "firstName": "Rasmus",
                            "lastName": "Ursem",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. K. Ursem"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 24
                            }
                        ],
                        "text": "evolutionary algorithms [7]) could also be used."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7131045,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "faa9e1fd52f583db0f54025f244f9f4707070b46",
            "isKey": false,
            "numCitedBy": 6704,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This is a progress report describing my research during the last one and a half year, performed during part A of my Ph.D. study. The research field is multi-objective optimization using evolutionary algorithms, and the reseach has taken place in a collaboration with Aarhus Univerity, Grundfos and the Alexandra Institute. My research so far has been focused on two main areas, i) multi-objective evolutionary algorithms (MOEAs) with different variation operators, and ii) decreasing the cardinality of the resulting population of MOEAs. The outcome of the former is a comparative analysis of MOEA versions using different variation operators on a suite of test problems. The latter area has given rise to both a new branch of multiobjective optimization (MOO) called MODCO (Multi-Objective Distinct Candidates Optimization) and a new MOEA which makes it possible for the user to directly set the cardinality of the resulting set of solutions. To motivate and cover my previous and future work, the progress report is divided into three main parts:"
            },
            "slug": "Multi-objective-Optimization-using-Evolutionary-Ursem",
            "title": {
                "fragments": [],
                "text": "Multi-objective Optimization using Evolutionary Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "A new branch of multiobjective optimization (MOO) called MODCO (Multi-Objective Distinct Candidates Optimization) and a new MOEA which makes it possible for the user to directly set the cardinality of the resulting set of solutions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722400"
                        ],
                        "name": "Rob A. Rutenbar",
                        "slug": "Rob-A.-Rutenbar",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Rutenbar",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rob A. Rutenbar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "genetic algorithms [42], simulated annealing [43], and particle swarm techniques [44])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 39933695,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "bfc2588a9455f22040d2d95b70fb146e9da0c342",
            "isKey": false,
            "numCitedBy": 604,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "A brief introduction is given to the actual mechanics of simulated annealing, and a simple example from an IC layout is used to illustrate how these ideas can be applied. The complexities and tradeoffs involved in attacking a realistically complex design problem are illustrated by dissecting two very different annealing algorithms for VLSI chip floorplanning. Several current research problems aimed at determining more precisely how and why annealing algorithms work are examined. Some philosophical issues raised by the introduction of annealing are discussed.<<ETX>>"
            },
            "slug": "Simulated-annealing-algorithms:-an-overview-Rutenbar",
            "title": {
                "fragments": [],
                "text": "Simulated annealing algorithms: an overview"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A brief introduction is given to the actual mechanics of simulated annealing, and a simple example from an IC layout is used to illustrate how these ideas can be applied."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Circuits and Devices Magazine"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30366261"
                        ],
                        "name": "G. Moore",
                        "slug": "G.-Moore",
                        "structuredName": {
                            "firstName": "Gordon",
                            "lastName": "Moore",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Moore"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6519532,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9634af346c2356a8d40cbb939c49a7a7a9ba62fc",
            "isKey": false,
            "numCitedBy": 10709,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The future of integrated electronics is the future of electronics itself. The advantages of integration will bring about a proliferation of electronics, pushing this science into many new areas. Integrated circuits will lead to such wonders as home computers\u2014or at least terminals connected to a central computer\u2014automatic controls for automobiles, and personal portable communications equipment. The electronic wristwatch needs only a display to be feasible today. But the biggest potential lies in the production of large systems. In telephone communications, integrated circuits in digital filters will separate channels on multiplex equipment. Integrated circuits will also switch telephone circuits and perform data processing. Computers will be more powerful, and will be organized in completely different ways. For example, memories built of integrated electronics may be distributed throughout the machine instead of being concentrated in a central unit. In addition, the improved reliability made possible by integrated circuits will allow the construction of larger processing units. Machines similar to those in existence today will be built at lower costs and with faster turnaround."
            },
            "slug": "Cramming-More-Components-Onto-Integrated-Circuits-Moore",
            "title": {
                "fragments": [],
                "text": "Cramming More Components Onto Integrated Circuits"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "The future of integrated electronics is the future of electronics itself, and the advantages of integration will bring about a proliferation of electronics, pushing this science into many new areas."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69924093"
                        ],
                        "name": "S. Hyakin",
                        "slug": "S.-Hyakin",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Hyakin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hyakin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[1\u20136]), biologically-inspired models are routinely among the highest-performing artificial vision systems in practical tests of object and face recognition [7\u201312]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60577818,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "045310b06e8a3983a363a118cc9dcc3f292970b4",
            "isKey": false,
            "numCitedBy": 9899,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Simon Haykin Neural Networks A Comprehensive Foundation. Neural Networks A Comprehensive Foundation Simon S. Neural Networks A Comprehensive Foundation Simon S. Neural Networks A Comprehensive Foundation. Neural Networks Association for Computing Machinery. Book Review Neural Networks A Comprehensive Foundation. Neural Networks A Comprehensive Foundation Pearson. Neural networks a comprehensive foundation. Neural Networks a Comprehensive Foundation AbeBooks. Neural networks a comprehensive foundation solutions. cdn preterhuman net. Neural Networks A Comprehensive Foundation Goodreads. Neural Networks A Comprehensive Foundation Amazon it. Neural Networks A Comprehensive Foundation Amazon co uk. Neural Networks A Comprehensive Foundation 3rd Edition. Neural Networks A Comprehensive Foundation Simon. Neural Networks A Comprehensive Foundation amazon com. Neural networks a comprehensive foundation Academia edu. Neural Networks A Comprehensive Foundation Amazon. neural networks a comprehensive foundation simon haykin. Simon Haykin Neural Networks A Comprehensive Foundation. Neural Networks A comprehensive Foundation 2 ed. Simon haykin neural networks a comprehensive foundation pdf. Buy Neural Networks A Comprehensive Foundation Book. Neural networks a comprehensive foundation 2e book. Neural Networks A Comprehensive Foundation. NEURAL NETWORKS A COMPREHENSIVE FOUNDATION SIMON. Neural Networks a Comprehensive Foundation by Haykin Simon. Neural Networks A Comprehensive Foundation pdf PDF Drive. Neural Networks A Comprehensive Foundation amazon ca. Simon Haykin Neural Networks A Comprehensive Foundation. NEURAL NETWORKS A Comprehensive Foundation PDF. Neural Networks A Comprehensive Foundation pdf PDF Drive. Neural Networks A Comprehensive Foundation by Haykin. Neural Networks A Comprehensive Foundation 3rd Edition. Neural Networks A Comprehensive Foundation Simon S. Neural Networks A Comprehensive Foundation. Neural networks a comprehensive foundation Book 1994. Neural Networks A Comprehensive Foundation 2nd Edition. Neural Networks A Comprehensive Foundation S S Haykin. Neural Networks A Comprehensive Foundation International. Neural Networks A Comprehensive Foundation 2 e Pearson. Download Neural Networks A Comprehensive Foundation 2Nd. Neural Networks A comprehensive foundation Aalto"
            },
            "slug": "Neural-Networks:-A-Comprehensive-Foundation-Hyakin",
            "title": {
                "fragments": [],
                "text": "Neural Networks: A Comprehensive Foundation"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "Simon Haykin Neural Networks A Comprehensive Foundation Simon S. Haykin neural networks a comprehensive foundation pdf PDF Drive."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644050191"
                        ],
                        "name": "G. LoweDavid",
                        "slug": "G.-LoweDavid",
                        "structuredName": {
                            "firstName": "G",
                            "lastName": "LoweDavid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. LoweDavid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 361,
                                "start": 357
                            }
                        ],
                        "text": "For each validation set, the performance (averaged over 10 random splits; error bars represent standard error of the mean) is first plotted for V1-like and V1-like+ baseline models (see [10\u201312] for a detailed description of these two variants) (gray bars), and for five state-of-the-art vision systems (green bars): Scale Invariant Feature Transform (SIFT, [40]), Geometric Blur Descriptor (GB, [39]), Pyramidal Histogram of Gradients (PHOG, [37]), Pyramidal Histogram of Words (PHOW, [38]), and a biologically-inspired hierarchical model (\u2018\u2018Sparse Localized Features\u2019\u2019 SLF, [8])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 449,
                                "start": 445
                            }
                        ],
                        "text": "To facilitate comparison with other models in the literature, we obtained code for, or re-implemented five \u2018\u2018state of the art\u2019\u2019 object recognition algorithms from the extant literature: \u2018\u2018Pyramid Histogram of Oriented Gradients\u2019\u2019 (PHOG) [37], \u2018\u2018Pyramid Histogram of Words\u2019\u2019 (PHOW) (also known as the Spatial Pyramid [38]), the \u2018\u2018Geometric Blur\u2019\u2019 shape descriptors [39], the descriptors from the \u2018\u2018Scale Invariant Feature Transformation\u2019\u2019 (SIFT) [40], and the \u2018\u2018Sparse Localized Features\u2019\u2019 (SLF) features of Mutch and Lowe [8] (a sparse extension of the C2 features from the Serre et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 423,
                                "start": 419
                            }
                        ],
                        "text": "To facilitate comparison with other models in the literature, we obtained code for, or re-implemented five \u2018\u2018state of the art\u2019\u2019 object recognition algorithms from the extant literature: \u2018\u2018Pyramid Histogram of Oriented Gradients\u2019\u2019 (PHOG) [37], \u2018\u2018Pyramid Histogram of Words\u2019\u2019 (PHOW) (also known as the Spatial Pyramid [38]), the \u2018\u2018Geometric Blur\u2019\u2019 shape descriptors [39], the descriptors from the \u2018\u2018Scale Invariant Feature Transformation\u2019\u2019 (SIFT) [40], and the \u2018\u2018Sparse Localized Features\u2019\u2019 (SLF) features of Mutch and Lowe [8] (a sparse extension of the C2 features from the Serre et al. HMAX model [7])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 174065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4cab9c4b571761203ed4c3a4c5a07dd615f57a91",
            "isKey": false,
            "numCitedBy": 25505,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are ..."
            },
            "slug": "Distinctive-Image-Features-from-Scale-Invariant-LoweDavid",
            "title": {
                "fragments": [],
                "text": "Distinctive Image Features from Scale-Invariant Keypoints"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516040"
                        ],
                        "name": "Erik Lindholm",
                        "slug": "Erik-Lindholm",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Lindholm",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Erik Lindholm"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2782714"
                        ],
                        "name": "J. Nickolls",
                        "slug": "J.-Nickolls",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Nickolls",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nickolls"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3210618"
                        ],
                        "name": "S. Oberman",
                        "slug": "S.-Oberman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Oberman",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Oberman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1892041"
                        ],
                        "name": "John Montrym",
                        "slug": "John-Montrym",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Montrym",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Montrym"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 209
                            }
                        ],
                        "text": "linear filtering, local normalization) were created for both the IBM Cell Processor (PlayStation 3), and for NVIDIA graphics processing units (GPUs) using the Tesla Architecture and the CUDA programming model [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2793450,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "356869aa0ae8d598e956c7f2ae884bbf5009c98c",
            "isKey": false,
            "numCitedBy": 1411,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "To enable flexible, programmable graphics and high-performance computing, NVIDIA has developed the Tesla scalable unified graphics and parallel computing architecture. Its scalable parallel array of processors is massively multithreaded and programmable in C or via graphics APIs."
            },
            "slug": "NVIDIA-Tesla:-A-Unified-Graphics-and-Computing-Lindholm-Nickolls",
            "title": {
                "fragments": [],
                "text": "NVIDIA Tesla: A Unified Graphics and Computing Architecture"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "To enable flexible, programmable graphics and high-performance computing, NVIDIA has developed the Tesla scalable unified graphics and parallel computing architecture, which is massively multithreaded and programmable in C or via graphics APIs."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Micro"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742664"
                        ],
                        "name": "R. Poli",
                        "slug": "R.-Poli",
                        "structuredName": {
                            "firstName": "Riccardo",
                            "lastName": "Poli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Poli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143953756"
                        ],
                        "name": "J. Kennedy",
                        "slug": "J.-Kennedy",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Kennedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kennedy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787135"
                        ],
                        "name": "T. Blackwell",
                        "slug": "T.-Blackwell",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Blackwell",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Blackwell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "genetic algorithms [42], simulated annealing [43], and particle swarm techniques [44])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3114196,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d2ea871e1089efaea8feb0ce6c3b123304c5235",
            "isKey": false,
            "numCitedBy": 30416,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\nParticle swarm optimization (PSO) has undergone many changes since its introduction in 1995. As researchers have learned about the technique, they have derived new versions, developed new applications, and published theoretical studies of the effects of the various parameters and aspects of the algorithm. This paper comprises a snapshot of particle swarming from the authors\u2019 perspective, including variations in the algorithm, current and ongoing research, applications and open problems.\n"
            },
            "slug": "Particle-swarm-optimization-Poli-Kennedy",
            "title": {
                "fragments": [],
                "text": "Particle swarm optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A snapshot of particle swarming from the authors\u2019 perspective, including variations in the algorithm, current and ongoing research, applications and open problems, is included."
            },
            "venue": {
                "fragments": [],
                "text": "Swarm Intelligence"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753279"
                        ],
                        "name": "J. Kurzak",
                        "slug": "J.-Kurzak",
                        "structuredName": {
                            "firstName": "Jakub",
                            "lastName": "Kurzak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kurzak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760511"
                        ],
                        "name": "A. Buttari",
                        "slug": "A.-Buttari",
                        "structuredName": {
                            "firstName": "Alfredo",
                            "lastName": "Buttari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Buttari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741269"
                        ],
                        "name": "P. Luszczek",
                        "slug": "P.-Luszczek",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Luszczek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Luszczek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708869"
                        ],
                        "name": "J. Dongarra",
                        "slug": "J.-Dongarra",
                        "structuredName": {
                            "firstName": "Jack",
                            "lastName": "Dongarra",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dongarra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", however, recently, advances in highly-parallel graphics processing hardware (such as high-end NVIDIA graphics cards, and the PlayStation 3's IBM Cell processor) have disrupted this status quo for some classes of computational problems. In particular, this new class of modern graphics processing hardware has enabled over hundred-fold speed-ups in some of the key computations that most biologically-inspired visual models share in common. As is already occurring in other scientific fields"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14306052,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "efbec7eae8b3857ba73bad82092aa72ecb342bd4",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Is real-world gaming technology the next big thing in the more academically based high-performance computing arena? The authors put PlayStation 3 to the test."
            },
            "slug": "The-PlayStation-3-for-High-Performance-Scientific-Kurzak-Buttari",
            "title": {
                "fragments": [],
                "text": "The PlayStation 3 for High-Performance Scientific Computing"
            },
            "venue": {
                "fragments": [],
                "text": "Computing in Science & Engineering"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The CMU multi-pose, illumination, and expression (Multi-PIE) face database Technical report, Tech. rep., Robotics Institute"
            },
            "venue": {
                "fragments": [],
                "text": "Learning generative visual models from few training examples: an incremental Bayesian approach tested on 101 object categories. Computer Vision and Pattern Recognition Conference (CVPR)"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 150
                            }
                        ],
                        "text": "The raw computation required to generate, train and screen these 7,500 models was completed in approximately one week, using 23 PlayStation 3 systems [41]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A high-throughput screening approach to discovering good forms of visual representation. Computational and Systems Neuroscience (COSYNE)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "genetic algorithms [42], simulated annealing [43], and particle swarm techniques [44])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multi-Objective Optimization Using Evolutionary Algorithms. Wiley"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Baker S The CMU multi-pose, illumination, and expression (Multi-PIE) face database"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 51
                            }
                        ],
                        "text": "As is already occurring in other scientific fields [14,15], the large quantitative performance improvements offered by this new class of hardware hold the potential to effect qualitative changes in how science is done."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The PlayStation 3 for HighPerformance Scientific Computing"
            },
            "venue": {
                "fragments": [],
                "text": "Computing in Science & Engineering"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "(D) A subset of the MultiPIE face test set [27] with the faces manually removed from the background, and composited onto random image backgrounds, with additional variation in position, scale, and planar rotation added."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "subset of the standard MultiPIE face recognition test set ([27]; here"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The CMU multi-pose, illumination, and expression (Multi-PIE) face database"
            },
            "venue": {
                "fragments": [],
                "text": "Technical report,"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "GPU computing. Proceedings of the IEEE"
            },
            "venue": {
                "fragments": [],
                "text": "GPU computing. Proceedings of the IEEE"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 399,
                                "start": 395
                            }
                        ],
                        "text": "For each validation set, the performance (averaged over 10 random splits; error bars represent standard error of the mean) is first plotted for V1-like and V1-like+ baseline models (see [10\u201312] for a detailed description of these two variants) (gray bars), and for five state-of-the-art vision systems (green bars): Scale Invariant Feature Transform (SIFT, [40]), Geometric Blur Descriptor (GB, [39]), Pyramidal Histogram of Gradients (PHOG, [37]), Pyramidal Histogram of Words (PHOW, [38]), and a biologically-inspired hierarchical model (\u2018\u2018Sparse Localized Features\u2019\u2019 SLF, [8])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 368,
                                "start": 364
                            }
                        ],
                        "text": "To facilitate comparison with other models in the literature, we obtained code for, or re-implemented five \u2018\u2018state of the art\u2019\u2019 object recognition algorithms from the extant literature: \u2018\u2018Pyramid Histogram of Oriented Gradients\u2019\u2019 (PHOG) [37], \u2018\u2018Pyramid Histogram of Words\u2019\u2019 (PHOW) (also known as the Spatial Pyramid [38]), the \u2018\u2018Geometric Blur\u2019\u2019 shape descriptors [39], the descriptors from the \u2018\u2018Scale Invariant Feature Transformation\u2019\u2019 (SIFT) [40], and the \u2018\u2018Sparse Localized Features\u2019\u2019 (SLF) features of Mutch and Lowe [8] (a sparse extension of the C2 features from the Serre et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Geometric blur and template matching"
            },
            "venue": {
                "fragments": [],
                "text": "Computer Vision and Pattern Recognition Conference (CVPR)"
            },
            "year": 2001
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 19,
            "methodology": 13
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 48,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/A-High-Throughput-Screening-Approach-to-Discovering-Pinto-Doukhan/d46fd54609e09bcd135fd28750003185a5ee4125?sort=total-citations"
}