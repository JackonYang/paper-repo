{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2706258"
                        ],
                        "name": "Pranav Rajpurkar",
                        "slug": "Pranav-Rajpurkar",
                        "structuredName": {
                            "firstName": "Pranav",
                            "lastName": "Rajpurkar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pranav Rajpurkar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2151810148"
                        ],
                        "name": "Jian Zhang",
                        "slug": "Jian-Zhang",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2787620"
                        ],
                        "name": "Konstantin Lopyrev",
                        "slug": "Konstantin-Lopyrev",
                        "structuredName": {
                            "firstName": "Konstantin",
                            "lastName": "Lopyrev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Konstantin Lopyrev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145419642"
                        ],
                        "name": "Percy Liang",
                        "slug": "Percy-Liang",
                        "structuredName": {
                            "firstName": "Percy",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Percy Liang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 46
                            }
                        ],
                        "text": ", 2016), and the Logistic Regression Baseline (Rajpurkar et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 33
                            }
                        ],
                        "text": "SQuAD reading comprehension task (Rajpurkar et al., 2016), in which systems answer questions about paragraphs from Wikipedia."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 50
                            }
                        ],
                        "text": "We focus on the\nSQuAD reading comprehension task (Rajpurkar et al., 2016), in which systems answer questions about paragraphs from Wikipedia."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 54
                            }
                        ],
                        "text": "For all experiments, we measure adversarial F1 score (Rajpurkar et al., 2016) across 1000 randomly sampled examples from the SQuAD development set (the test set is not publicly available)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 19
                            }
                        ],
                        "text": "The SQuAD dataset (Rajpurkar et al., 2016) contains 107,785 human-generated reading comprehension questions about Wikipedia articles."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 584,
                                "start": 556
                            }
                        ],
                        "text": "We also validate our major findings on twelve other published models with publicly available test-time code: ReasoNet Single and Ensemble versions (Shen et al., 2017), Mnemonic\nReader Single and Ensemble versions (Hu et al., 2017), Structural Embedding of Dependency Trees (SEDT) Single and Ensemble versions (Liu et al., 2017), jNet (Zhang et al., 2017), Ruminating Reader (Gong and Bowman, 2017), MultiPerspective Context Matching (MPCM) Single version (Wang et al., 2016), RaSOR (Lee et al., 2017), Dynamic Chunk Reader (DCR) (Yu et al., 2016), and the Logistic Regression Baseline (Rajpurkar et al., 2016)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 278,
                                "start": 255
                            }
                        ],
                        "text": "\u2026in paragraph-question pairs (p, q) and outputs an answer a\u0302, the standard accuracy over a test set Dtest is simply\nAcc(f) def= 1 |Dtest| \u2211\n(p,q,a)\u2208Dtest\nv((p, q, a), f),\nwhere v is the F1 score between the true answer a and the predicted answer f(p, q) (see Rajpurkar et al. (2016) for details)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 273
                            }
                        ],
                        "text": "\u2026et al., 2017), jNet (Zhang et al., 2017), Ruminating Reader (Gong and Bowman, 2017), MultiPerspective Context Matching (MPCM) Single version (Wang et al., 2016), RaSOR (Lee et al., 2017), Dynamic Chunk Reader (DCR) (Yu et al., 2016), and the Logistic Regression Baseline (Rajpurkar et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11816014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "05dd7254b632376973f3a1b4d39485da17814df5",
            "isKey": false,
            "numCitedBy": 4267,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. \nThe dataset is freely available at this https URL"
            },
            "slug": "SQuAD:-100,000+-Questions-for-Machine-Comprehension-Rajpurkar-Zhang",
            "title": {
                "fragments": [],
                "text": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
            },
            "tldr": {
                "abstractSimilarityScore": 35,
                "text": "A strong logistic regression model is built, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%)."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2544107"
                        ],
                        "name": "Kenton Lee",
                        "slug": "Kenton-Lee",
                        "structuredName": {
                            "firstName": "Kenton",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenton Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15652489"
                        ],
                        "name": "T. Kwiatkowski",
                        "slug": "T.-Kwiatkowski",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Kwiatkowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kwiatkowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144729897"
                        ],
                        "name": "Ankur P. Parikh",
                        "slug": "Ankur-P.-Parikh",
                        "structuredName": {
                            "firstName": "Ankur",
                            "lastName": "Parikh",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ankur P. Parikh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143790066"
                        ],
                        "name": "Dipanjan Das",
                        "slug": "Dipanjan-Das",
                        "structuredName": {
                            "firstName": "Dipanjan",
                            "lastName": "Das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dipanjan Das"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 170
                            }
                        ],
                        "text": "\u2026et al., 2017), jNet (Zhang et al., 2017), Ruminating Reader (Gong and Bowman, 2017), MultiPerspective Context Matching (MPCM) Single version (Wang et al., 2016), RaSOR (Lee et al., 2017), Dynamic Chunk Reader (DCR) (Yu et al., 2016), and the Logistic Regression Baseline (Rajpurkar et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 15
                            }
                        ],
                        "text": ", 2016), RaSOR (Lee et al., 2017), Dynamic Chunk Reader (DCR) (Yu et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 481,
                                "start": 476
                            }
                        ],
                        "text": "We also validate our major findings on twelve other published models with publicly available test-time code: ReasoNet Single and Ensemble versions (Shen et al., 2017), Mnemonic\nReader Single and Ensemble versions (Hu et al., 2017), Structural Embedding of Dependency Trees (SEDT) Single and Ensemble versions (Liu et al., 2017), jNet (Zhang et al., 2017), Ruminating Reader (Gong and Bowman, 2017), MultiPerspective Context Matching (MPCM) Single version (Wang et al., 2016), RaSOR (Lee et al., 2017), Dynamic Chunk Reader (DCR) (Yu et al., 2016), and the Logistic Regression Baseline (Rajpurkar et al., 2016)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6537520,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97e6ed1f7e5de0034f71c370c01f59c87aaf9a72",
            "isKey": false,
            "numCitedBy": 139,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The reading comprehension task, that asks questions about a given evidence document, is a central problem in natural language understanding. Recent formulations of this task have typically focused on answer selection from a set of candidates pre-defined manually or through the use of an external NLP pipeline. However, Rajpurkar et al. (2016) recently released the SQUAD dataset in which the answers can be arbitrary strings from the supplied text. In this paper, we focus on this answer extraction task, presenting a novel model architecture that efficiently builds fixed length representations of all spans in the evidence document with a recurrent network. We show that scoring explicit span representations significantly improves performance over other approaches that factor the prediction into separate predictions about words or start and end markers. Our approach improves upon the best published results of Wang & Jiang (2016) by 5% and decreases the error of Rajpurkar et al.\u2019s baseline by > 50%."
            },
            "slug": "Learning-Recurrent-Span-Representations-for-Lee-Kwiatkowski",
            "title": {
                "fragments": [],
                "text": "Learning Recurrent Span Representations for Extractive Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper presents a novel model architecture that efficiently builds fixed length representations of all spans in the evidence document with a recurrent network, and shows that scoring explicit span representations significantly improves performance over other approaches that factor the prediction into separate predictions about words or start and end markers."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2992833"
                        ],
                        "name": "Shuohang Wang",
                        "slug": "Shuohang-Wang",
                        "structuredName": {
                            "firstName": "Shuohang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuohang Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144924150"
                        ],
                        "name": "Jing Jiang",
                        "slug": "Jing-Jiang",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Jiang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5592690,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff1861b71eaedba46cb679bbe2c585dbe18f9b19",
            "isKey": false,
            "numCitedBy": 541,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Machine comprehension of text is an important problem in natural language processing. A recently released dataset, the Stanford Question Answering Dataset (SQuAD), offers a large number of real questions and their answers created by humans through crowdsourcing. SQuAD provides a challenging testbed for evaluating machine comprehension algorithms, partly because compared with previous datasets, in SQuAD the answers do not come from a small set of candidate answers and they have variable lengths. We propose an end-to-end neural architecture for the task. The architecture is based on match-LSTM, a model we proposed previously for textual entailment, and Pointer Net, a sequence-to-sequence model proposed by Vinyals et al.(2015) to constrain the output tokens to be from the input sequences. We propose two ways of using Pointer Net for our task. Our experiments show that both of our two models substantially outperform the best results obtained by Rajpurkar et al.(2016) using logistic regression and manually crafted features."
            },
            "slug": "Machine-Comprehension-Using-Match-LSTM-and-Answer-Wang-Jiang",
            "title": {
                "fragments": [],
                "text": "Machine Comprehension Using Match-LSTM and Answer Pointer"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This work proposes an end-to-end neural architecture for the Stanford Question Answering Dataset (SQuAD), based on match-LSTM, a model previously proposed previously for textual entailment, and Pointer Net, a sequence- to-sequence model proposed by Vinyals et al.(2015) to constrain the output tokens to be from the input sequences."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3319373"
                        ],
                        "name": "Dirk Weissenborn",
                        "slug": "Dirk-Weissenborn",
                        "structuredName": {
                            "firstName": "Dirk",
                            "lastName": "Weissenborn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dirk Weissenborn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39344644"
                        ],
                        "name": "Georg Wiese",
                        "slug": "Georg-Wiese",
                        "structuredName": {
                            "firstName": "Georg",
                            "lastName": "Wiese",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Georg Wiese"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9704479"
                        ],
                        "name": "Laura Seiffe",
                        "slug": "Laura-Seiffe",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Seiffe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laura Seiffe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 0
                            }
                        ],
                        "text": "Weissenborn et al. (2017) argue that many SQuAD questions can be answered with heuristics based on type and keyword-matching."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2592133,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46a7afc2b23bb3406fb64c36b6f2696145b54f24",
            "isKey": false,
            "numCitedBy": 176,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent development of large-scale question answering (QA) datasets triggered a substantial amount of research into end-to-end neural architectures for QA. Increasingly complex systems have been conceived without comparison to simpler neural baseline systems that would justify their complexity. In this work, we propose a simple heuristic that guides the development of neural baseline systems for the extractive QA task. We find that there are two ingredients necessary for building a high-performing neural QA system: first, the awareness of question words while processing the context and second, a composition function that goes beyond simple bag-of-words modeling, such as recurrent neural networks. Our results show that FastQA, a system that meets these two requirements, can achieve very competitive performance compared with existing models. We argue that this surprising finding puts results of previous systems and the complexity of recent QA datasets into perspective."
            },
            "slug": "Making-Neural-QA-as-Simple-as-Possible-but-not-Weissenborn-Wiese",
            "title": {
                "fragments": [],
                "text": "Making Neural QA as Simple as Possible but not Simpler"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes a simple heuristic that guides the development of neural baseline systems for the extractive QA task and finds that there are two ingredients necessary for building a high-performing neural QA system: the awareness of question words while processing the context and a composition function that goes beyond simple bag-of-words modeling, such as recurrent neural networks."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2129425"
                        ],
                        "name": "Denis Paperno",
                        "slug": "Denis-Paperno",
                        "structuredName": {
                            "firstName": "Denis",
                            "lastName": "Paperno",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Denis Paperno"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067996"
                        ],
                        "name": "Germ\u00e1n Kruszewski",
                        "slug": "Germ\u00e1n-Kruszewski",
                        "structuredName": {
                            "firstName": "Germ\u00e1n",
                            "lastName": "Kruszewski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Germ\u00e1n Kruszewski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2672644"
                        ],
                        "name": "Angeliki Lazaridou",
                        "slug": "Angeliki-Lazaridou",
                        "structuredName": {
                            "firstName": "Angeliki",
                            "lastName": "Lazaridou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Angeliki Lazaridou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422089"
                        ],
                        "name": "Q. N. Pham",
                        "slug": "Q.-N.-Pham",
                        "structuredName": {
                            "firstName": "Quan",
                            "lastName": "Pham",
                            "middleNames": [
                                "Ngoc"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Q. N. Pham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145040726"
                        ],
                        "name": "R. Bernardi",
                        "slug": "R.-Bernardi",
                        "structuredName": {
                            "firstName": "Raffaella",
                            "lastName": "Bernardi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bernardi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422247"
                        ],
                        "name": "Sandro Pezzelle",
                        "slug": "Sandro-Pezzelle",
                        "structuredName": {
                            "firstName": "Sandro",
                            "lastName": "Pezzelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sandro Pezzelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145283199"
                        ],
                        "name": "Marco Baroni",
                        "slug": "Marco-Baroni",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Baroni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Baroni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807810"
                        ],
                        "name": "Gemma Boleda",
                        "slug": "Gemma-Boleda",
                        "structuredName": {
                            "firstName": "Gemma",
                            "lastName": "Boleda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gemma Boleda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144151273"
                        ],
                        "name": "R. Fern\u00e1ndez",
                        "slug": "R.-Fern\u00e1ndez",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Fern\u00e1ndez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fern\u00e1ndez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Paperno et al. (2016) constructed the LAMBADA dataset, which tests the ability of language models to handle long-range dependencies."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 198
                            }
                        ],
                        "text": "However, models can succeed in this paradigm by recognizing patterns that happen to be predictive on most of the test examples, while ignoring deeper, more difficult phenomena (Rimell et al., 2009; Paperno et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2381275,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "77fb0b7aef619dfac650423d4677170df2158e0d",
            "isKey": false,
            "numCitedBy": 155,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAMBADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse. We show that LAMBADA exemplifies a wide range of linguistic phenomena, and that none of several state-of-the-art language models reaches accuracy above 1% on this novel benchmark. We thus propose LAMBADA as a challenging test set, meant to encourage the development of new models capable of genuine understanding of broad context in natural language text."
            },
            "slug": "The-LAMBADA-dataset:-Word-prediction-requiring-a-Paperno-Kruszewski",
            "title": {
                "fragments": [],
                "text": "The LAMBADA dataset: Word prediction requiring a broad discourse context"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that LAMBADA exemplifies a wide range of linguistic phenomena, and that none of several state-of-the-art language models reaches accuracy above 1% on this novel benchmark."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108792993"
                        ],
                        "name": "Junbei Zhang",
                        "slug": "Junbei-Zhang",
                        "structuredName": {
                            "firstName": "Junbei",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junbei Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1854999"
                        ],
                        "name": "Xiao-Dan Zhu",
                        "slug": "Xiao-Dan-Zhu",
                        "structuredName": {
                            "firstName": "Xiao-Dan",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiao-Dan Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146380510"
                        ],
                        "name": "Qian Chen",
                        "slug": "Qian-Chen",
                        "structuredName": {
                            "firstName": "Qian",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qian Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1860774"
                        ],
                        "name": "Lirong Dai",
                        "slug": "Lirong-Dai",
                        "structuredName": {
                            "firstName": "Lirong",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lirong Dai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144572674"
                        ],
                        "name": "Si Wei",
                        "slug": "Si-Wei",
                        "structuredName": {
                            "firstName": "Si",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Si Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36357862"
                        ],
                        "name": "Hui Jiang",
                        "slug": "Hui-Jiang",
                        "structuredName": {
                            "firstName": "Hui",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hui Jiang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 14
                            }
                        ],
                        "text": ", 2017), jNet (Zhang et al., 2017), Ruminating Reader (Gong and Bowman, 2017), MultiPerspective Context Matching (MPCM) Single version (Wang et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 145
                            }
                        ],
                        "text": "\u2026and Ensemble versions (Hu et al., 2017), Structural Embedding of Dependency Trees (SEDT) Single and Ensemble versions (Liu et al., 2017), jNet (Zhang et al., 2017), Ruminating Reader (Gong and Bowman, 2017), MultiPerspective Context Matching (MPCM) Single version (Wang et al., 2016), RaSOR\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11056052,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "62f88e8fc3b44c5627f2b4721b08498d78103893",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The last several years have seen intensive interest in exploring neural-network-based models for machine comprehension (MC) and question answering (QA). In this paper, we approach the problems by closely modelling questions in a neural network framework. We first introduce syntactic information to help encode questions. We then view and model different types of questions and the information shared among them as an adaptation task and proposed adaptation models for them. On the Stanford Question Answering Dataset (SQuAD), we show that these approaches can help attain better results over a competitive baseline."
            },
            "slug": "Exploring-Question-Understanding-and-Adaptation-in-Zhang-Zhu",
            "title": {
                "fragments": [],
                "text": "Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper first introduces syntactic information to help encode questions and then view and model different types of questions and the information shared among them as an adaptation task and proposed adaptation models for them."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49298465"
                        ],
                        "name": "Jiwei Li",
                        "slug": "Jiwei-Li",
                        "structuredName": {
                            "firstName": "Jiwei",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiwei Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145768639"
                        ],
                        "name": "Will Monroe",
                        "slug": "Will-Monroe",
                        "structuredName": {
                            "firstName": "Will",
                            "lastName": "Monroe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Will Monroe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10238549"
                        ],
                        "name": "Tianlin Shi",
                        "slug": "Tianlin-Shi",
                        "structuredName": {
                            "firstName": "Tianlin",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tianlin Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152857609"
                        ],
                        "name": "S\u00e9bastien Jean",
                        "slug": "S\u00e9bastien-Jean",
                        "structuredName": {
                            "firstName": "S\u00e9bastien",
                            "lastName": "Jean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S\u00e9bastien Jean"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1863425"
                        ],
                        "name": "Alan Ritter",
                        "slug": "Alan-Ritter",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Ritter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alan Ritter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746807"
                        ],
                        "name": "Dan Jurafsky",
                        "slug": "Dan-Jurafsky",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Jurafsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan Jurafsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 25
                            }
                        ],
                        "text": "Bowman et al. (2016) and Li et al. (2017) used such a setup for sentence and dialogue generation, respectively."
                    },
                    "intents": []
                }
            ],
            "corpusId": 98180,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "176f1d608b918eec8dc4b75e7b6e0acaba84a447",
            "isKey": false,
            "numCitedBy": 741,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "We apply adversarial training to open-domain dialogue generation, training a system to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning problem where we jointly train two systems: a generative model to produce response sequences, and a discriminator\u2014analagous to the human evaluator in the Turing test\u2014 to distinguish between the human-generated dialogues and the machine-generated ones. In this generative adversarial network approach, the outputs from the discriminator are used to encourage the system towards more human-like dialogue. Further, we investigate models for adversarial evaluation that uses success in fooling an adversary as a dialogue evaluation metric, while avoiding a number of potential pitfalls. Experimental results on several metrics, including adversarial evaluation, demonstrate that the adversarially-trained system generates higher-quality responses than previous baselines"
            },
            "slug": "Adversarial-Learning-for-Neural-Dialogue-Generation-Li-Monroe",
            "title": {
                "fragments": [],
                "text": "Adversarial Learning for Neural Dialogue Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This work applies adversarial training to open-domain dialogue generation, training a system to produce sequences that are indistinguishable from human-generated dialogue utterances, and investigates models for adversarial evaluation that uses success in fooling an adversary as a dialogue evaluation metric, while avoiding a number of potential pitfalls."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8367832"
                        ],
                        "name": "Minghao Hu",
                        "slug": "Minghao-Hu",
                        "structuredName": {
                            "firstName": "Minghao",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minghao Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49236691"
                        ],
                        "name": "Yuxing Peng",
                        "slug": "Yuxing-Peng",
                        "structuredName": {
                            "firstName": "Yuxing",
                            "lastName": "Peng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuxing Peng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767521"
                        ],
                        "name": "Xipeng Qiu",
                        "slug": "Xipeng-Qiu",
                        "structuredName": {
                            "firstName": "Xipeng",
                            "lastName": "Qiu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xipeng Qiu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 54
                            }
                        ],
                        "text": ", 2017), Mnemonic Reader Single and Ensemble versions (Hu et al., 2017), Structural Embedding of Dependency Trees (SEDT) Single and Ensemble versions (Liu et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 146
                            }
                        ],
                        "text": "\u2026with publicly available test-time code: ReasoNet Single and Ensemble versions (Shen et al., 2017), Mnemonic\nReader Single and Ensemble versions (Hu et al., 2017), Structural Embedding of Dependency Trees (SEDT) Single and Ensemble versions (Liu et al., 2017), jNet (Zhang et al., 2017),\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 50
                            }
                        ],
                        "text": "It is noteworthy that the Mnemonic Reader models (Hu et al., 2017) outperform the other models by about 6 F1 points."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 195345829,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "525f65936c331b0b766c7aea0eae64c595704c50",
            "isKey": true,
            "numCitedBy": 39,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, several end-to-end neural models have been proposed for machine comprehension tasks. Typically, these models use attention mechanisms to capture the complicated interaction between the context and the query and then point the boundary of answer. To better point the correct answer, we introduce the Mnemonic Reader for machine comprehension tasks, which enhance the attention reader in two aspects. Firstly, we use a self-alignment attention to model the long-distance dependency among context words, and obtain query-aware and selfaware contextual representation for each word in the context. Second, we use a memory-based query-dependent pointer to predict the answer, which integrates both explicit and implicit query information, such as query category. Our experimental evaluations show that our model obtains the state-of-the-art result on the large-scale machine comprehension benchmarks SQuAD."
            },
            "slug": "Mnemonic-Reader-for-Machine-Comprehension-Hu-Peng",
            "title": {
                "fragments": [],
                "text": "Mnemonic Reader for Machine Comprehension"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The Mnemonic Reader is introduced for machine comprehension tasks, which uses a self-alignment attention to model the long-distance dependency among context words, and obtains query-aware and selfaware contextual representation for each word in the context."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144365875"
                        ],
                        "name": "Noah A. Smith",
                        "slug": "Noah-A.-Smith",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Smith",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah A. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 199
                            }
                        ],
                        "text": "Generative adversarial models pit a generative model, whose goal is to generate realistic outputs, against a discriminative model, whose goal is to distinguish the generator\u2019s outputs from real data (Smith, 2012; Goodfellow et al., 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1512061,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ab3aea9285204b3daa5b4abd47852f853efd1307",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We now have a rich and growing set of modeling tools and algorithms for inducing linguistic structure from text that is less than fully annotated. In this paper, we discuss some of the weaknesses of our current methodology. We present a new abstract framework for evaluating natural language processing (NLP) models in general and unsupervised NLP models in particular. The central idea is to make explicit certain adversarial roles among researchers, so that the different roles in an evaluation are more clearly defined and performers of all roles are offered ways to make measurable contributions to the larger goal. Adopting this approach may help to characterize model successes and failures by encouraging earlier consideration of error analysis. The framework can be instantiated in a variety of ways, simulating some familiar intrinsic and extrinsic evaluations as well as some new evaluations. This paper presents a new approach to evaluating computational models of natural language, based on adversarial roles performed by different researchers or their models. We begin inx2 with a brief review of current evaluation strategies in NLP. We then turn to coupled adversarial evaluations inspired by perplexity (x3) and the traditional roles of linguists (x4). The two-performer setup is formalized inx5. We then consider the origins of the data and growing awareness of the importance of context on language use (x6) and provide a three-performer setup, in which a third performer manages data selection (x7). We close with a few open questions (x8)."
            },
            "slug": "Adversarial-Evaluation-for-Models-of-Natural-Smith",
            "title": {
                "fragments": [],
                "text": "Adversarial Evaluation for Models of Natural Language"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new abstract framework for evaluating natural language processing (NLP) models in general and unsupervised NLP models in particular is presented, to make explicit certain adversarial roles among researchers, so that the different roles in an evaluation are more clearly defined and performers of all roles are offered ways to make measurable contributions to the larger goal."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40296541"
                        ],
                        "name": "Zhiguo Wang",
                        "slug": "Zhiguo-Wang",
                        "structuredName": {
                            "firstName": "Zhiguo",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhiguo Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2013337"
                        ],
                        "name": "Haitao Mi",
                        "slug": "Haitao-Mi",
                        "structuredName": {
                            "firstName": "Haitao",
                            "lastName": "Mi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haitao Mi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1836135"
                        ],
                        "name": "W. Hamza",
                        "slug": "W.-Hamza",
                        "structuredName": {
                            "firstName": "Wael",
                            "lastName": "Hamza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Hamza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707117"
                        ],
                        "name": "Radu Florian",
                        "slug": "Radu-Florian",
                        "structuredName": {
                            "firstName": "Radu",
                            "lastName": "Florian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radu Florian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 108
                            }
                        ],
                        "text": ", 2017), Ruminating Reader (Gong and Bowman, 2017), MultiPerspective Context Matching (MPCM) Single version (Wang et al., 2016), RaSOR (Lee et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 148
                            }
                        ],
                        "text": "\u2026(Liu et al., 2017), jNet (Zhang et al., 2017), Ruminating Reader (Gong and Bowman, 2017), MultiPerspective Context Matching (MPCM) Single version (Wang et al., 2016), RaSOR (Lee et al., 2017), Dynamic Chunk Reader (DCR) (Yu et al., 2016), and the Logistic Regression Baseline (Rajpurkar et al.,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6980296,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e94697b98b707f557436e025bdc8498fa261d3bc",
            "isKey": false,
            "numCitedBy": 147,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous machine comprehension (MC) datasets are either too small to train end-to-end deep learning models, or not difficult enough to evaluate the ability of current MC techniques. The newly released SQuAD dataset alleviates these limitations, and gives us a chance to develop more realistic MC models. Based on this dataset, we propose a Multi-Perspective Context Matching (MPCM) model, which is an end-to-end system that directly predicts the answer beginning and ending points in a passage. Our model first adjusts each word-embedding vector in the passage by multiplying a relevancy weight computed against the question. Then, we encode the question and weighted passage by using bi-directional LSTMs. For each point in the passage, our model matches the context of this point against the encoded question from multiple perspectives and produces a matching vector. Given those matched vectors, we employ another bi-directional LSTM to aggregate all the information and predict the beginning and ending points. Experimental result on the test set of SQuAD shows that our model achieves a competitive result on the leaderboard."
            },
            "slug": "Multi-Perspective-Context-Matching-for-Machine-Wang-Mi",
            "title": {
                "fragments": [],
                "text": "Multi-Perspective Context Matching for Machine Comprehension"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A Multi-Perspective Context Matching (MPCM) model is proposed, which is an end-to-end system that directly predicts the answer beginning and ending points in a passage."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752875"
                        ],
                        "name": "Yelong Shen",
                        "slug": "Yelong-Shen",
                        "structuredName": {
                            "firstName": "Yelong",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yelong Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2421691"
                        ],
                        "name": "Po-Sen Huang",
                        "slug": "Po-Sen-Huang",
                        "structuredName": {
                            "firstName": "Po-Sen",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Po-Sen Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800422"
                        ],
                        "name": "Jianfeng Gao",
                        "slug": "Jianfeng-Gao",
                        "structuredName": {
                            "firstName": "Jianfeng",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianfeng Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109136147"
                        ],
                        "name": "Weizhu Chen",
                        "slug": "Weizhu-Chen",
                        "structuredName": {
                            "firstName": "Weizhu",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weizhu Chen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 148
                            }
                        ],
                        "text": "We also validate our major findings on twelve other published models with publicly available test-time code: ReasoNet Single and Ensemble versions (Shen et al., 2017), Mnemonic\nReader Single and Ensemble versions (Hu et al., 2017), Structural Embedding of Dependency Trees (SEDT) Single and Ensemble\u2026"
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 147
                            }
                        ],
                        "text": "We also validate our major findings on twelve other published models with publicly available test-time code: ReasoNet Single and Ensemble versions (Shen et al., 2017), Mnemonic Reader Single and Ensemble versions (Hu et al."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 6300274,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c636a2dd242908fe2e598a1077c0c57bfdea8633",
            "isKey": false,
            "numCitedBy": 290,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Teaching a computer to read and answer general questions pertaining to a document is a challenging yet unsolved problem. In this paper, we describe a novel neural network architecture called the Reasoning Network (ReasoNet) for machine comprehension tasks. ReasoNets make use of multiple turns to effectively exploit and then reason over the relation among queries, documents, and answers. Different from previous approaches using a fixed number of turns during inference, ReasoNets introduce a termination state to relax this constraint on the reasoning depth. With the use of reinforcement learning, ReasoNets can dynamically determine whether to continue the comprehension process after digesting intermediate results, or to terminate reading when it concludes that existing information is adequate to produce an answer. ReasoNets achieve superior performance in machine comprehension datasets, including unstructured CNN and Daily Mail datasets, the Stanford SQuAD dataset, and a structured Graph Reachability dataset."
            },
            "slug": "ReasoNet:-Learning-to-Stop-Reading-in-Machine-Shen-Huang",
            "title": {
                "fragments": [],
                "text": "ReasoNet: Learning to Stop Reading in Machine Comprehension"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel neural network architecture called the Reasoning Network (ReasoNet) for machine comprehension tasks, which makes use of multiple turns to effectively exploit and then reason over the relation among queries, documents, and answers."
            },
            "venue": {
                "fragments": [],
                "text": "CoCo@NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144207288"
                        ],
                        "name": "R. Liu",
                        "slug": "R.-Liu",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145919378"
                        ],
                        "name": "Junjie Hu",
                        "slug": "Junjie-Hu",
                        "structuredName": {
                            "firstName": "Junjie",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junjie Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149191538"
                        ],
                        "name": "Wei Wei",
                        "slug": "Wei-Wei",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111906048"
                        ],
                        "name": "Zi Yang",
                        "slug": "Zi-Yang",
                        "structuredName": {
                            "firstName": "Zi",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zi Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144287919"
                        ],
                        "name": "Eric Nyberg",
                        "slug": "Eric-Nyberg",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Nyberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Nyberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 86
                            }
                        ],
                        "text": ", 2017), Structural Embedding of Dependency Trees (SEDT) Single and Ensemble versions (Liu et al., 2017), jNet (Zhang et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 150
                            }
                        ],
                        "text": "\u20262017), Mnemonic\nReader Single and Ensemble versions (Hu et al., 2017), Structural Embedding of Dependency Trees (SEDT) Single and Ensemble versions (Liu et al., 2017), jNet (Zhang et al., 2017), Ruminating Reader (Gong and Bowman, 2017), MultiPerspective Context Matching (MPCM) Single version\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2265207,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2ad6ae5f57844c4474cd3f318c3cdc469994fae",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep neural networks for machine comprehension typically utilizes only word or character embeddings without explicitly taking advantage of structured linguistic information such as constituency trees and dependency trees. In this paper, we propose structural embedding of syntactic trees (SEST), an algorithm framework to utilize structured information and encode them into vector representations that can boost the performance of algorithms for the machine comprehension. We evaluate our approach using a state-of-the-art neural attention model on the SQuAD dataset. Experimental results demonstrate that our model can accurately identify the syntactic boundaries of the sentences and extract answers that are syntactically coherent over the baseline methods."
            },
            "slug": "Structural-Embedding-of-Syntactic-Trees-for-Machine-Liu-Hu",
            "title": {
                "fragments": [],
                "text": "Structural Embedding of Syntactic Trees for Machine Comprehension"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Experimental results demonstrate that the proposed structural embedding of syntactic trees (SEST) can accurately identify the syntactic boundaries of the sentences and extract answers that are syntactically coherent over the baseline methods."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152847920"
                        ],
                        "name": "Yang Yu",
                        "slug": "Yang-Yu",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47527881"
                        ],
                        "name": "Wei Zhang",
                        "slug": "Wei-Zhang",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064634"
                        ],
                        "name": "K. Hasan",
                        "slug": "K.-Hasan",
                        "structuredName": {
                            "firstName": "Kazi",
                            "lastName": "Hasan",
                            "middleNames": [
                                "Saidul"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hasan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2482533"
                        ],
                        "name": "Mo Yu",
                        "slug": "Mo-Yu",
                        "structuredName": {
                            "firstName": "Mo",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mo Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144028698"
                        ],
                        "name": "Bing Xiang",
                        "slug": "Bing-Xiang",
                        "structuredName": {
                            "firstName": "Bing",
                            "lastName": "Xiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bing Xiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145218984"
                        ],
                        "name": "Bowen Zhou",
                        "slug": "Bowen-Zhou",
                        "structuredName": {
                            "firstName": "Bowen",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bowen Zhou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 36
                            }
                        ],
                        "text": ", 2017), Dynamic Chunk Reader (DCR) (Yu et al., 2016), and the Logistic Regression Baseline (Rajpurkar et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 217
                            }
                        ],
                        "text": "\u2026et al., 2017), jNet (Zhang et al., 2017), Ruminating Reader (Gong and Bowman, 2017), MultiPerspective Context Matching (MPCM) Single version (Wang et al., 2016), RaSOR (Lee et al., 2017), Dynamic Chunk Reader (DCR) (Yu et al., 2016), and the Logistic Regression Baseline (Rajpurkar et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 527,
                                "start": 524
                            }
                        ],
                        "text": "We also validate our major findings on twelve other published models with publicly available test-time code: ReasoNet Single and Ensemble versions (Shen et al., 2017), Mnemonic\nReader Single and Ensemble versions (Hu et al., 2017), Structural Embedding of Dependency Trees (SEDT) Single and Ensemble versions (Liu et al., 2017), jNet (Zhang et al., 2017), Ruminating Reader (Gong and Bowman, 2017), MultiPerspective Context Matching (MPCM) Single version (Wang et al., 2016), RaSOR (Lee et al., 2017), Dynamic Chunk Reader (DCR) (Yu et al., 2016), and the Logistic Regression Baseline (Rajpurkar et al., 2016)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17654651,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0680f04750b1e257ffdd161e85382031dc73ea7f",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes dynamic chunk reader (DCR), an end-to-end neural reading comprehension (RC) model that is able to extract and rank a set of answer candidates from a given document to answer questions. DCR is able to predict answers of variable lengths, whereas previous neural RC models primarily focused on predicting single tokens or entities. DCR encodes a document and an input question with recurrent neural networks, and then applies a word-by-word attention mechanism to acquire question-aware representations for the document, followed by the generation of chunk representations and a ranking module to propose the top-ranked chunk as the answer. Experimental results show that DCR could achieve a 66.3% Exact match and 74.7% F1 score on the Stanford Question Answering Dataset."
            },
            "slug": "End-to-End-Answer-Chunk-Extraction-and-Ranking-for-Yu-Zhang",
            "title": {
                "fragments": [],
                "text": "End-to-End Answer Chunk Extraction and Ranking for Reading Comprehension"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "This paper proposes dynamic chunk reader (DCR), an end-to-end neural reading comprehension model that is able to extract and rank a set of answer candidates from a given document to answer questions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4418074"
                        ],
                        "name": "Minjoon Seo",
                        "slug": "Minjoon-Seo",
                        "structuredName": {
                            "firstName": "Minjoon",
                            "lastName": "Seo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minjoon Seo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2684226"
                        ],
                        "name": "Aniruddha Kembhavi",
                        "slug": "Aniruddha-Kembhavi",
                        "structuredName": {
                            "firstName": "Aniruddha",
                            "lastName": "Kembhavi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aniruddha Kembhavi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143787583"
                        ],
                        "name": "Ali Farhadi",
                        "slug": "Ali-Farhadi",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Farhadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ali Farhadi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2548384"
                        ],
                        "name": "Hannaneh Hajishirzi",
                        "slug": "Hannaneh-Hajishirzi",
                        "structuredName": {
                            "firstName": "Hannaneh",
                            "lastName": "Hajishirzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hannaneh Hajishirzi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 97
                            }
                        ],
                        "text": "When developing and testing our methods, we focused on two published model architectures: BiDAF (Seo et al., 2016) and Match-LSTM (Wang and Jiang, 2016)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 57
                            }
                        ],
                        "text": "Ad-\n6 All previous experiments used parameters released by Seo et al. (2016)\nversarial training (Goodfellow et al., 2015) can be used for any model trained with stochastic gradient descent, but it requires generating new adversarial examples at every iteration; this is feasible for images, where\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8535316,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4",
            "isKey": false,
            "numCitedBy": 1722,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test."
            },
            "slug": "Bidirectional-Attention-Flow-for-Machine-Seo-Kembhavi",
            "title": {
                "fragments": [],
                "text": "Bidirectional Attention Flow for Machine Comprehension"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The BIDAF network is introduced, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153440022"
                        ],
                        "name": "Ian J. Goodfellow",
                        "slug": "Ian-J.-Goodfellow",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Goodfellow",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian J. Goodfellow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789737"
                        ],
                        "name": "Jonathon Shlens",
                        "slug": "Jonathon-Shlens",
                        "structuredName": {
                            "firstName": "Jonathon",
                            "lastName": "Shlens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathon Shlens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574060"
                        ],
                        "name": "Christian Szegedy",
                        "slug": "Christian-Szegedy",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Szegedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Szegedy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 95
                            }
                        ],
                        "text": "Ad-\n6 All previous experiments used parameters released by Seo et al. (2016)\nversarial training (Goodfellow et al., 2015) can be used for any model trained with stochastic gradient descent, but it requires generating new adversarial examples at every iteration; this is feasible for images, where\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 148
                            }
                        ],
                        "text": "In image classification, adversarial examples are commonly generated by adding an imperceptible amount of noise to the input (Szegedy et al., 2014; Goodfellow et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 19
                            }
                        ],
                        "text": "versarial training (Goodfellow et al., 2015) can be used for any model trained with stochastic gradient descent, but it requires generating new adversarial examples at every iteration; this is feasible for images, where fast gradient-based adversaries exist, but is infeasible for domains where only slower adversaries are available."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 124
                            }
                        ],
                        "text": "\u201cProbabilistic\u201d query access is still weaker than access to gradients, as is common in computer vision (Szegedy et al., 2014; Goodfellow et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 200
                            }
                        ],
                        "text": "Prior work in computer vision adds imperceptible adversarial perturbations to input images, relying on the fact that such small perturbations cannot change an image\u2019s true label (Szegedy et al., 2014; Goodfellow et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6706414,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bee044c8e8903fb67523c1f8c105ab4718600cdb",
            "isKey": true,
            "numCitedBy": 10110,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset."
            },
            "slug": "Explaining-and-Harnessing-Adversarial-Examples-Goodfellow-Shlens",
            "title": {
                "fragments": [],
                "text": "Explaining and Harnessing Adversarial Examples"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is argued that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature, supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723404"
                        ],
                        "name": "Nitin Madnani",
                        "slug": "Nitin-Madnani",
                        "structuredName": {
                            "firstName": "Nitin",
                            "lastName": "Madnani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nitin Madnani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752326"
                        ],
                        "name": "B. Dorr",
                        "slug": "B.-Dorr",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Dorr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Dorr"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 55
                            }
                        ],
                        "text": "As discussed previously, paraphrase generation systems (Madnani and Dorr, 2010) could be used for adversarial evaluation on a wide range of language tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 67
                            }
                        ],
                        "text": "For language, the direct analogue would be to paraphrase the input (Madnani and Dorr, 2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17652653,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7324f18f8fa0ef857f03ff486df0f2ac51d99afb",
            "isKey": false,
            "numCitedBy": 278,
            "numCiting": 148,
            "paperAbstract": {
                "fragments": [],
                "text": "The task of paraphrasing is inherently familiar to speakers of all languages. Moreover, the task of automatically generating or extracting semantic equivalences for the various units of language\u2014words, phrases, and sentences\u2014is an important part of natural language processing (NLP) and is being increasingly employed to improve the performance of several NLP applications. In this article, we attempt to conduct a comprehensive and application-independent survey of data-driven phrasal and sentential paraphrase generation methods, while also conveying an appreciation for the importance and potential use of paraphrases in the field of NLP research. Recent work done in manual and automatic construction of paraphrase corpora is also examined. We also discuss the strategies used for evaluating paraphrase generation techniques and briefly explore some future trends in paraphrase generation."
            },
            "slug": "Generating-Phrasal-and-Sentential-Paraphrases:-A-of-Madnani-Dorr",
            "title": {
                "fragments": [],
                "text": "Generating Phrasal and Sentential Paraphrases: A Survey of Data-Driven Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A comprehensive and application-independent survey of data-driven phrasal and sentential paraphrase generation methods is conducted, while also conveying an appreciation for the importance and potential use of paraphrases in the field of NLP research."
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2554988"
                        ],
                        "name": "Laura Rimell",
                        "slug": "Laura-Rimell",
                        "structuredName": {
                            "firstName": "Laura",
                            "lastName": "Rimell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laura Rimell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144523372"
                        ],
                        "name": "S. Clark",
                        "slug": "S.-Clark",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145332819"
                        ],
                        "name": "Mark Steedman",
                        "slug": "Mark-Steedman",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Steedman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Steedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 177
                            }
                        ],
                        "text": "However, models can succeed in this paradigm by recognizing patterns that happen to be predictive on most of the test examples, while ignoring deeper, more difficult phenomena (Rimell et al., 2009; Paperno et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Rimell et al. (2009) showed that dependency parsers that seem very accurate by standard metrics perform poorly on a subset of the test data that has unbounded dependency construc-\ntions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9357023,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f00e6007cdf7dafb8dc48ed2518dc6356013bf1c",
            "isKey": false,
            "numCitedBy": 114,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a new parser evaluation corpus containing around 700 sentences annotated with unbounded dependencies, from seven different grammatical constructions. We run a series of off-the-shelf parsers on the corpus to evaluate how well state-of-the-art parsing technology is able to recover such dependencies. The overall results range from 25% accuracy to 59%. These low scores call into question the validity of using Parseval scores as a general measure of parsing capability. We discuss the importance of parsers being able to recover unbounded dependencies, given their relatively low frequency in corpora. We also analyse the various errors made on these constructions by one of the more successful parsers."
            },
            "slug": "Unbounded-Dependency-Recovery-for-Parser-Evaluation-Rimell-Clark",
            "title": {
                "fragments": [],
                "text": "Unbounded Dependency Recovery for Parser Evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A new parser evaluation corpus containing around 700 sentences annotated with unbounded dependencies, from seven different grammatical constructions is introduced, to evaluate how well state-of-the-art parsing technology is able to recover such dependencies."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3644767"
                        ],
                        "name": "Samuel R. Bowman",
                        "slug": "Samuel-R.-Bowman",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Bowman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samuel R. Bowman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2546951"
                        ],
                        "name": "L. Vilnis",
                        "slug": "L.-Vilnis",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Vilnis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Vilnis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2555924"
                        ],
                        "name": "Andrew M. Dai",
                        "slug": "Andrew-M.-Dai",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Dai",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew M. Dai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1944541"
                        ],
                        "name": "R. J\u00f3zefowicz",
                        "slug": "R.-J\u00f3zefowicz",
                        "structuredName": {
                            "firstName": "Rafal",
                            "lastName": "J\u00f3zefowicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. J\u00f3zefowicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751569"
                        ],
                        "name": "Samy Bengio",
                        "slug": "Samy-Bengio",
                        "structuredName": {
                            "firstName": "Samy",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samy Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Bowman et al. (2016) and Li et al. (2017) used such a setup for sentence and dialogue generation, respectively."
                    },
                    "intents": []
                }
            ],
            "corpusId": 748227,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d82b55c35c8673774a708353838918346f6c006f",
            "isKey": false,
            "numCitedBy": 1746,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling."
            },
            "slug": "Generating-Sentences-from-a-Continuous-Space-Bowman-Vilnis",
            "title": {
                "fragments": [],
                "text": "Generating Sentences from a Continuous Space"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work introduces and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences that allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6392744"
                        ],
                        "name": "Yichen Gong",
                        "slug": "Yichen-Gong",
                        "structuredName": {
                            "firstName": "Yichen",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yichen Gong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3644767"
                        ],
                        "name": "Samuel R. Bowman",
                        "slug": "Samuel-R.-Bowman",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Bowman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samuel R. Bowman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 27
                            }
                        ],
                        "text": ", 2017), Ruminating Reader (Gong and Bowman, 2017), MultiPerspective Context Matching (MPCM) Single version (Wang et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 373,
                                "start": 356
                            }
                        ],
                        "text": "We also validate our major findings on twelve other published models with publicly available test-time code: ReasoNet Single and Ensemble versions (Shen et al., 2017), Mnemonic\nReader Single and Ensemble versions (Hu et al., 2017), Structural Embedding of Dependency Trees (SEDT) Single and Ensemble versions (Liu et al., 2017), jNet (Zhang et al., 2017), Ruminating Reader (Gong and Bowman, 2017), MultiPerspective Context Matching (MPCM) Single version (Wang et al., 2016), RaSOR (Lee et al., 2017), Dynamic Chunk Reader (DCR) (Yu et al., 2016), and the Logistic Regression Baseline (Rajpurkar et al., 2016)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11074199,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c50cd7df4271ef94a0a60894f0e2cf4ef89fb912",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "To answer the question in machine comprehension (MC) task, the models need to establish the interaction between the question and the context. To tackle the problem that the single-pass model cannot reflect on and correct its answer, we present Ruminating Reader. Ruminating Reader adds a second pass of attention and a novel information fusion component to the Bi-Directional Attention Flow model (BiDAF). We propose novel layer structures that construct a query aware context vector representation and fuse encoding representation with intermediate representation on top of BiDAF model. We show that a multi-hop attention mechanism can be applied to a bi-directional attention structure. In experiments on SQuAD, we find that the Reader outperforms the BiDAF baseline by 2.1 F1 score and 2.7 EM score. Our analysis shows that different hops of the attention have different responsibilities in selecting answers."
            },
            "slug": "Ruminating-Reader:-Reasoning-with-Gated-Multi-hop-Gong-Bowman",
            "title": {
                "fragments": [],
                "text": "Ruminating Reader: Reasoning with Gated Multi-hop Attention"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Ruminating Reader adds a second pass of attention and a novel information fusion component to the Bi-Directional Attention Flow model (BiDAF) and proposes novel layer structures that construct a query aware context vector representation and fuse encoding representation with intermediate representation on top of BiDAF model."
            },
            "venue": {
                "fragments": [],
                "text": "QA@ACL"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422908"
                        ],
                        "name": "Robin Jia",
                        "slug": "Robin-Jia",
                        "structuredName": {
                            "firstName": "Robin",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robin Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145419642"
                        ],
                        "name": "Percy Liang",
                        "slug": "Percy-Liang",
                        "structuredName": {
                            "firstName": "Percy",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Percy Liang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 123
                            }
                        ],
                        "text": "We could also adversarially generate new examples by combining multiple existing ones, in the spirit of Data Recombination (Jia and Liang, 2016)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7218315,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7eac64a8410976759445cce235469163d23ee65",
            "isKey": false,
            "numCitedBy": 378,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Modeling crisp logical regularities is crucial in semantic parsing, making it difficult for neural models with no task-specific prior knowledge to achieve good results. In this paper, we introduce data recombination, a novel framework for injecting such prior knowledge into a model. From the training data, we induce a high-precision synchronous context-free grammar, which captures important conditional independence properties commonly found in semantic parsing. We then train a sequence-to-sequence recurrent network (RNN) model with a novel attention-based copying mechanism on datapoints sampled from this grammar, thereby teaching the model about these structural properties. Data recombination improves the accuracy of our RNN model on three semantic parsing datasets, leading to new state-of-the-art performance on the standard GeoQuery dataset for models with comparable supervision."
            },
            "slug": "Data-Recombination-for-Neural-Semantic-Parsing-Jia-Liang",
            "title": {
                "fragments": [],
                "text": "Data Recombination for Neural Semantic Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Data recombination improves the accuracy of the RNN model on three semantic parsing datasets, leading to new state-of-the-art performance on the standard GeoQuery dataset for models with comparable supervision."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2701535"
                        ],
                        "name": "Nina Narodytska",
                        "slug": "Nina-Narodytska",
                        "structuredName": {
                            "firstName": "Nina",
                            "lastName": "Narodytska",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nina Narodytska"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7993151"
                        ],
                        "name": "S. Kasiviswanathan",
                        "slug": "S.-Kasiviswanathan",
                        "structuredName": {
                            "firstName": "Shiva",
                            "lastName": "Kasiviswanathan",
                            "middleNames": [
                                "Prasad"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kasiviswanathan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 47
                            }
                        ],
                        "text": "In contrast with prior work in computer vision (Papernot et al., 2017; Narodytska and Kasiviswanathan, 2016; Moosavi-Dezfooli et al., 2017), ADDONESENT does not require any access to the model or to any training data: it generates adversarial examples based solely on the intuition that existing models are overly stable."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 12443915,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a76bf5895759af6a561d81a0e2960cd00f1167e",
            "isKey": false,
            "numCitedBy": 172,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep neural networks are powerful and popular learning models that achieve state-of-the-art pattern recognition performance on many computer vision, speech, and language processing tasks. However, these networks have also been shown susceptible to carefully crafted adversarial perturbations which force misclassification of the inputs. Adversarial examples enable adversaries to subvert the expected system behavior leading to undesired consequences and could pose a security risk when these systems are deployed in the real world.\n\nIn this work, we focus on deep convolutional neural networks and demonstrate that adversaries can easily craft adversarial examples even without any internal knowledge of the target network. Our attacks treat the network as an oracle (black-box) and only assume that the output of the network can be observed on the probed inputs. Our first attack is based on a simple idea of adding perturbation to a randomly selected single pixel or a small set of them. We then improve the effectiveness of this attack by carefully constructing a small set of pixels to perturb by using the idea of greedy local-search. Our proposed attacks also naturally extend to a stronger notion of misclassification. Our extensive experimental results illustrate that even these elementary attacks can reveal a deep neural network's vulnerabilities. The simplicity and effectiveness of our proposed schemes mean that they could serve as a litmus test while designing robust networks."
            },
            "slug": "Simple-Black-Box-Adversarial-Perturbations-for-Deep-Narodytska-Kasiviswanathan",
            "title": {
                "fragments": [],
                "text": "Simple Black-Box Adversarial Perturbations for Deep Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work focuses on deep convolutional neural networks and demonstrates that adversaries can easily craft adversarial examples even without any internal knowledge of the target network."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143845796"
                        ],
                        "name": "Jeffrey Pennington",
                        "slug": "Jeffrey-Pennington",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Pennington",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey Pennington"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 163
                            }
                        ],
                        "text": "We replace nouns and adjectives with antonyms from WordNet (Fellbaum, 1998), and change named entities and numbers to the nearest word in GloVe word vector space2 (Pennington et al., 2014) with the same part of speech."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 148
                            }
                        ],
                        "text": "\u2026and adjectives with antonyms from WordNet (Fellbaum, 1998), and change named entities and numbers to the nearest word in GloVe word vector space2 (Pennington et al., 2014) with the same part of speech.3 If no words are changed during this step, the adversary gives up and immediately returns the\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1957433,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f37e1b62a767a307c046404ca96bc140b3e68cb5",
            "isKey": false,
            "numCitedBy": 22540,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition."
            },
            "slug": "GloVe:-Global-Vectors-for-Word-Representation-Pennington-Socher",
            "title": {
                "fragments": [],
                "text": "GloVe: Global Vectors for Word Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods and produces a vector space with meaningful substructure."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574060"
                        ],
                        "name": "Christian Szegedy",
                        "slug": "Christian-Szegedy",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Szegedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Szegedy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2563432"
                        ],
                        "name": "Wojciech Zaremba",
                        "slug": "Wojciech-Zaremba",
                        "structuredName": {
                            "firstName": "Wojciech",
                            "lastName": "Zaremba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wojciech Zaremba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143627859"
                        ],
                        "name": "Joan Bruna",
                        "slug": "Joan-Bruna",
                        "structuredName": {
                            "firstName": "Joan",
                            "lastName": "Bruna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joan Bruna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761978"
                        ],
                        "name": "D. Erhan",
                        "slug": "D.-Erhan",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Erhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Erhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153440022"
                        ],
                        "name": "Ian J. Goodfellow",
                        "slug": "Ian-J.-Goodfellow",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Goodfellow",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian J. Goodfellow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 178
                            }
                        ],
                        "text": "Prior work in computer vision adds imperceptible adversarial perturbations to input images, relying on the fact that such small perturbations cannot change an image\u2019s true label (Szegedy et al., 2014; Goodfellow et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 93
                            }
                        ],
                        "text": "In computer vision, adversarial examples that fool one model also tend to fool other models (Szegedy et al., 2014; Moosavi-Dezfooli et al., 2017); we investigate whether the same pattern holds for us."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 126
                            }
                        ],
                        "text": "In image classification, adversarial examples are commonly generated by adding an imperceptible amount of noise to the input (Szegedy et al., 2014; Goodfellow et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 102
                            }
                        ],
                        "text": "\u201cProbabilistic\u201d query access is still weaker than access to gradients, as is common in computer vision (Szegedy et al., 2014; Goodfellow et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 604334,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d891dc72cbd40ffaeefdc79f2e7afe1e530a23ad",
            "isKey": true,
            "numCitedBy": 8885,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. \nFirst, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. \nSecond, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input."
            },
            "slug": "Intriguing-properties-of-neural-networks-Szegedy-Zaremba",
            "title": {
                "fragments": [],
                "text": "Intriguing properties of neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is found that there is no distinction between individual highlevel units and random linear combinations of high level units, according to various methods of unit analysis, and it is suggested that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1967156"
                        ],
                        "name": "Nicolas Papernot",
                        "slug": "Nicolas-Papernot",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Papernot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Papernot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144061974"
                        ],
                        "name": "P. Mcdaniel",
                        "slug": "P.-Mcdaniel",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Mcdaniel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Mcdaniel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153440022"
                        ],
                        "name": "Ian J. Goodfellow",
                        "slug": "Ian-J.-Goodfellow",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Goodfellow",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian J. Goodfellow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680133"
                        ],
                        "name": "S. Jha",
                        "slug": "S.-Jha",
                        "structuredName": {
                            "firstName": "Somesh",
                            "lastName": "Jha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Jha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144643812"
                        ],
                        "name": "Z. B. Celik",
                        "slug": "Z.-B.-Celik",
                        "structuredName": {
                            "firstName": "Z.",
                            "lastName": "Celik",
                            "middleNames": [
                                "Berkay"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. B. Celik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144231976"
                        ],
                        "name": "A. Swami",
                        "slug": "A.-Swami",
                        "structuredName": {
                            "firstName": "Ananthram",
                            "lastName": "Swami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Swami"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 47
                            }
                        ],
                        "text": "In contrast with prior work in computer vision (Papernot et al., 2017; Narodytska and Kasiviswanathan, 2016; Moosavi-Dezfooli et al., 2017), ADDONESENT does not require any access to the model or to any training data: it generates adversarial examples based solely on the intuition that existing models are overly stable."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 48
                            }
                        ],
                        "text": "In contrast with prior work in computer vision (Papernot et al., 2017; Narodytska and Kasiviswanathan, 2016; Moosavi-Dezfooli et al., 2017), ADDONESENT does not require any access to the model or to any training data: it generates adversarial examples based solely on the intuition that existing\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 14321358,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "94187ef33e34af2cdb42502083c6f9b4c3f5ba6b",
            "isKey": false,
            "numCitedBy": 423,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "Advances in deep learning have led to the broad adoption of Deep Neural Networks (DNNs) to a range of important machine learning problems, e.g., guiding autonomous vehicles, speech recognition, malware detection. Yet, machine learning models, including DNNs, were shown to be vulnerable to adversarial samples-subtly (and often humanly indistinguishably) modified malicious inputs crafted to compromise the integrity of their outputs. Adversarial examples thus enable adversaries to manipulate system behaviors. Potential attacks include attempts to control the behavior of vehicles, have spam content identified as legitimate content, or have malware identified as legitimate software. Adversarial examples are known to transfer from one model to another, even if the second model has a different architecture or was trained on a different set. We introduce the first practical demonstration that this cross-model transfer phenomenon enables attackers to control a remotely hosted DNN with no access to the model, its parameters, or its training data. In our demonstration, we only assume that the adversary can observe outputs from the target DNN given inputs chosen by the adversary. We introduce the attack strategy of fitting a substitute model to the input-output pairs in this manner, then crafting adversarial examples based on this auxiliary model. We evaluate the approach on existing DNN datasets and real-world settings. In one experiment, we force a DNN supported by MetaMind (one of the online APIs for DNN classifiers) to mis-classify inputs at a rate of 84.24%. We conclude with experiments exploring why adversarial samples transfer between DNNs, and a discussion on the applicability of our attack when targeting machine learning algorithms distinct from DNNs."
            },
            "slug": "Practical-Black-Box-Attacks-against-Deep-Learning-Papernot-Mcdaniel",
            "title": {
                "fragments": [],
                "text": "Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work introduces the first practical demonstration that cross-model transfer phenomenon enables attackers to control a remotely hosted DNN with no access to the model, its parameters, or its training data, and introduces the attack strategy of fitting a substitute model to the input-output pairs in this manner, then crafting adversarial examples based on this auxiliary model."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143634377"
                        ],
                        "name": "H. Levesque",
                        "slug": "H.-Levesque",
                        "structuredName": {
                            "firstName": "Hector",
                            "lastName": "Levesque",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Levesque"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 112
                            }
                        ],
                        "text": "Quantifying the extent to which a computer system exhibits intelligent behavior is a longstanding problem in AI (Levesque, 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12325453,
            "fieldsOfStudy": [
                "Psychology",
                "Art"
            ],
            "id": "6d334cb557f5addd3fc080134483125eea0b7a90",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-our-best-behaviour-Levesque",
            "title": {
                "fragments": [],
                "text": "On our best behaviour"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403182206"
                        ],
                        "name": "Seyed-Mohsen Moosavi-Dezfooli",
                        "slug": "Seyed-Mohsen-Moosavi-Dezfooli",
                        "structuredName": {
                            "firstName": "Seyed-Mohsen",
                            "lastName": "Moosavi-Dezfooli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seyed-Mohsen Moosavi-Dezfooli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33054064"
                        ],
                        "name": "Alhussein Fawzi",
                        "slug": "Alhussein-Fawzi",
                        "structuredName": {
                            "firstName": "Alhussein",
                            "lastName": "Fawzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alhussein Fawzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145602557"
                        ],
                        "name": "Omar Fawzi",
                        "slug": "Omar-Fawzi",
                        "structuredName": {
                            "firstName": "Omar",
                            "lastName": "Fawzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omar Fawzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703189"
                        ],
                        "name": "P. Frossard",
                        "slug": "P.-Frossard",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Frossard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frossard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 47
                            }
                        ],
                        "text": "In contrast with prior work in computer vision (Papernot et al., 2017; Narodytska and Kasiviswanathan, 2016; Moosavi-Dezfooli et al., 2017), ADDONESENT does not require any access to the model or to any training data: it generates adversarial examples based solely on the intuition that existing models are overly stable."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 115
                            }
                        ],
                        "text": "In computer vision, adversarial examples that fool one model also tend to fool other models (Szegedy et al., 2014; Moosavi-Dezfooli et al., 2017); we investigate whether the same pattern holds for us."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 109
                            }
                        ],
                        "text": "In contrast with prior work in computer vision (Papernot et al., 2017; Narodytska and Kasiviswanathan, 2016; Moosavi-Dezfooli et al., 2017), ADDONESENT does not require any access to the model or to any training data: it generates adversarial examples based solely on the intuition that existing\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 11558223,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "16aa01ca0834a924c25faad5d8bfef3fd1acfcfe",
            "isKey": false,
            "numCitedBy": 1605,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images."
            },
            "slug": "Universal-Adversarial-Perturbations-Moosavi-Dezfooli-Fawzi",
            "title": {
                "fragments": [],
                "text": "Universal Adversarial Perturbations"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers and outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153440022"
                        ],
                        "name": "Ian J. Goodfellow",
                        "slug": "Ian-J.-Goodfellow",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Goodfellow",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian J. Goodfellow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403025868"
                        ],
                        "name": "Jean Pouget-Abadie",
                        "slug": "Jean-Pouget-Abadie",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Pouget-Abadie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean Pouget-Abadie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153583218"
                        ],
                        "name": "Mehdi Mirza",
                        "slug": "Mehdi-Mirza",
                        "structuredName": {
                            "firstName": "Mehdi",
                            "lastName": "Mirza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mehdi Mirza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113742925"
                        ],
                        "name": "Bing Xu",
                        "slug": "Bing-Xu",
                        "structuredName": {
                            "firstName": "Bing",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bing Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1393680089"
                        ],
                        "name": "David Warde-Farley",
                        "slug": "David-Warde-Farley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Warde-Farley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Warde-Farley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1955694"
                        ],
                        "name": "Sherjil Ozair",
                        "slug": "Sherjil-Ozair",
                        "structuredName": {
                            "firstName": "Sherjil",
                            "lastName": "Ozair",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sherjil Ozair"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760871"
                        ],
                        "name": "Aaron C. Courville",
                        "slug": "Aaron-C.-Courville",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Courville",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron C. Courville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 212
                            }
                        ],
                        "text": "Generative adversarial models pit a generative model, whose goal is to generate realistic outputs, against a discriminative model, whose goal is to distinguish the generator\u2019s outputs from real data (Smith, 2012; Goodfellow et al., 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1033682,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54e325aee6b2d476bbbb88615ac15e251c6e8214",
            "isKey": false,
            "numCitedBy": 29669,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to \u00bd everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples."
            },
            "slug": "Generative-Adversarial-Nets-Goodfellow-Pouget-Abadie",
            "title": {
                "fragments": [],
                "text": "Generative Adversarial Nets"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A new framework for estimating generative models via an adversarial process, in which two models are simultaneously train: a generative model G that captures the data distribution and a discriminative model D that estimates the probability that a sample came from the training data rather than G."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786843"
                        ],
                        "name": "A. Globerson",
                        "slug": "A.-Globerson",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Globerson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Globerson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9330607"
                        ],
                        "name": "S. Roweis",
                        "slug": "S.-Roweis",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Roweis",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roweis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For certain classes of models and adversaries, efficient training strategies exist: for example, Globerson and Roweis (2006) train classifiers that are optimally robust to adversarial feature deletion."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2116006,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6dd9bb6b38e5b84616e207f00a181dbadce06937",
            "isKey": false,
            "numCitedBy": 338,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "When constructing a classifier from labeled data, it is important not to assign too much weight to any single input feature, in order to increase the robustness of the classifier. This is particularly important in domains with nonstationary feature distributions or with input sensor failures. A common approach to achieving such robustness is to introduce regularization which spreads the weight more evenly between the features. However, this strategy is very generic, and cannot induce robustness specifically tailored to the classification task at hand. In this work, we introduce a new algorithm for avoiding single feature over-weighting by analyzing robustness using a game theoretic formalization. We develop classifiers which are optimally resilient to deletion of features in a minimax sense, and show how to construct such classifiers using quadratic programming. We illustrate the applicability of our methods on spam filtering and handwritten digit recognition tasks, where feature deletion is indeed a realistic noise model."
            },
            "slug": "Nightmare-at-test-time:-robust-learning-by-feature-Globerson-Roweis",
            "title": {
                "fragments": [],
                "text": "Nightmare at test time: robust learning by feature deletion"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new algorithm for avoiding single feature over-weighting is introduced by analyzing robustness using a game theoretic formalization, and classifiers which are optimally resilient to deletion of features in a minimax sense are developed."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760868"
                        ],
                        "name": "M. Surdeanu",
                        "slug": "M.-Surdeanu",
                        "structuredName": {
                            "firstName": "Mihai",
                            "lastName": "Surdeanu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Surdeanu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144661918"
                        ],
                        "name": "John Bauer",
                        "slug": "John-Bauer",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Bauer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Bauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784228"
                        ],
                        "name": "J. Finkel",
                        "slug": "J.-Finkel",
                        "structuredName": {
                            "firstName": "Jenny",
                            "lastName": "Finkel",
                            "middleNames": [
                                "Rose"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Finkel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105138"
                        ],
                        "name": "Steven Bethard",
                        "slug": "Steven-Bethard",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Bethard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven Bethard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2240597"
                        ],
                        "name": "David McClosky",
                        "slug": "David-McClosky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McClosky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David McClosky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 157
                            }
                        ],
                        "text": "If none of the nearest 100 words satisfy this, we just return the single closest word.\nof 26 types, corresponding to NER and POS tags from Stanford CoreNLP (Manning et al., 2014), plus a few custom categories (e.g., abbreviations), and manually associate a fake answer with each type."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 69
                            }
                        ],
                        "text": "of 26 types, corresponding to NER and POS tags from Stanford CoreNLP (Manning et al., 2014), plus a few custom categories (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14068874,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f5102ec3f70d0dea98c957cc2cab4d15d83a2da",
            "isKey": false,
            "numCitedBy": 6058,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural language analysis. This toolkit is quite widely used, both in the research NLP community and also among commercial and government users of open source NLP technology. We suggest that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage."
            },
            "slug": "The-Stanford-CoreNLP-Natural-Language-Processing-Manning-Surdeanu",
            "title": {
                "fragments": [],
                "text": "The Stanford CoreNLP Natural Language Processing Toolkit"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The design and use of the Stanford CoreNLP toolkit is described, an extensible pipeline that provides core natural language analysis, and it is suggested that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3021654"
                        ],
                        "name": "Daniel Lowd",
                        "slug": "Daniel-Lowd",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Lowd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Lowd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50004012"
                        ],
                        "name": "Christopher Meek",
                        "slug": "Christopher-Meek",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Meek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Meek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6259400,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "165e4c6467eeae8974739a731045bdff07d906fc",
            "isKey": false,
            "numCitedBy": 700,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Many classification tasks, such as spam filtering, intrusion detection, and terrorism detection, are complicated by an adversary who wishes to avoid detection. Previous work on adversarial classification has made the unrealistic assumption that the attacker has perfect knowledge of the classifier [2]. In this paper, we introduce the adversarial classifier reverse engineering (ACRE) learning problem, the task of learning sufficient information about a classifier to construct adversarial attacks. We present efficient algorithms for reverse engineering linear classifiers with either continuous or Boolean features and demonstrate their effectiveness using real data from the domain of spam filtering."
            },
            "slug": "Adversarial-learning-Lowd-Meek",
            "title": {
                "fragments": [],
                "text": "Adversarial learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper introduces the adversarial classifier reverse engineering (ACRE) learning problem, the task of learning sufficient information about a classifier to construct adversarial attacks, and presents efficient algorithms for reverse engineering linear classifiers with either continuous or Boolean features."
            },
            "venue": {
                "fragments": [],
                "text": "KDD '05"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1923209"
                        ],
                        "name": "Nilesh N. Dalvi",
                        "slug": "Nilesh-N.-Dalvi",
                        "structuredName": {
                            "firstName": "Nilesh",
                            "lastName": "Dalvi",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nilesh N. Dalvi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740213"
                        ],
                        "name": "Pedro M. Domingos",
                        "slug": "Pedro-M.-Domingos",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Domingos",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro M. Domingos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2674444"
                        ],
                        "name": "Mausam",
                        "slug": "Mausam",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Mausam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mausam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144074891"
                        ],
                        "name": "Sumit K. Sanghai",
                        "slug": "Sumit-K.-Sanghai",
                        "structuredName": {
                            "firstName": "Sumit",
                            "lastName": "Sanghai",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sumit K. Sanghai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144130626"
                        ],
                        "name": "D. Verma",
                        "slug": "D.-Verma",
                        "structuredName": {
                            "firstName": "Deepak",
                            "lastName": "Verma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Verma"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Dalvi et al. (2004) formulated such tasks as a game between a classifier and an adversary, and analyzed optimal strategies for each player."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14134514,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d94f56e7ef07b6e785b830e41da30c257b9a6ece",
            "isKey": false,
            "numCitedBy": 855,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Essentially all data mining algorithms assume that the data-generating process is independent of the data miner's activities. However, in many domains, including spam detection, intrusion detection, fraud detection, surveillance and counter-terrorism, this is far from the case: the data is actively manipulated by an adversary seeking to make the classifier produce false negatives. In these domains, the performance of a classifier can degrade rapidly after it is deployed, as the adversary learns to defeat it. Currently the only solution to this is repeated, manual, ad hoc reconstruction of the classifier. In this paper we develop a formal framework and algorithms for this problem. We view classification as a game between the classifier and the adversary, and produce a classifier that is optimal given the adversary's optimal strategy. Experiments in a spam detection domain show that this approach can greatly outperform a classifier learned in the standard way, and (within the parameters of the problem) automatically adapt the classifier to the adversary's evolving manipulations."
            },
            "slug": "Adversarial-classification-Dalvi-Domingos",
            "title": {
                "fragments": [],
                "text": "Adversarial classification"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper views classification as a game between the classifier and the adversary, and produces a classifier that is optimal given the adversary's optimal strategy, and experiments show that this approach can greatly outperform a classifiers learned in the standard way."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721801"
                        ],
                        "name": "C. Fellbaum",
                        "slug": "C.-Fellbaum",
                        "structuredName": {
                            "firstName": "Christiane",
                            "lastName": "Fellbaum",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Fellbaum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 88
                            }
                        ],
                        "text": ", we would change \u201cABC\u201d to \u201cNBC\u201d (a nearby word in vector space) and \u201cdomestic\u201d to \u201cforeign\u201d (a WordNet antonym), resulting in the question, \u201cWhat NBC division handles foreign television distribution?\u201d"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 59
                            }
                        ],
                        "text": "We replace nouns and adjectives with antonyms from WordNet (Fellbaum, 1998), and change named entities and numbers to the nearest word in GloVe word vector space2 (Pennington et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 51
                            }
                        ],
                        "text": "We replace nouns and adjectives with antonyms from WordNet (Fellbaum, 1998), and change named entities and numbers to the nearest word in GloVe word vector space2 (Pennington et al., 2014) with the same part of speech.3 If no words are changed during this step, the adversary gives up and immediately returns the original example."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5958691,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d87ceda3042f781c341ac17109d1e94a717f5f60",
            "isKey": true,
            "numCitedBy": 13575,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Part 1 The lexical database: nouns in WordNet, George A. Miller modifiers in WordNet, Katherine J. Miller a semantic network of English verbs, Christiane Fellbaum design and implementation of the WordNet lexical database and searching software, Randee I. Tengi. Part 2: automated discovery of WordNet relations, Marti A. Hearst representing verb alterations in WordNet, Karen T. Kohl et al the formalization of WordNet by methods of relational concept analysis, Uta E. Priss. Part 3 Applications of WordNet: building semantic concordances, Shari Landes et al performance and confidence in a semantic annotation task, Christiane Fellbaum et al WordNet and class-based probabilities, Philip Resnik combining local context and WordNet similarity for word sense identification, Claudia Leacock and Martin Chodorow using WordNet for text retrieval, Ellen M. Voorhees lexical chains as representations of context for the detection and correction of malapropisms, Graeme Hirst and David St-Onge temporal indexing through lexical chaining, Reem Al-Halimi and Rick Kazman COLOR-X - using knowledge from WordNet for conceptual modelling, J.F.M. Burg and R.P. van de Riet knowledge processing on an extended WordNet, Sanda M. Harabagiu and Dan I Moldovan appendix - obtaining and using WordNet."
            },
            "slug": "WordNet-:-an-electronic-lexical-database-Fellbaum",
            "title": {
                "fragments": [],
                "text": "WordNet : an electronic lexical database"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "The lexical database: nouns in WordNet, Katherine J. Miller a semantic network of English verbs, and applications of WordNet: building semantic concordances are presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "But examples optimized w.r.t a particular model transfer much better"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u2022 Do adversarial examples transfer for NLP models? \u2022 Randomly generated ones do not"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Are adversarial inputs syntactically similar to the original input? Simple check: evaluate perplexity w.r.t the model"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "\u2022 Is it easy to construct adversarial examples for NLP models? \u2022 Can be done even with random perturbation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 36
                            }
                        ],
                        "text": "The Build It, Break It shared task (Bender et al., 2017) encourages researchers to adversarially design minimal pairs to fool sentiment analysis and semantic role labeling systems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Build it, break it: The language edition"
            },
            "venue": {
                "fragments": [],
                "text": "https://bibinlp. umiacs.umd.edu/."
            },
            "year": 2017
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 21,
            "methodology": 11,
            "result": 4
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 37,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Adversarial-Examples-for-Evaluating-Reading-Systems-Jia-Liang/ffb949d3493c3b2f3c9acf9c75cb03938933ddf0?sort=total-citations"
}