{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145797336"
                        ],
                        "name": "Iain Murray",
                        "slug": "Iain-Murray",
                        "structuredName": {
                            "firstName": "Iain",
                            "lastName": "Murray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iain Murray"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Models learned by CD1 are often reasonable generative models of the data [3], but if learning is continued with CD25, the resulting generative models are much better [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 458722,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "08d0ea90b53aba0008d25811268fe46562cfb38c",
            "isKey": false,
            "numCitedBy": 459,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep Belief Networks (DBN's) are generative models that contain many layers of hidden variables. Efficient greedy algorithms for learning and approximate inference have allowed these models to be applied successfully in many application domains. The main building block of a DBN is a bipartite undirected graphical model called a restricted Boltzmann machine (RBM). Due to the presence of the partition function, model selection, complexity control, and exact maximum likelihood learning in RBM's are intractable. We show that Annealed Importance Sampling (AIS) can be used to efficiently estimate the partition function of an RBM, and we present a novel AIS scheme for comparing RBM's with different architectures. We further show how an AIS estimator, along with approximate inference, can be used to estimate a lower bound on the log-probability that a DBN model with multiple hidden layers assigns to the test data. This is, to our knowledge, the first step towards obtaining quantitative results that would allow us to directly assess the performance of Deep Belief Networks as generative models of data."
            },
            "slug": "On-the-quantitative-analysis-of-deep-belief-Salakhutdinov-Murray",
            "title": {
                "fragments": [],
                "text": "On the quantitative analysis of deep belief networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that Annealed Importance Sampling (AIS) can be used to efficiently estimate the partition function of an RBM, and a novel AIS scheme for comparing RBM's with different architectures is presented."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308557"
                        ],
                        "name": "S. Hochreiter",
                        "slug": "S.-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hochreiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "However, this disadvantage is common to many other probabilistic models, and it can be partially alleviated using techniques such as the long short term memory RNN [6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1915014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44d2abe2175df8153f465f6c39b68b76a0d40ab9",
            "isKey": false,
            "numCitedBy": 51717,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms."
            },
            "slug": "Long-Short-Term-Memory-Hochreiter-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Long Short-Term Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel, efficient, gradient based method called long short-term memory (LSTM) is introduced, which can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 198
                            }
                        ],
                        "text": "It follows from these equations that the TRBM is a directed graphical model that has an (undirecte d) RBM at each timestep (a related directed sequence of Boltzmann Machines has been considere d in [8])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2054939,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "80c330eee12decb84aaebcc85dc7ce414134ad61",
            "isKey": false,
            "numCitedBy": 138,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an efficient learning procedure for multilayer generative models that combine the best aspects of Markov random fields and deep, directed belief nets. The generative models can be learned one layer at a time and when learning is complete they have a very fast inference procedure for computing a good approximation to the posterior distribution in all of the hidden layers. Each hidden layer has its own MRF whose energy function is modulated by the top-down directed connections from the layer above. To generate from the model, each layer in turn must settle to equilibrium given its top-down input. We show that this type of model is good at capturing the statistics of patches of natural images."
            },
            "slug": "Modeling-image-patches-with-a-directed-hierarchy-of-Osindero-Hinton",
            "title": {
                "fragments": [],
                "text": "Modeling image patches with a directed hierarchy of Markov random fields"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "An efficient learning procedure for multilayer generative models that combine the best aspects of Markov random fields and deep, directed belief nets is described and it is shown that this type of model is good at capturing the statistics of patches of natural images."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2309950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8978cf7574ceb35f4c3096be768c7547b28a35d0",
            "isKey": false,
            "numCitedBy": 13412,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind."
            },
            "slug": "A-Fast-Learning-Algorithm-for-Deep-Belief-Nets-Hinton-Osindero",
            "title": {
                "fragments": [],
                "text": "A Fast Learning Algorithm for Deep Belief Nets"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A fast, greedy algorithm is derived that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2837110,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c74e230a5a6fd5e2db6ace765ce38afe65f96214",
            "isKey": false,
            "numCitedBy": 242,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a new family of non-linear sequence models that are substantially more powerful than hidden Markov models or linear dynamical systems. Our models have simple approximate inference and learning procedures that work well in practice. Multilevel representations of sequential data can be learned one hidden layer at a time, and adding extra hidden layers improves the resulting generative models. The models can be trained with very high-dimensional, very non-linear data such as raw pixel sequences. Their performance is demonstrated using synthetic video sequences of two balls bouncing in a box."
            },
            "slug": "Learning-Multilevel-Distributed-Representations-for-Sutskever-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning Multilevel Distributed Representations for High-Dimensional Sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A new family of non-linear sequence models that are substantially more powerful than hidden Markov models or linear dynamical systems are described, and their performance is demonstrated using synthetic video sequences of two balls bouncing in a box."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 180
                            }
                        ],
                        "text": "The common disadvantage of the RTRBM is that it is a recurrent neural network, a ty pe of model known to have difficulties learning to use its hidden units to their full potential [2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206457500,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0be39ee052d246ae99c082a565aba25b811be2d",
            "isKey": false,
            "numCitedBy": 6144,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered."
            },
            "slug": "Learning-long-term-dependencies-with-gradient-is-Bengio-Simard",
            "title": {
                "fragments": [],
                "text": "Learning long-term dependencies with gradient descent is difficult"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work shows why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases, and exposes a trade-off between efficient learning by gradient descent and latching on information for long periods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2957517"
                        ],
                        "name": "T. Tieleman",
                        "slug": "T.-Tieleman",
                        "structuredName": {
                            "firstName": "Tijmen",
                            "lastName": "Tieleman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tieleman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": ", [16, 19], including [15], which w orks well for the RBM)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7330145,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "73d6a26f407db77506959fdf3f7b853e44f3844a",
            "isKey": false,
            "numCitedBy": 902,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "A new algorithm for training Restricted Boltzmann Machines is introduced. The algorithm, named Persistent Contrastive Divergence, is different from the standard Contrastive Divergence algorithms in that it aims to draw samples from almost exactly the model distribution. It is compared to some standard Contrastive Divergence and Pseudo-Likelihood algorithms on the tasks of modeling and classifying various types of data. The Persistent Contrastive Divergence algorithm outperforms the other algorithms, and is equally fast and simple."
            },
            "slug": "Training-restricted-Boltzmann-machines-using-to-the-Tieleman",
            "title": {
                "fragments": [],
                "text": "Training restricted Boltzmann machines using approximations to the likelihood gradient"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A new algorithm for training Restricted Boltzmann Machines is introduced, which is compared to some standard Contrastive Divergence and Pseudo-Likelihood algorithms on the tasks of modeling and classifying various types of data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 80
                            }
                        ],
                        "text": "We will approximate the gradients with respect to the RBM\u2019s parameters using the Contrastive Divergence [3] learning procedure, CDn, whose updates are computed by the following algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 74
                            }
                        ],
                        "text": "Models learned by CD 1 are often reasonable generative models of the data [3], but i f learning is continued with CD25, the resulting generative models are much better [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 13
                            }
                        ],
                        "text": "The Temporal Restricted Boltzmann Machine (TRBM) is a probabilistic model for sequences that is able to successfully model (i.e., generate nice-looking samples of) several very high dimensional sequences, such as motioncapture data and the pixels of low resolution videos of balls bouncing in a box."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 95
                            }
                        ],
                        "text": "The learning consists of learning a conditional RBM at each timestep, which is easilydone with Contrastive Divergence (CD) [3]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 175
                            }
                        ],
                        "text": "This means that\u2207O = \u2207 log Q(vT1 ) can be computed with the backpropagation through time algorithm [11], where the contribution of the gradient from eachtimestep is computed with Contrastive Divergence."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 131
                            }
                        ],
                        "text": "As a probabilistic model, the TRBM is a directed graphical mo del consisting of a sequence of Restricted Boltzmann Machines (RBMs) [3], where the state of o ne r more previous RBMs determines the biases of the RBM in next timestep."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 168
                            }
                        ],
                        "text": "Despite the similarity,exact inference is very easy in the RTRBM and computing the gradient of the log likelihood is feasible (up to the error introduced by the use of Contrastive Divergence)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 13
                            }
                        ],
                        "text": "The Temporal Restricted Boltzmann Machine [14, 13] is a recently introduced probabilistic model that has the ability to accurately model complex probability distributions over high-dimensionalsequences."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 93
                            }
                        ],
                        "text": "As a probabilistic model, the TRBM is a directed graphical model consisting of a sequence of Restricted Boltzmann Machines (RBMs) [3], where the state of one r more previous RBMs determines the biases of the RBM in next timestep."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 124
                            }
                        ],
                        "text": "The learning consists of learning a conditional RBM at each timestep, which is easily done with Contrastive Divergence (CD) [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 81
                            }
                        ],
                        "text": "The building block of the TRBM and the RTRBM is the Restricted Boltzmann Machine [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 105
                            }
                        ],
                        "text": "We will approximate the gradients with respect to the RBM\u2019s p arameters using the Contrastive Divergence [3] learning procedure, CD n, whose updates are computed by the following algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207596505,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9360e5ce9c98166bb179ad479a9d2919ff13d022",
            "isKey": false,
            "numCitedBy": 4572,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual expert models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called contrastive divergence whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data."
            },
            "slug": "Training-Products-of-Experts-by-Minimizing-Hinton",
            "title": {
                "fragments": [],
                "text": "Training Products of Experts by Minimizing Contrastive Divergence"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A product of experts (PoE) is an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary because it is hard even to approximate the derivatives of the renormalization term in the combination rule."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145039030"
                        ],
                        "name": "J. Platt",
                        "slug": "J.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Platt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153379696"
                        ],
                        "name": "T. Hofmann",
                        "slug": "T.-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hofmann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 64
                            }
                        ],
                        "text": "Part of the Gaussian noise was removed in a manner described in [14] in both models ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 199
                            }
                        ],
                        "text": "To demonstrate that the RTRBM lear ns to use the hidden units to store information, we did not use delay-taps for the RTRBM nor the T RBM, which causes the results to be worse (but not much) than in [14, 13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 48
                            }
                        ],
                        "text": "If delay-taps are a llowed, then the results of [14, 13] show that there is little benefit from the hidden-to-hidden c o nections (which are W ), making the comparison between the RTRBM and the TRBM uninteresting."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 4
                            }
                        ],
                        "text": "The Temporal Restricted Boltzmann Machine (TRBM) is a probabilistic model for sequences that is able to successfully model (i.e., generate nice-looking samples of) several very high dimensional sequences, such as motioncapture data and the pixels of low resolution videos of balls bouncing in a box."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 66
                            }
                        ],
                        "text": "It was shown to be able to generate realistic motion capture data [14], and low resolu tion videos of 2 balls bouncing in a box [13], as well as complete and denoise such sequences."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 15
                            }
                        ],
                        "text": "The results in [14, 13] were obtained using TRBMs that had several delay-taps, so each hi dden unit could directly observe several previous timesteps."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 42
                            }
                        ],
                        "text": "The Temporal Restricted Boltzmann Machine [14, 13] is a recently introduced probabilistic mod el that has the ability to accurately model complex probability distributions over high-dimensional sequences."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "The data was pre proc ssed to be invariant to isometries [14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 4
                            }
                        ],
                        "text": "The Temporal Restricted Boltzmann Machine [14, 13] is a recently introduced probabilistic model that has the ability to accurately model complex probability distributions over high-dimensionalsequences."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 4
                            }
                        ],
                        "text": "See [18, 14] for more details and generalizations."
                    },
                    "intents": []
                }
            ],
            "corpusId": 125256517,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "576104ba976841628c67d2d794c9a64ba876eb87",
            "isKey": true,
            "numCitedBy": 307,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a non-linear generative model for human motion data that uses an undirected model with binary latent variables and real-valued \u201cvisible\u201d variables that represent joint angles. The latent and visible variables at each time step receive directed connections from the visible variables at the last few time-steps. Such an architecture makes on-line inference efficient and allows us to use a simple approximate learning procedure. After training, the model finds a single set of parameters that simultaneously capture several different kinds of motion. We demonstrate the power of our approach by synthesizing various motion sequences and by performing on-line filling in of data lost during motion capture."
            },
            "slug": "Modeling-Human-Motion-Using-Binary-Latent-Variables-Sch\u00f6lkopf-Platt",
            "title": {
                "fragments": [],
                "text": "Modeling Human Motion Using Binary Latent Variables"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "A non-linear generative model for human motion data that uses an undirected model with binary latent variables and real-valued \u201cvisible\u201d variables that represent joint angles that makes on-line inference efficient and allows for a simple approximate learning procedure."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144639556"
                        ],
                        "name": "Graham W. Taylor",
                        "slug": "Graham-W.-Taylor",
                        "structuredName": {
                            "firstName": "Graham",
                            "lastName": "Taylor",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Graham W. Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9330607"
                        ],
                        "name": "S. Roweis",
                        "slug": "S.-Roweis",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Roweis",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roweis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14962437,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "497a80b2813cffb17f46af50e621a71505094528",
            "isKey": false,
            "numCitedBy": 513,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a non-linear generative model for human motion data that uses an undirected model with binary latent variables and real-valued \"visible\" variables that represent joint angles. The latent and visible variables at each time step receive directed connections from the visible variables at the last few time-steps. Such an architecture makes on-line inference efficient and allows us to use a simple approximate learning procedure. After training, the model finds a single set of parameters that simultaneously capture several different kinds of motion. We demonstrate the power of our approach by synthesizing various motion sequences and by performing on-line filling in of data lost during motion capture."
            },
            "slug": "Modeling-Human-Motion-Using-Binary-Latent-Variables-Taylor-Hinton",
            "title": {
                "fragments": [],
                "text": "Modeling Human Motion Using Binary Latent Variables"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A non-linear generative model for human motion data that uses an undirected model with binary latent variables and real-valued \"visible\" variables that represent joint angles that makes on-line inference efficient and allows for a simple approximate learning procedure."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 205001834,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "052b1d8ce63b07fec3de9dbb583772d860b7c769",
            "isKey": false,
            "numCitedBy": 20335,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1."
            },
            "slug": "Learning-representations-by-back-propagating-errors-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning representations by back-propagating errors"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "Back-propagation repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector, which helps to represent important features of the task domain."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721860"
                        ],
                        "name": "M. Wainwright",
                        "slug": "M.-Wainwright",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Wainwright",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wainwright"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123333909"
                        ],
                        "name": "M.I. Jordan",
                        "slug": "M.I.-Jordan",
                        "structuredName": {
                            "firstName": "M.I.",
                            "lastName": "Jordan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M.I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 124
                            }
                        ],
                        "text": "In contrast, the statement h \u2190 P (H) means that each h is set to the real value P (H = 1), so this is a \u201cmean-field\u201d update [9, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207178945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d98d0d1900b13b87aa4ffd6b69c046beb63f0434",
            "isKey": false,
            "numCitedBy": 3901,
            "numCiting": 303,
            "paperAbstract": {
                "fragments": [],
                "text": "The formalism of probabilistic graphical models provides a unifying framework for capturing complex dependencies among random variables, and building large-scale multivariate statistical models. Graphical models have become a focus of research in many statistical, computational and mathematical fields, including bioinformatics, communication theory, statistical physics, combinatorial optimization, signal and image processing, information retrieval and statistical machine learning. Many problems that arise in specific instances \u2014 including the key problems of computing marginals and modes of probability distributions \u2014 are best studied in the general setting. Working with exponential family representations, and exploiting the conjugate duality between the cumulant function and the entropy for exponential families, we develop general variational representations of the problems of computing likelihoods, marginal probabilities and most probable configurations. We describe how a wide variety of algorithms \u2014 among them sum-product, cluster variational methods, expectation-propagation, mean field methods, max-product and linear programming relaxation, as well as conic programming relaxations \u2014 can all be understood in terms of exact or approximate forms of these variational representations. The variational approach provides a complementary alternative to Markov chain Monte Carlo as a general source of approximation methods for inference in large-scale statistical models."
            },
            "slug": "Graphical-Models,-Exponential-Families,-and-Wainwright-Jordan",
            "title": {
                "fragments": [],
                "text": "Graphical Models, Exponential Families, and Variational Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The variational approach provides a complementary alternative to Markov chain Monte Carlo as a general source of approximation methods for inference in large-scale statistical models."
            },
            "venue": {
                "fragments": [],
                "text": "Found. Trends Mach. Learn."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3198578"
                        ],
                        "name": "J. Yedidia",
                        "slug": "J.-Yedidia",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Yedidia",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Yedidia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30400079"
                        ],
                        "name": "Yair Weiss",
                        "slug": "Yair-Weiss",
                        "structuredName": {
                            "firstName": "Yair",
                            "lastName": "Weiss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yair Weiss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 121439686,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "05a281df21f4cb2b01e7751c50a4cba3ae0b992f",
            "isKey": false,
            "numCitedBy": 1568,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "\"Inference\" problems arise in statistical physics, computer vision, error-correcting coding theory, and AI. We explain the principles behind the belief propagation (BP) algorithm, which is an efficient way to solve inference problems based on passing local messages. We develop a unified approach, with examples, notation, and graphical models borrowed from the relevant disciplines.We explain the close connection between the BP algorithm and the Bethe approximation of statistical physics. In particular, we show that BP can only converge to a fixed point that is also a stationary point of the Bethe approximation to the free energy. This result helps explaining the successes of the BP algorithm and enables connections to be made with variational approaches to approximate inference.The connection of BP with the Bethe approximation also suggests a way to construct new message-passing algorithms based on improvements to Bethe's approximation introduced Kikuchi and others. The new generalized belief propagation (GBP) algorithms are significantly more accurate than ordinary BP for some problems. We illustrate how to construct GBP algorithms with a detailed example."
            },
            "slug": "Understanding-belief-propagation-and-its-Yedidia-Freeman",
            "title": {
                "fragments": [],
                "text": "Understanding belief propagation and its generalizations"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that BP can only converge to a fixed point that is also a stationary point of the Bethe approximation to the free energy, which enables connections to be made with variational approaches to approximate inference."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761978"
                        ],
                        "name": "D. Erhan",
                        "slug": "D.-Erhan",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Erhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Erhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760871"
                        ],
                        "name": "Aaron C. Courville",
                        "slug": "Aaron-C.-Courville",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Courville",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron C. Courville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32837403"
                        ],
                        "name": "J. Bergstra",
                        "slug": "J.-Bergstra",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bergstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bergstra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 157
                            }
                        ],
                        "text": "A good mod el f r these data sources could be useful for finding an abstract representation that is helpful for so lving \u201cnatural\u201d discrimination tasks (see [4, 7] for an example of this approach for the non-sequential case)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14805281,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8012351bc5ebce4a4b3039bbbba3ce393bc3315",
            "isKey": false,
            "numCitedBy": 973,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks."
            },
            "slug": "An-empirical-evaluation-of-deep-architectures-on-of-Larochelle-Erhan",
            "title": {
                "fragments": [],
                "text": "An empirical evaluation of deep architectures on problems with many factors of variation"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A series of experiments indicate that these models with deep architectures show promise in solving harder learning problems that exhibit many factors of variation."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398315116"
                        ],
                        "name": "M. Rosen-Zvi",
                        "slug": "M.-Rosen-Zvi",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Rosen-Zvi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rosen-Zvi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2388827,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2184fb6d32bc46f252b940035029273563c4fc82",
            "isKey": false,
            "numCitedBy": 502,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Directed graphical models with one layer of observed random variables and one or more layers of hidden random variables have been the dominant modelling paradigm in many research fields. Although this approach has met with considerable success, the causal semantics of these models can make it difficult to infer the posterior distribution over the hidden variables. In this paper we propose an alternative two-layer model based on exponential family distributions and the semantics of undirected models. Inference in these \"exponential family harmoniums\" is fast while learning is performed by minimizing contrastive divergence. A member of this family is then studied as an alternative probabilistic model for latent semantic indexing. In experiments it is shown that they perform well on document retrieval tasks and provide an elegant solution to searching with keywords."
            },
            "slug": "Exponential-Family-Harmoniums-with-an-Application-Welling-Rosen-Zvi",
            "title": {
                "fragments": [],
                "text": "Exponential Family Harmoniums with an Application to Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An alternative two-layer model based on exponential family distributions and the semantics of undirected models is proposed, which performs well on document retrieval tasks and provides an elegant solution to searching with keywords."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1701422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d7d0e8c4791700defd4b0df82a26b50055346e0",
            "isKey": false,
            "numCitedBy": 8758,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in \"blind\" signal processing."
            },
            "slug": "An-Information-Maximization-Approach-to-Blind-and-Bell-Sejnowski",
            "title": {
                "fragments": [],
                "text": "An Information-Maximization Approach to Blind Separation and Blind Deconvolution"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is suggested that information maximization provides a unifying framework for problems in \"blind\" signal processing and dependencies of information transfer on time delays are derived."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721860"
                        ],
                        "name": "M. Wainwright",
                        "slug": "M.-Wainwright",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Wainwright",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wainwright"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701607"
                        ],
                        "name": "A. Willsky",
                        "slug": "A.-Willsky",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Willsky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Willsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 2
                            }
                        ],
                        "text": ", [16, 19], including [15], which w orks well for the RBM)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5749684,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "f178383deb578992b2a62844a08a6451cbad16ed",
            "isKey": false,
            "numCitedBy": 453,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new class of upper bounds on the log partition function of a Markov random field (MRF). This quantity plays an important role in various contexts, including approximating marginal distributions, parameter estimation, combinatorial enumeration, statistical decision theory, and large-deviations bounds. Our derivation is based on concepts from convex duality and information geometry: in particular, it exploits mixtures of distributions in the exponential domain, and the Legendre mapping between exponential and mean parameters. In the special case of convex combinations of tree-structured distributions, we obtain a family of variational problems, similar to the Bethe variational problem, but distinguished by the following desirable properties: i) they are convex, and have a unique global optimum; and ii) the optimum gives an upper bound on the log partition function. This optimum is defined by stationary conditions very similar to those defining fixed points of the sum-product algorithm, or more generally, any local optimum of the Bethe variational problem. As with sum-product fixed points, the elements of the optimizing argument can be used as approximations to the marginals of the original model. The analysis extends naturally to convex combinations of hypertree-structured distributions, thereby establishing links to Kikuchi approximations and variants."
            },
            "slug": "A-new-class-of-upper-bounds-on-the-log-partition-Wainwright-Jaakkola",
            "title": {
                "fragments": [],
                "text": "A new class of upper bounds on the log partition function"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A new class of upper bounds on the log partition function of a Markov random field (MRF) is introduced, based on concepts from convex duality and information geometry, and the Legendre mapping between exponential and mean parameters is exploited."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746662"
                        ],
                        "name": "D. Sontag",
                        "slug": "D.-Sontag",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Sontag",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sontag"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", [16, 12, 19], including [15], which works well for the RBM)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2185784,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5bb686506dd11caa959244038e134cf16c8807be",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We give a new class of outer bounds on the marginal polytope, and propose a cutting-plane algorithm for efficiently optimizing over these constraints. When combined with a concave upper bound on the entropy, this gives a new variational inference algorithm for probabilistic inference in discrete Markov Random Fields (MRFs). Valid constraints on the marginal polytope are derived through a series of projections onto the cut polytope. As a result, we obtain tighter upper bounds on the log-partition function. We also show empirically that the approximations of the marginals are significantly more accurate when using the tighter outer bounds. Finally, we demonstrate the advantage of the new constraints for finding the MAP assignment in protein structure prediction."
            },
            "slug": "New-Outer-Bounds-on-the-Marginal-Polytope-Sontag-Jaakkola",
            "title": {
                "fragments": [],
                "text": "New Outer Bounds on the Marginal Polytope"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A new class of outer bounds on the marginal polytope is given, and a cutting-plane algorithm for efficiently optimizing over these constraints is proposed, which gives a new variational inference algorithm for probabilistic inference in discrete Markov Random Fields (MRFs)."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1658773,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e",
            "isKey": false,
            "numCitedBy": 14645,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such \u201cautoencoder\u201d networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data."
            },
            "slug": "Reducing-the-Dimensionality-of-Data-with-Neural-Hinton-Salakhutdinov",
            "title": {
                "fragments": [],
                "text": "Reducing the Dimensionality of Data with Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work describes an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779592"
                        ],
                        "name": "G. Lakemeyer",
                        "slug": "G.-Lakemeyer",
                        "structuredName": {
                            "firstName": "Gerhard",
                            "lastName": "Lakemeyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lakemeyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145304209"
                        ],
                        "name": "B. Nebel",
                        "slug": "B.-Nebel",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Nebel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Nebel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 2
                            }
                        ],
                        "text": ", [16, 19], including [15], which w orks well for the RBM)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 58737134,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "329ea533c0834195fc43f0d7c523c5ac7b0b0001",
            "isKey": false,
            "numCitedBy": 623,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface Chapter 1 Robotic Mapping: A Survey Sebastian Thrun Chapter 2 D-Learning: What Learning in Dogs Tells Us About Building Characters That Learn What They Ought to Learn Bruce Blumberg Chapter 3 Identifying Semantic Relations in Text Daniel Gildea and Daniel Jurafsky Chapter 4 Planning with Generic Types Derek Long and Maria Fox Chapter 5 Bayesian Inference of Visual Motion Boundaries David J. Fleet, Michael J. Black, and Oscar Nestares, CSIC Chapter 6 Qualitative Spatio-Temporal Representation and Reasoning: A Computational Perspective Frank Wolter and Michael Zakharyaschev Chapter 7 Extending Virtual Humans to Support Team Training in Virtual Reality Jeff Rickel and W. Lewis Johnson Chapter 8 Understanding Belief Propagation and Its Generalizations Jonathan Yedidia, William T. Freeman, and Yair Weiss Chapter 9 Learning Theory and Language Modeling David McAllester and Robert E. Schapire Chapter 10 A First-Order-Logic Davis-Putnam-Logemann-Loveland Procedure Peter Baumgartner Chapter 11 New Tractable Constraint Classes from Old David Cohen, Peter Jeavons, and Richard Gault Chapter 12 User-Oriented Evaluation Methods for Information Retrieval: A Case Study Based on Conceptual Models for Query Expansion Jaana Kekalainen and Kalervo Jarvelin Chapter 13 Data Mining for Manufacturing Control: An Application in Optimizing IC Test Tony Fountain, Thomas Dietterich"
            },
            "slug": "Exploring-artificial-intelligence-in-the-new-Lakemeyer-Nebel",
            "title": {
                "fragments": [],
                "text": "Exploring artificial intelligence in the new millennium"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The aim of this book is to contribute towards the development of a coherent model of human-computer interaction that can be applied to solve the challenge of integrating human and machine interaction in the rapidly changing environment."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13618539,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5",
            "isKey": false,
            "numCitedBy": 24804,
            "numCiting": 98,
            "paperAbstract": {
                "fragments": [],
                "text": "The fabric comprises a novel type of netting which will have particular utility in screening out mosquitoes and like insects and pests. The fabric is defined of voids having depth as well as width and length. The fabric is usable as a material from which to form clothing for wear, or bed coverings, or sleeping bags, etc., besides use simply as a netting."
            },
            "slug": "A-Tutorial-on-Hidden-Markov-Models-and-Selected-Rabiner",
            "title": {
                "fragments": [],
                "text": "A Tutorial on Hidden Markov Models and Selected Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The fabric comprises a novel type of netting which will have particular utility in screening out mosquitoes and like insects and pests."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47804034"
                        ],
                        "name": "Meng-Chang Lee",
                        "slug": "Meng-Chang-Lee",
                        "structuredName": {
                            "firstName": "Meng-Chang",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Meng-Chang Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 71
                            }
                        ],
                        "text": "In its simplest form, the TRBM can be viewed as a Hidden Markov Model (HMM) [10] with an exponentially large state space that has an extremely compact parameterization of the transition and the emission probabilities."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 198
                            }
                        ],
                        "text": "The TRBM defines a probability distributionP (V T1 = v T 1 , H T 1 = h T 1 ) by the equation\nP (vT1 , h T 1 ) =\nT\u220f\nt=2\nP (vt, ht|ht\u22121)P0(v1, h1) (4)\nwhich is identical to the defining equation of the HMM."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 41
                            }
                        ],
                        "text": "be viewed as a Hidden Markov Model (HMM) [9] with an exponentially large state space that has an extremely compact parameterization of the transition and the emission probabilities."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 50096699,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc08562b7757f6be3bc97ab694f738e25065f4bb",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "|Full Text: PDF (444 KB) 11. A survey on sensor networks Akyildiz, I.F.; Weilian Su; Sankarasubramaniam, Y.; Cayirci, E. Communications Magazine, IEEE Volume 40, Issue 8, Date: Aug 2002, Pages: 102114 Digital Object Identifier 10.1109/MCOM.2002.1024422 Abstract |Full Text: PDF (990 KB) 12. Realization of the next-generation network Chae-Sub Lee; Knight, D. Communications Magazine, IEEE Volume 43, Issue 10, Date: Oct. 2005, Pages: 3441 Digital Object Identifier 10.1109/MCOM.2005.1522122 Abstract |Full Text: PDF (151 KB) 13. A survey on wireless mesh networks Akyildiz, I.F.; Xudong Wang Communications Magazine, IEEE Volume 43, Issue 9, Date: Sept. 2005, Pages: S23S30 Digital Object Identifier 10.1109/MCOM.2005.1509968 Abstract |Full Text: PDF (138 KB) 14. A 2.4 GHz CMOS sub-sampling mixer with integrated filtering Pekau, H.; Haslett, J.W. Solid-State Circuits, IEEE Journal of Volume 40, Issue 11, Date: Nov. 2005, Pages: 21592166 Digital Object Identifier 10.1109/JSSC.2005.857364 Abstract |Full Text: PDF (784 KB) Page 2 of 14 Top Articles 12/15/2005 http://ieeexplore.ieee.org/Xplore/toparticles.jsp 15. TDCS, OFDM, and MC-CDMA: a brief tutorial Chakravarthy, V.; Nunez, A.S.; Stephens, J.P.; Shaw, A.K.; Temple, M.A. Communications Magazine, IEEE Volume 43, Issue 9, Date: Sept. 2005, Pages: S11S16 Digital Object Identifier 10.1109/MCOM.2005.1509966 Abstract |Full Text: PDF (128 KB) 16. A simple transmit diversity technique for wireless communications Alamouti, S.M. Selected Areas in Communications, IEEE Journal on Volume 16, Issue 8, Date: Oct 1998, Pages: 1451-1458 Digital Object Identifier 10.1109/49.730453 Abstract |Full Text: PDF (224 KB) 17. Why software fails [software failure] Charette, R.N. Spectrum, IEEE Volume 42, Issue 9, Date: Sept. 2005, Pages: 4249 Digital Object Identifier 10.1109/MSPEC.2005.1502528 Abstract |Full Text: PDF (9468 KB) 18. A road to future broadband wireless access: MIMO-OFDM-Based air interface Hongwei Yang Communications Magazine, IEEE Volume 43, Issue 1, Date: Jan. 2005, Pages: 5360 Digital Object Identifier 10.1109/MCOM.2005.1381875(410) 4 Abstract |Full Text: PDF (524 KB) 19. Remotely powered addressable UHF RFID integrated system Curty, J.-P.; Joehl, N.; Dehollain, C.; Declercq, M.J. Solid-State Circuits, IEEE Journal of Volume 40, Issue 11, Date: Nov. 2005, Pages: 21932202 Digital Object Identifier 10.1109/JSSC.2005.857352 Abstract |Full Text: PDF (728 KB) 20. IEEE standard for software test documentation IEEE Std 829-1998 Volume , Issue , Date: 16 Dec 1998, Pages: Abstract |Full Text: PDF (592 KB)|Full Text: PDF (592 KB) 21. Generalized principal component analysis (GPCA) Vidal, R.; Yi Ma; Sastry, S. Pattern Analysis and Machine Intelligence, IEEE Transactions on Volume 27, Issue 12, Date: Dec. 2005, Pages: 19451959 Digital Object Identifier 10.1109/TPAMI.2005.244 Abstract |Full Text: PDF (872 KB) 22. NGN architecture: generic principles, functional architecture, and implementation Knightson, K.; Morita, N.; Towle, T. Communications Magazine, IEEE Volume 43, Issue 10, Date: Oct. 2005, Pages: 4956 Digital Object Identifier 10.1109/MCOM.2005.1522124 Abstract |Full Text: PDF (131 KB) Page 3 of 14 Top Articles 12/15/2005 http://ieeexplore.ieee.org/Xplore/toparticles.jsp 23. An auto-I/Q calibrated CMOS transceiver for 802.11g Yong-Hsiang Hsieh; Wei-Yi Hu; Shin-Ming Lin; Chao-Liang Chen; Wen-Kai Li; Sao-Jie Chen; Chen, D.J. Solid-State Circuits, IEEE Journal of Volume 40, Issue 11, Date: Nov. 2005, Pages: 21872192 Digital Object Identifier 10.1109/JSSC.2005.857348 Abstract |Full Text: PDF (1080 KB) 24. IEEE Standard for Local and Metropolitan Area Networks Part 16: Air Interface for Fixed Broadband Wireless Access Systems IEEE Std 802.16-2004 (Revision of IEEE Std 802.16-2001) Volume , Issue , Date: 2004, Pages: 0_1857 Abstract |Full Text: PDF (6002 KB)|Full Text: PDF (6002 KB) 25. IEEE standard for information technologytelecommunications and information exchange between systemslocal and metropolitan area networksspecific require Part II: wireless LAN medium access control (MAC) and physical layer (PHY) specifications IEEE Std 802.11g-2003 (Amendment to IEEE Std 802.11, 1999 Edn. (Reaff 2003) as amended by IEEE Stds 802.11a-1999, 802.11b-1999, 802.11b-1999/Cor 1-2001, and 802 2001) Volume , Issue , Date: 2003, Pages: i67 Abstract |Full Text: PDF (1754 KB)|Full Text: PDF (1754 KB) 26. A 10 Gb/s BiCMOS adaptive cable equalizer Guangyu Evelina Zhang; Green, M.M. Solid-State Circuits, IEEE Journal of Volume 40, Issue 11, Date: Nov. 2005, Pages: 21322140 Digital Object Identifier 10.1109/JSSC.2005.857354 Abstract |Full Text: PDF (1304 KB) 27. A 1.2-V-only 900-mW 10 gb ethernet transceiver and XAUI interface with robust VCO tuning technique Hyung-Rok Lee; Moon-Sang Hwang; Bong-Joon Lee; Young-Deok Kim; Dohwan Oh; Jaeha Kim; Sang-Hyun Lee; Deog-Kyoon Jeong; Kim, W. Solid-State Circuits, IEEE Journal of Volume 40, Issue 11, Date: Nov. 2005, Pages: 21482158 Digital Object Identifier 10.1109/JSSC.2005.857360 Abstract |Full Text: PDF (2384 KB) 28. Differential current-mode sensing for efficient on-chip global signaling Tzartzanis, N.; Walker, W.W. Solid-State Circuits, IEEE Journal of Volume 40, Issue 11, Date: Nov. 2005, Pages: 21412147 Digital Object Identifier 10.1109/JSSC.2005.857351 Abstract |Full Text: PDF (672 KB) 29. IEEE Std 802.11-1997 Information Technologytelecommunications And Information exchange Between Systems-Local And Metropolitan Area Networks-specific Requirements-part 11: Wireless Lan Medium Access Control (MAC) And Physical Layer (PHY) Specifications IEEE Std 802.11-1997 Volume , Issue , Date: 18 Nov 1997, Pages: i-445 Abstract |Full Text: PDF (25764 KB)|Full Text: PDF (25764 KB) 30. QoS routing for wireless ad hoc networks: problems, algorithms, and protocols Baoxian Zhang; Mouftah, H.T. Communications Magazine, IEEE Page 4 of 14 Top Articles 12/15/2005 http://ieeexplore.ieee.org/Xplore/toparticles.jsp Volume 43, Issue 10, Date: Oct. 2005, Pages: 110117 Digital Object Identifier 10.1109/MCOM.2005.1522133 Abstract |Full Text: PDF (141 KB) 31. On the performance analysis of the minimum-blocking and bandwidth-reallocation channel-assignment (MBCA/BRCA) methods for quality-of-service routing suppo mobile multimedia ad hoc networks Aggelou, G. Vehicular Technology, IEEE Transactions on Volume 53, Issue 3, Date: May 2004, Pages: 770782 Digital Object Identifier 10.1109/TVT.2004.825696 Abstract |Full Text: PDF (1368 KB) 32. Standards for the next-generation network Maeda, Y.; Moore, B. Communications Magazine, IEEE Volume 43, Issue 10, Date: Oct. 2005, Pages: 3333 Digital Object Identifier 10.1109/MCOM.2005.1522121 Full Text: PDF (287 KB) 33. A monotonic digitally controlled delay element Maymandi-Nejad, M.; Sachdev, M. Solid-State Circuits, IEEE Journal of Volume 40, Issue 11, Date: Nov. 2005, Pages: 22122219 Digital Object Identifier 10.1109/JSSC.2005.857370 Abstract |Full Text: PDF (560 KB) 34. RFID privacy: an overview of problems and proposed solutions Garfinkel, S.L.; Juels, A.; Pappu, R. Security & Privacy Magazine, IEEE Volume 3, Issue 3, Date: May-June 2005, Pages: 3443 Digital Object Identifier 10.1109/MSP.2005.78 Abstract |Full Text: PDF (1520 KB) 35. Mobility support for IP-Based networks Jie Li; Hsiao-Hwa Chen Communications Magazine, IEEE Volume 43, Issue 10, Date: Oct. 2005, Pages: 127132 Digital Object Identifier 10.1109/MCOM.2005.1522136 Abstract |Full Text: PDF (124 KB) 36. Comparison frequency doubling and charge pump matching techniques for dual-band /spl Delta//spl Sigma/ fractional-N frequency synthesizer Hyungki Huh; Yido Koo; Kang-Yoon Lee; Yeonkyeong Ok; Sungho Lee; Daehyun Kwon; Jeongwoo Lee; Joonbae Park; Kyeongho Lee; Deog-Kyoon Jeong; Kim, W. Solid-State Circuits, IEEE Journal of Volume 40, Issue 11, Date: Nov. 2005, Pages: 22282236 Digital Object Identifier 10.1109/JSSC.2005.857368 Abstract |Full Text: PDF (984 KB) 37. Survey of clustering algorithms Rui Xu; Wunsch, D., II Neural Networks, IEEE Transactions on Volume 16, Issue 3, Date: May 2005, Pages: 645678 Digital Object Identifier 10.1109/TNN.2005.845141 Abstract |Full Text: PDF (1544 KB) 38. Who killed the virtual case file? [case management software] Page 5 of 14 Top Articles 12/15/2005 http://ieeexplore.ieee.org/Xplore/toparticles.jsp Goldstein, H. Spectrum, IEEE Volume 42, Issue 9, Date: Sept. 2005, Pages: 2435 Digital Object Identifier 10.1109/MSPEC.2005.1502526 Abstract |Full Text: PDF (11485 KB) 39. MOS operational amplifier design-a tutorial overview Gray, P.R.; Meyer, R.G. Solid-State Circuits, IEEE Journal of Volume 17, Issue 6, Date: Dec 1982, Pages: 969982 Abstract |Full Text: PDF (1536 KB)|Full Text: PDF (1536 KB) 40. Broadband MIMO-OFDM wireless communications Stuber, G.L.; Barry, J.R.; McLaughlin, S.W.; Ye Li; Ingram, M.A.; Pratt, T.G. Proceedings of the IEEE Volume 92, Issue 2, Date: Feb 2004, Pages: 271294 Digital Object Identifier 10.1109/JPROC.2003.821912 Abstract |Full Text: PDF (1200 KB)| Full Text: HTML 41. SOA without Web services: a pragmatic implementation of SOA for financial transactions systems Duan, Z.; Bose, S.; Stirpe, P.A.; Shoniregun, C.; Logvynovskiy, A. Services Computing, 2005 IEEE International Conference on Volume 1, Issue , Date: 11-15 July 2005, Pages: 243250 vol.1 Digital Object Identifier 10.1109/SCC.2005.94 Abstract |Full Text: PDF (296 KB) 42. Overview of the H.264/AVC video coding standard Wiegand, T.; Sullivan, G.J.; Bjntegaard, G.; Luthra, A. Circuits and Systems for Video Technology, IEEE Transactions on Volume 13, Issue 7, Date: July 2003, Pages: 560576 Digital Object Identifier 10.1109/TCSVT.2003.815165 Abstract |Full Text: PDF (904 KB) 43. Applications of RFID technology Raza, N.; Bradshaw, V.; Hague, M. RFID Technology (Ref. No. 1999/123), IEE Colloquium on Volume , Issue , Date: 1999, Pages: 1/1-1/5 Abstract |Full Text: PDF (160 KB)|Full Text: PDF (160 KB) 44. Ultra-wideband communications: an idea whose time has come Liuqing Yang; Giannakis,"
            },
            "slug": "Top-100-Documents-Browse-Search-Ieee-Xplore-Guide-a-Lee",
            "title": {
                "fragments": [],
                "text": "Top 100 Documents Browse Search Ieee Xplore Guide Support Top 100 Documents Accessed: Nov 2005 a Tutorial on Hidden Markov Models and Selected Applications Inspeech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A road to future broadband wireless access: MIMO-OFDM-Based air interface Hongwei Yang Communications Magazine, IEEE Volume 43, Issue 1, Date: Jan. 2005, Pages: 5360 Digital Object Identifier 10.1109/MCOM.2005."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 48
                            }
                        ],
                        "text": "If delay-taps are a llowed, then the results of [14, 13] show that there is little benefit from the hidden-to-hidden c o nections (which are W ), making the comparison between the RTRBM and the TRBM uninteresting."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 4
                            }
                        ],
                        "text": "The Temporal Restricted Boltzmann Machine (TRBM) is a probabilistic model for sequences that is able to successfully model (i.e., generate nice-looking samples of) several very high dimensional sequences, such as motioncapture data and the pixels of low resolution videos of balls bouncing in a box."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 128
                            }
                        ],
                        "text": "It was shown to be able to generate realistic motion capture data [14], and low resolu tion videos of 2 balls bouncing in a box [13], as well as complete and denoise such sequences."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 199
                            }
                        ],
                        "text": "To demonstrate that th e RTRBM learns to use the hidden units to store information, we did not use delay-taps for the RTRBM no r the TRBM, which causes the results to be worse (but not much) than in [14, 13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 42
                            }
                        ],
                        "text": "The Temporal Restricted Boltzmann Machine [14, 13] is a recently introduced probabilistic mod el that has the ability to accurately model complex probability distributions over high-dimensional sequences."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "The approximate inference procedure used in [13] was heuristic and was not even derived from a variational principle."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 4
                            }
                        ],
                        "text": "The Temporal Restricted Boltzmann Machine [14, 13] is a recently introduced probabilistic model that has the ability to accurately model complex probability distributions over high-dimensionalsequences."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 79
                            }
                        ],
                        "text": "The real-values in the vide os are the conditional probabilities of the pixels [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 187
                            }
                        ],
                        "text": "The statement h \u223c P (H) means that h is sampled from the factorial distribution P (H), so eachh is set to1 with (2)This is a slightly simplified description of the inference procedure in [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 72
                            }
                        ],
                        "text": "This difficulty necessitated the use of a heuristic inference procedure [13], which is based on the observation that the distributionP (Ht|h t\u22121 1 , v t 1) = P (Ht|ht\u22121, vt) is factorial by definition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 15
                            }
                        ],
                        "text": "The results in [14, 13] were obtained using TRBMs that had several delay-taps, which mea ns that each hidden unit could directly observe several previous timesteps."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning multilevel distributed repre sentations for high-dimensional sequences.Proceeding of the Eleventh International Conference on Artificial Intellige  nce and Statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 74
                            }
                        ],
                        "text": "This is useful because it is easyto compute gradients with respect to the RNN\u2019s parameters using the backpropagation through time algorithm [11]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 90
                            }
                        ],
                        "text": "The backpropagation thorugh time is simply the usual backpropagation algorithm applied to RNNs. Specifically, the algorithm first computes all ofrt, and then computes\u2202O/\u2202rt, which is a sum of on the contribution from the future timesteps and the contribu ion from the current timestep."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 160
                            }
                        ],
                        "text": "However, this disadvantage is common to many other probabilistic models, and it can be partially alleviated using techniques such as the long short term memory RNN [6]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "This is a valid definition of an RNN whose cumulative objective for the sequencevT1 is\nO = T\u2211\nt=1\nlog Q(vt|Ht\u22121 = rt\u22121) (10)\nwhereQ(v1|H0 = r0) = Q0(v1)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 89
                            }
                        ],
                        "text": "The equalityQ(Vt|v t\u22121 1 ) = Q(Vt|h(v)t\u22121) allows us to define a recurrent neural network (RNN) [11] whose parameters are identical to those of the RTRBM, and whose cost function is equal to the log likelihood of the RTRBM."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 96
                            }
                        ],
                        "text": "The equalityQ(Vt|v t\u22121 1 ) = Q(Vt|h(v)t\u22121) allows us to define a recurrent neural network (RNN) [11] whose parameters are identical to those of the RTRBM, an d whose cost function is equal to the log likelihood of the RTRBM."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "This means that \u2207O = \u2207 log Q(v 1 ) can be computed with the backpropagation through time algorithm [11], where the contribution of the gradient from each timestep is computed with Contrastive Divergence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "The RNN has a pair of variables at each timestep,{(vt, rt)}Tt=1, wherevt are the input variables andrt are the RNN\u2019s hidden variables (all of which are deterministic)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "The RNN attempts to probabilistically predict the next timestep from its history using the marginal distribution of the RBMQ(Vt+1|Ht = rt), so its objective function at timet is defined to belog Q(vt+1|Ht = rt), whereQ depends on the RNN\u2019s parameters\nin the same way it depends on the RTRBM\u2019s parameters (recall that the two sets of parameters being identical)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 142
                            }
                        ],
                        "text": "This is useful because it is easy to compute gradients with respect to the RNN\u2019s parameters using the backpropagation through tim e algorithm [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learni ng representations by back-propagating"
            },
            "venue": {
                "fragments": [],
                "text": "errors. Nature,"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "This means that \u2207O = \u2207 log Q(v 1 ) can be computed with the backpropagation through time algorithm [10], where the contribution of the gradient from each timestep is computed with Contrastive Divergence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 74
                            }
                        ],
                        "text": "This is useful because it is easyto compute gradients with respect to the RNN\u2019s parameters using the backpropagation through time algorithm [11]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 90
                            }
                        ],
                        "text": "The backpropagation thorugh time is simply the usual backpropagation algorithm applied to RNNs. Specifically, the algorithm first computes all ofrt, and then computes\u2202O/\u2202rt, which is a sum of on the contribution from the future timesteps and the contribu ion from the current timestep."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 160
                            }
                        ],
                        "text": "However, this disadvantage is common to many other probabilistic models, and it can be partially alleviated using techniques such as the long short term memory RNN [6]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "This is a valid definition of an RNN whose cumulative objective for the sequencevT1 is\nO = T\u2211\nt=1\nlog Q(vt|Ht\u22121 = rt\u22121) (10)\nwhereQ(v1|H0 = r0) = Q0(v1)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 89
                            }
                        ],
                        "text": "The equalityQ(Vt|v t\u22121 1 ) = Q(Vt|h(v)t\u22121) allows us to define a recurrent neural network (RNN) [11] whose parameters are identical to those of the RTRBM, and whose cost function is equal to the log likelihood of the RTRBM."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 96
                            }
                        ],
                        "text": "The equalityQ(Vt|v t\u22121 1 ) = Q(Vt|h(v)t\u22121) allows us to define a recurrent neural network (RNN) [10] whose parameters are identical to those of the RTRBM, an d whose cost function is equal to the log likelihood of the RTRBM."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 142
                            }
                        ],
                        "text": "This is useful because it is easy to compute gradients with respect to the RNN\u2019s parameters using the backpropagation through tim e algorithm [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "The RNN has a pair of variables at each timestep,{(vt, rt)}Tt=1, wherevt are the input variables andrt are the RNN\u2019s hidden variables (all of which are deterministic)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "The RNN attempts to probabilistically predict the next timestep from its history using the marginal distribution of the RBMQ(Vt+1|Ht = rt), so its objective function at timet is defined to belog Q(vt+1|Ht = rt), whereQ depends on the RNN\u2019s parameters\nin the same way it depends on the RTRBM\u2019s parameters (recall that the two sets of parameters being identical)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning represe ntations by back-propagating"
            },
            "venue": {
                "fragments": [],
                "text": "errors. Nature,"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "The realvalues in the videos are the conditional probabilities of th e pixels [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 199
                            }
                        ],
                        "text": "To demonstrate that the RTRBM lear ns to use the hidden units to store information, we did not use delay-taps for the RTRBM nor the T RBM, which causes the results to be worse (but not much) than in [14, 13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 48
                            }
                        ],
                        "text": "If delay-taps are a llowed, then the results of [14, 13] show that there is little benefit from the hidden-to-hidden c o nections (which are W ), making the comparison between the RTRBM and the TRBM uninteresting."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 4
                            }
                        ],
                        "text": "The Temporal Restricted Boltzmann Machine (TRBM) is a probabilistic model for sequences that is able to successfully model (i.e., generate nice-looking samples of) several very high dimensional sequences, such as motioncapture data and the pixels of low resolution videos of balls bouncing in a box."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 128
                            }
                        ],
                        "text": "It was shown to be able to generate realistic motion capture data [14], and low resolu tion videos of 2 balls bouncing in a box [13], as well as complete and denoise such sequences."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 15
                            }
                        ],
                        "text": "The results in [14, 13] were obtained using TRBMs that had several delay-taps, so each hi dden unit could directly observe several previous timesteps."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 42
                            }
                        ],
                        "text": "The Temporal Restricted Boltzmann Machine [14, 13] is a recently introduced probabilistic mod el that has the ability to accurately model complex probability distributions over high-dimensional sequences."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "As a result, the approximate inference procedure used in [13] was heuris tic and was not even derived from a variational principle."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 4
                            }
                        ],
                        "text": "The Temporal Restricted Boltzmann Machine [14, 13] is a recently introduced probabilistic model that has the ability to accurately model complex probability distributions over high-dimensionalsequences."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning multilevel dist  ributed representations for high-dimensional sequences.Proceeding of the Eleventh International Conference on Art  ificial Intelligence and Statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 74
                            }
                        ],
                        "text": "This is useful because it is easyto compute gradients with respect to the RNN\u2019s parameters using the backpropagation through time algorithm [11]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 90
                            }
                        ],
                        "text": "The backpropagation thorugh time is simply the usual backpropagation algorithm applied to RNNs. Specifically, the algorithm first computes all ofrt, and then computes\u2202O/\u2202rt, which is a sum of on the contribution from the future timesteps and the contribu ion from the current timestep."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 160
                            }
                        ],
                        "text": "However, this disadvantage is common to many other probabilistic models, and it can be partially alleviated using techniques such as the long short term memory RNN [6]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "This is a valid definition of an RNN whose cumulative objective for the sequencevT1 is\nO = T\u2211\nt=1\nlog Q(vt|Ht\u22121 = rt\u22121) (10)\nwhereQ(v1|H0 = r0) = Q0(v1)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 89
                            }
                        ],
                        "text": "The equalityQ(Vt|v t\u22121 1 ) = Q(Vt|h(v)t\u22121) allows us to define a recurrent neural network (RNN) [11] whose parameters are identical to those of the RTRBM, and whose cost function is equal to the log likelihood of the RTRBM."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 165
                            }
                        ],
                        "text": "However, this disadvantage is common to many other probabilistic models, and it can be partially all eviated using techniques such as the long short term memory RNN [6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "The RNN has a pair of variables at each timestep,{(vt, rt)}Tt=1, wherevt are the input variables andrt are the RNN\u2019s hidden variables (all of which are deterministic)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "The RNN attempts to probabilistically predict the next timestep from its history using the marginal distribution of the RBMQ(Vt+1|Ht = rt), so its objective function at timet is defined to belog Q(vt+1|Ht = rt), whereQ depends on the RNN\u2019s parameters\nin the same way it depends on the RTRBM\u2019s parameters (recall that the two sets of parameters being identical)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Long Short-Term Memor  y"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "This means that \u2207O = \u2207 log Q(v 1 ) can be computed with the backpropagation through time algorithm [10], where the contribution of the gradient from each timestep is computed with Contrastive Divergence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 96
                            }
                        ],
                        "text": "The equalityQ(Vt|v t\u22121 1 ) = Q(Vt|h(v)t\u22121) allows us to define a recurrent neural network (RNN) [10] whose parameters are identical to those of the RTRBM, and whose cost function is equal to the log likelihood of the RTRBM."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 74
                            }
                        ],
                        "text": "This is useful because it is easyto compute gradients with respect to the RNN\u2019s parameters using the backpropagation through time algorithm [11]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 90
                            }
                        ],
                        "text": "The backpropagation thorugh time is simply the usual backpropagation algorithm applied to RNNs. Specifically, the algorithm first computes all ofrt, and then computes\u2202O/\u2202rt, which is a sum of on the contribution from the future timesteps and the contribu ion from the current timestep."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 160
                            }
                        ],
                        "text": "However, this disadvantage is common to many other probabilistic models, and it can be partially alleviated using techniques such as the long short term memory RNN [6]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 141
                            }
                        ],
                        "text": "This is useful because it is easy to compute gradients with respect to the RNN\u2019s parameters using the backpropagation through time algorithm [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "This is a valid definition of an RNN whose cumulative objective for the sequencevT1 is\nO = T\u2211\nt=1\nlog Q(vt|Ht\u22121 = rt\u22121) (10)\nwhereQ(v1|H0 = r0) = Q0(v1)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 89
                            }
                        ],
                        "text": "The equalityQ(Vt|v t\u22121 1 ) = Q(Vt|h(v)t\u22121) allows us to define a recurrent neural network (RNN) [11] whose parameters are identical to those of the RTRBM, and whose cost function is equal to the log likelihood of the RTRBM."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "The RNN has a pair of variables at each timestep,{(vt, rt)}Tt=1, wherevt are the input variables andrt are the RNN\u2019s hidden variables (all of which are deterministic)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "The RNN attempts to probabilistically predict the next timestep from its history using the marginal distribution of the RBMQ(Vt+1|Ht = rt), so its objective function at timet is defined to belog Q(vt+1|Ht = rt), whereQ depends on the RNN\u2019s parameters\nin the same way it depends on the RTRBM\u2019s parameters (recall that the two sets of parameters being identical)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning representations by back-propagating"
            },
            "venue": {
                "fragments": [],
                "text": "errors. Nature,"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 80
                            }
                        ],
                        "text": "We will approximate the gradients with respect to the RBM\u2019s parameters using the Contrastive Divergence [3] learning procedure, CDn, whose updates are computed by the following algorithm."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 74
                            }
                        ],
                        "text": "Models learned by CD 1 are often reasonable generative models of the data [3], but i f learning is continued with CD25, the resulting generative models are much better [11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 13
                            }
                        ],
                        "text": "The Temporal Restricted Boltzmann Machine (TRBM) is a probabilistic model for sequences that is able to successfully model (i.e., generate nice-looking samples of) several very high dimensional sequences, such as motioncapture data and the pixels of low resolution videos of balls bouncing in a box."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 95
                            }
                        ],
                        "text": "The learning consists of learning a conditional RBM at each timestep, which is easilydone with Contrastive Divergence (CD) [3]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 175
                            }
                        ],
                        "text": "This means that\u2207O = \u2207 log Q(vT1 ) can be computed with the backpropagation through time algorithm [11], where the contribution of the gradient from eachtimestep is computed with Contrastive Divergence."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 131
                            }
                        ],
                        "text": "As a probabilistic model, the TRBM is a directed graphical mo del consisting of a sequence of Restricted Boltzmann Machines (RBMs) [3], where the state of o ne r more previous RBMs determines the biases of the RBM in next timestep."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 168
                            }
                        ],
                        "text": "Despite the similarity,exact inference is very easy in the RTRBM and computing the gradient of the log likelihood is feasible (up to the error introduced by the use of Contrastive Divergence)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 13
                            }
                        ],
                        "text": "The Temporal Restricted Boltzmann Machine [14, 13] is a recently introduced probabilistic model that has the ability to accurately model complex probability distributions over high-dimensionalsequences."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 93
                            }
                        ],
                        "text": "As a probabilistic model, the TRBM is a directed graphical model consisting of a sequence of Restricted Boltzmann Machines (RBMs) [3], where the state of one r more previous RBMs determines the biases of the RBM in next timestep."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 124
                            }
                        ],
                        "text": "The learning consists of learning a conditional RBM at each timestep, which is easily done with Contrastive Divergence (CD) [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 81
                            }
                        ],
                        "text": "The building block of the TRBM and the RTRBM is the Restricted Boltzmann Machine [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 105
                            }
                        ],
                        "text": "We will approximate the gradients with respect to the RBM\u2019s p arameters using the Contrastive Divergence [3] learning procedure, CD n, whose updates are computed by the following algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Training Products of Experts by Minimizing Contrastive D  ivergence.Neural Computation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055557361"
                        ],
                        "name": "C. Peterson",
                        "slug": "C.-Peterson",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Peterson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Peterson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110869996"
                        ],
                        "name": "James R. Anderson",
                        "slug": "James-R.-Anderson",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Anderson",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James R. Anderson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 3851750,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "607802a4067cb7738bac85d3ca3386f859e637b9",
            "isKey": false,
            "numCitedBy": 499,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Mean-Field-Theory-Learning-Algorithm-for-Neural-Peterson-Anderson",
            "title": {
                "fragments": [],
                "text": "A Mean Field Theory Learning Algorithm for Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145778742"
                        ],
                        "name": "B. Juang",
                        "slug": "B.-Juang",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Juang",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Juang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "In its simplest form, the TRBM can be v iewed as a Hidden Markov Model (HMM) [10] with an exponentially large state space that has a n extremely compact parameterization of the transition and the emission probabilities."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 71
                            }
                        ],
                        "text": "In its simplest form, the TRBM can be viewed as a Hidden Markov Model (HMM) [10] with an exponentially large state space that has an extremely compact parameterization of the transition and the emission probabilities."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 198
                            }
                        ],
                        "text": "The TRBM defines a probability distributionP (V T1 = v T 1 , H T 1 = h T 1 ) by the equation\nP (vT1 , h T 1 ) =\nT\u220f\nt=2\nP (vt, ht|ht\u22121)P0(v1, h1) (4)\nwhich is identical to the defining equation of the HMM."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60838227,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "81d734347d5d6732be09493180387bd640d3490f",
            "isKey": false,
            "numCitedBy": 625,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-tutorial-on-Hidden-Markov-Models-Rabiner-Juang",
            "title": {
                "fragments": [],
                "text": "A tutorial on Hidden Markov Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 60
                            }
                        ],
                        "text": "The R BM also plays a critical role in deep belief networks [4, 5], but we do not use this connection in this paper."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 157
                            }
                        ],
                        "text": "A good mod el f r these data sources could be useful for finding an abstract representation that is helpful for so lving \u201cnatural\u201d discrimination tasks (see [4, 7] for an example of this approach for the non-sequential case)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Fast Learning Al gorithm for Deep Belief Nets.Neural Computation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Understanding beliefpropagation and its generalizations"
            },
            "venue": {
                "fragments": [],
                "text": "Exploring Artificial Intelligence in the New Millennium"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 4
                            }
                        ],
                        "text": "See [18, 14] for more details and generalizations."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Exponential fa mily harmoniums with an application to information retrieval.Advances in Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 88
                            }
                        ],
                        "text": "The reason inference is easy is similar to the reaso n inference in square ICAs is easy [1]: There is auniqueand aneasily computablevalue of the hidden variables that has a nonzero posterior probability."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An Information-Maximization Approa ch to Blind Separation and Blind Deconvolution.Neural Computation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 65
                            }
                        ],
                        "text": "The R BM also plays a critical role in deep belief networks [4], [5], but we do not u se this connection in this paper."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reducing the Dimensionality of Da ta with Neural Networks.Science"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning representations by backpropagating errors"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 88
                            }
                        ],
                        "text": "The reason inference is easy is similar to the reaso n inference in square ICAs is easy [1]: There is auniqueand aneasily computablevalue of the hidden variables that has a nonzero posterior probability."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An Information-Maximizat ion Approach to Blind Separation and Blind Deconvolution.Neural Computation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 168
                            }
                        ],
                        "text": "Models learned by CD 1 are often reasonable generative models of the data [3], but i f learning is continued with CD25, the resulting generative models are much better [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the quantitative ana  lysis of deep belief networks"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of the International Conference on Machine Learning  ,"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 168
                            }
                        ],
                        "text": "Models learned by CD 1 are often reasonable generative models of the data [3], but i f learning is continued with CD25, the resulting generative models are much better [11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the quantitative analysis of dee p belief networks"
            },
            "venue": {
                "fragments": [],
                "text": "InProceedings of the International Conference on Machine Learning  ,"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 123
                            }
                        ],
                        "text": "In contrast, the statement h\u2190 P (H) means that each h is set to the real value P (H = 1), so this is a \u201cmean-field\u201d update [8, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A mean field theory learning algor  ithm for neural networks.Complex Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 124
                            }
                        ],
                        "text": "In contrast, the statement h \u2190 P (H) means that each h is set to the real value P (H = 1), so this is a \u201cmean-field\u201d update [9, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A mean field theory learnin  g algorithm for neural networks. Complex Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 60
                            }
                        ],
                        "text": "The R BM also plays a critical role in deep belief networks [4, 5], but we do not use this connection in this paper."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reducing the Dimens  io ality of Data with"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks. Science,"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning representations by backpropagating errors"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "New Outer Bounds on the Marginal Poly tope.Advances in Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 4
                            }
                        ],
                        "text": "See [18, 14] for more details and generalizations."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Exponential family harmon iums with an application to information retrieval.Advances in Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 24,
            "methodology": 11,
            "result": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 46,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/The-Recurrent-Temporal-Restricted-Boltzmann-Machine-Sutskever-Hinton/0228810a988f6b8f06337e14f564e2fd3f6e1056?sort=total-citations"
}