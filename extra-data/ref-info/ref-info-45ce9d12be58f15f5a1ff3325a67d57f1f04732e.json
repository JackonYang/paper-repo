{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712991"
                        ],
                        "name": "M. Flickner",
                        "slug": "M.-Flickner",
                        "structuredName": {
                            "firstName": "Myron",
                            "lastName": "Flickner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Flickner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733393"
                        ],
                        "name": "H. Sawhney",
                        "slug": "H.-Sawhney",
                        "structuredName": {
                            "firstName": "Harpreet",
                            "lastName": "Sawhney",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Sawhney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152883679"
                        ],
                        "name": "J. Ashley",
                        "slug": "J.-Ashley",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Ashley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ashley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1391129943"
                        ],
                        "name": "Qian Huang",
                        "slug": "Qian-Huang",
                        "structuredName": {
                            "firstName": "Qian",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qian Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786444"
                        ],
                        "name": "B. Dom",
                        "slug": "B.-Dom",
                        "structuredName": {
                            "firstName": "Byron",
                            "lastName": "Dom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Dom"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087139"
                        ],
                        "name": "M. Gorkani",
                        "slug": "M.-Gorkani",
                        "structuredName": {
                            "firstName": "Monika",
                            "lastName": "Gorkani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gorkani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39311329"
                        ],
                        "name": "J. Hafner",
                        "slug": "J.-Hafner",
                        "structuredName": {
                            "firstName": "Jim",
                            "lastName": "Hafner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hafner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2499047"
                        ],
                        "name": "Denis Lee",
                        "slug": "Denis-Lee",
                        "structuredName": {
                            "firstName": "Denis",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Denis Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143867341"
                        ],
                        "name": "D. Petkovic",
                        "slug": "D.-Petkovic",
                        "structuredName": {
                            "firstName": "Dragutin",
                            "lastName": "Petkovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Petkovic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144028064"
                        ],
                        "name": "David Steele",
                        "slug": "David-Steele",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Steele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Steele"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70341848"
                        ],
                        "name": "P. Yanker",
                        "slug": "P.-Yanker",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Yanker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Yanker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 14
                            }
                        ],
                        "text": "In the second [6], the user provides value-examples for one or more visual features, something like: an image with about 30% green and 40% blue, with a grass-like texture in the green part."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 110716,
            "fieldsOfStudy": [
                "Computer Science",
                "Art"
            ],
            "id": "dc139f901c869f80b54b41f89d5b7f35c7dfa3c7",
            "isKey": false,
            "numCitedBy": 4258,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Research on ways to extend and improve query methods for image databases is widespread. We have developed the QBIC (Query by Image Content) system to explore content-based retrieval methods. QBIC allows queries on large image and video databases based on example images, user-constructed sketches and drawings, selected color and texture patterns, camera and object motion, and other graphical information. Two key properties of QBIC are (1) its use of image and video content-computable properties of color, texture, shape and motion of images, videos and their objects-in the queries, and (2) its graphical query language, in which queries are posed by drawing, selecting and other graphical means. This article describes the QBIC system and demonstrates its query capabilities. QBIC technology is part of several IBM products. >"
            },
            "slug": "Query-by-Image-and-Video-Content:-The-QBIC-System-Flickner-Sawhney",
            "title": {
                "fragments": [],
                "text": "Query by Image and Video Content: The QBIC System"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The QBIC system is described and its query capabilities are demonstrated, which allows queries on large image and video databases based on example images, user-constructed sketches and drawings, selected color and texture patterns, camera and object motion, and other graphical information."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8196487"
                        ],
                        "name": "A. Bimbo",
                        "slug": "A.-Bimbo",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Bimbo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bimbo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680785"
                        ],
                        "name": "E. Vicario",
                        "slug": "E.-Vicario",
                        "structuredName": {
                            "firstName": "Enrico",
                            "lastName": "Vicario",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Vicario"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787305"
                        ],
                        "name": "Daniele Zingoni",
                        "slug": "Daniele-Zingoni",
                        "structuredName": {
                            "firstName": "Daniele",
                            "lastName": "Zingoni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniele Zingoni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 13
                            }
                        ],
                        "text": "Some systems [3] disregard the whole problem of information extraction from videos and assume that symbolic descriptions of image sequences are available."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11650164,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "35403186e09d23c644d795c162c2664182d6cc82",
            "isKey": false,
            "numCitedBy": 152,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The emergence of advanced multimedia applications is emphasizing the relevance of retrieval by contents within databases of images and image sequences. Matching the inherent visuality of the information stored in such databases, visual specification by example provides an effective and natural way to express content-oriented queries. To support this querying approach, the system must be able to interpret example scenes reproducing the contents of images and sequences to be retrieved, and to match them against the actual contents of the database. In the accomplishment of this task, to avoid a direct access to raw image data, the system must be provided with an appropriate description language supporting the representation of the contents of pictorial data. An original language for the symbolic representation of the contents of image sequences is presented. This language, referred to as spatio-temporal logic, comprises a framework for the qualitative representation of the contents of image sequences, which allows for treatment and operation of content structures at a higher level than pixels or image features. Organization and operation principles of a prototype system exploiting spatio-temporal logic to support querying by example through visual iconic interaction are expounded. >"
            },
            "slug": "Symbolic-Description-and-Visual-Querying-of-Image-Bimbo-Vicario",
            "title": {
                "fragments": [],
                "text": "Symbolic Description and Visual Querying of Image Sequences Using Spatio-Temporal Logic"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Knowl. Data Eng."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679040"
                        ],
                        "name": "Shi-Kuo Chang",
                        "slug": "Shi-Kuo-Chang",
                        "structuredName": {
                            "firstName": "Shi-Kuo",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shi-Kuo Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46891469"
                        ],
                        "name": "A. Hsu",
                        "slug": "A.-Hsu",
                        "structuredName": {
                            "firstName": "Arding",
                            "lastName": "Hsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hsu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 27
                            }
                        ],
                        "text": "In a visual query language [2], the user visually places object icons to specify relative locations, orientations, and sizes of objects within the desired image."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 38822583,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee3deeb9c2f49b27c3fe7c222f153a610c33aa16",
            "isKey": false,
            "numCitedBy": 210,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "A conceptual framework for image information systems is presented. Current research topics are surveyed, and application examples are presented. The design issues for the next generation of active image systems are discussed. It is suggested that the next generation of active image information systems should be designed on the basis of notions of generalized icons and active indexes, resulting in smart images. >"
            },
            "slug": "Image-Information-Systems:-Where-Do-We-Go-From-Here-Chang-Hsu",
            "title": {
                "fragments": [],
                "text": "Image Information Systems: Where Do We Go From Here?"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is suggested that the next generation of active image information systems should be designed on the basis of notions of generalized icons and active indexes, resulting in smart images."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Knowl. Data Eng."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145507148"
                        ],
                        "name": "H. Lieberman",
                        "slug": "H.-Lieberman",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Lieberman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Lieberman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14582289,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "03e8102fe2d707e8c2ee624b655557dfde084767",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "A NEW paradigm is not just something that\u2019s a good idea. There are plenty of merely good ideas, but a new paradigm must go beyond simple innovation. A new paradigm is often introduced to solve a particular problem, but it must do more than that. It must fundamentally change the way we look at problems we have seen in the past. It must give us a new framework for thinking about problems in the future. It changes our priorities and values, changes our ideas about what to pay attention to and what to consider important. Thomas Kuhn wrote most eloquently about the impact of new paradigms in science in The Structure of Scientific Revolutions [7]. He traced the history of new paradigms such as quantum mechanics in physics. Such new paradigms are introduced in response to perceived problems with current practice, are advocated by enthusiastic iconoclasts, usually meet entrenched opposition, are accepted only slowly, and eventually become standard practice themselves. How does a new paradigm come about? Of course, the appearance of new paradigms is inherently unpredictable, but one thing that can be said is that the synergy between seemingly different disciplines is often a fertile ground for the growth of new paradigms. Here, I don\u2019t mean the kind of shallow interdisciplinary activity in which an expert in one field blithely assumes he or she can make pronouncements in another. Rather, when experts in different fields look with admiration and curiosity at each other\u2019s domains, and search for commonalities and fresh perspectives on their own, truly new paradigms can result. Intelligent Graphics \u2022 H e n r y L i e b e r m a n"
            },
            "slug": "Intelligent-graphics-Lieberman",
            "title": {
                "fragments": [],
                "text": "Intelligent graphics"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The synergy between seemingly different disciplines is often a fertile ground for the growth of new paradigms, and when experts in different fields look with admiration and curiosity at each other\u2019s domains, and search for commonalities and fresh perspectives on their own, truly new Paradigms can result."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780935"
                        ],
                        "name": "B. Moghaddam",
                        "slug": "B.-Moghaddam",
                        "structuredName": {
                            "firstName": "Baback",
                            "lastName": "Moghaddam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Moghaddam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738894"
                        ],
                        "name": "T. Starner",
                        "slug": "T.-Starner",
                        "structuredName": {
                            "firstName": "Thad",
                            "lastName": "Starner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Starner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 101
                            }
                        ],
                        "text": "A well-known VIR research system is the eigenface image database developed at MIT\u2019s Media Laboratory [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 136280,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0bf5d558220d39698ce96d59ee5772e8e1a0663",
            "isKey": false,
            "numCitedBy": 2234,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe experiments with eigenfaces for recognition and interactive search in a large-scale face database. Accurate visual recognition is demonstrated using a database of O(10/sup 3/) faces. The problem of recognition under general viewing orientation is also examined. A view-based multiple-observer eigenspace technique is proposed for use in face recognition under variable pose. In addition, a modular eigenspace description technique is used which incorporates salient features such as the eyes, nose and mouth, in an eigenfeature layer. This modular representation yields higher recognition rates as well as a more robust framework for face recognition. An automatic feature extraction technique using feature eigentemplates is also demonstrated.<<ETX>>"
            },
            "slug": "View-based-and-modular-eigenspaces-for-face-Pentland-Moghaddam",
            "title": {
                "fragments": [],
                "text": "View-based and modular eigenspaces for face recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A modular eigenspace description technique is used which incorporates salient features such as the eyes, nose and mouth, in an eigenfeature layer, which yields higher recognition rates as well as a more robust framework for face recognition."
            },
            "venue": {
                "fragments": [],
                "text": "1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1822658"
                        ],
                        "name": "C. Mooers",
                        "slug": "C.-Mooers",
                        "structuredName": {
                            "firstName": "Calvin",
                            "lastName": "Mooers",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Mooers"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "the term information retrieval [10] to describe the process through"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14489024,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2bb8062ce4d951c0768a38fc7501a5e457828c54",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The mechanical organization of knowledge for retrieval of stored information can no longer neglect the developments of point\u2010to\u2010point communication theory, since both deal with information and handle it by machines. The most versatile retrieval systems are those which delegate a separate tally to each information item, and which impress marks on the tally for the machine to read and to use for selective purposes. Coding is the relationship between these marks and the intellectual content of the information items. Coding determines the complexity of the selective machine and the utility of the whole process. A set of invariant coding principles is stated which define maximum coding efficiency for any tally selecting machines, and parallels are drawn between these principles and the conclusions of modern point\u2010to\u2010point communication theory. Zatocoding is defined\u2014the system which superimposes random subject code patterns on the tally\u2014and it is found to obey each of the invariant principles of coding efficiency while still allowing the simplest possible selector machine structure."
            },
            "slug": "Zatocoding-applied-to-mechanical-organization-of-Mooers",
            "title": {
                "fragments": [],
                "text": "Zatocoding applied to mechanical organization of knowledge"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A set of invariant coding principles is stated which define maximum coding efficiency for any tally selecting machines, and parallels are drawn between these principles and the conclusions of modern point\u2010to\u2010point communication theory."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1951
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2160177"
                        ],
                        "name": "N. Dimitrova",
                        "slug": "N.-Dimitrova",
                        "structuredName": {
                            "firstName": "Nevenka",
                            "lastName": "Dimitrova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dimitrova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703912"
                        ],
                        "name": "F. Golshani",
                        "slug": "F.-Golshani",
                        "structuredName": {
                            "firstName": "Forouzan",
                            "lastName": "Golshani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Golshani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 13
                            }
                        ],
                        "text": "Some systems [4] use the motionencoding in compressed video formats (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14324045,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54ecdf01c1bbbe8106cd27a35aed672c3564ef34",
            "isKey": false,
            "numCitedBy": 139,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Like other types of digital information, video sequences must be classified based on the semantics of their contents. A more-precise and completer extraction of semantic information will result in a more-effective classification. The most-discernible difference between still images and moving pictures stems from movements and variations. Thus, to go from the realm of still-image repositories to video databases, we must be able to deal with motion. Particularly, we need the ability to classify objects appearing in a video sequence based on their characteristics and features such as shape or color, as well as their movements. By describing the movements that we derive from the process of motion analysis, we introduce a dual hierarchy consisting of spatial and temporal parts for video sequence representation. This gives us the flexibility to examine arbitrary sequences of frames at various levels of abstraction and to retrieve the associated temporal information (say, object trajectories) in addition to the spatial representation. Our algorithm for motion detection uses the motion compensation component of the MPEG video-encoding scheme and then computes trajectories for objects of interest. The specification of a language for retrieval of video based on the spatial as well as motion characteristics is presented."
            },
            "slug": "Motion-recovery-for-video-content-classification-Dimitrova-Golshani",
            "title": {
                "fragments": [],
                "text": "Motion recovery for video content classification"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The specification of a language for retrieval of video based on the spatial as well as motion characteristics is presented and the algorithm for motion detection uses the motion compensation component of the MPEG video-encoding scheme and then computes trajectories for objects of interest."
            },
            "venue": {
                "fragments": [],
                "text": "TOIS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3221648"
                        ],
                        "name": "A. F. Cardenas",
                        "slug": "A.-F.-Cardenas",
                        "structuredName": {
                            "firstName": "Alfonso",
                            "lastName": "Cardenas",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. F. Cardenas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2825247"
                        ],
                        "name": "I. T. Ieong",
                        "slug": "I.-T.-Ieong",
                        "structuredName": {
                            "firstName": "Ion",
                            "lastName": "Ieong",
                            "middleNames": [
                                "Tim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. T. Ieong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743681"
                        ],
                        "name": "R. Taira",
                        "slug": "R.-Taira",
                        "structuredName": {
                            "firstName": "Ricky",
                            "lastName": "Taira",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Taira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "119971773"
                        ],
                        "name": "R. Barker",
                        "slug": "R.-Barker",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Barker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Barker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34368734"
                        ],
                        "name": "C. Breant",
                        "slug": "C.-Breant",
                        "structuredName": {
                            "firstName": "Claudine",
                            "lastName": "Breant",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Breant"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 34
                            }
                        ],
                        "text": "A textual query lan\u00adguage, such as \nPICQUERY+ [1], has constructs to compose a visual description through textually specified attributes \nand operators to specify spatial, temporal, evolutionary (e.g., splits into) relationships."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "A textual query language, such as PICQUERY+ [1], has constructs to \u201ccompose\u201d a visual description through textually specified attributes and operators to specify spatial, temporal, \u201cevolutionary\u201d (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 36
                            }
                        ],
                        "text": "The knowledge-based object-oriented PICQUERY+ \nlanguage system."
                    },
                    "intents": []
                }
            ],
            "corpusId": 27453566,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d6e8f49186b9d3222d31c5b894dfde05ba2994e6",
            "isKey": true,
            "numCitedBy": 117,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "PICQUERY/sup +/, a high-level domain-independent query language for pictorial and alphanumeric database management, is introduced. The PICQUERY/sup +/ language and its underlying stacked image data model are enhanced with major advances that include: convenient specification of the data domain space among a multimedia database federation, visualization of underlying data models, knowledge-based hierarchies, and domain rules, understanding of high-level abstract data types, ability to perform data object matches based on imprecise or fuzzy descriptors, imprecise relational correlators, and temporal and object evolutionary events, specification of alphanumeric and image processing algorithms on data, and specification of alphanumeric and image visualization methods for user presentation. The power of PICQUERY/sup +/ is illustrated using examples drawn from the medical imaging domain. A graphical menu-driven user interface is demonstrated for this domain as an example of the menu interface capabilities of PICQUERY/sup +/. >"
            },
            "slug": "The-Knowledge-Based-Object-Oriented-PICQUERY+-Cardenas-Ieong",
            "title": {
                "fragments": [],
                "text": "The Knowledge-Based Object-Oriented PICQUERY+ Language"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The PICQUERY/sup +/ language and its underlying stacked image data model are enhanced with major advances that include convenient specification of the data domain space among a multimedia database federation, visualization of underlying data models, knowledge-based hierarchies, and domain rules."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Knowl. Data Eng."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690709"
                        ],
                        "name": "A. Hampapur",
                        "slug": "A.-Hampapur",
                        "structuredName": {
                            "firstName": "Arun",
                            "lastName": "Hampapur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hampapur"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 141
                            }
                        ],
                        "text": "For example, an abrupt scene change can be modeled by finding major discontinuities in time-plots of cumulative pixel difference over frames [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59736721,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "62fc2d50b51f270ea835aec0dd1e70da64e2f4de",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Video is the most effective means of communicating and storing audio visual information. The usability of video acquires a new dimension when it is augmented with content based access. Video data management systems manage video collections while providing content based access. The design of such systems is the topic of this thesis. Managing video data adds a new dimension of complexity to the data modeling, insertion, organization and retrieval tasks. The modeling and insertion of video are the focus of this work. \nVideo data models are application specific representations designed to facilitate typical data access patterns. A set of video applications are analyzed to arrive at the design of a data model. The data model uses a temporally segmented representation of video with attached descriptions called features. Multimedia authoring and video production is used as an example application to design video insertion procedures. \nVideo segmentation comprises the first task in video insertion. Segmentation is formulated as feature based classification. The design of the feature detectors is based on video production models derived from standard movie and video production techniques. Feature detectors for effects like fades, dissolves and page translates have been designed. A discriminant function combines the feature detector results to achieve video segmentation. \nVideo indexing comprises the second task in insertion. This is formulated as feature based classification. A methodology for designing feature based indexing schemes are proposed. Indexing schemes are ranked based on an efficacy measure. The indexing scheme uses computational constrains and poses classification as a design problem. An image motion based video classification is used to design a cinematographic video indexing scheme. \nThe segmentation and indexing algorithms are incorporated into a prototype system which operates on video from commercial cable television. The thesis presents results of experiments with half hour of video data."
            },
            "slug": "Designing-video-data-management-systems-Hampapur",
            "title": {
                "fragments": [],
                "text": "Designing video data management systems"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The segmentation and indexing algorithms are incorporated into a prototype system which operates on video from commercial cable television, and results of experiments with half hour of video data are presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144171208"
                        ],
                        "name": "S. Iyengar",
                        "slug": "S.-Iyengar",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Iyengar",
                            "middleNames": [
                                "Sitharama"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Iyengar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32583901"
                        ],
                        "name": "R. Kashyap",
                        "slug": "R.-Kashyap",
                        "structuredName": {
                            "firstName": "Rangasami",
                            "lastName": "Kashyap",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kashyap"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 42615435,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b1927b9ad49453a5b3de4e2d5f24ea1e00ac82c1",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Guest-Editors'-Introduction:-Image-Databases-Iyengar-Kashyap",
            "title": {
                "fragments": [],
                "text": "Guest Editors' Introduction: Image Databases"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Software Eng."
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 130
                            }
                        ],
                        "text": "The simplest visual features that can be computed are based on pixel values of raw data, and several early image database systems [8] used pixels as the basis of their data models."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Special section on imagedatabase systems"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Software Eng"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The WebSeek demonstration web page, ADVENT laboratory"
            },
            "venue": {
                "fragments": [],
                "text": "The WebSeek demonstration web page, ADVENT laboratory"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 202
                            }
                        ],
                        "text": "The most noticeable aspect in these rough \u201cmeasures\u201d is that the users\u2019 judgment of goodness is based on how much of the retrieved data is good rather than on how much of the relevant data is retrieved [5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Efficient and effective querying by visual content"
            },
            "venue": {
                "fragments": [],
                "text": "J. Intell. Inf. Syst"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Amarnath Gupta (amarnath@virage.com) is a senior software scientist in Virage, Inc., a developer and vendor of VIR systems"
            },
            "venue": {
                "fragments": [],
                "text": "Amarnath Gupta (amarnath@virage.com) is a senior software scientist in Virage, Inc., a developer and vendor of VIR systems"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Designing video data management systems The Univ. of Michigan"
            },
            "venue": {
                "fragments": [],
                "text": "Designing video data management systems The Univ. of Michigan"
            },
            "year": 1995
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 8,
            "methodology": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 15,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Visual-information-retrieval-Gupta-Jain/45ce9d12be58f15f5a1ff3325a67d57f1f04732e?sort=total-citations"
}