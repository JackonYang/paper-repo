{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1906895"
                        ],
                        "name": "M. Andriluka",
                        "slug": "M.-Andriluka",
                        "structuredName": {
                            "firstName": "Mykhaylo",
                            "lastName": "Andriluka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Andriluka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145920814"
                        ],
                        "name": "S. Roth",
                        "slug": "S.-Roth",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1430002,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "573c11e7e00389a033787984223ced536e15c904",
            "isKey": false,
            "numCitedBy": 454,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Non-rigid object detection and articulated pose estimation are two related and challenging problems in computer vision. Numerous models have been proposed over the years and often address different special cases, such as pedestrian detection or upper body pose estimation in TV footage. This paper shows that such specialization may not be necessary, and proposes a generic approach based on the pictorial structures framework. We show that the right selection of components for both appearance and spatial modeling is crucial for general applicability and overall performance of the model. The appearance of body parts is modeled using densely sampled shape context descriptors and discriminatively trained AdaBoost classifiers. Furthermore, we interpret the normalized margin of each classifier as likelihood in a generative model. Non-Gaussian relationships between parts are represented as Gaussians in the coordinate system of the joint between parts. The marginal posterior of each part is inferred using belief propagation. We demonstrate that such a model is equally suitable for both detection and pose estimation tasks, outperforming the state of the art on three recently proposed datasets."
            },
            "slug": "Pictorial-structures-revisited:-People-detection-Andriluka-Roth",
            "title": {
                "fragments": [],
                "text": "Pictorial structures revisited: People detection and articulated pose estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper proposes a generic approach based on the pictorial structures framework, and demonstrates that such a model is equally suitable for both detection and pose estimation tasks, outperforming the state of the art on three recently proposed datasets."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9133363"
                        ],
                        "name": "Benjamin Sapp",
                        "slug": "Benjamin-Sapp",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Sapp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Sapp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726415"
                        ],
                        "name": "Alexander Toshev",
                        "slug": "Alexander-Toshev",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Toshev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Toshev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14602050,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "63b32a382c5108f2c316320ed38c006c485cd3f8",
            "isKey": false,
            "numCitedBy": 227,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of articulated human pose estimation by learning a coarse-to-fine cascade of pictorial structure models. While the fine-level state-space of poses of individual parts is too large to permit the use of rich appearance models, most possibilities can be ruled out by efficient structured models at a coarser scale. We propose to learn a sequence of structured models at different pose resolutions, where coarse models filter the pose space for the next level via their max-marginals. The cascade is trained to prune as much as possible while preserving true poses for the final level pictorial structure model. The final level uses much more expensive segmentation, contour and shape features in the model for the remaining filtered set of candidates. We evaluate our framework on the challenging Buffy and PASCAL human pose datasets, improving the state-of-the-art."
            },
            "slug": "Cascaded-Models-for-Articulated-Pose-Estimation-Sapp-Toshev",
            "title": {
                "fragments": [],
                "text": "Cascaded Models for Articulated Pose Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work proposes to learn a sequence of structured models at different pose resolutions, where coarse models filter the pose space for the next level via their max-marginals, and trains the cascade to prune as much as possible while preserving true poses for the final level pictorial structure model."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769383"
                        ],
                        "name": "Lubomir D. Bourdev",
                        "slug": "Lubomir-D.-Bourdev",
                        "structuredName": {
                            "firstName": "Lubomir",
                            "lastName": "Bourdev",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lubomir D. Bourdev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9320620,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55b29a2505149d06d8c1d616cd30edca40cb029c",
            "isKey": false,
            "numCitedBy": 1048,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the classic problems of detection, segmentation and pose estimation of people in images with a novel definition of a part, a poselet. We postulate two criteria (1) It should be easy to find a poselet given an input image (2) it should be easy to localize the 3D configuration of the person conditioned on the detection of a poselet. To permit this we have built a new dataset, H3D, of annotations of humans in 2D photographs with 3D joint information, inferred using anthropometric constraints. This enables us to implement a data-driven search procedure for finding poselets that are tightly clustered in both 3D joint configuration space as well as 2D image appearance. The algorithm discovers poselets that correspond to frontal and profile faces, pedestrians, head and shoulder views, among others. Each poselet provides examples for training a linear SVM classifier which can then be run over the image in a multiscale scanning mode. The outputs of these poselet detectors can be thought of as an intermediate layer of nodes, on top of which one can run a second layer of classification or regression. We show how this permits detection and localization of torsos or keypoints such as left shoulder, nose, etc. Experimental results show that we obtain state of the art performance on people detection in the PASCAL VOC 2007 challenge, among other datasets. We are making publicly available both the H3D dataset as well as the poselet parameters for use by other researchers."
            },
            "slug": "Poselets:-Body-part-detectors-trained-using-3D-pose-Bourdev-Malik",
            "title": {
                "fragments": [],
                "text": "Poselets: Body part detectors trained using 3D human pose annotations"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A new dataset, H3D, is built of annotations of humans in 2D photographs with 3D joint information, inferred using anthropometric constraints, to address the classic problems of detection, segmentation and pose estimation of people in images with a novel definition of a part, a poselet."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8170470,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6dd0597f8513dc100cd0bc1b493768cde45098a9",
            "isKey": false,
            "numCitedBy": 525,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the machine vision task of pose estimation from static images, specifically for the case of articulated objects. This problem is hard because of the large number of degrees of freedom to be estimated. Following a established line of research, pose estimation is framed as inference in a probabilistic model. In our experience however, the success of many approaches often lie in the power of the features. Our primary contribution is a novel casting of visual inference as an iterative parsing process, where one sequentially learns better and better features tuned to a particular image. We show quantitative results for human pose estimation on a database of over 300 images that suggest our algorithm is competitive with or surpasses the state-of-the-art. Since our procedure is quite general (it does not rely on face or skin detection), we also use it to estimate the poses of horses in the Weizmann database."
            },
            "slug": "Learning-to-parse-images-of-articulated-bodies-Ramanan",
            "title": {
                "fragments": [],
                "text": "Learning to parse images of articulated bodies"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This work considers the machine vision task of pose estimation from static images, specifically for the case of articulated objects, and casts visual inference as an iterative parsing process, where one sequentially learns better and better features tuned to a particular image."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38916673"
                        ],
                        "name": "B. Yao",
                        "slug": "B.-Yao",
                        "structuredName": {
                            "firstName": "Bangpeng",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7352553,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "927432c50d920e647260c67506859d7845c7f729",
            "isKey": false,
            "numCitedBy": 576,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Detecting objects in cluttered scenes and estimating articulated human body parts are two challenging problems in computer vision. The difficulty is particularly pronounced in activities involving human-object interactions (e.g. playing tennis), where the relevant object tends to be small or only partially visible, and the human body parts are often self-occluded. We observe, however, that objects and human poses can serve as mutual context to each other \u2013 recognizing one facilitates the recognition of the other. In this paper we propose a new random field model to encode the mutual context of objects and human poses in human-object interaction activities. We then cast the model learning task as a structure learning problem, of which the structural connectivity between the object, the overall human pose, and different body parts are estimated through a structure search approach, and the parameters of the model are estimated by a new max-margin algorithm. On a sports data set of six classes of human-object interactions [12], we show that our mutual context model significantly outperforms state-of-the-art in detecting very difficult objects and human poses."
            },
            "slug": "Modeling-mutual-context-of-object-and-human-pose-in-Yao-Fei-Fei",
            "title": {
                "fragments": [],
                "text": "Modeling mutual context of object and human pose in human-object interaction activities"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new random field model is proposed to encode the mutual context of objects and human poses in human-object interaction activities and it is shown that this mutual context model significantly outperforms state-of-the-art in detecting very difficult objects andhuman poses."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2143685864"
                        ],
                        "name": "Yi Yang",
                        "slug": "Yi-Yang",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3509338,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bf49f2789c72a8301c4dfbb5eabca76c92ed35ef",
            "isKey": false,
            "numCitedBy": 1117,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a method for human pose estimation in static images based on a novel representation of part models. Notably, we do not use articulated limb parts, but rather capture orientation with a mixture of templates for each part. We describe a general, flexible mixture model for capturing contextual co-occurrence relations between parts, augmenting standard spring models that encode spatial relations. We show that such relations can capture notions of local rigidity. When co-occurrence and spatial relations are tree-structured, our model can be efficiently optimized with dynamic programming. We present experimental results on standard benchmarks for pose estimation that indicate our approach is the state-of-the-art system for pose estimation, outperforming past work by 50% while being orders of magnitude faster."
            },
            "slug": "Articulated-pose-estimation-with-flexible-Yang-Ramanan",
            "title": {
                "fragments": [],
                "text": "Articulated pose estimation with flexible mixtures-of-parts"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A general, flexible mixture model for capturing contextual co-occurrence relations between parts, augmenting standard spring models that encode spatial relations, and it is shown that such relations can capture notions of local rigidity."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46396571"
                        ],
                        "name": "Yang Wang",
                        "slug": "Yang-Wang",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10771328"
                        ],
                        "name": "Greg Mori",
                        "slug": "Greg-Mori",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Mori"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9823069,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5edfa28559c054b23acc43ce0f975a04ae27b331",
            "isKey": false,
            "numCitedBy": 155,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Tree-structured models have been widely used for human pose estimation, in either 2D or 3D. While such models allow efficient learning and inference, they fail to capture additional dependencies between body parts, other than kinematic constraints between connected parts. In this paper, we consider the use of multiple tree models, rather than a single tree model for human pose estimation. Our model can alleviate the limitations of a single tree-structured model by combining information provided across different tree models. The parameters of each individual tree model are trained via standard learning algorithms in a single tree-structured model. Different tree models can be combined in a discriminative fashion by a boosting procedure. We present experimental results showing the improvement of our approaches on two different datasets. On the first dataset, we use our multiple tree framework for occlusion reasoning. On the second dataset, we combine multiple deformable trees for capturing spatial constraints between non-connected body parts."
            },
            "slug": "Multiple-Tree-Models-for-Occlusion-and-Spatial-in-Wang-Mori",
            "title": {
                "fragments": [],
                "text": "Multiple Tree Models for Occlusion and Spatial Constraints in Human Pose Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This model can alleviate the limitations of a single tree-structured model by combining information provided across different tree models, and combines multiple deformable trees for capturing spatial constraints between non-connected body parts."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49835695"
                        ],
                        "name": "Long Zhu",
                        "slug": "Long-Zhu",
                        "structuredName": {
                            "firstName": "Long",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Long Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144034946"
                        ],
                        "name": "Yuanhao Chen",
                        "slug": "Yuanhao-Chen",
                        "structuredName": {
                            "firstName": "Yuanhao",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuanhao Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6960041,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "120081166fc0d780c84e198622d638152a7cdf3e",
            "isKey": false,
            "numCitedBy": 294,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a latent hierarchical structural learning method for object detection. An object is represented by a mixture of hierarchical tree models where the nodes represent object parts. The nodes can move spatially to allow both local and global shape deformations. The models can be trained discriminatively using latent structural SVM learning, where the latent variables are the node positions and the mixture component. But current learning methods are slow, due to the large number of parameters and latent variables, and have been restricted to hierarchies with two layers. In this paper we describe an incremental concave-convex procedure (iCCCP) which allows us to learn both two and three layer models efficiently. We show that iCCCP leads to a simple training algorithm which avoids complex multi-stage layer-wise training, careful part selection, and achieves good performance without requiring elaborate initialization. We perform object detection using our learnt models and obtain performance comparable with state-of-the-art methods when evaluated on challenging public PASCAL datasets. We demonstrate the advantages of three layer hierarchies - outperforming Felzenszwalb et al.'s two layer models on all 20 classes."
            },
            "slug": "Latent-hierarchical-structural-learning-for-object-Zhu-Chen",
            "title": {
                "fragments": [],
                "text": "Latent hierarchical structural learning for object detection"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper describes an incremental concave-convex procedure (iCCCP) which allows us to learn both two and three layer models efficiently and demonstrates the advantages of three layer hierarchies - outperforming Felzenszwalb et al.'s two layer models on all 20 classes."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2273228"
                        ],
                        "name": "Catalin Ionescu",
                        "slug": "Catalin-Ionescu",
                        "structuredName": {
                            "firstName": "Catalin",
                            "lastName": "Ionescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Catalin Ionescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144651486"
                        ],
                        "name": "Liefeng Bo",
                        "slug": "Liefeng-Bo",
                        "structuredName": {
                            "firstName": "Liefeng",
                            "lastName": "Bo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liefeng Bo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781120"
                        ],
                        "name": "C. Sminchisescu",
                        "slug": "C.-Sminchisescu",
                        "structuredName": {
                            "firstName": "Cristian",
                            "lastName": "Sminchisescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sminchisescu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 421335,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "06e2736b002bfa9875368bea4baddc577ac77eb3",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an integrated model for visual object localization and continuous state estimation in a discriminative structured prediction framework. While existing discriminative \u2018prediction through time\u2019 methods have showed remarkable versatility for visual reconstruction and tracking problems, they tend to assume that the input is known (or the object is segmented) a condition that can rarely be accommodated in images of real scenes. Our structural Support Vector Machine (structSVM) framework offers an end-to-end training and inference framework that overcomes these limitations by consistently searching both in the space of possible inputs (effectively an efficient form of object localization) and in the space of possible structured outputs, given those inputs. We demonstrate the potential of this methodology for 3d human pose reconstruction in monocular images both in the HumanEva benchmark, where 3d ground truth is available, and qualitatively, in un-instrumented images of real scenes.1"
            },
            "slug": "Structural-SVM-for-visual-localization-and-state-Ionescu-Bo",
            "title": {
                "fragments": [],
                "text": "Structural SVM for visual localization and continuous state estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The potential of the structural Support Vector Machine framework for 3d human pose reconstruction in monocular images both in the HumanEva benchmark, where 3d ground truth is available, and qualitatively, in un-instrumented images of real scenes."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 12th International Conference on Computer Vision"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684865"
                        ],
                        "name": "Guillaume Bouchard",
                        "slug": "Guillaume-Bouchard",
                        "structuredName": {
                            "firstName": "Guillaume",
                            "lastName": "Bouchard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guillaume Bouchard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756114"
                        ],
                        "name": "B. Triggs",
                        "slug": "B.-Triggs",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Triggs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Triggs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 15
                            }
                        ],
                        "text": "Detecting people using mutually consistent poselet activations."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6193293,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3452d42804cbea13b8b252d08ec249b008e8df93",
            "isKey": false,
            "numCitedBy": 174,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a generative model that codes the geometry and appearance of generic visual object categories as a loose hierarchy of parts, with probabilistic spatial relations linking parts to subparts, soft assignment of subparts to parts, and scale invariant keypoint based local features at the lowest level of the hierarchy. The method is designed to efficiently handle categories containing hundreds of redundant local features, such as those returned by current key-point detectors. This robustness allows it to outperform constellation style models, despite their stronger spatial models. The model is initialized by robust bottom-up voting over location-scale pyramids, and optimized by expectation-maximization. Training is rapid, and objects do not need to be marked in the training images. Experiments on several popular datasets show the method's ability to capture complex natural object classes."
            },
            "slug": "Hierarchical-part-based-visual-object-Bouchard-Triggs",
            "title": {
                "fragments": [],
                "text": "Hierarchical part-based visual object categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "A generative model that codes the geometry and appearance of generic visual object categories as a loose hierarchy of parts, with probabilistic spatial relations linking parts to subparts, soft assignment of subparts to parts, and scale invariant keypoint based local features at the lowest level of the hierarchy."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769383"
                        ],
                        "name": "Lubomir D. Bourdev",
                        "slug": "Lubomir-D.-Bourdev",
                        "structuredName": {
                            "firstName": "Lubomir",
                            "lastName": "Bourdev",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lubomir D. Bourdev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35208858"
                        ],
                        "name": "Subhransu Maji",
                        "slug": "Subhransu-Maji",
                        "structuredName": {
                            "firstName": "Subhransu",
                            "lastName": "Maji",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Subhransu Maji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710872"
                        ],
                        "name": "T. Brox",
                        "slug": "T.-Brox",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Brox",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Brox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 918485,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c8d905b121e3e1f27d1f72195e27b7c8ac1a4386",
            "isKey": false,
            "numCitedBy": 403,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Bourdev and Malik (ICCV 09) introduced a new notion of parts, poselets, constructed to be tightly clustered both in the configuration space of keypoints, as well as in the appearance space of image patches. In this paper we develop a new algorithm for detecting people using poselets. Unlike that work which used 3D annotations of keypoints, we use only 2D annotations which are much easier for naive human annotators. The main algorithmic contribution is in how we use the pattern of poselet activations. Individual poselet activations are noisy, but considering the spatial context of each can provide vital disambiguating information, just as object detection can be improved by considering the detection scores of nearby objects in the scene. This can be done by training a two-layer feed-forward network with weights set using a max margin technique. The refined poselet activations are then clustered into mutually consistent hypotheses where consistency is based on empirically determined spatial keypoint distributions. Finally, bounding boxes are predicted for each person hypothesis and shape masks are aligned to edges in the image to provide a segmentation. To the best of our knowledge, the resulting system is the current best performer on the task of people detection and segmentation with an average precision of 47.8% and 40.5% respectively on PASCAL VOC 2009."
            },
            "slug": "Detecting-People-Using-Mutually-Consistent-Poselet-Bourdev-Maji",
            "title": {
                "fragments": [],
                "text": "Detecting People Using Mutually Consistent Poselet Activations"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new algorithm for detecting people using poselets is developed which uses only 2D annotations which are much easier for naive human annotators and is the current best performer on the task of people detection and segmentation."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789756"
                        ],
                        "name": "B. Leibe",
                        "slug": "B.-Leibe",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Leibe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Leibe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732672"
                        ],
                        "name": "A. Leonardis",
                        "slug": "A.-Leonardis",
                        "structuredName": {
                            "firstName": "Ale\u0161",
                            "lastName": "Leonardis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Leonardis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48920094"
                        ],
                        "name": "B. Schiele",
                        "slug": "B.-Schiele",
                        "structuredName": {
                            "firstName": "Bernt",
                            "lastName": "Schiele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Schiele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 5
                            }
                        ],
                        "text": "Detecting people using mutually consistent poselet activations."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6533591,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "38101fac622a70b78f13625fc6502000b8756d3a",
            "isKey": false,
            "numCitedBy": 1036,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for object categorization in real-world scenes. Following a common consensus in the field, we do not assume that a figure- ground segmentation is available prior to recognition. However, in contrast to most standard approaches for object class recognition, our approach automati- cally segments the object as a result of the categorization. This combination of recognition and segmentation into one process is made pos- sible by our use of an Implicit Shape Model, which integrates both into a common probabilistic framework. In addition to the recognition and segmentation result, it also generates a per-pixel confidence measure specifying the area that supports a hypothesis and how much it can be trusted. We use this confidence to derive a nat- ural extension of the approach to handle multiple objects in a scene and resolve ambiguities between overlapping hypotheses with a novel MDL-based criterion. In addition, we present an extensive evaluation of our method on a standard dataset for car detection and compare its performance to existing methods from the literature. Our results show that the proposed method significantly outper- forms previously published methods while needing one order of magnitude less training examples. Finally, we present results for articulated objects, which show that the proposed method can categorize and segment unfamiliar objects in differ- ent articulations and with widely varying texture patterns, even under significant partial occlusion."
            },
            "slug": "Combined-Object-Categorization-and-Segmentation-an-Leibe-Leonardis",
            "title": {
                "fragments": [],
                "text": "Combined Object Categorization and Segmentation With an Implicit Shape Model"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Results for articulated objects, which show that the proposed method can categorize and segment unfamiliar objects in differ- ent articulations and with widely varying texture patterns, even under significant partial occlusion."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46396571"
                        ],
                        "name": "Yang Wang",
                        "slug": "Yang-Wang",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2828154"
                        ],
                        "name": "Duan Tran",
                        "slug": "Duan-Tran",
                        "structuredName": {
                            "firstName": "Duan",
                            "lastName": "Tran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Duan Tran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2928799"
                        ],
                        "name": "Zicheng Liao",
                        "slug": "Zicheng-Liao",
                        "structuredName": {
                            "firstName": "Zicheng",
                            "lastName": "Liao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zicheng Liao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15097822,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "13784695d22601ac2e202e125e9bb00949e91d66",
            "isKey": false,
            "numCitedBy": 169,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of human parsing with part-based models. Most previous work in part-based models only considers rigid parts (e.g. torso, head, half limbs) guided by human anatomy. We argue that this representation of parts is not necessarily appropriate for human parsing. In this paper, we introduce hierarchical poselets\u2013a new representation for human parsing. Hierarchical poselets can be rigid parts, but they can also be parts that cover large portions of human bodies (e.g. torso + left arm). In the extreme case, they can be the whole bodies. We develop a structured model to organize poselets in a hierarchical way and learn the model parameters in a max-margin framework. We demonstrate the superior performance of our proposed approach on two datasets with aggressive pose variations."
            },
            "slug": "Learning-hierarchical-poselets-for-human-parsing-Wang-Tran",
            "title": {
                "fragments": [],
                "text": "Learning hierarchical poselets for human parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A structured model to organize poselets in a hierarchical way and learn the model parameters in a max-margin framework and demonstrates the superior performance of the proposed approach on two datasets with aggressive pose variations."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685089"
                        ],
                        "name": "Pedro F. Felzenszwalb",
                        "slug": "Pedro-F.-Felzenszwalb",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Felzenszwalb",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro F. Felzenszwalb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689002"
                        ],
                        "name": "David A. McAllester",
                        "slug": "David-A.-McAllester",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McAllester",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. McAllester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3198903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e79272fe3d65197100eae8be9fec6469107969ae",
            "isKey": false,
            "numCitedBy": 9374,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL data sets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin-sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI--SVM in terms of latent variables. A latent SVM is semiconvex, and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function."
            },
            "slug": "Object-Detection-with-Discriminatively-Trained-Part-Felzenszwalb-Girshick",
            "title": {
                "fragments": [],
                "text": "Object Detection with Discriminatively Trained Part Based Models"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "An object detection system based on mixtures of multiscale deformable part models that is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges is described."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2312794"
                        ],
                        "name": "X. Lan",
                        "slug": "X.-Lan",
                        "structuredName": {
                            "firstName": "Xiangyang",
                            "lastName": "Lan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Lan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713089"
                        ],
                        "name": "D. Huttenlocher",
                        "slug": "D.-Huttenlocher",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Huttenlocher",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Huttenlocher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 622540,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f1a6af296b99e2c6cd58a49533b49f3c7cdab02c",
            "isKey": false,
            "numCitedBy": 190,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Tree structured models have been widely used for determining the pose of a human body, from either 2D or 3D data. While such models can effectively represent the kinematic constraints of the skeletal structure, they do not capture additional constraints such as coordination of the limbs. Tree structured models thus miss an important source of information about human body pose, as limb coordination is necessary for balance while standing, walking, or running, as well as being evident in other activities such as dancing and throwing. In this paper, we consider the use of undirected graphical models that augment a tree structure with latent variables in order to account for coordination between limbs. We refer to these as common-factor models, since they are constructed by using factor analysis to identify additional correlations in limb position that are not accounted for by the kinematic tree structure. These common-factor models have an underlying tree structure and thus a variant of the standard Viterbi algorithm for a tree can be applied for efficient estimation. We present some experimental results contrasting common-factor models with tree models, and quantify the improvement in pose estimation for 2D image data."
            },
            "slug": "Beyond-trees:-common-factor-models-for-2D-human-Lan-Huttenlocher",
            "title": {
                "fragments": [],
                "text": "Beyond trees: common-factor models for 2D human pose recovery"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Undirected graphical models that augment a tree structure with latent variables in order to account for coordination between limbs are considered, since these common-factor models have an underlying tree structure and thus a variant of the standard Viterbi algorithm for a tree can be applied for efficient estimation."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685089"
                        ],
                        "name": "Pedro F. Felzenszwalb",
                        "slug": "Pedro-F.-Felzenszwalb",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Felzenszwalb",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro F. Felzenszwalb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713089"
                        ],
                        "name": "D. Huttenlocher",
                        "slug": "D.-Huttenlocher",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Huttenlocher",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Huttenlocher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2277383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd9ab441df8b24f473a3635370c69620b00c1e60",
            "isKey": false,
            "numCitedBy": 2423,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a computationally efficient framework for part-based modeling and recognition of objects. Our work is motivated by the pictorial structure models introduced by Fischler and Elschlager. The basic idea is to represent an object by a collection of parts arranged in a deformable configuration. The appearance of each part is modeled separately, and the deformable configuration is represented by spring-like connections between pairs of parts. These models allow for qualitative descriptions of visual appearance, and are suitable for generic recognition problems. We address the problem of using pictorial structure models to find instances of an object in an image as well as the problem of learning an object model from training examples, presenting efficient algorithms in both cases. We demonstrate the techniques by learning models that represent faces and human bodies and using the resulting models to locate the corresponding objects in novel images."
            },
            "slug": "Pictorial-Structures-for-Object-Recognition-Felzenszwalb-Huttenlocher",
            "title": {
                "fragments": [],
                "text": "Pictorial Structures for Object Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A computationally efficient framework for part-based modeling and recognition of objects, motivated by the pictorial structure models introduced by Fischler and Elschlager, that allows for qualitative descriptions of visual appearance and is suitable for generic recognition problems."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31786895"
                        ],
                        "name": "M. Eichner",
                        "slug": "M.-Eichner",
                        "structuredName": {
                            "firstName": "Marcin",
                            "lastName": "Eichner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Eichner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143865718"
                        ],
                        "name": "V. Ferrari",
                        "slug": "V.-Ferrari",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Ferrari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Ferrari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2437110,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "406767a9ea73cb77867aff9e73df40180185471a",
            "isKey": false,
            "numCitedBy": 250,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel approach for estimating body part appearance models for pictorial structures. We learn latent relationships between the appearance of different body parts from annotated images, which then help in estimating better appearance models on novel images. The learned appearance models are general, in that they can be plugged into any pictorial structure engine. In a comprehensive evaluation we demonstrate the bene\ufb01ts brought by the new appearance models to an existing articulated human pose estimation algorithm, on hundreds of highly challenging images from the TV series Buffy the vampire slayer and the PASCAL VOC 2008 challenge."
            },
            "slug": "Better-Appearance-Models-for-Pictorial-Structures-Eichner-Ferrari",
            "title": {
                "fragments": [],
                "text": "Better Appearance Models for Pictorial Structures"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "In a comprehensive evaluation, the bene\ufb01ts brought by the new appearance models to an existing articulated human pose estimation algorithm are demonstrated, on hundreds of highly challenging images from the TV series Buffy the vampire slayer and the PASCAL VOC 2008 challenge."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731948"
                        ],
                        "name": "Paul A. Viola",
                        "slug": "Paul-A.-Viola",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Viola",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul A. Viola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111328101"
                        ],
                        "name": "Michael Jones",
                        "slug": "Michael-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Detecting people using mutually consistent poselet activations."
                    },
                    "intents": []
                }
            ],
            "corpusId": 235084,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4cb4d685b47001652b29dc41c1b3e786277e7647",
            "isKey": false,
            "numCitedBy": 4016,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a visual object detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the \u201cIntegral Image\u201d which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features and yields extremely efficient classifiers [4]. The third contribution is a method for combining classifiers in a \u201ccascade\u201d which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. A set of experiments in the domain of face detection are presented. The system yields face detection performance comparable to the best previous systems [16, 11, 14, 10, 1]. Implemented on a conventional desktop, face detection proceeds at 15 frames per second. Author email: fPaul.Viola,Mike.J.Jonesg@compaq.com c Compaq Computer Corporation, 2001 This work may not be copied or reproduced in whole or in part for any commercial purpose. Permission to copy in whole or in part without payment of fee is granted for nonprofit educational and research purposes provided that all such whole or partial copies include the following: a notice that such copying is by permission of the Cambridge Research Laboratory of Compaq Computer Corporation in Cambridge, Massachusetts; an acknowledgment of the authors and individual contributors to the work; and all applicable portions of the copyright notice. Copying, reproducing, or republishing for any other purpose shall require a license with payment of fee to the Cambridge Research Laboratory. All rights reserved. CRL Technical reports are available on the CRL\u2019s web page at http://crl.research.compaq.com. Compaq Computer Corporation Cambridge Research Laboratory One Cambridge Center Cambridge, Massachusetts 02142 USA"
            },
            "slug": "Robust-Real-time-Object-Detection-Viola-Jones",
            "title": {
                "fragments": [],
                "text": "Robust Real-time Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A visual object detection framework that is capable of processing images extremely rapidly while achieving high detection rates is described, with the introduction of a new image representation called the \u201cIntegral Image\u201d which allows the features used by the detector to be computed very quickly."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731948"
                        ],
                        "name": "Paul A. Viola",
                        "slug": "Paul-A.-Viola",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Viola",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul A. Viola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111328101"
                        ],
                        "name": "Michael Jones",
                        "slug": "Michael-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2796017,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b09ec0b350f8352bce46a2f5bf7ae97c83a7b9ca",
            "isKey": false,
            "numCitedBy": 11227,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a face detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the \u201cIntegral Image\u201d which allows the features used by our detector to be computed very quickly. The second is a simple and efficient classifier which is built using the AdaBoost learning algorithm (Freund and Schapire, 1995) to select a small number of critical visual features from a very large set of potential features. The third contribution is a method for combining classifiers in a \u201ccascade\u201d which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions. A set of experiments in the domain of face detection is presented. The system yields face detection performance comparable to the best previous systems (Sung and Poggio, 1998; Rowley et al., 1998; Schneiderman and Kanade, 2000; Roth et al., 2000). Implemented on a conventional desktop, face detection proceeds at 15 frames per second."
            },
            "slug": "Robust-Real-Time-Face-Detection-Viola-Jones",
            "title": {
                "fragments": [],
                "text": "Robust Real-Time Face Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new image representation called the \u201cIntegral Image\u201d is introduced which allows the features used by the detector to be computed very quickly and a method for combining classifiers in a \u201ccascade\u201d which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49835695"
                        ],
                        "name": "Long Zhu",
                        "slug": "Long-Zhu",
                        "structuredName": {
                            "firstName": "Long",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Long Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144034946"
                        ],
                        "name": "Yuanhao Chen",
                        "slug": "Yuanhao-Chen",
                        "structuredName": {
                            "firstName": "Yuanhao",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuanhao Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2141538523"
                        ],
                        "name": "Yifei Lu",
                        "slug": "Yifei-Lu",
                        "structuredName": {
                            "firstName": "Yifei",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yifei Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49044595"
                        ],
                        "name": "Chenxi Lin",
                        "slug": "Chenxi-Lin",
                        "structuredName": {
                            "firstName": "Chenxi",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chenxi Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 675197,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47dfddafed43bc5afef93ac90ea3376a02046151",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel structure learning method, Max Margin AND/OR graph (MM-AOG), for parsing the human body into parts and recovering their poses. Our method represents the human body and its parts by an AND/OR graph, which is a multi-level mixture of Markov random fields (MRFs). Max-margin learning, which is a generalization of the training algorithm for support vector machines (SVMs), is used to learn the parameters of the AND/OR graph model discriminatively. There are four advantages from this combination of AND/OR graphs and max-margin learning. Firstly, the AND/OR graph allows us to handle enormous articulated poses with a compact graphical model. Secondly, max-margin learning has more discriminative power than the traditional maximum likelihood approach. Thirdly, the parameters of the AND/OR graph model are optimized globally. In particular, the weights of the appearance model for individual nodes and the relative importance of spatial relationships between nodes are learnt simultaneously. Finally, the kernel trick can be used to handle high dimensional features and to enable complex similarity measure of shapes. We perform comparison experiments on the base ball datasets, showing significant improvements over state of the art methods."
            },
            "slug": "Max-Margin-AND/OR-Graph-learning-for-parsing-the-Zhu-Chen",
            "title": {
                "fragments": [],
                "text": "Max Margin AND/OR Graph learning for parsing the human body"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "A novel structure learning method, Max Margin AND/OR graph (MM-AOG), for parsing the human body into parts and recovering their poses, showing significant improvements over state of the art methods."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685089"
                        ],
                        "name": "Pedro F. Felzenszwalb",
                        "slug": "Pedro-F.-Felzenszwalb",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Felzenszwalb",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro F. Felzenszwalb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713089"
                        ],
                        "name": "D. Huttenlocher",
                        "slug": "D.-Huttenlocher",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Huttenlocher",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Huttenlocher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12212153,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1233f38bddaebafe9f4ae676bb2f8671f6c4821a",
            "isKey": false,
            "numCitedBy": 724,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe linear-time algorithms for solving a class of problems that involve transforming a cost function on a grid using spatial information. These problems can be viewed as a generalization of classical distance transforms of binary images, where the binary image is replaced by an arbitrary function on a grid. Alternatively they can be viewed in terms of the minimum convolution of two functions, which is an important operation in grayscale morphology. A consequence of our techniques is a simple and fast method for computing the Euclidean distance transform of a binary image. Our algorithms are also applicable to Viterbi decoding, belief propagation, and optimal control."
            },
            "slug": "Distance-Transforms-of-Sampled-Functions-Felzenszwalb-Huttenlocher",
            "title": {
                "fragments": [],
                "text": "Distance Transforms of Sampled Functions"
            },
            "venue": {
                "fragments": [],
                "text": "Theory Comput."
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765700"
                        ],
                        "name": "Ioannis Tsochantaridis",
                        "slug": "Ioannis-Tsochantaridis",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Tsochantaridis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ioannis Tsochantaridis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783941"
                        ],
                        "name": "Y. Altun",
                        "slug": "Y.-Altun",
                        "structuredName": {
                            "firstName": "Yasemin",
                            "lastName": "Altun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Altun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 564746,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93aa298b40bb3ec23c25239089284fdf61ded917",
            "isKey": false,
            "numCitedBy": 1455,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning general functional dependencies is one of the main goals in machine learning. Recent progress in kernel-based methods has focused on designing flexible and powerful input representations. This paper addresses the complementary issue of problems involving complex outputs such as multiple dependent output variables and structured output spaces. We propose to generalize multiclass Support Vector Machine learning in a formulation that involves features extracted jointly from inputs and outputs. The resulting optimization problem is solved efficiently by a cutting plane algorithm that exploits the sparseness and structural decomposition of the problem. We demonstrate the versatility and effectiveness of our method on problems ranging from supervised grammar learning and named-entity recognition, to taxonomic text classification and sequence alignment."
            },
            "slug": "Support-vector-machine-learning-for-interdependent-Tsochantaridis-Hofmann",
            "title": {
                "fragments": [],
                "text": "Support vector machine learning for interdependent and structured output spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes to generalize multiclass Support Vector Machine learning in a formulation that involves features extracted jointly from inputs and outputs, and demonstrates the versatility and effectiveness of the method on problems ranging from supervised grammar learning and named-entity recognition, to taxonomic text classification and sequence alignment."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153281777"
                        ],
                        "name": "D. Marr",
                        "slug": "D.-Marr",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Marr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Marr"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 53917384,
            "fieldsOfStudy": [
                "Materials Science"
            ],
            "id": "52df763d69ed5fe609a4a88f07e97a6cfaadf59f",
            "isKey": false,
            "numCitedBy": 3089,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "An additive composition which can be adapted into conventional automatic dishwasher detergents for reducing foam during use thereof, comprising:"
            },
            "slug": "Vision:-A-computational-investigation-into-the-Marr",
            "title": {
                "fragments": [],
                "text": "Vision: A computational investigation into the human representation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743045"
                        ],
                        "name": "S. Ullman",
                        "slug": "S.-Ullman",
                        "structuredName": {
                            "firstName": "Shimon",
                            "lastName": "Ullman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ullman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 149
                            }
                        ],
                        "text": "\u2026u\nr R\ne su\nlt P\na sc\na l\nD a\nta se\nt\nDetected\nobject\nMissed Ground\nTruth Object\n0 0.2 0.4 0.6 0.8 1 1.2 0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nre ca\nll\nFPPI\ncat tail\nhead\ntorso forelegs\nLSVM[4]\n0 0.5 1 1.5 2 2.5 3 0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nFPPI\ndog tail\nhead\ntorso\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 96437026,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "107d7f20473bf84efd9d97c63f3488bd76a7b6e9",
            "isKey": true,
            "numCitedBy": 241,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Computational-Investigation-into-the-Human-and-of-Ullman-Poggio",
            "title": {
                "fragments": [],
                "text": "A Computational Investigation into the Human Representation and Processing of Visual Information"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A stochastic grammar of images"
            },
            "venue": {
                "fragments": [],
                "text": "Found. Trends. Comput. Graph. Vis"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Technical report of articulated part-based model"
            },
            "venue": {
                "fragments": [],
                "text": "Technical report of articulated part-based model"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The PASCAL VOC2010 Results"
            },
            "venue": {
                "fragments": [],
                "text": "The PASCAL VOC2010 Results"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {},
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 27,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Articulated-part-based-model-for-joint-object-and-Sun-Savarese/d4a7c54390a3be6822e22e3ead0ad88399cbb188?sort=total-citations"
}