{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2656315"
                        ],
                        "name": "Leipeng Hao",
                        "slug": "Leipeng-Hao",
                        "structuredName": {
                            "firstName": "Leipeng",
                            "lastName": "Hao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Leipeng Hao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777642"
                        ],
                        "name": "Liangcai Gao",
                        "slug": "Liangcai-Gao",
                        "structuredName": {
                            "firstName": "Liangcai",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liangcai Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3412797"
                        ],
                        "name": "Xiaohan Yi",
                        "slug": "Xiaohan-Yi",
                        "structuredName": {
                            "firstName": "Xiaohan",
                            "lastName": "Yi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaohan Yi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143830636"
                        ],
                        "name": "Zhi Tang",
                        "slug": "Zhi-Tang",
                        "structuredName": {
                            "firstName": "Zhi",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi Tang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[2] technique, the Faster-RCNN computes region proposals itself and then helps in determining whether the selected area is a table or not."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[2] presented deep learning based approach for table detection."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[2] presented an approach for table detection in documents using deep learning."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2870724,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "06bf934004b6f93711298f905b1e447683a8d0b9",
            "isKey": true,
            "numCitedBy": 78,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Because of the better performance of deep learning on many computer vision tasks, researchers in the area of document analysis and recognition begin to adopt this technique into their work. In this paper, we propose a novel method for table detection in PDF documents based on convolutional neutral networks, one of the most popular deep learning models. In the proposed method, some table-like areas are selected first by some loose rules, and then the convolutional networks are built and refined to determine whether the selected areas are tables or not. Besides, the visual features of table areas are directly extracted and utilized through the convolutional networks, while the non-visual information (e.g. characters, rendering instructions) contained in original PDF documents is also taken into consideration to help achieve better recognition results. The primary experimental results show that the approach is effective in table detection."
            },
            "slug": "A-Table-Detection-Method-for-PDF-Documents-Based-on-Hao-Gao",
            "title": {
                "fragments": [],
                "text": "A Table Detection Method for PDF Documents Based on Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel method for table detection in PDF documents based on convolutional neutral networks, one of the most popular deep learning models, which shows that the approach is effective in table detection."
            },
            "venue": {
                "fragments": [],
                "text": "2016 12th IAPR Workshop on Document Analysis Systems (DAS)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688013"
                        ],
                        "name": "F. Shafait",
                        "slug": "F.-Shafait",
                        "structuredName": {
                            "firstName": "Faisal",
                            "lastName": "Shafait",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shafait"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108985360"
                        ],
                        "name": "Raymond W. Smith",
                        "slug": "Raymond-W.-Smith",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Smith",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raymond W. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "We have used open sourced UNLV dataset [3] as used in [10] to make a fair comparison of both methodologies."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[10] presented another approach for table detection in heterogeneous documents."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[10] technique (Tesseract), a commercial engine (Abbyy Cloud OCR SDK) and our method is shown in Table I."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[10] to make a fair comparison with their technique."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 37
                            }
                        ],
                        "text": "The evaluation measures described in [10] have been employed."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 85
                            }
                        ],
                        "text": "The formula for finding the overlapped region between two bounding boxes is given by [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[10] and a commercial engine, Abbyy Cloud OCR SDK [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2534837,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2dea52f04a90dc993d2aa30e75b5a5a535e7ca70",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Detecting tables in document images is important since not only do tables contain important information, but also most of the layout analysis methods fail in the presence of tables in the document image. Existing approaches for table detection mainly focus on detecting tables in single columns of text and do not work reliably on documents with varying layouts. This paper presents a practical algorithm for table detection that works with a high accuracy on documents with varying layouts (company reports, newspaper articles, magazine pages, ...). An open source implementation of the algorithm is provided as part of the Tesseract OCR engine. Evaluation of the algorithm on document images from publicly available UNLV dataset shows competitive performance in comparison to the table detection module of a commercial OCR system."
            },
            "slug": "Table-detection-in-heterogeneous-documents-Shafait-Smith",
            "title": {
                "fragments": [],
                "text": "Table detection in heterogeneous documents"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Evaluation of the algorithm on document images from publicly available UNLV dataset shows competitive performance in comparison to the table detection module of a commercial OCR system."
            },
            "venue": {
                "fragments": [],
                "text": "DAS '10"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676116"
                        ],
                        "name": "T. T. Anh",
                        "slug": "T.-T.-Anh",
                        "structuredName": {
                            "firstName": "Tran",
                            "lastName": "Anh",
                            "middleNames": [
                                "Tuan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. T. Anh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2096989346"
                        ],
                        "name": "Na In-Seop",
                        "slug": "Na-In-Seop",
                        "structuredName": {
                            "firstName": "Na",
                            "lastName": "In-Seop",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Na In-Seop"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1394855475"
                        ],
                        "name": "Kim\uff0c Soo-Hyung",
                        "slug": "Kim\uff0c-Soo-Hyung",
                        "structuredName": {
                            "firstName": "Kim\uff0c",
                            "lastName": "Soo-Hyung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kim\uff0c Soo-Hyung"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[17] presented a hybrid approach for table detection in document images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 24826377,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57089523465a963a45153a587b27854afadbd712",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a hybrid method consisting of three main stages for detecting tables in document images. Based on table structure, our system separates table into two main categories, ruling line table and non-ruling line table. In the first stage, the text and non-text elements in document are classified by a heuristic filter. Then, the white space analysis is used to group the text elements into text lines, while ruling line table candidates are identified from non-text elements. In the second stage, based on the text lines, text and non-text elements, a hybrid method which consist of the alternative bottom-up and top-down approaches is implemented to find the table region candidates. In the final stage, these candidates are examined to get the table regions by analyzing text lines and spare lines. Experimental results with the document database from the ICDAR2013 table competition show that the proposed method works better than the previous ones."
            },
            "slug": "A-hybrid-method-for-table-detection-from-document-Anh-In-Seop",
            "title": {
                "fragments": [],
                "text": "A hybrid method for table detection from document image"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A hybrid method consisting of the alternative bottom-up and top-down approaches is implemented to find the table region candidates by analyzing text lines and spare lines for detecting tables in document images."
            },
            "venue": {
                "fragments": [],
                "text": "2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115379063"
                        ],
                        "name": "Jing Fang",
                        "slug": "Jing-Fang",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Fang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Fang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777642"
                        ],
                        "name": "Liangcai Gao",
                        "slug": "Liangcai-Gao",
                        "structuredName": {
                            "firstName": "Liangcai",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liangcai Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064776984"
                        ],
                        "name": "Kun Bai",
                        "slug": "Kun-Bai",
                        "structuredName": {
                            "firstName": "Kun",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kun Bai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29035564"
                        ],
                        "name": "Ruiheng Qiu",
                        "slug": "Ruiheng-Qiu",
                        "structuredName": {
                            "firstName": "Ruiheng",
                            "lastName": "Qiu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ruiheng Qiu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070898875"
                        ],
                        "name": "Xin Tao",
                        "slug": "Xin-Tao",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Tao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xin Tao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087321561"
                        ],
                        "name": "Zhi Tang",
                        "slug": "Zhi-Tang",
                        "structuredName": {
                            "firstName": "Zhi",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi Tang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "Most of the techniques that were proposed in ICDAR 2013 [15], [27] are not data driven and are highly dependent on table layout and extraction of custom features"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10738490,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1839a3dc008765457f434ad6920fb28bbe669a92",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Table detection is always an important task of document analysis and recognition. In this paper, we propose a novel and effective table detection method via visual separators and geometric content layout information, targeting at PDF documents. The visual separators refer to not only the graphic ruling lines but also the white spaces to handle tables with or without ruling lines. Furthermore, we detect page columns in order to assist table region delimitation in complex layout pages. Evaluations of our algorithm on an e-Book dataset and a scientific document dataset show competitive performance. It is noteworthy that the proposed method has been successfully incorporated into a commercial software package for large-scale Chinese e-Book production."
            },
            "slug": "A-Table-Detection-Method-for-Multipage-PDF-via-and-Fang-Gao",
            "title": {
                "fragments": [],
                "text": "A Table Detection Method for Multipage PDF Documents via Visual Seperators and Tabular Structures"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A novel and effective table detection method via visual separators and geometric content layout information, targeting at PDF documents is proposed, successfully incorporated into a commercial software package for large-scale Chinese e-Book production."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Document Analysis and Recognition"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47756656"
                        ],
                        "name": "Jianying Hu",
                        "slug": "Jianying-Hu",
                        "structuredName": {
                            "firstName": "Jianying",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianying Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32225756"
                        ],
                        "name": "R. Kashi",
                        "slug": "R.-Kashi",
                        "structuredName": {
                            "firstName": "Ramanujan",
                            "lastName": "Kashi",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kashi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1828940"
                        ],
                        "name": "D. Lopresti",
                        "slug": "D.-Lopresti",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Lopresti",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lopresti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2859740"
                        ],
                        "name": "G. Wilfong",
                        "slug": "G.-Wilfong",
                        "structuredName": {
                            "firstName": "Gordon",
                            "lastName": "Wilfong",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wilfong"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "These measures include precision and recall [9], [24] that have been used for evaluating various table detection algorithms [1], [8], [24]\u2013[26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[9] presented an approach for table detection while assuming that input images are single columned."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 37293831,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "edbd577d793a083de4f337acac992ec7837609e0",
            "isKey": false,
            "numCitedBy": 90,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "An important step towards the goal of table understanding is a method for reliable table detection. This paper describes a general solution for detecting tables based on computing an optimal partitioning of a document into some number of tables. A dynamic programming algorithm is given to solve the resulting optimization problem. This high-level framework is independent of any particular table quality measure and independent of the document medium. Moreover, it does not rely on the presence of ruling lines or other table delimiters. We also present table quality measures based on white space correlation and vertical connected component analysis. These measures can be applied equally well to ASCII text and scanned images. We report on some preliminary experiments using this method to detect tables in both ASCII text and scanned images, yielding promising results. We present detailed evaluation of these results using three different criteria which by themselves pose interesting research questions."
            },
            "slug": "Medium-independent-table-detection-Hu-Kashi",
            "title": {
                "fragments": [],
                "text": "Medium-independent table detection"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A general solution for detecting tables based on computing an optimal partitioning of a document into some number of tables is described and a dynamic programming algorithm is given to solve the resulting optimization problem."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7232446"
                        ],
                        "name": "B. Gatos",
                        "slug": "B.-Gatos",
                        "structuredName": {
                            "firstName": "Basilios",
                            "lastName": "Gatos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Gatos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3148833"
                        ],
                        "name": "Dimitrios Danatsas",
                        "slug": "Dimitrios-Danatsas",
                        "structuredName": {
                            "firstName": "Dimitrios",
                            "lastName": "Danatsas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dimitrios Danatsas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748249"
                        ],
                        "name": "I. Pratikakis",
                        "slug": "I.-Pratikakis",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Pratikakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Pratikakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2397702"
                        ],
                        "name": "S. Perantonis",
                        "slug": "S.-Perantonis",
                        "structuredName": {
                            "firstName": "Stavros",
                            "lastName": "Perantonis",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Perantonis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[13] proposed an approach for table detection by finding area of intersection between the horizontal and vertical lines."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11485240,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "800ce7dc6b97a48b1e721f03269eb4a59adacabe",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a novel technique for automatic table detection in document images. Lines and tables are among the most frequent graphic, non-textual entities in documents and their detection is directly related to the OCR performance as well as to the document layout description. We propose a workflow for table detection that comprises three distinct steps: (i) image pre-processing; (ii) horizontal and vertical line detection and (iii) table detection. The efficiency of the proposed method is demonstrated by using a performance evaluation scheme which considers a great variety of documents such as forms, newspapers/magazines, scientific journals, tickets/bank cheques, certificates and handwritten documents."
            },
            "slug": "Automatic-Table-Detection-in-Document-Images-Gatos-Danatsas",
            "title": {
                "fragments": [],
                "text": "Automatic Table Detection in Document Images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The efficiency of the proposed method is demonstrated by using a performance evaluation scheme which considers a great variety of documents such as forms, newspapers/magazines, scientific journals, tickets/bank cheques, certificates and handwritten documents."
            },
            "venue": {
                "fragments": [],
                "text": "ICAPR"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787967"
                        ],
                        "name": "Gaurav Harit",
                        "slug": "Gaurav-Harit",
                        "structuredName": {
                            "firstName": "Gaurav",
                            "lastName": "Harit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gaurav Harit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2803087"
                        ],
                        "name": "Anukriti Bansal",
                        "slug": "Anukriti-Bansal",
                        "structuredName": {
                            "firstName": "Anukriti",
                            "lastName": "Bansal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anukriti Bansal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[12] proposed a technique for table detection based on the identification of unique table start and trailer pattern."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11568809,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6770ad067bc57fe7d3e075126eabcba8feeb83a7",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new approach to detect tabular structures present in document images and in low resolution video images. The algorithm for table detection is based on identifying the unique table start pattern and table trailer pattern. We have formulated perceptual attributes to characterize the patterns. The performance of our table detection system is tested on a set of document images picked from UW-III (University of Washington) dataset, UNLV dataset, video images of NPTEL videos, and our own dataset. Our approach demonstrates improved detection for different types of table layouts, with or without ruling lines. We have obtained correct table localization on pages with multiple tables aligned side-by-side."
            },
            "slug": "Table-detection-in-document-images-using-header-and-Harit-Bansal",
            "title": {
                "fragments": [],
                "text": "Table detection in document images using header and trailer patterns"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "This paper presents a new approach to detect tabular structures present in document images and in low resolution video images based on identifying the unique table start pattern and table trailer pattern and formulated perceptual attributes to characterize the patterns."
            },
            "venue": {
                "fragments": [],
                "text": "ICVGIP '12"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47756656"
                        ],
                        "name": "Jianying Hu",
                        "slug": "Jianying-Hu",
                        "structuredName": {
                            "firstName": "Jianying",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianying Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32225756"
                        ],
                        "name": "R. Kashi",
                        "slug": "R.-Kashi",
                        "structuredName": {
                            "firstName": "Ramanujan",
                            "lastName": "Kashi",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kashi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1828940"
                        ],
                        "name": "D. Lopresti",
                        "slug": "D.-Lopresti",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Lopresti",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lopresti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2859740"
                        ],
                        "name": "G. Wilfong",
                        "slug": "G.-Wilfong",
                        "structuredName": {
                            "firstName": "Gordon",
                            "lastName": "Wilfong",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wilfong"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 124
                            }
                        ],
                        "text": "These measures include precision and recall [9], [24] that have been used for evaluating various table detection algorithms [1], [8], [24]\u2013[26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7630958,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "af42cc46f93bc9cf413770af4f7243f54a31336e",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract. While techniques for evaluating the performance of lower-level document analysis tasks such as optical character recognition have gained acceptance in the literature, attempts to formalize the problem for higher-level algorithms, while receiving a fair amount of attention in terms of theory, have generally been less successful in practice, perhaps owing to their complexity. In this paper, we introduce intuitive, easy-to-implement evaluation schemes for the related problems of table detection and table structure recognition. We also present the results of several small experiments, demonstrating how well the methodologies work and the useful sorts of feedback they provide. We first consider the table detection problem. Here algorithms can yield various classes of errors, including non-table regions improperly labeled as tables (insertion errors), tables missed completely (deletion errors), larger tables broken into a number of smaller ones (splitting errors), and groups of smaller tables combined to form larger ones (merging errors). This leads naturally to the use of an edit distance approach for assessing the results of table detection. Next we address the problem of evaluating table structure recognition. Our model is based on a directed acyclic attribute graph, or table DAG. We describe a new paradigm, \u201cgraph probing,\u201d for comparing the results returned by the recognition system and the representation created during ground-truthing. Probing is in fact a general concept that could be applied to other document recognition tasks as well."
            },
            "slug": "Evaluating-the-performance-of-table-processing-Hu-Kashi",
            "title": {
                "fragments": [],
                "text": "Evaluating the performance of table processing algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An intuitive, easy-to-implement evaluation schemes for the related problems of table detection and table structure recognition are introduced and a new paradigm, \u201cgraph probing,\u201d is described for comparing the results returned by the recognition system and the representation created during ground-truthing."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal on Document Analysis and Recognition"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3320897"
                        ],
                        "name": "T. Kieninger",
                        "slug": "T.-Kieninger",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Kieninger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kieninger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 206776168,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "d25a61cc0cd816eba75864912fe2f6f44be3cecf",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper summarizes the core idea of the T-Recs table recognition system, an integrated system covering block-segmentation, table location and a model-free structural analysis of tables. T-Recs works on the output of commercial OCR systems that provide the word bounding box geometry together with the text itself (e.g. Xerox ScanWorX). While T-Recs performs well on a number of document categories, business letters still remained a challenging domain because the T-Recs location heuristics are mislead by their header or footer resulting in a low recognition precision. Business letters such as invoices are a very interesting domain for industrial applications due to the large amount of documents to be analyzed and the importance of the data carried within their tables. Hence, we developed a more restrictive approach which is implemented in the T-Recs++ prototype. This paper describes the ideas of the T-Recs++ location and also proposes a quality evaluation measure that reflects the bottom-up strategy of either T-Recs or T-Recs++. Finally, some results comparing both systems on a collection of business letters are given."
            },
            "slug": "Applying-the-T-Recs-table-recognition-system-to-the-Kieninger-Dengel",
            "title": {
                "fragments": [],
                "text": "Applying the T-Recs table recognition system to the business letter domain"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "This paper summarizes the core idea of the T-Recs table recognition system, an integrated system covering block-segmentation, table location and a model-free structural analysis of tables, and proposes a quality evaluation measure that reflects the bottom-up strategy of either T-recs or T- Recs++."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Sixth International Conference on Document Analysis and Recognition"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404115829"
                        ],
                        "name": "M. A. C. Akmal-Jahan",
                        "slug": "M.-A.-C.-Akmal-Jahan",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Akmal-Jahan",
                            "middleNames": [
                                "A.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. A. C. Akmal-Jahan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144074321"
                        ],
                        "name": "R. Ragel",
                        "slug": "R.-Ragel",
                        "structuredName": {
                            "firstName": "Roshan",
                            "lastName": "Ragel",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ragel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[16] presented a method that uses local thresholds for word spacing and line height for localization and extraction of table regions from document images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7334448,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8c12d52cb739f6e66dab07378c2a42fe24447623",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Pool of knowledge available to the mankind depends on the source of learning resources, which can vary from ancient printed documents to present electronic material. The rapid conversion of material available in traditional libraries to digital form needs a significant amount of work if we are to maintain the format and the look of the electronic documents as same as their printed counterparts. Most of the printed documents contain not only characters and its formatting but also some associated non text objects such as tables, charts and graphical objects. It is challenging to detect them and to concentrate on the format preservation of the contents while reproducing them. To address this issue, we propose an algorithm using local thresholds for word space and line height to locate and extract all categories of tables from scanned document images. From the experiments performed on 298 documents, we conclude that our algorithm has an overall accuracy of about 75% in detecting tables from the scanned document images. Since the algorithm does not completely depend on rule lines, it can detect all categories of tables in a range of scanned documents with different font types, styles and sizes to extract their formatting features. Moreover, the algorithm can be applied to locate tables in multi column layouts with small modification in layout analysis. Treating tables with their existing formatting features will tremendously help the reproducing of printed documents for reprinting and updating purposes."
            },
            "slug": "Locating-tables-in-scanned-documents-for-and-Akmal-Jahan-Ragel",
            "title": {
                "fragments": [],
                "text": "Locating tables in scanned documents for reconstructing and republishing"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An algorithm using local thresholds for word space and line height to locate and extract all categories of tables from scanned document images is proposed, which has an overall accuracy of about 75% in detecting tables from the scanned document image images."
            },
            "venue": {
                "fragments": [],
                "text": "7th International Conference on Information and Automation for Sustainability"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2032184078"
                        ],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "1) Region Proposal Network: As described in [18], Region Proposal Network (RPN) takes the transformed image as an input and returns an output of a set of rectangular object proposals, each with an objectness score."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 55
                            }
                        ],
                        "text": "We have used Caffe based implementation of Faster RCNN [18] to fine tune on our images."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "For detection, our approach employed Faster R-CNN [18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "This architecture [18] is naturally implemented with an n\u00d7n convolutional layer followed by two 1\u00d71 convolutional layers for regression and classification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "Faster R-CNN [18] was initially proposed for natural images."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10328909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "424561d8585ff8ebce7d5d07de8dbf7aae5e7270",
            "isKey": true,
            "numCitedBy": 32561,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available"
            },
            "slug": "Faster-R-CNN:-Towards-Real-Time-Object-Detection-Ren-He",
            "title": {
                "fragments": [],
                "text": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work introduces a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals and further merge RPN and Fast R-CNN into a single network by sharing their convolutionAL features."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48799969"
                        ],
                        "name": "Matthew D. Zeiler",
                        "slug": "Matthew-D.-Zeiler",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Zeiler",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew D. Zeiler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "has used Zeiler and Fergus model (ZF) [23] and Simonyan and Zisserman model (VGG-16) in their experiments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3960646,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a2a770d23b4a171fa81de62a78a3deb0588f238",
            "isKey": false,
            "numCitedBy": 11811,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets."
            },
            "slug": "Visualizing-and-Understanding-Convolutional-Zeiler-Fergus",
            "title": {
                "fragments": [],
                "text": "Visualizing and Understanding Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel visualization technique is introduced that gives insight into the function of intermediate feature layers and the operation of the classifier in large Convolutional Network models, used in a diagnostic role to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40019398"
                        ],
                        "name": "T. Kasar",
                        "slug": "T.-Kasar",
                        "structuredName": {
                            "firstName": "Thotreingam",
                            "lastName": "Kasar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kasar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32179591"
                        ],
                        "name": "Philippine Barlas",
                        "slug": "Philippine-Barlas",
                        "structuredName": {
                            "firstName": "Philippine",
                            "lastName": "Barlas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philippine Barlas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143680806"
                        ],
                        "name": "S\u00e9bastien Adam",
                        "slug": "S\u00e9bastien-Adam",
                        "structuredName": {
                            "firstName": "S\u00e9bastien",
                            "lastName": "Adam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S\u00e9bastien Adam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712446"
                        ],
                        "name": "Cl\u00e9ment Chatelain",
                        "slug": "Cl\u00e9ment-Chatelain",
                        "structuredName": {
                            "firstName": "Cl\u00e9ment",
                            "lastName": "Chatelain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cl\u00e9ment Chatelain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690399"
                        ],
                        "name": "T. Paquet",
                        "slug": "T.-Paquet",
                        "structuredName": {
                            "firstName": "Thierry",
                            "lastName": "Paquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Paquet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "Most of the techniques that were proposed in ICDAR 2013 [15], [27] are not data driven and are highly dependent on table layout and extraction of custom features"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "Kasar [15] presented a method to locate tables by identifying column and row line separators."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18367100,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a389e078d490bde455f9ba1e58902818598bd870",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method to detect table regions in document images by identifying the column and row line-separators and their properties. The method employs a run-length approach to identify the horizontal and vertical lines present in the input image. From each group of intersecting horizontal and vertical lines, a set of 26 low-level features are extracted and an SVM classifier is used to test if it belongs to a table or not. The performance of the method is evaluated on a heterogeneous corpus of French, English and Arabic documents that contain various types of table structures and compared with that of the Tesseract OCR system."
            },
            "slug": "Learning-to-Detect-Tables-in-Scanned-Document-Using-Kasar-Barlas",
            "title": {
                "fragments": [],
                "text": "Learning to Detect Tables in Scanned Document Images Using Line Information"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The performance of the method is evaluated on a heterogeneous corpus of French, English and Arabic documents that contain various types of table structures and compared with that of the Tesseract OCR system."
            },
            "venue": {
                "fragments": [],
                "text": "2013 12th International Conference on Document Analysis and Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2595112"
                        ],
                        "name": "Sekhar Mandal",
                        "slug": "Sekhar-Mandal",
                        "structuredName": {
                            "firstName": "Sekhar",
                            "lastName": "Mandal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sekhar Mandal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105636642"
                        ],
                        "name": "S. Chowdhury",
                        "slug": "S.-Chowdhury",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Chowdhury",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chowdhury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152167018"
                        ],
                        "name": "Amit Kumar Das",
                        "slug": "Amit-Kumar-Das",
                        "structuredName": {
                            "firstName": "Amit Kumar",
                            "lastName": "Das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amit Kumar Das"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784810"
                        ],
                        "name": "B. Chanda",
                        "slug": "B.-Chanda",
                        "structuredName": {
                            "firstName": "Bhabatosh",
                            "lastName": "Chanda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Chanda"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 27098537,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "86a11c997be11cc9ea43ae018d7970c9257be1eb",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The requirement of detection and identification of tables from document images is crucial to any document image analysis and digital library system. In this paper we report a very simple but extremely powerful approach to detect tables present in document pages. The algorithm relies on the observation that the tables have distinct columns which implies that gaps between the fields are substantially larger than the gaps between the words in text lines. This deceptively simple observation has led to the design of a simple but powerful table detection system with low computation cost. Moreover, mathematical foundation of the approach is also established including formation of a regular expression for ease of implementation."
            },
            "slug": "A-simple-and-effective-table-detection-system-from-Mandal-Chowdhury",
            "title": {
                "fragments": [],
                "text": "A simple and effective table detection system from document images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A very simple but extremely powerful approach to detect tables present in document pages based on the observation that the tables have distinct columns which implies that gaps between the fields are substantially larger than the gap between the words in text lines."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Document Analysis and Recognition (IJDAR)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105434437"
                        ],
                        "name": "Scott Tupaj",
                        "slug": "Scott-Tupaj",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Tupaj",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott Tupaj"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110396962"
                        ],
                        "name": "Zhong Shi",
                        "slug": "Zhong-Shi",
                        "structuredName": {
                            "firstName": "Zhong",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhong Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145923790"
                        ],
                        "name": "D. H. Chang",
                        "slug": "D.-H.-Chang",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Chang",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. H. Chang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[11] proposed an OCR based table detection technique."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18379904,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ebeade30e54c102bf7d806739ea0f963ba07a20e",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents work done in locating and extracting tables and their contents from document images. While most research in the area of table analysis and recognition has focused on analyzing the raster image, our approach builds upon the advances in optical character recognition (OCR) software to preserve the layout of tabular data by means of white space. By using methods to analyze the geometry, syntax, and the semantics of the character data, as well as utilizing some well-known image processing techniques, we are able to 1) isolate embedded tables from documents, and 2) identify table components uch as title blocks, table entries, and footer blocks. Furthermore, the table analysis techniques presented in this paper can also be applied when analyzing blocks of text isolated by traditional methods such as connected component analysis[1] or bounding box [2]."
            },
            "slug": "Extracting-Tabular-Information-From-Text-Files-Tupaj-Shi",
            "title": {
                "fragments": [],
                "text": "Extracting Tabular Information From Text Files"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The approach builds upon the advances in optical character recognition software to preserve the layout of tabular data by means of white space to isolate embedded tables from documents and identify table components uch as title blocks, table entries, and footer blocks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115625054"
                        ],
                        "name": "Yalin Wang",
                        "slug": "Yalin-Wang",
                        "structuredName": {
                            "firstName": "Yalin",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yalin Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710238"
                        ],
                        "name": "R. Haralick",
                        "slug": "R.-Haralick",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Haralick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Haralick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744200"
                        ],
                        "name": "I. T. Phillips",
                        "slug": "I.-T.-Phillips",
                        "structuredName": {
                            "firstName": "Ihsin",
                            "lastName": "Phillips",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. T. Phillips"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 129
                            }
                        ],
                        "text": "These measures include precision and recall [9], [24] that have been used for evaluating various table detection algorithms [1], [8], [24]\u2013[26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 628973,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd7e243fb7041c582a479116a11126904f2be010",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We first describe an automatic table ground truth generation system which can efficiently generate a large amount of accurate table ground truth suitable for the development of table detection algorithms. Then a novel background analysis-based, coarse-to-fine table identification algorithm and an X-Y cut table decomposition algorithm are described. We discuss an experimental protocol to evaluate the table detection algorithms. For a total of 1,125 document pages having 518 table entities and a total of 10,941 cell entities, our table detection algorithm takes line, word segmentation results as input and obtains around 90% cell correct detection rates."
            },
            "slug": "Automatic-table-ground-truth-generation-and-a-table-Wang-Haralick",
            "title": {
                "fragments": [],
                "text": "Automatic table ground truth generation and a background-analysis-based table structure extraction method"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "An automatic table groundtruth generation system which can efficiently generate a large amount of accurate table ground truth suitable for the development of table detection algorithms and an X-Y cut table decomposition algorithm are described."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of Sixth International Conference on Document Analysis and Recognition"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688013"
                        ],
                        "name": "F. Shafait",
                        "slug": "F.-Shafait",
                        "structuredName": {
                            "firstName": "Faisal",
                            "lastName": "Shafait",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Shafait"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51027911"
                        ],
                        "name": "Daniel Keysers",
                        "slug": "Daniel-Keysers",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Keysers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Keysers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733858"
                        ],
                        "name": "T. Breuel",
                        "slug": "T.-Breuel",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Breuel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Breuel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 139
                            }
                        ],
                        "text": "These measures include precision and recall [9], [24] that have been used for evaluating various table detection algorithms [1], [8], [24]\u2013[26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8746521,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41d424d8b63e1f2a94f348d897bc19a817257894",
            "isKey": false,
            "numCitedBy": 203,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Informative benchmarks are crucial for optimizing the page segmentation step of an OCR system, frequently the performance limiting step for overall OCR system performance. We show that current evaluation scores are insufficient for diagnosing specific errors in page segmentation and fail to identify some classes of serious segmentation errors altogether. This paper introduces a vectorial score that is sensitive to, and identifies, the most important classes of segmentation errors (over, under, and mis-segmentation) and what page components (lines, blocks, etc.) are affected. Unlike previous schemes, our evaluation method has a canonical representation of ground-truth data and guarantees pixel-accurate evaluation results for arbitrary region shapes. We present the results of evaluating widely used segmentation algorithms (x-y cut, smearing, whitespace analysis, constrained text-line finding, docstrum, and Voronoi) on the UW-III database and demonstrate that the new evaluation scheme permits the identification of several specific flaws in individual segmentation methods."
            },
            "slug": "Performance-Evaluation-and-Benchmarking-of-Six-Page-Shafait-Keysers",
            "title": {
                "fragments": [],
                "text": "Performance Evaluation and Benchmarking of Six-Page Segmentation Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A vectorial score that is sensitive to, and identifies, the most important classes of segmentation errors (over, under, and mis-segmentation) and what page components (lines, blocks, etc.) are affected."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "We have used Caffe based implementation of Faster RCNN [18] to fine tune on our images."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "RPN shares common set of convolutional layers with detector module of Faster-RCNN."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "Then, this paper uses Faster Recurrent Convolutional Neural Network (Faster RCNN) as the deep learning module."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "In contrary to Hao et al. [2] technique, the Faster-RCNN computes region proposals itself and then helps in determining whether the selected area is a table or not."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 121
                            }
                        ],
                        "text": "The region proposals are fed to the second module which is the detector module that was originally proposed in Fast RCNN [22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206770307,
            "fieldsOfStudy": [
                "Computer Science",
                "Environmental Science"
            ],
            "id": "7ffdbc358b63378f07311e883dddacc9faeeaf4b",
            "isKey": true,
            "numCitedBy": 14072,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn."
            },
            "slug": "Fast-R-CNN-Girshick",
            "title": {
                "fragments": [],
                "text": "Fast R-CNN"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection that builds on previous work to efficiently classify object proposals using deep convolutional networks."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3320897"
                        ],
                        "name": "T. Kieninger",
                        "slug": "T.-Kieninger",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Kieninger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kieninger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58220542,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1fdbf577a1b8d6e78556a509326bbf304af5780",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Document organization may be described in different ways. The physical presentation on one hand by means of a nested layout structure representing a \u201cpart-of\u201d-relationship, e.g. of text blocks, text line segments, words, and characters. The composition of meaningful entities, such as title, author, address, or abstract, on the other, in terms of a logical structure. Both views to the contents of a document are complementary. They relate to each other as being explicitely given by publication guidelines where the position and dimensions of logical objects are precisely described. Moreover such \u201cpublication guidelines\u201d more or less hold for various types of documents. Although they are intuitively understandable for human beings, they are hard to formalize explicitly because of the freedom originators and authors of documents have in order to incorporate their own creativity. However, humans use these intrinsic layout features to get first hints towards the logical meaning of document information. This is even obvious when considering the very difficult document examples in Figure 1 where no text is given but it is nevertheless possible to generate hypothesis for logical meaning."
            },
            "slug": "Table-Recognition-and-Labeling-Using-Intrinsic-Kieninger-Dengel",
            "title": {
                "fragments": [],
                "text": "Table Recognition and Labeling Using Intrinsic Layout Features"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Although intrinsic layout features of documents are intuitively understandable for human beings, they are hard to formalize explicitly because of the freedom originators and authors of documents have in order to incorporate their own creativity."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3320897"
                        ],
                        "name": "T. Kieninger",
                        "slug": "T.-Kieninger",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Kieninger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kieninger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 49
                            }
                        ],
                        "text": "These measures include precision and recall [9], [24] that have been used for evaluating various table detection algorithms [1], [8], [24]\u2013[26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17912915,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "05b21303b2acf90b3d08144a67448299e76d894c",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "After we developed a model free table recognition system we had the desire to automatically register the effect of minor changes to parameters upon the overall performance quality of our system in order to tune parameters. Therefore we developed a complete benchmarking environment, containing a user front-end to acquire ground truth data as well as mechanisms to evaluate the quality of the recognition results. The tasks involved in the analysis systems were the locating of table regions, identification of cells and mapping of cells to rows and columns. This paper presents our approach towards the comparison of recognition results with the ground truth. The established definitions of recall and precision did not meet our requirements, as we wanted to register even smallest improvements (or changes in general) in the results, even when both results were imperfect. We therefore extended the measures recall and precision in order to deal with recognition probabilities of objects rather than just with Boolean values."
            },
            "slug": "An-approach-towards-benchmarking-of-table-structure-Kieninger-Dengel",
            "title": {
                "fragments": [],
                "text": "An approach towards benchmarking of table structure recognition results"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A complete benchmarking environment, containing a user front-end to acquire ground truth data as well as mechanisms to evaluate the quality of the recognition results, is developed, presenting the approach towards the comparison of recognition results with the ground truth."
            },
            "venue": {
                "fragments": [],
                "text": "Eighth International Conference on Document Analysis and Recognition (ICDAR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49915337"
                        ],
                        "name": "A. C. E. Silva",
                        "slug": "A.-C.-E.-Silva",
                        "structuredName": {
                            "firstName": "Ana",
                            "lastName": "Silva",
                            "middleNames": [
                                "Costa",
                                "e"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. C. E. Silva"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "Costa e Silva [14] presented a technique for table detection using Hidden Markov Models (HMMs)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7260147,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "166e2f3ff52620ffc975acca6281915995498605",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Hidden Markov Models (HMM) are probabilistic graphical models for interdependent classification. In this paper we experiment with different ways of combining the components of an HMM for document analysis applications, in particular for finding tables in text. We show: a) how to integrate different document structure finders into the HMM; b) that transition probabilities should vary along the chain to embed general knowledge axioms of our field, c) some emission energies can be selectively ignored, and d) emission and transition probabilities can be weighed differently. We conclude these changes increase the expressiveness and usability of HMMs in our field."
            },
            "slug": "Learning-Rich-Hidden-Markov-Models-in-Document-Silva",
            "title": {
                "fragments": [],
                "text": "Learning Rich Hidden Markov Models in Document Analysis: Table Location"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This paper experiments with different ways of combining the components of an HMM for document analysis applications, in particular for finding tables in text, and shows how to integrate different document structure finders into the HMM."
            },
            "venue": {
                "fragments": [],
                "text": "2009 10th International Conference on Document Analysis and Recognition"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40270396"
                        ],
                        "name": "R. Fabbri",
                        "slug": "R.-Fabbri",
                        "structuredName": {
                            "firstName": "Ricardo",
                            "lastName": "Fabbri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fabbri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145882231"
                        ],
                        "name": "L. F. D. Costa",
                        "slug": "L.-F.-D.-Costa",
                        "structuredName": {
                            "firstName": "Luciano",
                            "lastName": "Costa",
                            "middleNames": [
                                "da",
                                "Fontoura"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. F. D. Costa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1934737"
                        ],
                        "name": "Julio C. Torelli",
                        "slug": "Julio-C.-Torelli",
                        "structuredName": {
                            "firstName": "Julio",
                            "lastName": "Torelli",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julio C. Torelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689373"
                        ],
                        "name": "O. Bruno",
                        "slug": "O.-Bruno",
                        "structuredName": {
                            "firstName": "Odemir",
                            "lastName": "Bruno",
                            "middleNames": [
                                "Martinez"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bruno"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12824109,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c3e09f71d4d0ec31ade6ce666fe2a1d0317e0ff",
            "isKey": false,
            "numCitedBy": 461,
            "numCiting": 148,
            "paperAbstract": {
                "fragments": [],
                "text": "The distance transform (DT) is a general operator forming the basis of many methods in computer vision and geometry, with great potential for practical applications. However, all the optimal algorithms for the computation of the exact Euclidean DT (EDT) were proposed only since the 1990s. In this work, state-of-the-art sequential 2D EDT algorithms are reviewed and compared, in an effort to reach more solid conclusions regarding their differences in speed and their exactness. Six of the best algorithms were fully implemented and compared in practice."
            },
            "slug": "2D-Euclidean-distance-transform-algorithms:-A-Fabbri-Costa",
            "title": {
                "fragments": [],
                "text": "2D Euclidean distance transform algorithms: A comparative survey"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "In this work, state-of-the-art sequential 2D EDT algorithms are reviewed and compared, in an effort to reach more solid conclusions regarding their differences in speed and their exactness."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143811174"
                        ],
                        "name": "H. Breu",
                        "slug": "H.-Breu",
                        "structuredName": {
                            "firstName": "Heinz",
                            "lastName": "Breu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Breu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054718690"
                        ],
                        "name": "Joseph Gil",
                        "slug": "Joseph-Gil",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Gil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph Gil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143950908"
                        ],
                        "name": "D. Kirkpatrick",
                        "slug": "D.-Kirkpatrick",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Kirkpatrick",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kirkpatrick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27379268"
                        ],
                        "name": "M. Werman",
                        "slug": "M.-Werman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Werman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Werman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 100
                            }
                        ],
                        "text": "It then computes Euclidean distance transform, linear distance transform and max distance transform [19]\u2013[21] on"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "Distance transform [19]\u2013 [21] is a derived representation of digital image."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 22636389,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5281ab66c98d241fafb61f27a8071b4e7185afca",
            "isKey": false,
            "numCitedBy": 329,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Two linear time (and hence asymptotically optimal) algorithms for computing the Euclidean distance transform of a two-dimensional binary image are presented. The algorithms are based on the construction and regular sampling of the Voronoi diagram whose sites consist of the unit (feature) pixels in the image. The first algorithm, which is of primarily theoretical interest, constructs the complete Voronoi diagram. The second, more practical, algorithm constructs the Voronoi diagram where it intersects the horizontal lines passing through the image pixel centers. Extensions to higher dimensional images and to other distance functions are also discussed. >"
            },
            "slug": "Linear-Time-Euclidean-Distance-Algorithms-Breu-Gil",
            "title": {
                "fragments": [],
                "text": "Linear Time Euclidean Distance Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "Two linear time algorithms for computing the Euclidean distance transform of a two-dimensional binary image are presented based on the construction and regular sampling of the Voronoi diagram whose sites consist of the unit pixels in the image."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1872690"
                        ],
                        "name": "I. Ragnemalm",
                        "slug": "I.-Ragnemalm",
                        "structuredName": {
                            "firstName": "Ingemar",
                            "lastName": "Ragnemalm",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Ragnemalm"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 105
                            }
                        ],
                        "text": "It then computes Euclidean distance transform, linear distance transform and max distance transform [19]\u2013[21] on"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "Distance transform [19]\u2013 [21] is a derived representation of digital image."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 34705215,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c96f60daca70f1da37a01c8ae64ed5292d0ee7ce",
            "isKey": false,
            "numCitedBy": 165,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Euclidean-distance-transform-in-arbitrary-Ragnemalm",
            "title": {
                "fragments": [],
                "text": "The Euclidean distance transform in arbitrary dimensions"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71619391"
                        ],
                        "name": "\u0e2d\u0e19\u0e34\u0e23\u0e38\u0e18 \u0e2a\u0e37\u0e1a\u0e2a\u0e34\u0e07\u0e2b\u0e4c",
                        "slug": "\u0e2d\u0e19\u0e34\u0e23\u0e38\u0e18-\u0e2a\u0e37\u0e1a\u0e2a\u0e34\u0e07\u0e2b\u0e4c",
                        "structuredName": {
                            "firstName": "\u0e2d\u0e19\u0e34\u0e23\u0e38\u0e18",
                            "lastName": "\u0e2a\u0e37\u0e1a\u0e2a\u0e34\u0e07\u0e2b\u0e4c",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u0e2d\u0e19\u0e34\u0e23\u0e38\u0e18 \u0e2a\u0e37\u0e1a\u0e2a\u0e34\u0e07\u0e2b\u0e4c"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "As the dataset is small so we have used transfer learning approach [28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 64641472,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b42b1bfdc262bf99e9484e2e9df94df216b96374",
            "isKey": false,
            "numCitedBy": 10618,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Data-Mining-Practical-Machine-Learning-Tools-and-\u0e2a\u0e37\u0e1a\u0e2a\u0e34\u0e07\u0e2b\u0e4c",
            "title": {
                "fragments": [],
                "text": "Data Mining Practical Machine Learning Tools and Techniques"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "OCR SDK engine"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2017
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Table detection in heterogeneous documents , \u201d in Proceedings of the 9 th IAPR International Workshop on Document Analysis Systems"
            },
            "venue": {
                "fragments": [],
                "text": "ACM"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A paper-to-html table converting system"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of document analysis systems (DAS), vol. 98, 1998."
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 102
                            }
                        ],
                        "text": "Additionally, we have used publicly available UNLV dataset for evaluation of our proposed methodology [3] where it gives better results than Tesseract\u2019s table detection system."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 107
                            }
                        ],
                        "text": "In order to evaluate the performance of the proposed methodology, we chose publicly available UNLV dataset [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 39
                            }
                        ],
                        "text": "We have used open sourced UNLV dataset [3] as used in [10] to make a fair comparison of both methodologies."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Table ground truth for the UW3 and UNLV datasets"
            },
            "venue": {
                "fragments": [],
                "text": "[Online; accessed 7-April-2017]. [Online]. Available: http://www.iapr-tc11.org/mediawiki/index.php?title=Table_ Ground_Truth_for_the_UW3_and_UNLV_datasets"
            },
            "year": 2017
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 8,
            "methodology": 20
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 29,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Table-Detection-Using-Deep-Learning-Gilani-Qasim/9d6931dd0ba9e492b0ceab00268ca4be62ef663a"
}