{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085881785"
                        ],
                        "name": "Irko",
                        "slug": "Irko",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Irko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Irko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2098039661"
                        ],
                        "name": "Ronzi",
                        "slug": "Ronzi",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Ronzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2079052885"
                        ],
                        "name": "Alter",
                        "slug": "Alter",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Alter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105909865"
                        ],
                        "name": "Rescenzi",
                        "slug": "Rescenzi",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Rescenzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rescenzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1394029917"
                        ],
                        "name": "aolo",
                        "slug": "aolo",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "aolo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "aolo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083151706"
                        ],
                        "name": "Erialdo",
                        "slug": "Erialdo",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Erialdo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Erialdo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 60
                            }
                        ],
                        "text": "The proof, which is rather straightforward, can be found in [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10239105,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5afa5c75d5b11ddecaee5594e502ae2c04ee4f2b",
            "isKey": false,
            "numCitedBy": 1,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a formal framework for an unsupervised approach tackling two problems simultaneously: the data extraction problem, for generating the extraction rules needed to gain data from web pages, and the data integration problem, to integrate the data coming from several partially redundant web sources. We motivate the approach by showing its advantages with regard to the traditional waterfall approach, in which data are extracted upfront, before the integration starts without any mutual dependency between the two tasks. In this paper, we focus on data exposed by structured and redundant web sources. We introduce novel polynomial algorithms to solve the stated problems and formally prove their correctness. Along the way, we precisely characterize the amount of redundancy needed by our algorithm to produce a solution, and present experimental results to show the benefits of our approach w.r.t. state-of-the-art solutions."
            },
            "slug": "Via-della-Vasca-Navale-,-79-00146-Roma-,-Italy-and-Irko-Ronzi",
            "title": {
                "fragments": [],
                "text": "Via della Vasca Navale , 79 00146 Roma , Italy Extraction and Integration of Partially Overlapping Web Sources"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Novel polynomial algorithms are introduced to solve the stated problems and formally prove their correctness and the amount of redundancy needed by the algorithm to produce a solution, and experimental results are presented to show the benefits of the approach w.r.t. state-of-the-art solutions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725561"
                        ],
                        "name": "Michael J. Cafarella",
                        "slug": "Michael-J.-Cafarella",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Cafarella",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Cafarella"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770962"
                        ],
                        "name": "A. Halevy",
                        "slug": "A.-Halevy",
                        "structuredName": {
                            "firstName": "Alon",
                            "lastName": "Halevy",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Halevy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786879"
                        ],
                        "name": "Nodira Khoussainova",
                        "slug": "Nodira-Khoussainova",
                        "structuredName": {
                            "firstName": "Nodira",
                            "lastName": "Khoussainova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nodira Khoussainova"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 156
                            }
                        ],
                        "text": "Information extraction systems, such as ReVerb [21], tackle the two issues synergically and aim at extracting and integrating huge amounts of data from the Web."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 275,
                                "start": 272
                            }
                        ],
                        "text": "In addition, it was able to extract and integrate more attributes than those specified in the SWDE dataset, such as (we report one example per domain): Transmissions for Autos, ListPrice for Books, MegaPixel for Cameras, RunningTime for Movies, BirthDate for NBA Players, Website for Restaurants, and Address for Universities."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Web-scale information extraction with vertex."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 34
                            }
                        ],
                        "text": "In this section, we present WEIR (Web-Extraction and Integration of Redundant data), our algorithm for solving the Abstract Relation Discovery problem."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 5
                            }
                        ],
                        "text": "In IIWeb, 2003."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Web data extraction and integration is still an expensive process, which needs human supervision in many steps to achieve high quality results."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 168
                            }
                        ],
                        "text": "Also, we observe that it is rather easy to collect detail pages from these websites by means of a crawler based on set-expansion techniques (e.g., [6]) for the surface Web, or by using form-filling techniques (e.g., [28]) for the hidden Web."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 101
                            }
                        ],
                        "text": "These technical challenges are reflected in systems designed for web data extraction and integration [9, 23, 24, 30], where ad-hoc user input (such as annotated pages) is required to achieve acceptable results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 61
                            }
                        ],
                        "text": "Although the impressive number of sources and domains on the Web has given rise to several research proposals for automatically extracting and integrating web data, so far extraction and integration problems have been mainly tackled separately: many researchers have proposed techniques for extracting data from the Web [12], while others have concentrated their solutions on integrating the extracted data [5]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 382,
                                "start": 379
                            }
                        ],
                        "text": "The paper makes the following contributions: (i) we formulate an abstract generative model that characterizes partially overlapping data-intensive web sources; (ii) based on the model, we introduce a formal setting to state the data extraction and integration problem, exploiting the redundancy of information among the sources; (iii) we propose an unsupervised algorithm, WEIR (Web-Extraction and Integration of Redundant data), to solve the stated problem, and formally study its correctness; (iv) we show robustness and performance of our approach against alternative solutions in an experimental evaluation with real-world websites."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 7
                            }
                        ],
                        "text": "On the Web, for many domains this is a natural assumption, and the identifiers can be derived during the page harvesting phase, or automatically extracted from the page collections starting from a small seed set."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Webtables: exploring the power of tables on the web."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 43
                            }
                        ],
                        "text": "The massive exploitation of the structured Web has been studied for data published in HTML tables and lists [10, 20]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "Labeling Data Extracted from the Web."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 38
                            }
                        ],
                        "text": "We collected 40 data sources from the Web over four domain entities: soccer players, stock quotes, video games, and books.5 Detail pages for the video games and soccer players domains were gathered by means of a crawler based on a set expansion technique [6]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 31
                            }
                        ],
                        "text": "It is well recognized that the Web is a valuable source of information and that making use of its data is an incredible opportunity to create knowledge with both scientific and commercial implications."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 107
                            }
                        ],
                        "text": "A large body of works has tackled the challenge of extracting and integrating structured data from the Web [9, 23, 30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Web data extraction has been addressed in multiple works in the last decade (see [12] for a survey)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "OCTOPUS [9] and CIMPLE [30] support users in the creation of datasets from web data by means of a set of operators to perform search, extraction, data cleaning and integration."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 117
                            }
                        ],
                        "text": "The problem of finding labels for web data has been studied in [17], which exploits the redundancy of information on Web."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Web data extraction involve several tasks: source discovery, wrapper generation, data integration, and data cleaning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 7
                            }
                        ],
                        "text": "On the Web, redundant sources publish data of many different types and with different unit of measures."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 8
                            }
                        ],
                        "text": "OCTOPUS [9] and CIMPLE [30] support users in the creation of datasets from web data by means of a set of operators"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9492731,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c687d5f26d78f2ba5e66e47af6db721c639f907",
            "isKey": true,
            "numCitedBy": 223,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The Web contains a vast amount of structured information such as HTML tables, HTML lists and deep-web databases; there is enormous potential in combining and re-purposing this data in creative ways. However, integrating data from this relational web raises several challenges that are not addressed by current data integration systems or mash-up tools. First, the structured data is usually not published cleanly and must be extracted (say, from an HTML list) before it can be used. Second, due to the vastness of the corpus, a user can never know all of the potentially-relevant databases ahead of time (much less write a wrapper or mapping for each one); the source databases must be discovered during the integration process. Third, some of the important information regarding the data is only present in its enclosing web page and needs to be extracted appropriately. \n \nThis paper describes Octopus, a system that combines search, extraction, data cleaning and integration, and enables users to create new data sets from those found on the Web. The key idea underlying Octopus is to offer the user a set of best-effort operators that automate the most labor-intensive tasks. For example, the Search operator takes a search-style keyword query and returns a set of relevance-ranked and similarity-clustered structured data sources on the Web; the Context operator helps the user specify the semantics of the sources by inferring attribute values that may not appear in the source itself, and the Extend operator helps the user find related sources that can be joined to add new attributes to a table. Octopus executes some of these operators automatically, but always allows the user to provide feedback and correct errors. We describe the algorithms underlying each of these operators and experiments that demonstrate their efficacy."
            },
            "slug": "Data-Integration-for-the-Relational-Web-Cafarella-Halevy",
            "title": {
                "fragments": [],
                "text": "Data Integration for the Relational Web"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Octopus is a system that combines search, extraction, data cleaning and integration, and enables users to create new data sets from those found on the Web, to offer the user a set of best-effort operators that automate the most labor-intensive tasks."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. VLDB Endow."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2339397"
                        ],
                        "name": "Michele Banko",
                        "slug": "Michele-Banko",
                        "structuredName": {
                            "firstName": "Michele",
                            "lastName": "Banko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michele Banko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725561"
                        ],
                        "name": "Michael J. Cafarella",
                        "slug": "Michael-J.-Cafarella",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Cafarella",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Cafarella"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144295318"
                        ],
                        "name": "S. Soderland",
                        "slug": "S.-Soderland",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Soderland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Soderland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50452701"
                        ],
                        "name": "M. Broadhead",
                        "slug": "M.-Broadhead",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Broadhead",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Broadhead"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "However, as argued in [4], these systems are designed for textual corpora, and they cannot extract data from in HTML templates, which conversely represent our target."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 61
                            }
                        ],
                        "text": "Recently, so called \u201copen\u201d information extraction approaches [4, 21] have been developed: they automatically discover phrases to extract new facts from sentences, and they are not constrained to learn an extractor only for the target relations specified by means of training samples."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207169186,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "498bb0efad6ec15dd09d941fb309aa18d6df9f5f",
            "isKey": false,
            "numCitedBy": 2290,
            "numCiting": 148,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditionally, Information Extraction (IE) has focused on satisfying precise, narrow, pre-specified requests from small homogeneous corpora (e.g., extract the location and time of seminars from a set of announcements). Shifting to a new domain requires the user to name the target relations and to manually create new extraction rules or hand-tag new training examples. This manual labor scales linearly with the number of target relations. This paper introduces Open IE (OIE), a new extraction paradigm where the system makes a single data-driven pass over its corpus and extracts a large set of relational tuples without requiring any human input. The paper also introduces TEXTRUNNER, a fully implemented, highly scalable OIE system where the tuples are assigned a probability and indexed to support efficient extraction and exploration via user queries. We report on experiments over a 9,000,000 Web page corpus that compare TEXTRUNNER with KNOWITALL, a state-of-the-art Web IE system. TEXTRUNNER achieves an error reduction of 33% on a comparable set of extractions. Furthermore, in the amount of time it takes KNOWITALL to perform extraction for a handful of pre-specified relations, TEXTRUNNER extracts a far broader set of facts reflecting orders of magnitude more relations, discovered on the fly. We report statistics on TEXTRUNNER\u2019s 11,000,000 highest probability tuples, and show that they contain over 1,000,000 concrete facts and over 6,500,000more abstract assertions."
            },
            "slug": "Open-Information-Extraction-from-the-Web-Banko-Cafarella",
            "title": {
                "fragments": [],
                "text": "Open Information Extraction from the Web"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Open IE (OIE), a new extraction paradigm where the system makes a single data-driven pass over its corpus and extracts a large set of relational tuples without requiring any human input, is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052599266"
                        ],
                        "name": "Qiang Hao",
                        "slug": "Qiang-Hao",
                        "structuredName": {
                            "firstName": "Qiang",
                            "lastName": "Hao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiang Hao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145625690"
                        ],
                        "name": "Rui Cai",
                        "slug": "Rui-Cai",
                        "structuredName": {
                            "firstName": "Rui",
                            "lastName": "Cai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rui Cai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145134722"
                        ],
                        "name": "Yanwei Pang",
                        "slug": "Yanwei-Pang",
                        "structuredName": {
                            "firstName": "Yanwei",
                            "lastName": "Pang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yanwei Pang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39089563"
                        ],
                        "name": "Lei Zhang",
                        "slug": "Lei-Zhang",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 125
                            }
                        ],
                        "text": "Finally, we compared WEIR against a state-of-the-art approach that takes as input human annotations to bootstrap the process [24]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "Figure 9: Comparison of WEIR vs [24] over the SWDE dataset: AUTOS (Model, Price, Engine, Fuel), BOOKS (Title, Author, ISBN, Publisher,"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 136
                            }
                        ],
                        "text": "Then, we compare WEIR against alternative unsupervised solutions, and against a system requiring human annotations as part of the input [24]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 101
                            }
                        ],
                        "text": "These technical challenges are reflected in systems designed for web data extraction and integration [9, 23, 24, 30], where ad-hoc user input (such as annotated pages) is required to achieve acceptable results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 79
                            }
                        ],
                        "text": "We used the dataset \u201cas-is\u201d and, to be comparable with the results reported in [24], we used their performance metrics that are based on precision and recall over the expected values in the golden dataset."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 11
                            }
                        ],
                        "text": "Similarly, [22, 23, 24, 31] require labeled examples to bootstrap the extraction process and, with the notable exception of [31], they can extract only data from the attributes annotated in the input pages (i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17002481,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5da5952da2fceac1823393ee2e0bdde0e0a02d2b",
            "isKey": true,
            "numCitedBy": 77,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Structured data, in the form of entities and associated attributes, has been a rich web resource for search engines and knowledge databases. To efficiently extract structured data from enormous websites in various verticals (e.g., books, restaurants), much research effort has been attracted, but most existing approaches either require considerable human effort or rely on strong features that lack of flexibility. We consider an ambitious scenario -- can we build a system that (1) is general enough to handle any vertical without re-implementation and (2) requires only one labeled example site from each vertical for training to automatically deal with other sites in the same vertical? In this paper, we propose a unified solution to demonstrate the feasibility of this scenario. Specifically, we design a set of weak but general features to characterize vertical knowledge (including attribute-specific semantics and inter-attribute layout relationships). Such features can be adopted in various verticals without redesign; meanwhile, they are weak enough to avoid overfitting of the learnt knowledge to seed sites. Given a new unseen site, the learnt knowledge is first applied to identify page-level candidate attribute values, while inevitably involve false positives. To remove noise, site-level information of the new site is then exploited to boost up the true values. The site-level information is derived in an unsupervised manner, without harm to the applicability of the solution. Promising experimental performance on 80 websites in 8 distinct verticals demonstrated the feasibility and flexibility of the proposed solution."
            },
            "slug": "From-one-tree-to-a-forest:-a-unified-solution-for-Hao-Cai",
            "title": {
                "fragments": [],
                "text": "From one tree to a forest: a unified solution for structured web data extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A set of weak but general features to characterize vertical knowledge (including attribute-specific semantics and inter-attribute layout relationships) are designed that can be adopted in various verticals without redesign; meanwhile, they are weak enough to avoid overfitting of the learnt knowledge to seed sites."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1819655"
                        ],
                        "name": "Shui-Lung Chuang",
                        "slug": "Shui-Lung-Chuang",
                        "structuredName": {
                            "firstName": "Shui-Lung",
                            "lastName": "Chuang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shui-Lung Chuang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143922493"
                        ],
                        "name": "K. Chang",
                        "slug": "K.-Chang",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Chang",
                            "middleNames": [
                                "Chen-Chuan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736467"
                        ],
                        "name": "ChengXiang Zhai",
                        "slug": "ChengXiang-Zhai",
                        "structuredName": {
                            "firstName": "ChengXiang",
                            "lastName": "Zhai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "ChengXiang Zhai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "An approach related to ours is TurboWrapper [13], which introduces a composite architecture including several wrapper generator systems aiming at improving the results of the single participating systems taken separately."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7503916,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3ce7caa291ea1f7e978bcdf6cec693719263fec",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "The deep Web presents a pressing need for integrating large numbers of dynamically evolving data sources. To be more automatic yet accurate in building an integration system, we observe two problems: First, across sequential tasks in integration, how can a wrapper (as an extraction task) consider the peer sources to facilitate the subsequent matching task? Second, across parallel sources, how can a wrapper leverage the peer wrappers or domain rules to enhance extraction accuracy? These issues, while seemingly unrelated, both boil down to the lack of \"context awareness\": Current automatic wrapper induction approaches generate a wrapper for one source at a time, in isolation, and thus inherently lack the awareness of the peer sources or domain knowledge in the context of integration. We propose the concept of context-aware wrappers that are amenable to matching and that can leverage peer wrappers or prior domain knowledge. Such context awareness inspires a synchronization framework to construct wrappers consistently and collaboratively across their mutual context. We draw the insight from turbo codes and develop the turbo syncer to interconnect extraction with matching, which together achieve context awareness in wrapping. Our experiments show that the turbo syncer can, on the one hand, enhance extraction consistency and thus increase matching accuracy (from 17--83% to 78--94% in F-measure) and, on the other hand, incorporate peer wrappers and domain knowledge seamlessly to reduce extraction errors (from 09--60% to 01--11%)."
            },
            "slug": "Context-Aware-Wrapping:-Synchronized-Data-Chuang-Chang",
            "title": {
                "fragments": [],
                "text": "Context-Aware Wrapping: Synchronized Data Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The concept of context-aware wrappers that are amenable to matching and that can leverage peer wrappers or prior domain knowledge is proposed and inspires a synchronization framework to construct wrappers consistently and collaboratively across their mutual context."
            },
            "venue": {
                "fragments": [],
                "text": "VLDB"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2627799"
                        ],
                        "name": "P. Gulhane",
                        "slug": "P.-Gulhane",
                        "structuredName": {
                            "firstName": "Pankaj",
                            "lastName": "Gulhane",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gulhane"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696519"
                        ],
                        "name": "R. Rastogi",
                        "slug": "R.-Rastogi",
                        "structuredName": {
                            "firstName": "Rajeev",
                            "lastName": "Rastogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rastogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757518"
                        ],
                        "name": "Srinivasan H. Sengamedu",
                        "slug": "Srinivasan-H.-Sengamedu",
                        "structuredName": {
                            "firstName": "Srinivasan",
                            "lastName": "Sengamedu",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Srinivasan H. Sengamedu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2990683"
                        ],
                        "name": "Ashwin Tengli",
                        "slug": "Ashwin-Tengli",
                        "structuredName": {
                            "firstName": "Ashwin",
                            "lastName": "Tengli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashwin Tengli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 156
                            }
                        ],
                        "text": "Information extraction systems, such as ReVerb [21], tackle the two issues synergically and aim at extracting and integrating huge amounts of data from the Web."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 275,
                                "start": 272
                            }
                        ],
                        "text": "In addition, it was able to extract and integrate more attributes than those specified in the SWDE dataset, such as (we report one example per domain): Transmissions for Autos, ListPrice for Books, MegaPixel for Cameras, RunningTime for Movies, BirthDate for NBA Players, Website for Restaurants, and Address for Universities."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Web-scale information extraction with vertex."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 34
                            }
                        ],
                        "text": "In this section, we present WEIR (Web-Extraction and Integration of Redundant data), our algorithm for solving the Abstract Relation Discovery problem."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 5
                            }
                        ],
                        "text": "In IIWeb, 2003."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Web data extraction and integration is still an expensive process, which needs human supervision in many steps to achieve high quality results."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 168
                            }
                        ],
                        "text": "Also, we observe that it is rather easy to collect detail pages from these websites by means of a crawler based on set-expansion techniques (e.g., [6]) for the surface Web, or by using form-filling techniques (e.g., [28]) for the hidden Web."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 101
                            }
                        ],
                        "text": "These technical challenges are reflected in systems designed for web data extraction and integration [9, 23, 24, 30], where ad-hoc user input (such as annotated pages) is required to achieve acceptable results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 61
                            }
                        ],
                        "text": "Although the impressive number of sources and domains on the Web has given rise to several research proposals for automatically extracting and integrating web data, so far extraction and integration problems have been mainly tackled separately: many researchers have proposed techniques for extracting data from the Web [12], while others have concentrated their solutions on integrating the extracted data [5]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 382,
                                "start": 379
                            }
                        ],
                        "text": "The paper makes the following contributions: (i) we formulate an abstract generative model that characterizes partially overlapping data-intensive web sources; (ii) based on the model, we introduce a formal setting to state the data extraction and integration problem, exploiting the redundancy of information among the sources; (iii) we propose an unsupervised algorithm, WEIR (Web-Extraction and Integration of Redundant data), to solve the stated problem, and formally study its correctness; (iv) we show robustness and performance of our approach against alternative solutions in an experimental evaluation with real-world websites."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 7
                            }
                        ],
                        "text": "On the Web, for many domains this is a natural assumption, and the identifiers can be derived during the page harvesting phase, or automatically extracted from the page collections starting from a small seed set."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Webtables: exploring the power of tables on the web."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 43
                            }
                        ],
                        "text": "The massive exploitation of the structured Web has been studied for data published in HTML tables and lists [10, 20]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "Labeling Data Extracted from the Web."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 11
                            }
                        ],
                        "text": "Similarly, [22, 23, 24, 31] require labeled examples to bootstrap the extraction process and, with the notable exception of [31], they can extract only data from the attributes annotated in the input pages (i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 38
                            }
                        ],
                        "text": "We collected 40 data sources from the Web over four domain entities: soccer players, stock quotes, video games, and books.5 Detail pages for the video games and soccer players domains were gathered by means of a crawler based on a set expansion technique [6]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 31
                            }
                        ],
                        "text": "It is well recognized that the Web is a valuable source of information and that making use of its data is an incredible opportunity to create knowledge with both scientific and commercial implications."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 107
                            }
                        ],
                        "text": "A large body of works has tackled the challenge of extracting and integrating structured data from the Web [9, 23, 30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Web data extraction has been addressed in multiple works in the last decade (see [12] for a survey)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 117
                            }
                        ],
                        "text": "The problem of finding labels for web data has been studied in [17], which exploits the redundancy of information on Web."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Web data extraction involve several tasks: source discovery, wrapper generation, data integration, and data cleaning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 7
                            }
                        ],
                        "text": "On the Web, redundant sources publish data of many different types and with different unit of measures."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9237436,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0bcf4adf0d69e9b9f8606eb42d90ea4558389e71",
            "isKey": true,
            "numCitedBy": 33,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel extraction approach that exploits content redundancy on the web to extract structured data from template-based web sites. We start by populating a seed database with records extracted from a few initial sites. We then identify values within the pages of each new site that match attribute values contained in the seed set of records. To match attribute values with diverse representations across sites, we define a new similarity metric that leverages the templatized structure of attribute content. Specifically, our metric discovers the matching pattern between attribute values from two sites, and uses this to ignore extraneous portions of attribute values when computing similarity scores. Further, to filter out noisy attribute value matches, we exploit the fact that attribute values occur at fixed positions within template-based sites. We develop an efficient Apriori-style algorithm to systematically enumerate attribute position configurations with sufficient matching values across pages. Finally, we conduct an extensive experimental study with real-life web data to demonstrate the effectiveness of our extraction approach."
            },
            "slug": "Exploiting-content-redundancy-for-web-information-Gulhane-Rastogi",
            "title": {
                "fragments": [],
                "text": "Exploiting content redundancy for web information extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A novel extraction approach that exploits content redundancy on the web to extract structured data from template-based web sites by developing an efficient Apriori-style algorithm to systematically enumerate attribute position configurations with sufficient matching values across pages."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. VLDB Endow."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1890500"
                        ],
                        "name": "Hazem Elmeleegy",
                        "slug": "Hazem-Elmeleegy",
                        "structuredName": {
                            "firstName": "Hazem",
                            "lastName": "Elmeleegy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hazem Elmeleegy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2224716"
                        ],
                        "name": "J. Madhavan",
                        "slug": "J.-Madhavan",
                        "structuredName": {
                            "firstName": "Jayant",
                            "lastName": "Madhavan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Madhavan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770962"
                        ],
                        "name": "A. Halevy",
                        "slug": "A.-Halevy",
                        "structuredName": {
                            "firstName": "Alon",
                            "lastName": "Halevy",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Halevy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 108
                            }
                        ],
                        "text": "The massive exploitation of the structured Web has been studied for data published in HTML tables and lists [10, 20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 908261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "50fd068ad5cb83265d6ad89f002277e88980d3c7",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "A large number of web pages contain data structured in the form of \u201clists\u201d. Many such lists can be further split into multi-column tables, which can then be used in more semantically meaningful tasks. However, harvesting relational tables from such lists can be a challenging task. The lists are manually generated and hence need not have well-defined templates\u2014they have inconsistent delimiters (if any) and often have missing information. We propose a novel technique for extracting tables from lists. The technique is domain independent and operates in a fully unsupervised manner. We first use multiple sources of information to split individual lines into multiple fields and then, compare the splits across multiple lines to identify and fix incorrect splits and bad alignments. In particular, we exploit a corpus of HTML tables, also extracted from the web, to identify likely fields and good alignments. For each extracted table, we compute an extraction score that reflects our confidence in the table\u2019s quality. We conducted an extensive experimental study using both real web lists and lists derived from tables on the web. The experiments demonstrate the ability of our technique to extract tables with high accuracy. In addition, we applied our technique on a large sample of about 100,000 lists crawled from the web. The analysis of the extracted tables has led us to believe that there are likely to be tens of millions of useful and query-able relational tables extractable from lists on the web."
            },
            "slug": "Harvesting-relational-tables-from-lists-on-the-web-Elmeleegy-Madhavan",
            "title": {
                "fragments": [],
                "text": "Harvesting relational tables from lists on the web"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work proposes a novel technique for extracting tables from lists that is domain independent and operates in a fully unsupervised manner, and believes that there are likely to be tens of millions of useful and query-able relational tables extractable from lists on the web."
            },
            "venue": {
                "fragments": [],
                "text": "The VLDB Journal"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720956"
                        ],
                        "name": "Chia-Hui Chang",
                        "slug": "Chia-Hui-Chang",
                        "structuredName": {
                            "firstName": "Chia-Hui",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chia-Hui Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2484984"
                        ],
                        "name": "Mohammed Kayed",
                        "slug": "Mohammed-Kayed",
                        "structuredName": {
                            "firstName": "Mohammed",
                            "lastName": "Kayed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohammed Kayed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2570984"
                        ],
                        "name": "M. Girgis",
                        "slug": "M.-Girgis",
                        "structuredName": {
                            "firstName": "Moheb",
                            "lastName": "Girgis",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Girgis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40241708"
                        ],
                        "name": "K. Shaalan",
                        "slug": "K.-Shaalan",
                        "structuredName": {
                            "firstName": "Khaled",
                            "lastName": "Shaalan",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Shaalan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 156
                            }
                        ],
                        "text": "Information extraction systems, such as ReVerb [21], tackle the two issues synergically and aim at extracting and integrating huge amounts of data from the Web."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 275,
                                "start": 272
                            }
                        ],
                        "text": "In addition, it was able to extract and integrate more attributes than those specified in the SWDE dataset, such as (we report one example per domain): Transmissions for Autos, ListPrice for Books, MegaPixel for Cameras, RunningTime for Movies, BirthDate for NBA Players, Website for Restaurants, and Address for Universities."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Web-scale information extraction with vertex."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 34
                            }
                        ],
                        "text": "In this section, we present WEIR (Web-Extraction and Integration of Redundant data), our algorithm for solving the Abstract Relation Discovery problem."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 5
                            }
                        ],
                        "text": "In IIWeb, 2003."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Web data extraction and integration is still an expensive process, which needs human supervision in many steps to achieve high quality results."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 168
                            }
                        ],
                        "text": "Also, we observe that it is rather easy to collect detail pages from these websites by means of a crawler based on set-expansion techniques (e.g., [6]) for the surface Web, or by using form-filling techniques (e.g., [28]) for the hidden Web."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 324,
                                "start": 320
                            }
                        ],
                        "text": "Although the impressive number of sources and domains on the Web has given rise to several research proposals for automatically extracting and integrating web data, so far extraction and integration problems have been mainly tackled separately: many researchers have proposed techniques for extracting data from the Web [12], while others have concentrated their solutions on integrating the extracted data [5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 382,
                                "start": 379
                            }
                        ],
                        "text": "The paper makes the following contributions: (i) we formulate an abstract generative model that characterizes partially overlapping data-intensive web sources; (ii) based on the model, we introduce a formal setting to state the data extraction and integration problem, exploiting the redundancy of information among the sources; (iii) we propose an unsupervised algorithm, WEIR (Web-Extraction and Integration of Redundant data), to solve the stated problem, and formally study its correctness; (iv) we show robustness and performance of our approach against alternative solutions in an experimental evaluation with real-world websites."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 7
                            }
                        ],
                        "text": "On the Web, for many domains this is a natural assumption, and the identifiers can be derived during the page harvesting phase, or automatically extracted from the page collections starting from a small seed set."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Webtables: exploring the power of tables on the web."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 43
                            }
                        ],
                        "text": "The massive exploitation of the structured Web has been studied for data published in HTML tables and lists [10, 20]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "Labeling Data Extracted from the Web."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 38
                            }
                        ],
                        "text": "We collected 40 data sources from the Web over four domain entities: soccer players, stock quotes, video games, and books.5 Detail pages for the video games and soccer players domains were gathered by means of a crawler based on a set expansion technique [6]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 31
                            }
                        ],
                        "text": "It is well recognized that the Web is a valuable source of information and that making use of its data is an incredible opportunity to create knowledge with both scientific and commercial implications."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 103
                            }
                        ],
                        "text": "A large body of works has tackled the challenge of extracting and integrating structured data from the Web [9, 23, 30]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "Web data extraction has been addressed in multiple works in the last decade (see [12] for a survey)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 117
                            }
                        ],
                        "text": "The problem of finding labels for web data has been studied in [17], which exploits the redundancy of information on Web."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Web data extraction involve several tasks: source discovery, wrapper generation, data integration, and data cleaning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 7
                            }
                        ],
                        "text": "On the Web, redundant sources publish data of many different types and with different unit of measures."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206742377,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3cc310717f422592c3cc9a046943777468c14358",
            "isKey": true,
            "numCitedBy": 877,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "The Internet presents a huge amount of useful information which is usually formatted for its users, which makes it difficult to extract relevant data from various sources. Therefore, the availability of robust, flexible information extraction (IE) systems that transform the Web pages into program-friendly structures such as a relational database will become a great necessity. Although many approaches for data extraction from Web pages have been developed, there has been limited effort to compare such tools. Unfortunately, in only a few cases can the results generated by distinct tools be directly compared since the addressed extraction tasks are different. This paper surveys the major Web data extraction approaches and compares them in three dimensions: the task domain, the automation degree, and the techniques used. The criteria of the first dimension explain why an IE system fails to handle some Web sites of particular structures. The criteria of the second dimension classify IE systems based on the techniques used. The criteria of the third dimension measure the degree of automation for IE systems. We believe these criteria provide qualitatively measures to evaluate various IE approaches"
            },
            "slug": "A-Survey-of-Web-Information-Extraction-Systems-Chang-Kayed",
            "title": {
                "fragments": [],
                "text": "A Survey of Web Information Extraction Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper surveys the major Web data extraction approaches and compares them in three dimensions: the task domain, the automation degree, and the techniques used and believes these criteria provide qualitatively measures to evaluate various IE approaches."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Knowledge and Data Engineering"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38087946"
                        ],
                        "name": "Anthony Fader",
                        "slug": "Anthony-Fader",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Fader",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anthony Fader"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35837717"
                        ],
                        "name": "Janara Christensen",
                        "slug": "Janara-Christensen",
                        "structuredName": {
                            "firstName": "Janara",
                            "lastName": "Christensen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Janara Christensen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144295318"
                        ],
                        "name": "S. Soderland",
                        "slug": "S.-Soderland",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Soderland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Soderland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2674444"
                        ],
                        "name": "Mausam",
                        "slug": "Mausam",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Mausam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mausam"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 61
                            }
                        ],
                        "text": "Recently, so called \u201copen\u201d information extraction approaches [4, 21] have been developed: they automatically discover phrases to extract new facts from sentences, and they are not constrained to learn an extractor only for the target relations specified by means of training samples."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "Information extraction systems, such as ReVerb [21], tackle the two issues synergically and aim at extracting and integrating huge amounts of data from the Web."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15515902,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "906901b15c93d0cbfdf6c9b6587c6a1b389ec386",
            "isKey": false,
            "numCitedBy": 489,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "How do we scale information extraction to the massive size and unprecedented heterogeneity of the Web corpus? Beginning in 2003, our KnowItAll project has sought to extract high-quality knowledge from the Web. \n \nIn 2007, we introduced the Open Information Extraction (Open IE) paradigm which eschews hand-labeled training examples, and avoids domain-specific verbs and nouns, to develop unlexicalized, domain-independent extractors that scale to the Web corpus. Open IE systems have extracted billions of assertions as the basis for both common-sense knowledge and novel question-answering systems. \n \nThis paper describes the second generation of Open IE systems, which rely on a novel model of how relations and their arguments are expressed in English sentences to double precision/recall compared with previous systems such as TEXTRUNNER and WOE."
            },
            "slug": "Open-Information-Extraction:-The-Second-Generation-Etzioni-Fader",
            "title": {
                "fragments": [],
                "text": "Open Information Extraction: The Second Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The second generation of Open IE systems are described, which rely on a novel model of how relations and their arguments are expressed in English sentences to double precision/recall compared with previous systems such as TEXTRUNNER and WOE."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2170598"
                        ],
                        "name": "Mark James Carman",
                        "slug": "Mark-James-Carman",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Carman",
                            "middleNames": [
                                "James"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark James Carman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745117"
                        ],
                        "name": "Craig A. Knoblock",
                        "slug": "Craig-A.-Knoblock",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Knoblock",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Craig A. Knoblock"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 174
                            }
                        ],
                        "text": "The different setting leads to different solutions: instead of learning simple matchings (element correspondences), it automatically learns Datalog mappings between websites [11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12459136,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68d0fa21844f862f9a5c96c8d301b0de3bd1405c",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "The Internet contains a very large number of information sources providing many types of data from weather forecasts to travel deals and financial information. These sources can be accessed via Web-forms, Web Services, RSS feeds and so on. In order to make automated use of these sources, we need to model them semantically, but writing semantic descriptions for Web Services is both tedious and error prone. In this paper we investigate the problem of automatically generating such models. We introduce a framework for learning Datalog definitions of Web sources. In order to learn these definitions, our system actively invokes the sources and compares the data they produce with that of known sources of information. It then performs an inductive logic search through the space of plausible source definitions in order to learn the best possible semantic model for each new source. In this paper we perform an empirical evaluation of the system using real-world Web sources. The evaluation demonstrates the effectiveness of the approach, showing that we can automatically learn complex models for real sources in reasonable time. We also compare our system with a complex schema matching system, showing that our approach can handle the kinds of problems tackled by the latter."
            },
            "slug": "Learning-Semantic-Definitions-of-Online-Information-Carman-Knoblock",
            "title": {
                "fragments": [],
                "text": "Learning Semantic Definitions of Online Information Sources"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper introduces a framework for learning Datalog definitions of Web sources, and performs an empirical evaluation of the system, showing that it can automatically learn complex models for real sources in reasonable time and compared with a complex schema matching system."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1923209"
                        ],
                        "name": "Nilesh N. Dalvi",
                        "slug": "Nilesh-N.-Dalvi",
                        "structuredName": {
                            "firstName": "Nilesh",
                            "lastName": "Dalvi",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nilesh N. Dalvi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2357165"
                        ],
                        "name": "Ashwin Machanavajjhala",
                        "slug": "Ashwin-Machanavajjhala",
                        "structuredName": {
                            "firstName": "Ashwin",
                            "lastName": "Machanavajjhala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashwin Machanavajjhala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144865353"
                        ],
                        "name": "B. Pang",
                        "slug": "B.-Pang",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Pang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Pang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "As observed in [19] and [27], these sources partially overlap, i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6820835,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9bdef4898c3cbdc196a4d076e9bd5df38b97f28d",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we analyze the nature and distribution of structured data on the Web. Web-scale information extraction, or the problem of creating structured tables using extraction from the entire web, is gathering lots of research interest. We perform a study to understand and quantify the value of Web-scale extraction, and how structured information is distributed amongst top aggregator websites and tail sites for various interesting domains. We believe this is the first study of its kind, and gives us new insights for information extraction over the Web."
            },
            "slug": "An-Analysis-of-Structured-Data-on-the-Web-Dalvi-Machanavajjhala",
            "title": {
                "fragments": [],
                "text": "An Analysis of Structured Data on the Web"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This is the first study of its kind to understand and quantify the value of Web-scale extraction, and how structured information is distributed amongst top aggregator websites and tail sites for various interesting domains."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. VLDB Endow."
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764746"
                        ],
                        "name": "Lorenzo Blanco",
                        "slug": "Lorenzo-Blanco",
                        "structuredName": {
                            "firstName": "Lorenzo",
                            "lastName": "Blanco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lorenzo Blanco"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791339"
                        ],
                        "name": "Valter Crescenzi",
                        "slug": "Valter-Crescenzi",
                        "structuredName": {
                            "firstName": "Valter",
                            "lastName": "Crescenzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Valter Crescenzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796590"
                        ],
                        "name": "P. Merialdo",
                        "slug": "P.-Merialdo",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Merialdo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Merialdo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802817"
                        ],
                        "name": "Paolo Papotti",
                        "slug": "Paolo-Papotti",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Papotti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paolo Papotti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 133
                            }
                        ],
                        "text": "In this work we focus on extraction and integration, but we developed an end-toend system that covers crawling [6] and data cleaning [7] as well."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 240028,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5eb52376ff4f288017a730fd0cd4001883ac951",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Several techniques have been developed to extract and integrate data from web sources. However, web data are inherently imprecise and uncertain. This paper addresses the issue of characterizing the uncertainty of data extracted from a number of inaccurate sources. We develop a probabilistic model to compute a probability distribution for the extracted values, and the accuracy of the sources. Our model considers the presence of sources that copy their contents from other sources, and manages the misleading consensus produced by copiers. We extend the models previously proposed in the literature by working on several attributes at a time to better leverage all the available evidence. We also report the results of several experiments on both synthetic and real-life data to show the effectiveness of the proposed approach."
            },
            "slug": "Probabilistic-Models-to-Reconcile-Complex-Data-from-Blanco-Crescenzi",
            "title": {
                "fragments": [],
                "text": "Probabilistic Models to Reconcile Complex Data from Inaccurate Data Sources"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A probabilistic model to compute a probability distribution for the extracted values, and the accuracy of the sources, is developed, which considers the presence of sources that copy their contents from other sources, and manages the misleading consensus produced by copiers."
            },
            "venue": {
                "fragments": [],
                "text": "CAiSE"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764746"
                        ],
                        "name": "Lorenzo Blanco",
                        "slug": "Lorenzo-Blanco",
                        "structuredName": {
                            "firstName": "Lorenzo",
                            "lastName": "Blanco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lorenzo Blanco"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791339"
                        ],
                        "name": "Valter Crescenzi",
                        "slug": "Valter-Crescenzi",
                        "structuredName": {
                            "firstName": "Valter",
                            "lastName": "Crescenzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Valter Crescenzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796590"
                        ],
                        "name": "P. Merialdo",
                        "slug": "P.-Merialdo",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Merialdo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Merialdo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802817"
                        ],
                        "name": "Paolo Papotti",
                        "slug": "Paolo-Papotti",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Papotti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paolo Papotti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 131
                            }
                        ],
                        "text": "Detail pages for the video games and soccer players domains were gathered by means of a crawler based on a set expansion technique [6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 2
                            }
                        ],
                        "text": ", [6]) for the surface Web, or by using form-filling techniques (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 111
                            }
                        ],
                        "text": "In this work we focus on extraction and integration, but we developed an end-toend system that covers crawling [6] and data cleaning [7] as well."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16544908,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89d2a745ca523739a8e2cf7a2ee6d4d8a534b78b",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Several web sites deliver a large number of pages, each publishing data about one instance of some real world entity, such as an athlete, a stock quote, a book. Although it is easy for a human reader to recognize these instances, current search engines are unaware of them. Technologies for the Semantic Web aim at achieving this goal; however, so far they have been of little help in this respect, as semantic publishing is very limited.\n We have developed a method to automatically search on the web for pages that publish data representing an instance of a certain conceptual entity. Our method takes as input a small set of sample pages: it automatically infers a description of the underlying conceptual entity and then searches the web for other pages containing data representing the same entity. We have implemented our method in a system prototype, which has been used to conduct several experiments that have produced interesting results."
            },
            "slug": "Supporting-the-automatic-construction-of-entity-Blanco-Crescenzi",
            "title": {
                "fragments": [],
                "text": "Supporting the automatic construction of entity aware search engines"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A method to automatically search on the web for pages that publish data representing an instance of a certain conceptual entity that is implemented in a system prototype, which has been used to conduct several experiments that have produced interesting results."
            },
            "venue": {
                "fragments": [],
                "text": "WIDM '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1923209"
                        ],
                        "name": "Nilesh N. Dalvi",
                        "slug": "Nilesh-N.-Dalvi",
                        "structuredName": {
                            "firstName": "Nilesh",
                            "lastName": "Dalvi",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nilesh N. Dalvi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683442"
                        ],
                        "name": "Ravi Kumar",
                        "slug": "Ravi-Kumar",
                        "structuredName": {
                            "firstName": "Ravi",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ravi Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144018773"
                        ],
                        "name": "Mohamed A. Soliman",
                        "slug": "Mohamed-A.-Soliman",
                        "structuredName": {
                            "firstName": "Mohamed",
                            "lastName": "Soliman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohamed A. Soliman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 98
                            }
                        ],
                        "text": "As related to data extraction, in order to craft production-level wrappers, supervised approaches [18, 22] require annotated pages, whereas unsupervised ones [3, 16] cannot provide accurate results without external feedback."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 42464,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "65ec5824f6a997df0322827285ee691510b4527a",
            "isKey": false,
            "numCitedBy": 139,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a generic framework to make wrapper induction algorithms tolerant to noise in the training data. This enables us to learn wrappers in a completely unsupervised manner from automatically and cheaply obtained noisy training data, e.g., using dictionaries and regular expressions. By removing the site-level supervision that wrapper-based techniques require, we are able to perform information extraction at web-scale, with accuracy unattained with existing unsupervised extraction techniques. Our system is used in production at Yahoo! and powers live applications."
            },
            "slug": "Automatic-Wrappers-for-Large-Scale-Web-Extraction-Dalvi-Kumar",
            "title": {
                "fragments": [],
                "text": "Automatic Wrappers for Large Scale Web Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "By removing the site-level supervision that wrapper-based techniques require, this work is able to perform information extraction at web-scale, with accuracy unattained with existing unsupervised extraction techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. VLDB Endow."
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690426"
                        ],
                        "name": "A. D. Silva",
                        "slug": "A.-D.-Silva",
                        "structuredName": {
                            "firstName": "Altigran",
                            "lastName": "Silva",
                            "middleNames": [
                                "Soares",
                                "da"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. D. Silva"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145690522"
                        ],
                        "name": "Denilson Barbosa",
                        "slug": "Denilson-Barbosa",
                        "structuredName": {
                            "firstName": "Denilson",
                            "lastName": "Barbosa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Denilson Barbosa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2458052"
                        ],
                        "name": "J. Cavalcanti",
                        "slug": "J.-Cavalcanti",
                        "structuredName": {
                            "firstName": "Jo\u00e3o",
                            "lastName": "Cavalcanti",
                            "middleNames": [
                                "M.",
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cavalcanti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3201474"
                        ],
                        "name": "Marco A. S. Sevalho",
                        "slug": "Marco-A.-S.-Sevalho",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Sevalho",
                            "middleNames": [
                                "A.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco A. S. Sevalho"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2467507,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "73bb3d5323bd0810e52b6ab2c805aa0f98005cc8",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider finding descriptive labels for anonymous, structured datasets, such as those produced by state-of-the-art Web wrappers. We give a probabilistic model to estimate the affinity between attributes and labels, and describe a method that uses a Web search engine to populate the model. We discuss a method for finding good candidate labels for unlabeled datasets. Ours is the first unsupervised labeling method that does not rely on mining the HTML pages containing the data. Experimental results with data from 8 different domains show that our methods achieve high accuracy even with very few search engine accesses."
            },
            "slug": "Labeling-Data-Extracted-from-the-Web-Silva-Barbosa",
            "title": {
                "fragments": [],
                "text": "Labeling Data Extracted from the Web"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The authors' is the first unsupervised labeling method that does not rely on mining the HTML pages containing the data, and achieves high accuracy even with very few search engine accesses."
            },
            "venue": {
                "fragments": [],
                "text": "OTM Conferences"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2887330"
                        ],
                        "name": "J. Ambite",
                        "slug": "J.-Ambite",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Ambite",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ambite"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2995884"
                        ],
                        "name": "Sirish Darbha",
                        "slug": "Sirish-Darbha",
                        "structuredName": {
                            "firstName": "Sirish",
                            "lastName": "Darbha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sirish Darbha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2165465"
                        ],
                        "name": "Aman Goel",
                        "slug": "Aman-Goel",
                        "structuredName": {
                            "firstName": "Aman",
                            "lastName": "Goel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aman Goel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745117"
                        ],
                        "name": "Craig A. Knoblock",
                        "slug": "Craig-A.-Knoblock",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Knoblock",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Craig A. Knoblock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782658"
                        ],
                        "name": "Kristina Lerman",
                        "slug": "Kristina-Lerman",
                        "structuredName": {
                            "firstName": "Kristina",
                            "lastName": "Lerman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristina Lerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2959971"
                        ],
                        "name": "Rahul Parundekar",
                        "slug": "Rahul-Parundekar",
                        "structuredName": {
                            "firstName": "Rahul",
                            "lastName": "Parundekar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rahul Parundekar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143740135"
                        ],
                        "name": "Thomas A. Russ",
                        "slug": "Thomas-A.-Russ",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Russ",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas A. Russ"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9295554,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b094d9184e16017175325cd4fc2e8e8656e30ca4",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The work on integrating sources and services in the Semantic Web assumes that the data is either already represented in RDF or OWL or is available through a Semantic Web Service. In practice, there is a tremendous amount of data on the Web that is not available through the Semantic Web. In this paper we present an approach to automatically discover and create new Semantic Web Services. The idea behind this approach is to start with a set of known sources and the corresponding semantic descriptions and then discover similar sources, extract the source data, build semantic descriptions of the sources, and then turn them into Semantic Web Services. We implemented an end-to-end solution to this problem in a system called Deimos and evaluated the system across five different domains. The results demonstrate that the system can automatically discover, learn semantic descriptions, and build Semantic Web Services with only example sources and their descriptions as input."
            },
            "slug": "Automatically-Constructing-Semantic-Web-Services-Ambite-Darbha",
            "title": {
                "fragments": [],
                "text": "Automatically Constructing Semantic Web Services from Online Sources"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "An approach to automatically discover and create new Semantic Web Services by starting with a set of known sources and the corresponding semantic descriptions and then discover similar sources, extract the source data, build semantic descriptions of the sources, and then turn them into SemanticWeb Services."
            },
            "venue": {
                "fragments": [],
                "text": "SEMWEB"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725561"
                        ],
                        "name": "Michael J. Cafarella",
                        "slug": "Michael-J.-Cafarella",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Cafarella",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Cafarella"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770962"
                        ],
                        "name": "A. Halevy",
                        "slug": "A.-Halevy",
                        "structuredName": {
                            "firstName": "Alon",
                            "lastName": "Halevy",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Halevy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111220343"
                        ],
                        "name": "D. Wang",
                        "slug": "D.-Wang",
                        "structuredName": {
                            "firstName": "Daisy",
                            "lastName": "Wang",
                            "middleNames": [
                                "Zhe"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48144872"
                        ],
                        "name": "Eugene Wu",
                        "slug": "Eugene-Wu",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145955157"
                        ],
                        "name": "Yang Zhang",
                        "slug": "Yang-Zhang",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15642206,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b5077161a6f55a0d18cdfa3abbb612663d08d69",
            "isKey": false,
            "numCitedBy": 655,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "The World-Wide Web consists of a huge number of unstructured documents, but it also contains structured data in the form of HTML tables. We extracted 14.1 billion HTML tables from Google's general-purpose web crawl, and used statistical classification techniques to find the estimated 154M that contain high-quality relational data. Because each relational table has its own \"schema\" of labeled and typed columns, each such table can be considered a small structured database. The resulting corpus of databases is larger than any other corpus we are aware of, by at least five orders of magnitude. \n \nWe describe the WEBTABLES system to explore two fundamental questions about this collection of databases. First, what are effective techniques for searching for structured data at search-engine scales? Second, what additional power can be derived by analyzing such a huge corpus? \n \nFirst, we develop new techniques for keyword search over a corpus of tables, and show that they can achieve substantially higher relevance than solutions based on a traditional search engine. Second, we introduce a new object derived from the database corpus: the attribute correlation statistics database (AcsDB) that records corpus-wide statistics on co-occurrences of schema elements. In addition to improving search relevance, the AcsDB makes possible several novel applications: schema auto-complete, which helps a database designer to choose schema elements; attribute synonym finding, which automatically computes attribute synonym pairs for schema matching; and join-graph traversal, which allows a user to navigate between extracted schemas using automatically-generated join links."
            },
            "slug": "WebTables:-exploring-the-power-of-tables-on-the-web-Cafarella-Halevy",
            "title": {
                "fragments": [],
                "text": "WebTables: exploring the power of tables on the web"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The WEBTABLES system develops new techniques for keyword search over a corpus of tables, and shows that they can achieve substantially higher relevance than solutions based on a traditional search engine."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. VLDB Endow."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39756936"
                        ],
                        "name": "Tak-Lam Wong",
                        "slug": "Tak-Lam-Wong",
                        "structuredName": {
                            "firstName": "Tak-Lam",
                            "lastName": "Wong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tak-Lam Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144594306"
                        ],
                        "name": "Wai Lam",
                        "slug": "Wai-Lam",
                        "structuredName": {
                            "firstName": "Wai",
                            "lastName": "Lam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wai Lam"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15287551,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "64ad134fa13221ed251d3d651298c636144201db",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a Bayesian learning framework for adapting information extraction wrappers with new attribute discovery, reducing human effort in extracting precise information from unseen Web sites. Our approach aims at automatically adapting the information extraction knowledge previously learned from a source Web site to a new unseen site, at the same time, discovering previously unseen attributes. Two kinds of text-related clues from the source Web site are considered. The first kind of clue is obtained from the extraction pattern contained in the previously learned wrapper. The second kind of clue is derived from the previously extracted or collected items. A generative model for the generation of the site-independent content information and the site-dependent layout format of the text fragments related to attribute values contained in a Web page is designed to harness the uncertainty involved. Bayesian learning and expectation-maximization (EM) techniques are developed under the proposed generative model for identifying new training data for learning the new wrapper for new unseen sites. Previously unseen attributes together with their semantic labels can also be discovered via another EM-based Bayesian learning based on the generative model. We have conducted extensive experiments from more than 30 real-world Web sites in three different domains to demonstrate the effectiveness of our framework."
            },
            "slug": "Learning-to-Adapt-Web-Information-Extraction-and-a-Wong-Lam",
            "title": {
                "fragments": [],
                "text": "Learning to Adapt Web Information Extraction Knowledge and Discovering New Attributes via a Bayesian Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper presents a Bayesian learning framework for adapting information extraction wrappers with new attribute discovery, reducing human effort in extracting precise information from unseen Web sites."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Knowledge and Data Engineering"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2627799"
                        ],
                        "name": "P. Gulhane",
                        "slug": "P.-Gulhane",
                        "structuredName": {
                            "firstName": "Pankaj",
                            "lastName": "Gulhane",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gulhane"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2136102"
                        ],
                        "name": "Amit Madaan",
                        "slug": "Amit-Madaan",
                        "structuredName": {
                            "firstName": "Amit",
                            "lastName": "Madaan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amit Madaan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3259494"
                        ],
                        "name": "Rupesh R. Mehta",
                        "slug": "Rupesh-R.-Mehta",
                        "structuredName": {
                            "firstName": "Rupesh",
                            "lastName": "Mehta",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rupesh R. Mehta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2311735"
                        ],
                        "name": "J. Ramamirtham",
                        "slug": "J.-Ramamirtham",
                        "structuredName": {
                            "firstName": "Jeyashankher",
                            "lastName": "Ramamirtham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ramamirtham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696519"
                        ],
                        "name": "R. Rastogi",
                        "slug": "R.-Rastogi",
                        "structuredName": {
                            "firstName": "Rajeev",
                            "lastName": "Rastogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rastogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1837802"
                        ],
                        "name": "Sandeepkumar Satpal",
                        "slug": "Sandeepkumar-Satpal",
                        "structuredName": {
                            "firstName": "Sandeepkumar",
                            "lastName": "Satpal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sandeepkumar Satpal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757518"
                        ],
                        "name": "Srinivasan H. Sengamedu",
                        "slug": "Srinivasan-H.-Sengamedu",
                        "structuredName": {
                            "firstName": "Srinivasan",
                            "lastName": "Sengamedu",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Srinivasan H. Sengamedu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2990683"
                        ],
                        "name": "Ashwin Tengli",
                        "slug": "Ashwin-Tengli",
                        "structuredName": {
                            "firstName": "Ashwin",
                            "lastName": "Tengli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashwin Tengli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2081450365"
                        ],
                        "name": "Charu Tiwari",
                        "slug": "Charu-Tiwari",
                        "structuredName": {
                            "firstName": "Charu",
                            "lastName": "Tiwari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charu Tiwari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 98
                            }
                        ],
                        "text": "As related to data extraction, in order to craft production-level wrappers, supervised approaches [18, 22] require annotated pages, whereas unsupervised ones [3, 16] cannot provide accurate results without external feedback."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 11
                            }
                        ],
                        "text": "Similarly, [22, 23, 24, 31] require labeled examples to bootstrap the extraction process and, with the notable exception of [31], they can extract only data from the attributes annotated in the input pages (i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13091007,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a12502ba5b9686e37b0ec9d86a2dc7f4b7022ac",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Vertex is a Wrapper Induction system developed at Yahoo! for extracting structured records from template-based Web pages. To operate at Web scale, Vertex employs a host of novel algorithms for (1) Grouping similar structured pages in a Web site, (2) Picking the appropriate sample pages for wrapper inference, (3) Learning XPath-based extraction rules that are robust to variations in site structure, (4) Detecting site changes by monitoring sample pages, and (5) Optimizing editorial costs by reusing rules, etc. The system is deployed in production and currently extracts more than 250 million records from more than 200 Web sites. To the best of our knowledge, Vertex is the first system to do high-precision information extraction at Web scale."
            },
            "slug": "Web-scale-information-extraction-with-vertex-Gulhane-Madaan",
            "title": {
                "fragments": [],
                "text": "Web-scale information extraction with vertex"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "Vertex is a Wrapper Induction system developed at Yahoo! for extracting structured records from template-based Web pages that is the first system to do high-precision information extraction at Web scale."
            },
            "venue": {
                "fragments": [],
                "text": "2011 IEEE 27th International Conference on Data Engineering"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791339"
                        ],
                        "name": "Valter Crescenzi",
                        "slug": "Valter-Crescenzi",
                        "structuredName": {
                            "firstName": "Valter",
                            "lastName": "Crescenzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Valter Crescenzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796590"
                        ],
                        "name": "P. Merialdo",
                        "slug": "P.-Merialdo",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Merialdo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Merialdo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 114
                            }
                        ],
                        "text": "For example, EXALG analyzes the co-occurrence of tokens in a large number of pages sharing a common template, and ROADRUNNER tries to incrementally align a set of sample pages to separate their underlying template from the embedded data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 158
                            }
                        ],
                        "text": "As related to data extraction, in order to craft production-level wrappers, supervised approaches [18, 22] require annotated pages, whereas unsupervised ones [3, 16] cannot provide accurate results without external feedback."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 71
                            }
                        ],
                        "text": "More specifically, we set the following configurations: (i) we rely on ROADRUNNER to infer the wrappers, and on our algorithm to compute the mappings over the relations produced by the wrappers (this is the RR configuration); (ii) we infer rules with our approach (running also the WEAK-REMOVAL procedure), and compute the mappings with the HAC algorithm (HAC configuration)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "The formalisms used by state-of-the-art unsupervised wrapper generator systems, such as ROADRUNNER [16] and EXALG [3], are expressive enough to define a complete wrapper for a vast majority of web sources."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 6
                            }
                        ],
                        "text": "Using ROADRUNNER and HAC we assembled three alternative systems."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "As wrapper generator system, we used the most recent implementation of ROADRUNNER [16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 28
                            }
                        ],
                        "text": "WEIR is more effective than ROADRUNNER in choosing the correct rules, thus being able to achieve better precision and recall."
                    },
                    "intents": []
                }
            ],
            "corpusId": 44480544,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f2264e359f3336163503f6eb587f4c1eb9bcd082",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Several studies have concentrated on the generation of wrappers for web data sources. As wrappers can be easily described as grammars, the grammatical inference heritage could play a significant role in this research field. Recent results have identified a new subclass of regular languages, called prefix mark-up languages, that nicely abstract the structures usually found in HTML pages of large web sites. This class has been proven to be identifiable in the limit, and a PTIME unsupervised learning algorithm has been previously developed. Unfortunately, many real-life web pages do not fall in this class of languages. In this article we analyze the roots of the problem and we propose a technique to transform pages in order to bring them into the class of prefix mark-up languages. In this way, we have a practical solution without renouncing to the formal background defined within the grammatical inference framework. We report on some experiments that we have conducted on real-life web pages to evaluate the approach; the results of this activity demonstrate the effectiveness of the presented techniques."
            },
            "slug": "WRAPPER-INFERENCE-FOR-AMBIGUOUS-WEB-PAGES-Crescenzi-Merialdo",
            "title": {
                "fragments": [],
                "text": "WRAPPER INFERENCE FOR AMBIGUOUS WEB PAGES"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This article proposes a technique to transform pages in order to bring them into the class of prefix mark-up languages, a practical solution without renouncing to the formal background defined within the grammatical inference framework."
            },
            "venue": {
                "fragments": [],
                "text": "Appl. Artif. Intell."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685296"
                        ],
                        "name": "Eugene Agichtein",
                        "slug": "Eugene-Agichtein",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Agichtein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Agichtein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684012"
                        ],
                        "name": "L. Gravano",
                        "slug": "L.-Gravano",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Gravano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gravano"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 36
                            }
                        ],
                        "text": "/td[2]/text() {38M, 46M} r(6): /html[1]/table[1]/tr[3]/td[2]/text() {38M, null}"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 26
                            }
                        ],
                        "text": "Early approaches (such as\nSnowball [1]) take a seed set of facts as input, then interleave pattern inference and relation extraction steps to expand the seed set."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 37
                            }
                        ],
                        "text": "/td[2]/text() {Dan, null} r(4): /html[1]/table[1]/tr[2]/td[2]/text() {Dan, 46M} r(5): //td[contains(text(),\u2019Volume\u2019)] /."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 35
                            }
                        ],
                        "text": "Early approaches (such as Snowball [1]) take a seed set of facts as input, then interleave pattern inference and relation extraction steps to expand the seed set."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "Snowball: extracting relations\nfrom large plain-text collections."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7579604,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cee045e890270abae65455667b292db355d53728",
            "isKey": true,
            "numCitedBy": 1365,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Text documents often contain valuable structured data that is hidden Yin regular English sentences. This data is best exploited infavailable as arelational table that we could use for answering precise queries or running data mining tasks.We explore a technique for extracting such tables from document collections that requires only a handful of training examples from users. These examples are used to generate extraction patterns, that in turn result in new tuples being extracted from the document collection.We build on this idea and present our Snowball system. Snowball introduces novel strategies for generating patterns and extracting tuples from plain-text documents.At each iteration of the extraction process, Snowball evaluates the quality of these patterns and tuples without human intervention,and keeps only the most reliable ones for the next iteration. In this paper we also develop a scalable evaluation methodology and metrics for our task, and present a thorough experimental evaluation of Snowball and comparable techniques over a collection of more than 300,000 newspaper documents."
            },
            "slug": "Snowball:-extracting-relations-from-large-Agichtein-Gravano",
            "title": {
                "fragments": [],
                "text": "Snowball: extracting relations from large plain-text collections"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper develops a scalable evaluation methodology and metrics for the task, and presents a thorough experimental evaluation of Snowball and comparable techniques over a collection of more than 300,000 newspaper documents."
            },
            "venue": {
                "fragments": [],
                "text": "DL '00"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144323862"
                        ],
                        "name": "Jaewoo Kang",
                        "slug": "Jaewoo-Kang",
                        "structuredName": {
                            "firstName": "Jaewoo",
                            "lastName": "Kang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jaewoo Kang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5151034"
                        ],
                        "name": "J. Naughton",
                        "slug": "J.-Naughton",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Naughton",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Naughton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 187
                            }
                        ],
                        "text": "As we mentioned, for other domains where such an assumption does not hold, we will need to generalize WEIR by relying on distance functions for opaque columns, such as those developed in [25, 26], which do not require to align the columns\u2019 elements."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10052039,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "afdb8e8ef5c2a0e91c9a5103e3ab01ad263d0130",
            "isKey": false,
            "numCitedBy": 262,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Most previous solutions to the schema matching problem rely in some fashion upon identifying \"similar\" column names in the schemas to be matched, or by recognizing common domains in the data stored in the schemas. While each of these approaches is valuable in many cases, they are not infallible, and there exist instances of the schema matching problem for which they do not even apply. Such problem instances typically arise when the column names in the schemas and the data in the columns are \"opaque\" or very difficult to interpret. In this paper we propose a two-step technique that works even in the presence of opaque column names and data values. In the first step, we measure the pair-wise attribute correlations in the tables to be matched and construct a dependency graph using mutual information as a measure of the dependency between attributes. In the second stage, we find matching node pairs in the dependency graphs by running a graph matching algorithm. We validate our approach with an experimental study, the results of which suggest that such an approach can be a useful addition to a set of (semi) automatic schema matching techniques."
            },
            "slug": "On-schema-matching-with-opaque-column-names-and-Kang-Naughton",
            "title": {
                "fragments": [],
                "text": "On schema matching with opaque column names and data values"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The results suggest that the two-step schema matching technique can be a useful addition to a set of (semi) automatic schema matching techniques."
            },
            "venue": {
                "fragments": [],
                "text": "SIGMOD '03"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3225963"
                        ],
                        "name": "Anuj R. Jaiswal",
                        "slug": "Anuj-R.-Jaiswal",
                        "structuredName": {
                            "firstName": "Anuj",
                            "lastName": "Jaiswal",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anuj R. Jaiswal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145780619"
                        ],
                        "name": "David J. Miller",
                        "slug": "David-J.-Miller",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Miller",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143930195"
                        ],
                        "name": "P. Mitra",
                        "slug": "P.-Mitra",
                        "structuredName": {
                            "firstName": "Prasenjit",
                            "lastName": "Mitra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Mitra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 187
                            }
                        ],
                        "text": "As we mentioned, for other domains where such an assumption does not hold, we will need to generalize WEIR by relying on distance functions for opaque columns, such as those developed in [25, 26], which do not require to align the columns\u2019 elements."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10875925,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b70723097bbf9dc063345715fd671113dabc9dbb",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Schema matching and value mapping across two heterogeneous information sources are critical tasks in applications involving data integration, data warehousing, and federation of databases. Before data can be integrated from multiple tables, the columns and the values appearing in the tables must be matched. The complexity of the problem grows quickly with the number of data attributes/columns to be matched and due to multiple semantics of data values. Traditional research has tackled schema matching and value mapping independently. We propose a novel method that optimizes embedded value mappings to enhance schema matching in the presence of opaque data values and column names. In this approach, the fitness objective for matching a pair of attributes from two schemas depends on the value mapping function for each of the two attributes. Suitable fitness objectives include the euclidean distance measure, which we use in our experimental study, as well as relative (cross) entropy. We propose a heuristic local descent optimization strategy that uses sorting and two-opt switching to jointly optimize value mappings and attribute matches. Our experiments show that our proposed technique outperforms earlier uninterpreted schema matching methods, and thus, should form a useful addition to a suite of (semi) automated tools for resolving structural heterogeneity."
            },
            "slug": "Uninterpreted-Schema-Matching-with-Embedded-Value-Jaiswal-Miller",
            "title": {
                "fragments": [],
                "text": "Uninterpreted Schema Matching with Embedded Value Mapping under Opaque Column Names and Data Values"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel method that optimizes embedded value mappings to enhance schema matching in the presence of opaque data values and column names is proposed, and should form a useful addition to a suite of (semi) automated tools for resolving structural heterogeneity."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Knowledge and Data Engineering"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34414997"
                        ],
                        "name": "Warren Shen",
                        "slug": "Warren-Shen",
                        "structuredName": {
                            "firstName": "Warren",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Warren Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2749312"
                        ],
                        "name": "Pedro DeRose",
                        "slug": "Pedro-DeRose",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "DeRose",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro DeRose"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065903749"
                        ],
                        "name": "R. McCann",
                        "slug": "R.-McCann",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "McCann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. McCann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3030274"
                        ],
                        "name": "A. Doan",
                        "slug": "A.-Doan",
                        "structuredName": {
                            "firstName": "AnHai",
                            "lastName": "Doan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Doan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709145"
                        ],
                        "name": "R. Ramakrishnan",
                        "slug": "R.-Ramakrishnan",
                        "structuredName": {
                            "firstName": "Raghu",
                            "lastName": "Ramakrishnan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ramakrishnan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 156
                            }
                        ],
                        "text": "Information extraction systems, such as ReVerb [21], tackle the two issues synergically and aim at extracting and integrating huge amounts of data from the Web."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 275,
                                "start": 272
                            }
                        ],
                        "text": "In addition, it was able to extract and integrate more attributes than those specified in the SWDE dataset, such as (we report one example per domain): Transmissions for Autos, ListPrice for Books, MegaPixel for Cameras, RunningTime for Movies, BirthDate for NBA Players, Website for Restaurants, and Address for Universities."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Web-scale information extraction with vertex."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 34
                            }
                        ],
                        "text": "In this section, we present WEIR (Web-Extraction and Integration of Redundant data), our algorithm for solving the Abstract Relation Discovery problem."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 5
                            }
                        ],
                        "text": "In IIWeb, 2003."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Web data extraction and integration is still an expensive process, which needs human supervision in many steps to achieve high quality results."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 168
                            }
                        ],
                        "text": "Also, we observe that it is rather easy to collect detail pages from these websites by means of a crawler based on set-expansion techniques (e.g., [6]) for the surface Web, or by using form-filling techniques (e.g., [28]) for the hidden Web."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 101
                            }
                        ],
                        "text": "These technical challenges are reflected in systems designed for web data extraction and integration [9, 23, 24, 30], where ad-hoc user input (such as annotated pages) is required to achieve acceptable results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 61
                            }
                        ],
                        "text": "Although the impressive number of sources and domains on the Web has given rise to several research proposals for automatically extracting and integrating web data, so far extraction and integration problems have been mainly tackled separately: many researchers have proposed techniques for extracting data from the Web [12], while others have concentrated their solutions on integrating the extracted data [5]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 382,
                                "start": 379
                            }
                        ],
                        "text": "The paper makes the following contributions: (i) we formulate an abstract generative model that characterizes partially overlapping data-intensive web sources; (ii) based on the model, we introduce a formal setting to state the data extraction and integration problem, exploiting the redundancy of information among the sources; (iii) we propose an unsupervised algorithm, WEIR (Web-Extraction and Integration of Redundant data), to solve the stated problem, and formally study its correctness; (iv) we show robustness and performance of our approach against alternative solutions in an experimental evaluation with real-world websites."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 7
                            }
                        ],
                        "text": "On the Web, for many domains this is a natural assumption, and the identifiers can be derived during the page harvesting phase, or automatically extracted from the page collections starting from a small seed set."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Webtables: exploring the power of tables on the web."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 43
                            }
                        ],
                        "text": "The massive exploitation of the structured Web has been studied for data published in HTML tables and lists [10, 20]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "Labeling Data Extracted from the Web."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 38
                            }
                        ],
                        "text": "We collected 40 data sources from the Web over four domain entities: soccer players, stock quotes, video games, and books.5 Detail pages for the video games and soccer players domains were gathered by means of a crawler based on a set expansion technique [6]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 31
                            }
                        ],
                        "text": "It is well recognized that the Web is a valuable source of information and that making use of its data is an incredible opportunity to create knowledge with both scientific and commercial implications."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 107
                            }
                        ],
                        "text": "A large body of works has tackled the challenge of extracting and integrating structured data from the Web [9, 23, 30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Web data extraction has been addressed in multiple works in the last decade (see [12] for a survey)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 16
                            }
                        ],
                        "text": "OCTOPUS [9] and CIMPLE [30] support users in the creation of datasets from web data by means of a set of operators to perform search, extraction, data cleaning and integration."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 117
                            }
                        ],
                        "text": "The problem of finding labels for web data has been studied in [17], which exploits the redundancy of information on Web."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Web data extraction involve several tasks: source discovery, wrapper generation, data integration, and data cleaning."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 7
                            }
                        ],
                        "text": "On the Web, redundant sources publish data of many different types and with different unit of measures."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "OCTOPUS [9] and CIMPLE [30] support users in the creation of datasets from web data by means of a set of operators"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5665679,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db8f75747cf3587fd2182f9f5ae468ec54788bb1",
            "isKey": true,
            "numCitedBy": 55,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Current approaches to develop information extraction (IE) programs have largely focused on producing precise IE results. As such, they suffer from three major limitations. First, it is often difficult to execute partially specified IE programs and obtain meaningful results, thereby producing a long \"debug loop\". Second, it often takes a long time before we can obtain the first meaningful result (by finishing and running a precise IE program), thereby rendering these approaches impractical for time-sensitive IE applications. Finally, by trying to write precise IE programs we may also waste a significant amount of effort, because an approximate result -- one that can be produced quickly -- may already be satisfactory in many IE settings.\n To address these limitations, we propose iFlex, an IE approach that relaxes the precise IE requirement to enable best-effort IE. In iFlex, a developer U uses a declarative language to quickly write an initial approximate IE program P with a possible-worlds semantics. Then iFlex evaluates P using an approximate query processor to quickly extract an approximate result. Next, U examines the result, and further refines P if necessary, to obtain increasingly more precise results. To refine P, U can enlist a next-effort assistant, which suggests refinements based on the data and the current version of P. Extensive experiments on real-world domains demonstrate the utility of the iFlex approach."
            },
            "slug": "Toward-best-effort-information-extraction-Shen-DeRose",
            "title": {
                "fragments": [],
                "text": "Toward best-effort information extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "iFlex, an IE approach that relaxes the precise IE requirement to enable best-effort IE, is proposed, in which a developer uses a declarative language to quickly write an initial approximate IE program P with a possible-worlds semantics."
            },
            "venue": {
                "fragments": [],
                "text": "SIGMOD Conference"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2224716"
                        ],
                        "name": "J. Madhavan",
                        "slug": "J.-Madhavan",
                        "structuredName": {
                            "firstName": "Jayant",
                            "lastName": "Madhavan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Madhavan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143701664"
                        ],
                        "name": "L. Afanasiev",
                        "slug": "L.-Afanasiev",
                        "structuredName": {
                            "firstName": "Loredana",
                            "lastName": "Afanasiev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Afanasiev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3013198"
                        ],
                        "name": "Lyublena Antova",
                        "slug": "Lyublena-Antova",
                        "structuredName": {
                            "firstName": "Lyublena",
                            "lastName": "Antova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lyublena Antova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770962"
                        ],
                        "name": "A. Halevy",
                        "slug": "A.-Halevy",
                        "structuredName": {
                            "firstName": "Alon",
                            "lastName": "Halevy",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Halevy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "The process of extracting and integrating data-intensive sources can be articulated as follows: (i) transform the set of web pages from each source into a relation by creating web wrappers, i.e., data extraction programs; (ii) integrate these relations by defining semantic mappings between the data\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 499698,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb0c56ef70a998c633b43bc4e470ee1983a7c0b8",
            "isKey": false,
            "numCitedBy": 106,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The Deep Web refers to content hidden behind HTML forms. In order to get to such content, a user has to perform a form submission with valid input values. The name Deep Web arises from the fact that such content was thought to be beyond the reach of search engines. The Deep Web is also believed to be the biggest source of structured data on the Web and hence accessing its contents has been a long standing challenge in the data management community [1, 8, 9, 13, 14, 18, 19]. Over the past few years, we have built a system that exposed content from the Deep Web to web-search users of Google.com. The results of our surfacing are now shown in over 1000 web-search queries per-second, and the content surfaced is in over 45 languages and in hundreds of domains. The algorithms underlying our system are described in [12]. In this paper we report some of our key observations in building our system and outline the main challenges we see in the further exploration and use of deep-web content. To understand the different efforts on providing access to deep-web content, we first present the rapidly changing landscape of different kinds of structured data that exist on the web and the relationships between them (Section 2). In fact, there appears to be some confusion about the term Deep Web \u2013 it has often been incorrectly used synonymously with structured data on the Web. The Deep Web is one (significant) source of data, much of which is structured, but not the only one. We describe the different types of structured data in the context of the varying search tasks that we can strive to support over them. Second, we discuss our choice of underlying approach in exposing deep-web content in a search engine. Most prior works on the Deep Web have espoused one of two main approaches. The first, known as virtual integration, follows the data integration paradigm. Here, we consider each deepweb site as a source in a data integration system. Users pose queries over a mediated schema that is exposed to them as a web form, and queries are routed to the relevant sites. The second, known as surfacing, attempts to"
            },
            "slug": "Harnessing-the-Deep-Web:-Present-and-Future-Madhavan-Afanasiev",
            "title": {
                "fragments": [],
                "text": "Harnessing the Deep Web: Present and Future"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper reports some of the key observations in building the system that exposed content from the Deep Web to web-search users of Google.com and discusses the choice of underlying approach in exposing deep-web content in a search engine."
            },
            "venue": {
                "fragments": [],
                "text": "CIDR"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737944"
                        ],
                        "name": "P. Bernstein",
                        "slug": "P.-Bernstein",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Bernstein",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bernstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2224716"
                        ],
                        "name": "J. Madhavan",
                        "slug": "J.-Madhavan",
                        "structuredName": {
                            "firstName": "Jayant",
                            "lastName": "Madhavan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Madhavan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747414"
                        ],
                        "name": "E. Rahm",
                        "slug": "E.-Rahm",
                        "structuredName": {
                            "firstName": "Erhard",
                            "lastName": "Rahm",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Rahm"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 23
                            }
                        ],
                        "text": "Despite recent results [5], the schema matching problem in our context is challenging because web data are"
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 81
                            }
                        ],
                        "text": "To generate mappings among the attributes we resort to an instancebased approach [5] that aggregates physical attributes with similar values into the same mapping."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 410,
                                "start": 407
                            }
                        ],
                        "text": "Although the impressive number of sources and domains on the Web has given rise to several research proposals for automatically extracting and integrating web data, so far extraction and integration problems have been mainly tackled separately: many researchers have proposed techniques for extracting data from the Web [12], while others have concentrated their solutions on integrating the extracted data [5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6302654,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55c186046feee5614cd15909dfcc587e0ff662d8",
            "isKey": false,
            "numCitedBy": 273,
            "numCiting": 95,
            "paperAbstract": {
                "fragments": [],
                "text": "In a paper published in the 2001 VLDB Conference, we proposed treating generic schema matching as an independent problem. We developed a taxonomy of existing techniques, a new schema matching algorithm, and an approach to comparative evaluation. Since then, the field has grown into a major research topic. We briefly summarize the new techniques that have been developed and applications of the techniques in the commercial world. We conclude by discussing future trends and recommendations for further work."
            },
            "slug": "Generic-schema-matching,-ten-years-later-Bernstein-Madhavan",
            "title": {
                "fragments": [],
                "text": "Generic schema matching, ten years later"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A taxonomy of existing techniques, a new schema matching algorithm, and an approach to comparative evaluation are developed, which summarizes the new techniques that have been developed and applications of the techniques in the commercial world."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. VLDB Endow."
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116235540"
                        ],
                        "name": "Xian Li",
                        "slug": "Xian-Li",
                        "structuredName": {
                            "firstName": "Xian",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xian Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145867172"
                        ],
                        "name": "Xin Dong",
                        "slug": "Xin-Dong",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Dong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xin Dong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073317592"
                        ],
                        "name": "Kenneth Lyons",
                        "slug": "Kenneth-Lyons",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Lyons",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Lyons"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38699354"
                        ],
                        "name": "W. Meng",
                        "slug": "W.-Meng",
                        "structuredName": {
                            "firstName": "Weiyi",
                            "lastName": "Meng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Meng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145860176"
                        ],
                        "name": "D. Srivastava",
                        "slug": "D.-Srivastava",
                        "structuredName": {
                            "firstName": "Divesh",
                            "lastName": "Srivastava",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Srivastava"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 104
                            }
                        ],
                        "text": "As observed in [19] and [27], these sources partially overlap, i.e., they provide redundant information both at the schema level (some attributes are published by several sources), and at the instance level (some objects are published by several sources)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "Similar issues apply to schema integration approaches based on clustering, where even a manually tuned algorithm cannot be applied to every scenario."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3133027,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46b416896cd48484603447e3aec38a719468f068",
            "isKey": false,
            "numCitedBy": 270,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The amount of useful information available on the Web has been growing at a dramatic pace in recent years and people rely more and more on the Web to fulfill their information needs. In this paper, we study truthfulness of Deep Web data in two domains where we believed data are fairly clean and data quality is important to people's lives: Stock and Flight. To our surprise, we observed a large amount of inconsistency on data from different sources and also some sources with quite low accuracy. We further applied on these two data sets state-of-the-art data fusion methods that aim at resolving conflicts and finding the truth, analyzed their strengths and limitations, and suggested promising research directions. We wish our study can increase awareness of the seriousness of conflicting data on the Web and in turn inspire more research in our community to tackle this problem."
            },
            "slug": "Truth-Finding-on-the-Deep-Web:-Is-the-Problem-Li-Dong",
            "title": {
                "fragments": [],
                "text": "Truth Finding on the Deep Web: Is the Problem Solved?"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This study studies truthfulness of Deep Web data in two domains where data are fairly clean and data quality is important to people's lives: Stock and Flight to observe a large amount of inconsistency on data from different sources and also some sources with quite low accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. VLDB Endow."
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50056360"
                        ],
                        "name": "William W. Cohen",
                        "slug": "William-W.-Cohen",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Cohen",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William W. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145969795"
                        ],
                        "name": "Pradeep Ravikumar",
                        "slug": "Pradeep-Ravikumar",
                        "structuredName": {
                            "firstName": "Pradeep",
                            "lastName": "Ravikumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pradeep Ravikumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684961"
                        ],
                        "name": "S. Fienberg",
                        "slug": "S.-Fienberg",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Fienberg",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fienberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "UnsmoothedJS, described in [14]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10625463,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9578679e028777dd709881f938114aa59fbbf481",
            "isKey": false,
            "numCitedBy": 1652,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Using an open-source, Java toolkit of name-matching methods, we experimentally compare string distance metrics on the task of matching entity names. We investigate a number of different metrics proposed by different communities, including edit-distance metrics, fast heuristic string comparators, token-based distance metrics, and hybrid methods. Overall, the best-performing method is a hybrid scheme combining a TFIDF weighting scheme, which is widely used in information retrieval, with the Jaro-Winkler string-distance scheme, which was developed in the probabilistic record linkage community."
            },
            "slug": "A-Comparison-of-String-Distance-Metrics-for-Tasks-Cohen-Ravikumar",
            "title": {
                "fragments": [],
                "text": "A Comparison of String Distance Metrics for Name-Matching Tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This work investigates a number of different metrics proposed by different communities, including edit-distance metrics, fast heuristic string comparators, token-based distance metrics, and hybrid methods, and finds the best-performing method is a hybrid scheme combining a TFIDF weighting scheme with the Jaro-Winkler string-distance scheme."
            },
            "venue": {
                "fragments": [],
                "text": "IIWeb"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725180"
                        ],
                        "name": "T. Cormen",
                        "slug": "T.-Cormen",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cormen",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cormen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145372049"
                        ],
                        "name": "C. Leiserson",
                        "slug": "C.-Leiserson",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Leiserson",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Leiserson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113911099"
                        ],
                        "name": "R. Rivest",
                        "slug": "R.-Rivest",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rivest",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rivest"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 222237163,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f01c4e51cb33f4bed8d37832dc1325ec5dedf49d",
            "isKey": false,
            "numCitedBy": 12423,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nThe updated new edition of the classic Introduction to Algorithms is intended primarily for use in undergraduate or graduate courses in algorithms or data structures. Like the first edition,this text can also be used for self-study by technical professionals since it discusses engineering issues in algorithm design as well as the mathematical aspects. \nIn its new edition,Introduction to Algorithms continues to provide a comprehensive introduction to the modern study of algorithms. The revision has been updated to reflect changes in the years since the book's original publication. New chapters on the role of algorithms in computing and on probabilistic analysis and randomized algorithms have been included. Sections throughout the book have been rewritten for increased clarity,and material has been added wherever a fuller explanation has seemed useful or new information warrants expanded coverage. \nAs in the classic first edition,this new edition of Introduction to Algorithms presents a rich variety of algorithms and covers them in considerable depth while making their design and analysis accessible to all levels of readers. Further,the algorithms are presented in pseudocode to make the book easily accessible to students from all programming language backgrounds. \nEach chapter presents an algorithm,a design technique,an application area,or a related topic. The chapters are not dependent on one another,so the instructor can organize his or her use of the book in the way that best suits the course's needs. Additionally,the new edition offers a 25% increase over the first edition in the number of problems,giving the book 155 problems and over 900 exercises thatreinforcethe concepts the students are learning."
            },
            "slug": "Introduction-to-Algorithms-Cormen-Leiserson",
            "title": {
                "fragments": [],
                "text": "Introduction to Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "The updated new edition of the classic Introduction to Algorithms is intended primarily for use in undergraduate or graduate courses in algorithms or data structures and presents a rich variety of algorithms and covers them in considerable depth while making their design and analysis accessible to all levels of readers."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803434"
                        ],
                        "name": "R. Larson",
                        "slug": "R.-Larson",
                        "structuredName": {
                            "firstName": "Ray",
                            "lastName": "Larson",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Larson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 104
                            }
                        ],
                        "text": "Our integration technique may be seen as a specialized agglomerative, hierarchical clustering algorithm [29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 58
                            }
                        ],
                        "text": "It can be seen as a hierarchical agglomerative clustering [29] that processes all the attributes from the sources."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 32493971,
            "fieldsOfStudy": [
                "Computer Science",
                "Environmental Science",
                "Psychology"
            ],
            "id": "5f3b50c6c826ad105163b09d53e1eb498a4b3994",
            "isKey": false,
            "numCitedBy": 7736,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction To Information Retrieval Overdrive Digital. Introduction To Information Retrieval. Introduction To Information Retrieval Putao Ufcg. Introduction To Information Retrieval Arbeitsbereiche. Introduction To Information Retrieval. Introduction To Information Retrieval Stanford Nlp Group. Introduction To Information Retrieval Cs Ucr Edu. Introduction To Information Retrieval By Christopher D. Introduction To Information Retrieval Book. Information Retrieval The Mit Press. Introduction Information Retrieval Uvm. Information Retrieval Lmu Munich. Introduction To Information Retrieval Stanford University. Introduction To Information Retrieval. Introduction To Information Retrieval Amp Models Slideshare. Introduction To Information Retrieval Kangwon Ac Kr. Information Retrieval. Introduction To Information Retrieval Assets. Introduction To Information Retrieval. Introduction To Information Retrieval"
            },
            "slug": "Introduction-to-Information-Retrieval-Larson",
            "title": {
                "fragments": [],
                "text": "Introduction to Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This chapter discusses Information Retrieval, the science and technology behind information retrieval and retrieval, and some of the techniques used in the retrieval of information."
            },
            "venue": {
                "fragments": [],
                "text": "J. Assoc. Inf. Sci. Technol."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725180"
                        ],
                        "name": "T. Cormen",
                        "slug": "T.-Cormen",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cormen",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cormen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145372049"
                        ],
                        "name": "C. Leiserson",
                        "slug": "C.-Leiserson",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Leiserson",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Leiserson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113911099"
                        ],
                        "name": "R. Rivest",
                        "slug": "R.-Rivest",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rivest",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rivest"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690758"
                        ],
                        "name": "C. Stein",
                        "slug": "C.-Stein",
                        "structuredName": {
                            "firstName": "Clifford",
                            "lastName": "Stein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Stein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60621753,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "611c2c20f6566946694d832e58bfd7b7cb58b66e",
            "isKey": false,
            "numCitedBy": 2760,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "If you had to buy just one text on algorithms, Introduction to Algorithms is a magnificent choice. The book begins by considering the mathematical foundations of the analysis of algorithms and maintains this mathematical rigor throughout the work. The tools developed in these opening sections are then applied to sorting, data structures, graphs, and a variety of selected algorithms including computational geometry, string algorithms, parallel models of computation, fast Fourier transforms (FFTs), and more. This book's strength lies in its encyclopedic range, clear exposition, and powerful analysis. Pseudo-code explanation of the algorithms coupled with proof of their accuracy makes this book is a great resource on the basic tools used to analyze the performance of algorithms."
            },
            "slug": "Introduction-to-Algorithms,-third-edition-Cormen-Leiserson",
            "title": {
                "fragments": [],
                "text": "Introduction to Algorithms, third edition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Pseudo-code explanation of the algorithms coupled with proof of their accuracy makes this book a great resource on the basic tools used to analyze the performance of algorithms."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2079828"
                        ],
                        "name": "Tobias D\u00f6nz",
                        "slug": "Tobias-D\u00f6nz",
                        "structuredName": {
                            "firstName": "Tobias",
                            "lastName": "D\u00f6nz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tobias D\u00f6nz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 13
                            }
                        ],
                        "text": "For example, EXALG analyzes the co-occurrence of tokens in a large number of pages sharing a common template, and ROADRUNNER tries to incrementally align a set of sample pages to separate their underlying template from the embedded data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 158
                            }
                        ],
                        "text": "As related to data extraction, in order to craft production-level wrappers, supervised approaches [18, 22] require annotated pages, whereas unsupervised ones [3, 16] cannot provide accurate results without external feedback."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 51
                            }
                        ],
                        "text": "/td[2]/text() {38M, 46M} r(6): /html[1]/table[1]/tr[3]/td[2]/text() {38M, null}"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 37
                            }
                        ],
                        "text": "Following the intuition developed in [3], we classify as template nodes the text"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 114
                            }
                        ],
                        "text": "The formalisms used by state-of-the-art unsupervised wrapper generator systems, such as ROADRUNNER [16] and EXALG [3], are expressive enough to define a complete wrapper for a vast majority of web sources."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13550720,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4fb16e487560e82aacc9cea9cb662c9eb324aa80",
            "isKey": true,
            "numCitedBy": 417,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Extracting-Structured-Data-from-Web-Pages-D\u00f6nz",
            "title": {
                "fragments": [],
                "text": "Extracting Structured Data from Web Pages"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Introduction to Algorithms (3"
            },
            "venue": {
                "fragments": [],
                "text": "ed.). MIT Press,"
            },
            "year": 2009
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 17,
            "methodology": 12,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 33,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Extraction-and-Integration-of-Partially-Overlapping-Bronzi-Crescenzi/131383aa1f91eb0e9578dcae80f4dfcfb0f11e3e?sort=total-citations"
}