{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47055692"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 205118721,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "266e07d0dd9a75b61e3632e9469993dbaf063f1c",
            "isKey": false,
            "numCitedBy": 882,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generalization-of-backpropagation-with-application-Werbos",
            "title": {
                "fragments": [],
                "text": "Generalization of backpropagation with application to a recurrent gas market model"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47055692"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14060545,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c33b70cf34814fdfe045026cc2a39fb9636d1b4a",
            "isKey": false,
            "numCitedBy": 283,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Some scientists have concluded that backpropagation is a specialized method for pattern classification, of little relevance to broader problems, to parallel computing, or to our understanding of the human brain. The author questions these beliefs and proposes development of a general theory of intelligence in which backpropagation and comparisons to the brain play a central role. He also points to a series of intermediate steps and applications leading up to the construction of such generalized systems, including past applications to social science which in some ways go beyond the work in AI as such. The author presents a condensed mathematical summary of that work. He begins by summarizing a generalized formulation of backpropagation, and then discusses network architectures and applications which it opens up.<<ETX>>"
            },
            "slug": "Backpropagation:-past-and-future-Werbos",
            "title": {
                "fragments": [],
                "text": "Backpropagation: past and future"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The author proposes development of a general theory of intelligence in which backpropagation and comparisons to the brain play a central role, and points to a series of intermediate steps and applications leading up to the construction of such generalized systems."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE 1988 International Conference on Neural Networks"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2319833"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 23072417,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8c393ec4cb7bc630d2bdd324547606daf343b76f",
            "isKey": false,
            "numCitedBy": 185,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Consistency-of-HDP-applied-to-a-simple-learning-Werbos",
            "title": {
                "fragments": [],
                "text": "Consistency of HDP applied to a simple reinforcement learning problem"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47055692"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35067038,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c7f9bddb6773be57ab671ebcbffcaaf039ad4cd0",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Generalized methods that are commonly used in neural-network research have made it possible for the US Energy Information Administration (EIA) to solve a gas-industry optimization problem on a personal computer that would previously have required a mainframe computer because of the run time required. The resulting model was used to produce EIA's official energy forecasts published in 1988. It is shown how backpropagation can be used by modelers with no special training in neurocomputing. Earlier applications of backpropagation to modeling and to EIA problems are reviewed that antedate the practical applications to neural networks. Finally, the relations between backpropagation, the current EIA model, and economic issues related to modeling and the gas industry are discussed. Among these issues are optimization subject to constraints, and competition and efficiency in gas supply. It is also shown how more recent formulations of backpropagation are a special case of the proposed formulation. >"
            },
            "slug": "Maximizing-long-term-gas-industry-profits-in-two-in-Werbos",
            "title": {
                "fragments": [],
                "text": "Maximizing long-term gas industry profits in two minutes in Lotus using neural network methods"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The relations between backpropagation, the current EIA model, and economic issues related to modeling and the gas industry are discussed and it is shown how more recent formulations of back Propagation are a special case of the proposed formulation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700974"
                        ],
                        "name": "Barak A. Pearlmutter",
                        "slug": "Barak-A.-Pearlmutter",
                        "structuredName": {
                            "firstName": "Barak",
                            "lastName": "Pearlmutter",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barak A. Pearlmutter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16813485,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "34468c0aa95a7aea212d8738ab899a69b2fc14c6",
            "isKey": false,
            "numCitedBy": 744,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Many neural network learning procedures compute gradients of the errors on the output layer of units after they have settled to their final values. We describe a procedure for finding E/wij, where E is an error functional of the temporal trajectory of the states of a continuous recurrent network and wij are the weights of that network. Computing these quantities allows one to perform gradient descent in the weights to minimize E. Simulations in which networks are taught to move through limit cycles are shown. This type of recurrent network seems particularly suited for temporally continuous domains, such as signal processing, control, and speech."
            },
            "slug": "Learning-State-Space-Trajectories-in-Recurrent-Pearlmutter",
            "title": {
                "fragments": [],
                "text": "Learning State Space Trajectories in Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A procedure for finding E/wij, where E is an error functional of the temporal trajectory of the states of a continuous recurrent network and wij are the weights of that network, which seems particularly suited for temporally continuous domains."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1822055"
                        ],
                        "name": "Raymond L. Watrous",
                        "slug": "Raymond-L.-Watrous",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Watrous",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raymond L. Watrous"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48462607"
                        ],
                        "name": "L. Shastri",
                        "slug": "L.-Shastri",
                        "structuredName": {
                            "firstName": "Lokendra",
                            "lastName": "Shastri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Shastri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12357500,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f1e40283ecd4633c36c70fbc8dbb14e9a4afb37f",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for learning phonetic features from speech data using connectionist networks is described. A temporal flow model is introduced in which sampled speech data flows through a parallel network from input to output units. The network uses hidden units with recurrent links to capture spectral/temporal characteristics of phonetic features. A supervised learning algorithm is presented which performs gradient descent in weight space using a coarse approximation of the desired output as an evaluation function. \n \nA simple connectionist network with recurrent links was trained on a single instance of the word pair \"no\" and \"go\", and successful learned a discriminatory mechanism. The trained network also correctly discriminated 98% of 25 other tokens of each word by the same speaker. A single integrated spectral feature was formed without segmentation of the input, and without a direct comparison of the two items."
            },
            "slug": "Learning-Phonetic-Features-Using-Connectionist-Watrous-Shastri",
            "title": {
                "fragments": [],
                "text": "Learning Phonetic Features Using Connectionist Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A method for learning phonetic features from speech data using connectionist networks is described and a supervised learning algorithm is presented which performs gradient descent in weight space using a coarse approximation of the desired output as an evaluation function."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38579456"
                        ],
                        "name": "D. Nguyen",
                        "slug": "D.-Nguyen",
                        "structuredName": {
                            "firstName": "Derrick",
                            "lastName": "Nguyen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Nguyen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1913418"
                        ],
                        "name": "B. Widrow",
                        "slug": "B.-Widrow",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Widrow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Widrow"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6020113,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c243f77e85185706abcb6f9a3b25348ad324759",
            "isKey": false,
            "numCitedBy": 563,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural networks can be used to solve highly nonlinear control problems. A two-layer neural network containing 26 adaptive neural elements has learned to back up a computer-simulated trailer truck to a loading dock, even when initially jackknifed. It is not yet known how to design a controller to perform this steering task. Nevertheless, the neural net was able to learn of its own accord to do this, regardless of initial conditions. Experience gained with the truck backer-upper should be applicable to a wide variety of nonlinear control problems.<<ETX>>"
            },
            "slug": "The-truck-backer-upper:-an-example-of-self-learning-Nguyen-Widrow",
            "title": {
                "fragments": [],
                "text": "The truck backer-upper: an example of self-learning in neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "A two-layer neural network containing 26 adaptive neural elements has learned to back up a computer-simulated trailer truck to a loading dock, even when initially jackknifed."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114709889"
                        ],
                        "name": "I. Poujaud",
                        "slug": "I.-Poujaud",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Poujaud",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Poujaud"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3078169"
                        ],
                        "name": "L. Personnaz",
                        "slug": "L.-Personnaz",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Personnaz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Personnaz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144097910"
                        ],
                        "name": "G. Dreyfus",
                        "slug": "G.-Dreyfus",
                        "structuredName": {
                            "firstName": "G\u00e9rard",
                            "lastName": "Dreyfus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Dreyfus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "116525682"
                        ],
                        "name": "Y. Le Cun",
                        "slug": "Y.-Le-Cun",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Le Cun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Le Cun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2093637,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "590bd948e06e9d07e305fe175c2a86d751ccac2d",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "An evaluation is made of several neural network classifiers, comparing their performance on a typical problem, namely handwritten digit recognition. For this purpose, the authors use a database of handwritten digits, with relatively uniform handwriting styles. The authors propose a novel way of organizing the network architectures by training several small networks so as to deal separately with subsets of the problem, and then combining the results. This approach works in conjunction with various techniques including: layered networks with one or several layers of adaptive connections, fully connected recursive networks, ad hoc networks with no adaptive connections, and architectures with second-degree polynomial decision surfaces.<<ETX>>"
            },
            "slug": "Comparing-different-neural-network-architectures-Guyon-Poujaud",
            "title": {
                "fragments": [],
                "text": "Comparing different neural network architectures for classifying handwritten digits"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The authors propose a novel way of organizing the network architectures by training several small networks so as to deal separately with subsets of the problem, and then combining the results."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "115945547"
                        ],
                        "name": "H. Sawai",
                        "slug": "H.-Sawai",
                        "structuredName": {
                            "firstName": "Hidefumi",
                            "lastName": "Sawai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Sawai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9293147"
                        ],
                        "name": "A. Waibei",
                        "slug": "A.-Waibei",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Waibei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721248"
                        ],
                        "name": "P. Haffner",
                        "slug": "P.-Haffner",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Haffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haffner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1970204"
                        ],
                        "name": "M. Miyatake",
                        "slug": "M.-Miyatake",
                        "structuredName": {
                            "firstName": "Masanori",
                            "lastName": "Miyatake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Miyatake"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9243990"
                        ],
                        "name": "K. Shikano",
                        "slug": "K.-Shikano",
                        "structuredName": {
                            "firstName": "Kiyohiro",
                            "lastName": "Shikano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Shikano"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14999196,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "8798a360b78b97f4ff96f1aaa1176d6157e4bc5e",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "To extend the performance of TDNNs (time-delay neural networks) to all phoneme recognition and word/continuous speech recognition, the authors present several techniques. First, they show that it is possible to scale up the TDNN to a large phonemic TDNN aimed at discriminating all phonemes without loss of recognition performance and without excessive training tokens. Second, the authors propose fast backpropagation learning methods which make it possible to train a large phonemic TDNN within 1.5 hours. Finally, they show several methods for spotting Japanese CV syllables/phonemes in input speech based on TDNNs: they constructed a TDNN which can discriminate a single CV syllable or phoneme. Syllable and phoneme spotting experiments show excellent results, with syllable and phoneme spotting rates of better than 96.7% and 92% correct, respectively.<<ETX>>"
            },
            "slug": "Parallelism,-hierarchy,-scaling-in-time-delay-for-Sawai-Waibei",
            "title": {
                "fragments": [],
                "text": "Parallelism, hierarchy, scaling in time-delay neural networks for spotting Japanese phonemes CV-syllables"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "To extend the performance of TDNNs (time-delay neural networks) to all phoneme recognition and word/continuous speech recognition, the authors present several techniques and constructed a TDNN which can discriminate a single CV syllable or phoneme."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800149"
                        ],
                        "name": "J. Dennis",
                        "slug": "J.-Dennis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Dennis",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dennis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795397"
                        ],
                        "name": "Bobby Schnabel",
                        "slug": "Bobby-Schnabel",
                        "structuredName": {
                            "firstName": "Bobby",
                            "lastName": "Schnabel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bobby Schnabel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 27578127,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e1053197256c6c3c0631377ec23a3f7dc1cb4781",
            "isKey": false,
            "numCitedBy": 7615,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface 1. Introduction. Problems to be considered Characteristics of 'real-world' problems Finite-precision arithmetic and measurement of error Exercises 2. Nonlinear Problems in One Variable. What is not possible Newton's method for solving one equation in one unknown Convergence of sequences of real numbers Convergence of Newton's method Globally convergent methods for solving one equation in one uknown Methods when derivatives are unavailable Minimization of a function of one variable Exercises 3. Numerical Linear Algebra Background. Vector and matrix norms and orthogonality Solving systems of linear equations-matrix factorizations Errors in solving linear systems Updating matrix factorizations Eigenvalues and positive definiteness Linear least squares Exercises 4. Multivariable Calculus Background Derivatives and multivariable models Multivariable finite-difference derivatives Necessary and sufficient conditions for unconstrained minimization Exercises 5. Newton's Method for Nonlinear Equations and Unconstrained Minimization. Newton's method for systems of nonlinear equations Local convergence of Newton's method The Kantorovich and contractive mapping theorems Finite-difference derivative methods for systems of nonlinear equations Newton's method for unconstrained minimization Finite difference derivative methods for unconstrained minimization Exercises 6. Globally Convergent Modifications of Newton's Method. The quasi-Newton framework Descent directions Line searches The model-trust region approach Global methods for systems of nonlinear equations Exercises 7. Stopping, Scaling, and Testing. Scaling Stopping criteria Testing Exercises 8. Secant Methods for Systems of Nonlinear Equations. Broyden's method Local convergence analysis of Broyden's method Implementation of quasi-Newton algorithms using Broyden's update Other secant updates for nonlinear equations Exercises 9. Secant Methods for Unconstrained Minimization. The symmetric secant update of Powell Symmetric positive definite secant updates Local convergence of positive definite secant methods Implementation of quasi-Newton algorithms using the positive definite secant update Another convergence result for the positive definite secant method Other secant updates for unconstrained minimization Exercises 10. Nonlinear Least Squares. The nonlinear least-squares problem Gauss-Newton-type methods Full Newton-type methods Other considerations in solving nonlinear least-squares problems Exercises 11. Methods for Problems with Special Structure. The sparse finite-difference Newton method Sparse secant methods Deriving least-change secant updates Analyzing least-change secant methods Exercises Appendix A. A Modular System of Algorithms for Unconstrained Minimization and Nonlinear Equations (by Robert Schnabel) Appendix B. Test Problems (by Robert Schnabel) References Author Index Subject Index."
            },
            "slug": "Numerical-methods-for-unconstrained-optimization-Dennis-Schnabel",
            "title": {
                "fragments": [],
                "text": "Numerical methods for unconstrained optimization and nonlinear equations"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Newton's Method for Nonlinear Equations and Unconstrained Minimization and methods for solving nonlinear least-squares problems with Special Structure."
            },
            "venue": {
                "fragments": [],
                "text": "Prentice Hall series in computational mathematics"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2213123"
                        ],
                        "name": "D. Shanno",
                        "slug": "D.-Shanno",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Shanno",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Shanno"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 9256912,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "27250b833d10ec7174c841171c5fc5e792c10a63",
            "isKey": false,
            "numCitedBy": 433,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Conjugate gradient methods are iterative methods for finding the minimizer of a scalar function fx of a vector variable x which do not update an approximation to the inverse Hessian matrix. This paper examines the effects of inexact linear searches on the methods and shows how the traditional Fletcher-Reeves and Polak-Ribiere algorithm may be modified in a form discovered by Perry to a sequence which can be interpreted as a memorytess BFGS algorithm. This algorithm may then be scaled optimally in the sense of Oren and Spedicalo. This scaling can be combined with Beale restarts and Powell's restart criterion. Computational results will show that this new method substantially outperforms known conjugate gradient methods on a wide class of problems."
            },
            "slug": "Conjugate-Gradient-Methods-with-Inexact-Searches-Shanno",
            "title": {
                "fragments": [],
                "text": "Conjugate Gradient Methods with Inexact Searches"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The traditional Fletcher-Reeves and Polak-Ribiere algorithm may be modified in a form discovered by Perry to a sequence which can be interpreted as a memorytess BFGS algorithm and this algorithm may then be scaled optimally in the sense of Oren and Spedicalo."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Oper. Res."
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47055692"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59694629,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa277dfe3645463a25432282563fca4891d846ea",
            "isKey": false,
            "numCitedBy": 261,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The following paper summarizes the major properties and applications of a collection of algorithms involving differentiation and optimization at minimum cost. The areas of application include the sensitivity analysis of models, new work in statistical or econometric estimation, optimization, artificial intelligence and neuron modelling. The details, references and derivations can be obtained by requesting \u201eSensitivity Analysis Methods for Nonlinear Systems\u201c from Forecast Analysis and Evaluation Team, Quality Assurance, OSS/EIA, Room 7413, Department of Energy, Washington, DC 20461."
            },
            "slug": "Applications-of-advances-in-nonlinear-sensitivity-Werbos",
            "title": {
                "fragments": [],
                "text": "Applications of advances in nonlinear sensitivity analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "The following paper summarizes the major properties and applications of a collection of algorithms involving differentiation and optimization at minimum cost, including the sensitivity analysis of models, new work in statistical or econometric estimation, optimization, artificial intelligence and neuron modelling."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35666339"
                        ],
                        "name": "J. Cramer",
                        "slug": "J.-Cramer",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Cramer",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cramer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2780316"
                        ],
                        "name": "B. Womack",
                        "slug": "B.-Womack",
                        "structuredName": {
                            "firstName": "Baxter",
                            "lastName": "Womack",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Womack"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 42511106,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d656089b7465f6a23dea7f94bd46cfa1020d6d82",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper exploits the adaptive transformational or mapping capabilities of neural networks in the design of a neural controller for dynamic systems. The controller makes no assumptions on the dynamics of the plant, only weak assumptions on the structures of the plant. Several controller architectures are presented with simulations demonstrating their ability to compensate unstable, non-linear, or time-varying systems."
            },
            "slug": "Adaptive-Control-Using-Neural-Networks-Cramer-Womack",
            "title": {
                "fragments": [],
                "text": "Adaptive Control Using Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "1991 American Control Conference"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18686894,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "909226ce00fbb74306da00911d48651383bf1ae8",
            "isKey": false,
            "numCitedBy": 164,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "Although general network learning rules are of undeniable interest, it is generally agreed that successful accounts of learning must incorporate domain-specific, a priori knowledge. Such knowledge might be used, for example, to determine the structure of a network or its initial weights. The author discusses a third possibility in which domain-specific knowledge is incorporated directly in a network learning rule via a set of constraints on activations. The approach uses the notion of a forward model to give constraints a domain-specific interpretation. This approach is demonstrated with several examples from the domain of motor learning.<<ETX>>"
            },
            "slug": "Generic-constraints-on-underspecified-target-Jordan",
            "title": {
                "fragments": [],
                "text": "Generic constraints on underspecified target trajectories"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The author discusses a third possibility in which domain-specific knowledge is incorporated directly in a network learning rule via a set of constraints on activations, which uses the notion of a forward model to give constraints a domain- specific interpretation."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62245742,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "111fd833a4ae576cfdbb27d87d2f8fc0640af355",
            "isKey": false,
            "numCitedBy": 19356,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-internal-representations-by-error-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning internal representations by error propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30320952"
                        ],
                        "name": "F. Pineda",
                        "slug": "F.-Pineda",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pineda",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Pineda"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 196070964,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d18b17417128322f86528f60d652a5b998a51409",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "GENERALIZATION-OF-BACKPROPAGATION-TO-RECURRENT-AND-Pineda",
            "title": {
                "fragments": [],
                "text": "GENERALIZATION OF BACKPROPAGATION TO RECURRENT AND HIGH-ORDER NETWORKS."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2319833"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 207975157,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "56623a496727d5c71491850e04512ddf4152b487",
            "isKey": false,
            "numCitedBy": 4468,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Beyond-Regression-:-\"New-Tools-for-Prediction-and-Werbos",
            "title": {
                "fragments": [],
                "text": "Beyond Regression : \"New Tools for Prediction and Analysis in the Behavioral Sciences"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2213123"
                        ],
                        "name": "D. Shanno",
                        "slug": "D.-Shanno",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Shanno",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Shanno"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60279543,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "b1d52187016aa943a854f8df6907d57ee0971713",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Recent-advances-in-numerical-techniques-for-large-Shanno",
            "title": {
                "fragments": [],
                "text": "Recent advances in numerical techniques for large scale optimization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068289963"
                        ],
                        "name": "L. B. Almeida",
                        "slug": "L.-B.-Almeida",
                        "structuredName": {
                            "firstName": "Lu\u00eds",
                            "lastName": "Almeida",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. B. Almeida"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58820035,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8be3f21ab796bd9811382b560507c1c679fae37f",
            "isKey": false,
            "numCitedBy": 325,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-learning-rule-for-asynchronous-perceptrons-with-a-Almeida",
            "title": {
                "fragments": [],
                "text": "A learning rule for asynchronous perceptrons with feedback in a combinatorial environment"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105353"
                        ],
                        "name": "K. Narendra",
                        "slug": "K.-Narendra",
                        "structuredName": {
                            "firstName": "Kumpati",
                            "lastName": "Narendra",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Narendra"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 54130823,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "7f04415dce6c6d7e0c7e363818a0c5cbffc91443",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adaptive-control-using-neural-networks-Narendra",
            "title": {
                "fragments": [],
                "text": "Adaptive control using neural networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17962678,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25d9530f5a1ae0630d451d754407fc9ab7ceb4b2",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adaptive-state-representation-and-estimation-using-Williams",
            "title": {
                "fragments": [],
                "text": "Adaptive state representation and estimation using recurrent connectionist networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "England, that emphasized mathematical physics, international political economy, and economics. He developed backpropagation for the Ph.D. degree in applied mathematics at Harvard"
            },
            "venue": {
                "fragments": [],
                "text": "He iscurrently Program Director for Neuroengineering and Emerging Technology Initiation at the National Science Foundation (NSF) and Secretary of the International Neural Network Society . While an Assistant Professor at the University of Maryland"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Une procedure d \u2018 apprentissage pour reseau a seuil assymetrique"
            },
            "venue": {
                "fragments": [],
                "text": "Proc . Cognitiva \u2019"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning howtheworld works: Specificationsfor predictive networks in robots and brains"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. 1987 /\u20acE\u20ac lnt. Conf. Syst"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Glorot , Understanding the difficulty of training deep feedforward neuralnetworks , AISTATS 2010 LeCun , Yann ; Bengio , Yoshua ; Hinton , Geoffrey ( 2015 ) . \" Deep learning \""
            },
            "venue": {
                "fragments": [],
                "text": "\" Efficient backprop . \" Neural networks : Tricks of the trade"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural Networks for Robotics and Control"
            },
            "venue": {
                "fragments": [],
                "text": "\u201dAdaptive control using neural networks,\u201d"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptive control using neural networks NeuralNetworks for Robotics andControl Maximizing long-term gas industry profits in two minutes in Lotus using neural network methods"
            },
            "venue": {
                "fragments": [],
                "text": "/FE\u20ac Trans. Syst"
            },
            "year": 1989
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {},
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 27,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Backpropagation-Through-Time:-What-It-Does-and-How-Werbos/1a3d22599028a05669e884f3eaf19a342e190a87?sort=total-citations"
}