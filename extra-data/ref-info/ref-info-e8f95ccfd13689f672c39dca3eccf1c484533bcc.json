{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792214"
                        ],
                        "name": "Daniel Povey",
                        "slug": "Daniel-Povey",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Povey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Povey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Vinyals and Povey (2012) make a similar empirical observation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 117
                            }
                        ],
                        "text": "HessianFree optimization (Martens, 2010; Sutskever et al., 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), natural gradient descent (Amari, 1997; Park et al., 2000), TONGA (Le Roux et al., 2008; Roux and Fitzgibbon, 2010) are just a few of such recently proposed\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 121
                            }
                        ],
                        "text": "Instead of using linear conjugate gradient descent for computing the inverse of the metric, Krylov Subspace Descent(KSD) Vinyals and Povey (2012) opts for restricting \u2206\u03b8 to a lower dimensional Krylov subspace given by Gx = \u2207L and then, using some other second order method like LBFGS, to solve for\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 47
                            }
                        ],
                        "text": "The result agrees with the observation made in Vinyals and Povey (2012), where Krylov Subspace Descent (KSD) was shown to converge faster than Hessian-Free."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 121
                            }
                        ],
                        "text": "Instead of using linear conjugate gradient descent for computing the inverse of the metric, Krylov Subspace Descent(KSD) Vinyals and Povey (2012) opts for restricting \u2206\u03b8 to a lower dimensional Krylov subspace given by Gx = \u2207L and then, using some other second order method like LBFGS, to solve for \u2206\u03b8 within this space."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 145
                            }
                        ],
                        "text": "We show the connection between natural gradient and three other recently proposed methods: Hessian-Free (Martens, 2010), Krylov Subspace Descent (Vinyals and Povey, 2012) and TONGA (Le Roux et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 277,
                                "start": 254
                            }
                        ],
                        "text": "One particularly interesting pipeline to scale up such algorithms was originally proposed in Pearlmutter (1994) \u2013 finetuned in Schraudolph (2002) \u2013 and represents the backbone behind both HessianFree optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 166
                            }
                        ],
                        "text": "Section 5 describes the connection between natural gradient and Hessian Free (Martens, 2010), section 6 looks at the relationship with Krylov Subspace Descent (KSD) (Vinyals and Povey, 2012)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 59
                            }
                        ],
                        "text": ", 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), natural gradient descent (Amari, 1997; Park et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 47
                            }
                        ],
                        "text": "The result agrees with the observation made in Vinyals and Povey (2012), where Krylov Subspace Descent\n(KSD) was shown to converge faster than Hessian-Free."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 6318468,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a98483785378bde7e2384a3035b2b501ee03654b",
            "isKey": true,
            "numCitedBy": 119,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a second order optimization method to learn models where both the dimensionality of the parameter space and the number of training samples is high. In our method, we construct on each iteration a Krylov subspace formed by the gradient and an approximation to the Hessian matrix, and then use a subset of the training data samples to optimize over this subspace. As with the Hessian Free (HF) method of Martens (2010), the Hessian matrix is never explicitly constructed, and is computed using a subset of data. In practice, as in HF, we typically use a positive definite substitute for the Hessian matrix such as the Gauss-Newton matrix. We investigate the effectiveness of our proposed method on deep neural networks, and compare its performance to widely used methods such as stochastic gradient descent, conjugate gradient descent and L-BFGS, and also to HF. Our method leads to faster convergence than either L-BFGS or HF, and generally performs better than either of them in cross-validation accuracy. It is also simpler and more general than HF, as it does not require a positive semidefinite approximation of the Hessian matrix to work well nor the setting of a damping parameter. The chief drawback versus HF is the need for memory to store a basis for the Krylov subspace."
            },
            "slug": "Krylov-Subspace-Descent-for-Deep-Learning-Vinyals-Povey",
            "title": {
                "fragments": [],
                "text": "Krylov Subspace Descent for Deep Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "This paper proposes a second order optimization method to learn models where both the dimensionality of the parameter space and the number of training samples is high, and builds on each iteration a Krylov subspace formed by the gradient and an approximation to the Hessian matrix."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3450996"
                        ],
                        "name": "Ryan Kiros",
                        "slug": "Ryan-Kiros",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Kiros",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan Kiros"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 43
                            }
                        ],
                        "text": "Our result is in the spirit of the work of Kiros (2013), though the exact approach of dealing with small minibatches is slightly different."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 17225395,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "daa083e60ed6a85ddbbf370f8cb58dee37520f3a",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Hessian-free (HF) optimization has been successfully used for training deep autoencoders and recurrent networks. HF uses the conjugate gradient algorithm to construct update directions through curvature-vector products that can be computed on the same order of time as gradients. In this paper we exploit this property and study stochastic HF with gradient and curvature mini-batches independent of the dataset size. We modify Martens' HF for these settings and integrate dropout, a method for preventing co-adaptation of feature detectors, to guard against overfitting. Stochastic Hessian-free optimization gives an intermediary between SGD and HF that achieves competitive performance on both classification and deep autoencoder experiments."
            },
            "slug": "Training-Neural-Networks-with-Stochastic-Kiros",
            "title": {
                "fragments": [],
                "text": "Training Neural Networks with Stochastic Hessian-Free Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Stochastic Hessian-free optimization gives an intermediary between SGD and HF that achieves competitive performance on both classification and deep autoencoder experiments."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145704247"
                        ],
                        "name": "James Martens",
                        "slug": "James-Martens",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Martens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Martens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Structural damping (Martens and Sutskever, 2011), a specific regularization term used to improve training of recurrent neural network, can also be explained from the natural gradient descent perspective."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "HessianFree Optimization (Martens, 2010; Martens and Sutskever, 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012; Mizutani and Demmel, 2003), natural gradient descent (Amari, 1997; Park et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9153163,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d6203718c15f137fda2f295c96269bc2b254644",
            "isKey": false,
            "numCitedBy": 585,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we resolve the long-outstanding problem of how to effectively train recurrent neural networks (RNNs) on complex and difficult sequence modeling problems which may contain long-term data dependencies. Utilizing recent advances in the Hessian-free optimization approach (Martens, 2010), together with a novel damping scheme, we successfully train RNNs on two sets of challenging problems. First, a collection of pathological synthetic datasets which are known to be impossible for standard optimization approaches (due to their extremely long-term dependencies), and second, on three natural and highly complex real-world sequence datasets where we find that our method significantly outperforms the previous state-of-the-art method for training neural sequence models: the Long Short-term Memory approach of Hochreiter and Schmidhuber (1997). Additionally, we offer a new interpretation of the generalized Gauss-Newton matrix of Schraudolph (2002) which is used within the HF approach of Martens."
            },
            "slug": "Learning-Recurrent-Neural-Networks-with-Martens-Sutskever",
            "title": {
                "fragments": [],
                "text": "Learning Recurrent Neural Networks with Hessian-Free Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This work solves the long-outstanding problem of how to effectively train recurrent neural networks on complex and difficult sequence modeling problems which may contain long-term data dependencies and offers a new interpretation of the generalized Gauss-Newton matrix of Schraudolph which is used within the HF approach of Martens."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145704247"
                        ],
                        "name": "James Martens",
                        "slug": "James-Martens",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Martens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Martens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 25
                            }
                        ],
                        "text": "HessianFree optimization (Martens, 2010; Sutskever et al., 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), natural gradient descent (Amari, 1997; Park et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 26
                            }
                        ],
                        "text": "HessianFree optimization (Martens, 2010; Sutskever et al., 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), natural gradient descent (Amari, 1997; Park et al., 2000), TONGA (Le Roux et al., 2008; Roux and Fitzgibbon, 2010) are just a few of such recently proposed\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 123
                            }
                        ],
                        "text": "Following the functional manifold interpretation of the algorithm, we can recover the LevenbergMarquardt heuristic used in Martens (2010) by considering a first order Taylor approximation, where for any function f ,\nf ( \u03b8t \u2212 \u03b7G\u22121 \u2202f(\u03b8t)\n\u2202\u03b8t\nT ) \u2248 f(\u03b8t)\u2212 \u03b7 \u2202f(\u03b8t)\n\u2202\u03b8t G\u22121\n\u2202f(\u03b8t)\n\u2202\u03b8t\nT\n(30)\nThis gives\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 388,
                                "start": 214
                            }
                        ],
                        "text": "One particularly interesting pipeline to scale up such algorithms was originally proposed in Pearlmutter (1994) \u2013 finetuned in Schraudolph (2002) \u2013 and represents the backbone behind both HessianFree optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012). The core idea behind it is to make use of the forward pass (renamed to R-operator in Pearlmutter (1994)) and backward pass of automatic differentiation to compute efficient products between Jacobian or Hessian matrices and vectors."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 146
                            }
                        ],
                        "text": "We have implemented natural gradient descent using a truncated Newton approach similar to the pipeline proposed by Pearlmutter (1994) and used by Martens (2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 89
                            }
                        ],
                        "text": "We carry out a benchmark on the Curves dataset, using the 6 layer deep auto-encoder from Martens (2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 150
                            }
                        ],
                        "text": "\u2026\u2202f(\u03b8t)\n\u2202\u03b8t\nT ) \u2248 f(\u03b8t)\u2212 \u03b7 \u2202f(\u03b8t)\n\u2202\u03b8t G\u22121\n\u2202f(\u03b8t)\n\u2202\u03b8t\nT\n(15)\nThis gives as the reduction ratio given by equation (31) which can be shown to behave identically with the one in Martens (2010).\n\u03c1 = f ( \u03b8t \u2212 \u03b7G\u22121 \u2202f(\u03b8t)\u2202\u03b8t T) \u2212 f(\u03b8t)\n\u2212\u03b7 \u2202f(\u03b8t)\u2202\u03b8t G \u22121 \u2202f(\u03b8t) \u2202\u03b8t\nT (16)\nStructural damping (Sutskever et\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 123
                            }
                        ],
                        "text": "Following the functional manifold interpretation of the algorithm, we can recover the LevenbergMarquardt heuristic used in Martens (2010) to adapt the damping factor by considering a first order Taylor approximation, where for any function f ,\nf ( \u03b8t \u2212 \u03b7G\u22121 \u2202f(\u03b8t)\n\u2202\u03b8t\nT ) \u2248 f(\u03b8t)\u2212 \u03b7 \u2202f(\u03b8t)\n\u2202\u03b8t\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 124
                            }
                        ],
                        "text": "Following the functional manifold interpretation of the algorithm, we can recover the Levenberg-Marquardt heuristic used in Martens (2010) to adapt the damping factor by considering a first order Taylor approximation, where for any function f ,"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 104
                            }
                        ],
                        "text": "We show the connection between natural gradient and three other recently proposed methods: Hessian-Free (Martens, 2010), Krylov Subspace Descent (Vinyals and Povey, 2012) and TONGA (Le Roux et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 210
                            }
                        ],
                        "text": "One particularly interesting pipeline to scale up such algorithms was originally proposed in Pearlmutter (1994) \u2013 finetuned in Schraudolph (2002) \u2013 and represents the backbone behind both HessianFree optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 54
                            }
                        ],
                        "text": "While Heskes (2000) precedes both Schraudolph (2002); Martens (2010) both Hessian Free and Krylov Subspace Descent are introduced as purely approximations to second order methods."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 62
                            }
                        ],
                        "text": "(16) which can be shown to behave identically with the one in Martens (2010)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 78
                            }
                        ],
                        "text": "Section 5 describes the connection between natural gradient and Hessian Free (Martens, 2010), section 6 looks at the relationship with Krylov Subspace Descent (KSD) (Vinyals and Povey, 2012)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 185
                            }
                        ],
                        "text": "\u2026for any function f ,\nf ( \u03b8t \u2212 \u03b7G\u22121 \u2202f(\u03b8t)\n\u2202\u03b8t\nT ) \u2248 f(\u03b8t)\u2212 \u03b7 \u2202f(\u03b8t)\n\u2202\u03b8t G\u22121\n\u2202f(\u03b8t)\n\u2202\u03b8t\nT\n(30)\nThis gives as the reduction ratio given by equation (31) which can be shown to behave identically with the one in Martens (2010).\n\u03c1 = f ( \u03b8t \u2212 \u03b7G\u22121 \u2202f(\u03b8t)\u2202\u03b8t T) \u2212 f(\u03b8t)\n\u2212\u03b7 \u2202f(\u03b8t)\u2202\u03b8t G \u22121 \u2202f(\u03b8t) \u2202\u03b8t\nT (31)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 11154521,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4c46347fbc272b21468efe3d9af34b4b2bad6684",
            "isKey": true,
            "numCitedBy": 845,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a 2nd-order optimization method based on the \"Hessian-free\" approach, and apply it to training deep auto-encoders. Without using pre-training, we obtain results superior to those reported by Hinton & Salakhutdinov (2006) on the same tasks they considered. Our method is practical, easy to use, scales nicely to very large datasets, and isn't limited in applicability to auto-encoders, or any specific model class. We also discuss the issue of \"pathological curvature\" as a possible explanation for the difficulty of deep-learning and how 2nd-order optimization, and our method in particular, effectively deals with it."
            },
            "slug": "Deep-learning-via-Hessian-free-optimization-Martens",
            "title": {
                "fragments": [],
                "text": "Deep learning via Hessian-free optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A 2nd-order optimization method based on the \"Hessian-free\" approach is developed, and applied to training deep auto-encoders, and results superior to those reported by Hinton & Salakhutdinov (2006) are obtained."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2755582"
                        ],
                        "name": "Guillaume Desjardins",
                        "slug": "Guillaume-Desjardins",
                        "structuredName": {
                            "firstName": "Guillaume",
                            "lastName": "Desjardins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guillaume Desjardins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1996134"
                        ],
                        "name": "Razvan Pascanu",
                        "slug": "Razvan-Pascanu",
                        "structuredName": {
                            "firstName": "Razvan",
                            "lastName": "Pascanu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Razvan Pascanu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760871"
                        ],
                        "name": "Aaron C. Courville",
                        "slug": "Aaron-C.-Courville",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Courville",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron C. Courville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 30
                            }
                        ],
                        "text": "We repeat the experiment from Erhan et al. (2010), using the NISTP dataset introduced in Bengio et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Desjardins et al. (2013) shows a straight forward application of natural gradient to Deep Boltzmann Machines."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18634770,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a7a2658df5d66541305962d4c9d43078adadac6",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces the Metric-Free Natural Gradient (MFNG) algorithm for training Boltzmann Machines. Similar in spirit to the Hessian-Free method of Martens [8], our algorithm belongs to the family of truncated Newton methods and exploits an efficient matrix-vector product to avoid explicitely storing the natural gradient metric $L$. This metric is shown to be the expected second derivative of the log-partition function (under the model distribution), or equivalently, the variance of the vector of partial derivatives of the energy function. We evaluate our method on the task of joint-training a 3-layer Deep Boltzmann Machine and show that MFNG does indeed have faster per-epoch convergence compared to Stochastic Maximum Likelihood with centering, though wall-clock performance is currently not competitive."
            },
            "slug": "Metric-Free-Natural-Gradient-for-Joint-Training-of-Desjardins-Pascanu",
            "title": {
                "fragments": [],
                "text": "Metric-Free Natural Gradient for Joint-Training of Boltzmann Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The Metric-Free Natural Gradient algorithm is introduced and it is shown that MFNG does indeed have faster per-epoch convergence compared to Stochastic Maximum Likelihood with centering, though wall-clock performance is currently not competitive."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5041704"
                        ],
                        "name": "Ana M. Gonz\u00e1lez",
                        "slug": "Ana-M.-Gonz\u00e1lez",
                        "structuredName": {
                            "firstName": "Ana",
                            "lastName": "Gonz\u00e1lez",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ana M. Gonz\u00e1lez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721097"
                        ],
                        "name": "J. R. Dorronsoro",
                        "slug": "J.-R.-Dorronsoro",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Dorronsoro",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Dorronsoro"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 0
                            }
                        ],
                        "text": "Gonzalez and Dorronsoro (2006) is more similar in spirit with this work, though their approach is defined for the diagonal form of the Fisher Information Matrix and differs in how they propose to compute a new conjugate direction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 0
                            }
                        ],
                        "text": "Gonzalez and Dorronsoro (2006); Honkela et al. (2010) address these issues by making the assumption that Gt\u22121 and Gt are identical (so dt\u22121 does not need to be transported)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 36880675,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db9574d9c4c7da04d22e3fa2268ba5271acbd597",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Natural-conjugate-gradient-training-of-multilayer-Gonz\u00e1lez-Dorronsoro",
            "title": {
                "fragments": [],
                "text": "Natural conjugate gradient training of multilayer perceptrons"
            },
            "venue": {
                "fragments": [],
                "text": "Neurocomputing"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761978"
                        ],
                        "name": "D. Erhan",
                        "slug": "D.-Erhan",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Erhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Erhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760871"
                        ],
                        "name": "Aaron C. Courville",
                        "slug": "Aaron-C.-Courville",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Courville",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron C. Courville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120247189"
                        ],
                        "name": "Pascal Vincent",
                        "slug": "Pascal-Vincent",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Vincent"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 30
                            }
                        ],
                        "text": "We repeat the experiment from Erhan et al. (2010), using the NISTP dataset introduced in Bengio et al. (2011) (which is just the NIST dataset plus deformations) and use 32.7M samples of this data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15796526,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d2336389dff3031910bd21dd1c44d1b4cd51725",
            "isKey": false,
            "numCitedBy": 1726,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. The main question investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the influence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments confirm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pre-training guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre-training."
            },
            "slug": "Why-Does-Unsupervised-Pre-training-Help-Deep-Erhan-Courville",
            "title": {
                "fragments": [],
                "text": "Why Does Unsupervised Pre-training Help Deep Learning?"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The results suggest that unsupervised pre-training guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre- training."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7245737"
                        ],
                        "name": "Nicolas Le Roux",
                        "slug": "Nicolas-Le-Roux",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Le Roux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Le Roux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47139824"
                        ],
                        "name": "A. Fitzgibbon",
                        "slug": "A.-Fitzgibbon",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Fitzgibbon",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fitzgibbon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 13
                            }
                        ],
                        "text": "Note that in Roux and Fitzgibbon (2010) a method for combining second order information and TONGA is proposed."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 247
                            }
                        ],
                        "text": "They usually can be split in different categories: those which make use of second order information, those which use the geometry of the underlying parameter manifold (e.g. natural gradient) or those that use the uncertainty in the gradient (e.g. TONGA)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 15
                            }
                        ],
                        "text": ", 2000), TONGA (Le Roux et al., 2008; Roux and Fitzgibbon, 2010) are just a few of such recently proposed algorithms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 220
                            }
                        ],
                        "text": "\u2026optimization (Martens, 2010; Sutskever et al., 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), natural gradient descent (Amari, 1997; Park et al., 2000), TONGA (Le Roux et al., 2008; Roux and Fitzgibbon, 2010) are just a few of such recently proposed algorithms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 127
                            }
                        ],
                        "text": "One particularly interesting pipeline to scale up such algorithms was originally proposed in Pearlmutter (1994) \u2013 finetuned in Schraudolph (2002) \u2013 and represents the backbone behind both HessianFree optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 202
                            }
                        ],
                        "text": "HessianFree optimization (Martens, 2010; Sutskever et al., 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), natural gradient descent (Amari, 1997; Park et al., 2000), TONGA (Le Roux et al., 2008; Roux and Fitzgibbon, 2010) are just a few of such recently proposed algorithms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 66
                            }
                        ],
                        "text": "Lastly we highlighted the difference between natural gradient and TONGA, and brought forward two new properties of the algorithm compared to stochastic gradient descent."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2546945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f7cc843c318d8862357485488971b26527ef1a8e",
            "isKey": true,
            "numCitedBy": 52,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Nowadays, for many tasks such as object recognition or language modeling, data is plentiful. As such, an important challenge has become to find learning algorithms which can make use of all the available data. In this setting, called \"large-scale learning\" by Bottou & Bousquet (2008), learning and optimization become different and powerful optimization algorithms are suboptimal learning algorithms. While most efforts are focused on adapting optimization algorithms for learning by efficiently using the information contained in the Hessian, Le Roux et al. (2008) exploited the special structure of the learning problem to achieve faster convergence. In this paper, we investigate a natural way of combining these two directions to yield fast and robust learning algorithms."
            },
            "slug": "A-fast-natural-Newton-method-Roux-Fitzgibbon",
            "title": {
                "fragments": [],
                "text": "A fast natural Newton method"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper investigates a natural way of combining the two directions of learning and optimization to yield fast and robust learning algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1407546424"
                        ],
                        "name": "J. Sohl-Dickstein",
                        "slug": "J.-Sohl-Dickstein",
                        "structuredName": {
                            "firstName": "Jascha",
                            "lastName": "Sohl-Dickstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sohl-Dickstein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 0
                            }
                        ],
                        "text": "Sohl-Dickstein (2012) explores this idea, defining natural gradient as doing whitening in the parameter space."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18568112,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5c4cb0448368a0f8f2a95d5a695e95539066aa56",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The natural gradient allows for more efficient gradient descent by removing dependencies and biases inherent in a function's parameterization. Several papers present the topic thoroughly and precisely. It remains a very difficult idea to get your head around however. The intent of this note is to provide simple intuition for the natural gradient and its use. We review how an ill conditioned parameter space can undermine learning, introduce the natural gradient by analogy to the more widely understood concept of signal whitening, and present tricks and specific prescriptions for applying the natural gradient to learning problems."
            },
            "slug": "The-Natural-Gradient-by-Analogy-to-Signal-and-and-Sohl-Dickstein",
            "title": {
                "fragments": [],
                "text": "The Natural Gradient by Analogy to Signal Whitening, and Recipes and Tricks for its Use"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This note reviews how an ill conditioned parameter space can undermine learning, introduces the natural gradient by analogy to the more widely understood concept of signal whitening, and presents tricks and specific prescriptions for applying thenatural gradient to learning problems."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7245737"
                        ],
                        "name": "Nicolas Le Roux",
                        "slug": "Nicolas-Le-Roux",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Le Roux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Le Roux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798462"
                        ],
                        "name": "Pierre-Antoine Manzagol",
                        "slug": "Pierre-Antoine-Manzagol",
                        "structuredName": {
                            "firstName": "Pierre-Antoine",
                            "lastName": "Manzagol",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre-Antoine Manzagol"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 201
                            }
                        ],
                        "text": "\u2026optimization (Martens, 2010; Sutskever et al., 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), natural gradient descent (Amari, 1997; Park et al., 2000), TONGA (Le Roux et al., 2008; Roux and Fitzgibbon, 2010) are just a few of such recently proposed algorithms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 6
                            }
                        ],
                        "text": "In Le Roux et al. (2008) one assumes that the gradients computed over different minibatches are distributed according to a Gaussian centered around the true gradient with some covariance matrix C."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 75
                            }
                        ],
                        "text": "While the probabilistic derivation requires the centered covariance, in Le Roux et al. (2008) it is argued that one can use the uncentered covariance U resulting in a simplified formula which is sometimes confused with the metric derived by Amari:\nU \u2248 E(x,t)\u223cq [( \u2202 log p(t|x) \u2202\u03b8 )T ( \u2202 log p(t|x)\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9666804,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6ed460701019072ee2e364a1a491f73dd931f27f",
            "isKey": false,
            "numCitedBy": 179,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Guided by the goal of obtaining an optimization algorithm that is both fast and yields good generalization, we study the descent direction maximizing the decrease in generalization error or the probability of not increasing generalization error. The surprising result is that from both the Bayesian and frequentist perspectives this can yield the natural gradient direction. Although that direction can be very expensive to compute we develop an efficient, general, online approximation to the natural gradient descent which is suited to large scale problems. We report experimental results showing much faster convergence in computation time and in number of iterations with TONGA (Topmoumoute Online natural Gradient Algorithm) than with stochastic gradient descent, even on very large datasets."
            },
            "slug": "Topmoumoute-Online-Natural-Gradient-Algorithm-Roux-Manzagol",
            "title": {
                "fragments": [],
                "text": "Topmoumoute Online Natural Gradient Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An efficient, general, online approximation to the natural gradient descent which is suited to large scale problems and much faster convergence in computation time and in number of iterations with TONGA than with stochastic gradient descent, even on very large datasets."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2051036"
                        ],
                        "name": "Hyeyoung Park",
                        "slug": "Hyeyoung-Park",
                        "structuredName": {
                            "firstName": "Hyeyoung",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hyeyoung Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693668"
                        ],
                        "name": "K. Fukumizu",
                        "slug": "K.-Fukumizu",
                        "structuredName": {
                            "firstName": "Kenji",
                            "lastName": "Fukumizu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukumizu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 111
                            }
                        ],
                        "text": ", 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), natural gradient descent (Amari, 1997; Park et al., 2000), TONGA (Le Roux et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 171
                            }
                        ],
                        "text": "\u2026optimization (Martens, 2010; Sutskever et al., 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), natural gradient descent (Amari, 1997; Park et al., 2000), TONGA (Le Roux et al., 2008; Roux and Fitzgibbon, 2010) are just a few of such recently proposed algorithms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 49
                            }
                        ],
                        "text": "This formula can be massaged further (similar to Park et al. (2000)) for specific activations and error functions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 237
                            }
                        ],
                        "text": "Natural gradient can be traced back to Amari\u2019s work on information geometry (Amari, 1985) and its application to various neural networks (Amari et al., 1992; Amari, 1997), though a more in depth introduction can be found in Amari (1998); Park et al. (2000); Arnold et al. (2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6471036,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55a5dbc05fe8e362e0050b36dbbc4886011c4a32",
            "isKey": true,
            "numCitedBy": 166,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adaptive-natural-gradient-learning-algorithms-for-Park-Amari",
            "title": {
                "fragments": [],
                "text": "Adaptive natural gradient learning algorithms for various stochastic models"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145766200"
                        ],
                        "name": "A. Honkela",
                        "slug": "A.-Honkela",
                        "structuredName": {
                            "firstName": "Antti",
                            "lastName": "Honkela",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Honkela"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3188310"
                        ],
                        "name": "M. Tornio",
                        "slug": "M.-Tornio",
                        "structuredName": {
                            "firstName": "Matti",
                            "lastName": "Tornio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Tornio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2785022"
                        ],
                        "name": "T. Raiko",
                        "slug": "T.-Raiko",
                        "structuredName": {
                            "firstName": "Tapani",
                            "lastName": "Raiko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Raiko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703769"
                        ],
                        "name": "J. Karhunen",
                        "slug": "J.-Karhunen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karhunen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 3
                            }
                        ],
                        "text": "In Honkela et al. (2008, 2010) a similar idea is proposed in the context of variational inference and specific assumptions on the form of p(\u03b8) are made."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3023232,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9295580c5bc93ee992b691f554e467661739b7f8",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Variational methods for approximate inference in machine learning often adapt a parametric probability distribution to optimize a given objective function. This view is especially useful when applying variational Bayes (VB) to models outside the conjugate-exponential family. For them, variational Bayesian expectation maximization (VB EM) algorithms are not easily available, and gradient-based methods are often used as alternatives. Traditional natural gradient methods use the Riemannian structure (or geometry) of the predictive distribution to speed up maximum likelihood estimation. We propose using the geometry of the variational approximating distribution instead to speed up a conjugate gradient method for variational learning and inference. The computational overhead is small due to the simplicity of the approximating distribution. Experiments with real-world speech data show significant speedups over alternative learning algorithms."
            },
            "slug": "Natural-Conjugate-Gradient-in-Variational-Inference-Honkela-Tornio",
            "title": {
                "fragments": [],
                "text": "Natural Conjugate Gradient in Variational Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes using the geometry of the variational approximating distribution instead to speed up a conjugate gradient method for variational learning and inference, and shows significant speedups over alternative learning algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "ICONIP"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 223
                            }
                        ],
                        "text": "Natural gradient can be traced back to Amari\u2019s work on information geometry (Amari, 1985) and its application to various neural networks (Amari et al., 1992; Amari, 1997), though a more in depth introduction can be found in Amari (1998); Park et al. (2000); Arnold et al. (2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207585383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a767a341364de1f75bea85e0b12ba7d3586a461",
            "isKey": false,
            "numCitedBy": 2730,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "When a parameter space has a certain underlying structure, the ordinary gradient of a function does not represent its steepest direction, but the natural gradient does. Information geometry is used for calculating the natural gradients in the parameter space of perceptrons, the space of matrices (for blind source separation), and the space of linear dynamical systems (for blind source deconvolution). The dynamical behavior of natural gradient online learning is analyzed and is proved to be Fisher efficient, implying that it has asymptotically the same performance as the optimal batch estimation of parameters. This suggests that the plateau phenomenon, which appears in the backpropagation learning algorithm of multilayer perceptrons, might disappear or might not be so serious when the natural gradient is used. An adaptive method of updating the learning rate is proposed and analyzed."
            },
            "slug": "Natural-Gradient-Works-Efficiently-in-Learning-Amari",
            "title": {
                "fragments": [],
                "text": "Natural Gradient Works Efficiently in Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The dynamical behavior of natural gradient online learning is analyzed and is proved to be Fisher efficient, implying that it has asymptotically the same performance as the optimal batch estimation of parameters."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 111
                            }
                        ],
                        "text": ", 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), natural gradient descent (Amari, 1997; Park et al., 2000), TONGA (Le Roux et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 158
                            }
                        ],
                        "text": "\u2026optimization (Martens, 2010; Sutskever et al., 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), natural gradient descent (Amari, 1997; Park et al., 2000), TONGA (Le Roux et al., 2008; Roux and Fitzgibbon, 2010) are just a few of such recently proposed algorithms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 137
                            }
                        ],
                        "text": "Natural gradient can be traced back to Amari\u2019s work on information geometry (Amari, 1985) and its application to various neural networks (Amari et al., 1992; Amari, 1997), though a more in depth introduction can be found in Amari (1998); Park et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 157
                            }
                        ],
                        "text": "Natural gradient can be traced back to Amari\u2019s work on information geometry (Amari, 1985) and its application to various neural networks (Amari et al., 1992; Amari, 1997), though a more in depth introduction can be found in Amari (1998); Park et al. (2000); Arnold et al. (2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7589577,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa0c75a9b5f39d166dd875005580687716a236bb",
            "isKey": true,
            "numCitedBy": 182,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The parameter space of neural networks has a Riemannian metric structure. The natural Riemannian gradient should be used instead of the conventional gradient, since the former denotes the true steepest descent direction of a loss function in the Riemannian space. The behavior of the stochastic gradient learning algorithm is much more effective if the natural gradient is used. The present paper studies the information-geometrical structure of perceptrons and other networks, and prove that the on-line learning method based on the natural gradient is asymptotically as efficient as the optimal batch algorithm. Adaptive modification of the learning constant is proposed and analyzed in terms of the Riemannian measure and is shown to be efficient. The natural gradient is finally applied to blind separation of mixtured independent signal sources."
            },
            "slug": "Neural-Learning-in-Structured-Parameter-Spaces-Amari",
            "title": {
                "fragments": [],
                "text": "Neural Learning in Structured Parameter Spaces - Natural Riemannian Gradient"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The present paper studies the information-geometrical structure of perceptrons and other networks, and proves that the on-line learning method based on the natural gradient is asymptotically as efficient as the optimal batch algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145704247"
                        ],
                        "name": "James Martens",
                        "slug": "James-Martens",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Martens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Martens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 148
                            }
                        ],
                        "text": "\u2026ratio given by equation (31) which can be shown to behave identically with the one in Martens (2010).\n\u03c1 = f ( \u03b8t \u2212 \u03b7G\u22121 \u2202f(\u03b8t)\u2202\u03b8t T) \u2212 f(\u03b8t)\n\u2212\u03b7 \u2202f(\u03b8t)\u2202\u03b8t G \u22121 \u2202f(\u03b8t) \u2202\u03b8t\nT (16)\nStructural damping (Sutskever et al., 2011), a specific regularization term used to improve training of recurrent\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 41
                            }
                        ],
                        "text": "HessianFree optimization (Martens, 2010; Sutskever et al., 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), natural gradient descent (Amari, 1997; Park et al., 2000), TONGA (Le Roux et al., 2008; Roux and Fitzgibbon, 2010) are just a few of such recently proposed\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 19
                            }
                        ],
                        "text": "Structural damping (Sutskever et al., 2011), a specific regularization term used to improve training of recurrent neural network, can also be explained from the natural gradient perspective."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 25
                            }
                        ],
                        "text": "HessianFree optimization (Martens, 2010; Sutskever et al., 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), natural gradient descent (Amari, 1997; Park et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8843166,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de",
            "isKey": true,
            "numCitedBy": 1254,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent Neural Networks (RNNs) are very powerful sequence models that do not enjoy widespread use because it is extremely difficult to train them properly. Fortunately, recent advances in Hessian-free optimization have been able to overcome the difficulties associated with training RNNs, making it possible to apply them successfully to challenging sequence problems. In this paper we demonstrate the power of RNNs trained with the new Hessian-Free optimizer (HF) by applying them to character-level language modeling tasks. The standard RNN architecture, while effective, is not ideally suited for such tasks, so we introduce a new RNN variant that uses multiplicative (or \"gated\") connections which allow the current input character to determine the transition matrix from one hidden state vector to the next. After training the multiplicative RNN with the HF optimizer for five days on 8 high-end Graphics Processing Units, we were able to surpass the performance of the best previous single method for character-level language modeling \u2013 a hierarchical non-parametric sequence model. To our knowledge this represents the largest recurrent neural network application to date."
            },
            "slug": "Generating-Text-with-Recurrent-Neural-Networks-Sutskever-Martens",
            "title": {
                "fragments": [],
                "text": "Generating Text with Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The power of RNNs trained with the new Hessian-Free optimizer by applying them to character-level language modeling tasks is demonstrated, and a new RNN variant that uses multiplicative connections which allow the current input character to determine the transition matrix from one hidden state vector to the next is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790356"
                        ],
                        "name": "T. Heskes",
                        "slug": "T.-Heskes",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Heskes",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Heskes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 6
                            }
                        ],
                        "text": "While Heskes (2000) precedes both Schraudolph (2002); Martens (2010) both Hessian Free and Krylov Subspace Descent are introduced as purely approximations to second order methods."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 33
                            }
                        ],
                        "text": "We make the additional note that Heskes (2000) makes similar algebraic manipulations as the ones provided in this section,"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 33
                            }
                        ],
                        "text": "We make the additional note that Heskes (2000) makes similar algebraic manipulations as the ones provided in this section, however for different reasons."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8035080,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "650319158e20a678bb6bc9352270c1f41a108479",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Several studies have shown that natural gradient descent for on-line learning is much more efficient than standard gradient descent. In this article, we derive natural gradients in a slightly different manner and discuss implications for batch-mode learning and pruning, linking them to existing algorithms such as Levenberg-Marquardt optimization and optimal brain surgeon. The Fisher matrix plays an important role in all these algorithms. The second half of the article discusses a layered approximation of the Fisher matrix specific to multilayered perceptrons. Using this approximation rather than the exact Fisher matrix, we arrive at much faster natural learning algorithms and more robust pruning procedures."
            },
            "slug": "On-Natural-Learning-and-Pruning-in-Multilayered-Heskes",
            "title": {
                "fragments": [],
                "text": "On Natural Learning and Pruning in Multilayered Perceptrons"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This article derives natural gradients in a slightly different manner and discusses implications for batch-mode learning and pruning, linking them to existing algorithms such as Levenberg-Marquardt optimization and optimal brain surgeon."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3227028"
                        ],
                        "name": "Fr\u00e9d\u00e9ric Bastien",
                        "slug": "Fr\u00e9d\u00e9ric-Bastien",
                        "structuredName": {
                            "firstName": "Fr\u00e9d\u00e9ric",
                            "lastName": "Bastien",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fr\u00e9d\u00e9ric Bastien"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47944877"
                        ],
                        "name": "Arnaud Bergeron",
                        "slug": "Arnaud-Bergeron",
                        "structuredName": {
                            "firstName": "Arnaud",
                            "lastName": "Bergeron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Arnaud Bergeron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1395619597"
                        ],
                        "name": "Nicolas Boulanger-Lewandowski",
                        "slug": "Nicolas-Boulanger-Lewandowski",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Boulanger-Lewandowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Boulanger-Lewandowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733858"
                        ],
                        "name": "T. Breuel",
                        "slug": "T.-Breuel",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Breuel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Breuel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3327616"
                        ],
                        "name": "Y. Chherawala",
                        "slug": "Y.-Chherawala",
                        "structuredName": {
                            "firstName": "Youssouf",
                            "lastName": "Chherawala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Chherawala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5723508"
                        ],
                        "name": "Moustapha Ciss\u00e9",
                        "slug": "Moustapha-Ciss\u00e9",
                        "structuredName": {
                            "firstName": "Moustapha",
                            "lastName": "Ciss\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Moustapha Ciss\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39977229"
                        ],
                        "name": "Myriam C\u00f4t\u00e9",
                        "slug": "Myriam-C\u00f4t\u00e9",
                        "structuredName": {
                            "firstName": "Myriam",
                            "lastName": "C\u00f4t\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Myriam C\u00f4t\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761978"
                        ],
                        "name": "D. Erhan",
                        "slug": "D.-Erhan",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Erhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Erhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35215951"
                        ],
                        "name": "Jeremy Eustache",
                        "slug": "Jeremy-Eustache",
                        "structuredName": {
                            "firstName": "Jeremy",
                            "lastName": "Eustache",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeremy Eustache"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3119801"
                        ],
                        "name": "Xavier Glorot",
                        "slug": "Xavier-Glorot",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Glorot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xavier Glorot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2090922238"
                        ],
                        "name": "X. Muller",
                        "slug": "X.-Muller",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Muller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Muller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2619783"
                        ],
                        "name": "Sylvain Pannetier Lebeuf",
                        "slug": "Sylvain-Pannetier-Lebeuf",
                        "structuredName": {
                            "firstName": "Sylvain",
                            "lastName": "Lebeuf",
                            "middleNames": [
                                "Pannetier"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sylvain Pannetier Lebeuf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1996134"
                        ],
                        "name": "Razvan Pascanu",
                        "slug": "Razvan-Pascanu",
                        "structuredName": {
                            "firstName": "Razvan",
                            "lastName": "Pascanu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Razvan Pascanu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2425018"
                        ],
                        "name": "S. Rifai",
                        "slug": "S.-Rifai",
                        "structuredName": {
                            "firstName": "Salah",
                            "lastName": "Rifai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Rifai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47918629"
                        ],
                        "name": "F. Savard",
                        "slug": "F.-Savard",
                        "structuredName": {
                            "firstName": "Fran\u00e7ois",
                            "lastName": "Savard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Savard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057963758"
                        ],
                        "name": "Guillaume Sicard",
                        "slug": "Guillaume-Sicard",
                        "structuredName": {
                            "firstName": "Guillaume",
                            "lastName": "Sicard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guillaume Sicard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 46
                            }
                        ],
                        "text": "(2010), using the NISTP dataset introduced in Bengio et al. (2011) (which is just the NIST dataset plus deformations) and use 32."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 89
                            }
                        ],
                        "text": "We repeat the experiment from Erhan et al. (2010), using the NISTP dataset introduced in Bengio et al. (2011) (which is just the NIST dataset plus deformations) and use 32.7M samples of this data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2462590,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "38fb40f6b6c2498069d2bd0352b8dc3377fde8f0",
            "isKey": false,
            "numCitedBy": 117,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent theoretical and empirical work in statistical machine learning has demonstrated the potential of learning algorithms for deep architectures, i.e., function classes obtained by composing multiple levels of representation. The hypothesis evaluated here is that intermediate levels of representation, because they can be shared across tasks and examples from different but related distributions, can yield even more benefits. Comparative experiments were performed on a large-scale handwritten character recognition setting with 62 classes (upper case, lower case, digits), using both a multi-task setting and perturbed examples in order to obtain out-ofdistribution examples. The results agree with the hypothesis, and show that a deep learner did beat previously published results and reached human-level performance."
            },
            "slug": "Deep-Learners-Benefit-More-from-Out-of-Distribution-Bengio-Bastien",
            "title": {
                "fragments": [],
                "text": "Deep Learners Benefit More from Out-of-Distribution Examples"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Results show that a deep learner did beat previously published results and reached human-level performance, and the hypothesis is that intermediate levels of representation, because they can be shared across tasks and examples from different but related distributions, can yield even more benefits."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145766200"
                        ],
                        "name": "A. Honkela",
                        "slug": "A.-Honkela",
                        "structuredName": {
                            "firstName": "Antti",
                            "lastName": "Honkela",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Honkela"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2785022"
                        ],
                        "name": "T. Raiko",
                        "slug": "T.-Raiko",
                        "structuredName": {
                            "firstName": "Tapani",
                            "lastName": "Raiko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Raiko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2267304"
                        ],
                        "name": "Mikael Kuusela",
                        "slug": "Mikael-Kuusela",
                        "structuredName": {
                            "firstName": "Mikael",
                            "lastName": "Kuusela",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikael Kuusela"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3188310"
                        ],
                        "name": "M. Tornio",
                        "slug": "M.-Tornio",
                        "structuredName": {
                            "firstName": "Matti",
                            "lastName": "Tornio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Tornio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703769"
                        ],
                        "name": "J. Karhunen",
                        "slug": "J.-Karhunen",
                        "structuredName": {
                            "firstName": "Juha",
                            "lastName": "Karhunen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Karhunen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 3
                            }
                        ],
                        "text": "In Honkela et al. (2008, 2010) a similar idea is proposed in the context of variational inference and specific assumptions on the form of p(\u03b8) are made."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 32
                            }
                        ],
                        "text": "Gonzalez and Dorronsoro (2006); Honkela et al. (2010) address these issues by making the assumption that Gt\u22121 and Gt are identical (so dt\u22121 does not need to be transported)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2505371,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be45343d13a9ea7f1d8adc8d0f9ba97564ac875c",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Variational Bayesian (VB) methods are typically only applied to models in the conjugate-exponential family using the variational Bayesian expectation maximisation (VB EM) algorithm or one of its variants. In this paper we present an efficient algorithm for applying VB to more general models. The method is based on specifying the functional form of the approximation, such as multivariate Gaussian. The parameters of the approximation are optimised using a conjugate gradient algorithm that utilises the Riemannian geometry of the space of the approximations. This leads to a very efficient algorithm for suitably structured approximations. It is shown empirically that the proposed method is comparable or superior in efficiency to the VB EM in a case where both are applicable. We also apply the algorithm to learning a nonlinear state-space model and a nonlinear factor analysis model for which the VB EM is not applicable. For these models, the proposed algorithm outperforms alternative gradient-based methods by a significant margin."
            },
            "slug": "Approximate-Riemannian-Conjugate-Gradient-Learning-Honkela-Raiko",
            "title": {
                "fragments": [],
                "text": "Approximate Riemannian Conjugate Gradient Learning for Fixed-Form Variational Bayes"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An efficient algorithm for applying VB to more general models based on specifying the functional form of the approximation, such as multivariate Gaussian, which outperforms alternative gradient-based methods by a significant margin."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144695232"
                        ],
                        "name": "S. Kakade",
                        "slug": "S.-Kakade",
                        "structuredName": {
                            "firstName": "Sham",
                            "lastName": "Kakade",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kakade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 90
                            }
                        ],
                        "text": "The algorithm has also been successfully applied in the reinforcement learning community (Kakade, 2001; Peters and Schaal, 2008) and for stochastic search (Sun et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 89
                            }
                        ],
                        "text": "We carry out a benchmark on the Curves dataset, using the 6 layer deep auto-encoder from Martens (2010). The dataset is small, has only 20K training examples of 784 dimensions each."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 75
                            }
                        ],
                        "text": "While the probabilistic derivation requires the centered covariance, in Le Roux et al. (2008) it is argued that one can use the uncentered covariance U resulting in a simplified formula which is sometimes confused with the metric derived by Amari:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 89
                            }
                        ],
                        "text": "The algorithm has also been successfully applied in the reinforcement learning community (Kakade, 2001; Peters and Schaal, 2008) and for stochastic search (Sun et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14540458,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b18833db0de9393d614d511e60821a1504fc6cd1",
            "isKey": true,
            "numCitedBy": 883,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris."
            },
            "slug": "A-Natural-Policy-Gradient-Kakade",
            "title": {
                "fragments": [],
                "text": "A Natural Policy Gradient"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This work provides a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space and shows drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700974"
                        ],
                        "name": "Barak A. Pearlmutter",
                        "slug": "Barak-A.-Pearlmutter",
                        "structuredName": {
                            "firstName": "Barak",
                            "lastName": "Pearlmutter",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barak A. Pearlmutter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 115
                            }
                        ],
                        "text": "We have implemented natural gradient descent using a truncated Newton approach similar to the pipeline proposed by Pearlmutter (1994) and used by Martens (2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 85
                            }
                        ],
                        "text": "The core idea behind it is to make use of the forward pass (renamed to R-operator in Pearlmutter (1994)) and backward pass of automatic differentiation to compute efficient products between Jacobian or Hessian matrices and vectors."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 93
                            }
                        ],
                        "text": "One particularly interesting pipeline to scale up such algorithms was originally proposed in Pearlmutter (1994) \u2013 finetuned in Schraudolph (2002) \u2013 and represents the backbone behind both HessianFree optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1251969,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6867b6b564462d6b902f68e0bfa58f4717ca1cc",
            "isKey": false,
            "numCitedBy": 586,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Just storing the Hessian H (the matrix of second derivatives 2E/wiwj of the error E with respect to each pair of weights) of a large neural network is difficult. Since a common use of a large matrix like H is to compute its product with various vectors, we derive a technique that directly calculates Hv, where v is an arbitrary vector. To calculate Hv, we first define a differential operator Rv{f(w)} = (/r)f(w rv)|r=0, note that Rv{w} = Hv and Rv{w} = v, and then apply Rv{} to the equations used to compute w. The result is an exact and numerically stable procedure for computing Hv, which takes about as much computation, and is about as local, as a gradient evaluation. We then apply the technique to a one pass gradient calculation algorithm (backpropagation), a relaxation gradient calculation algorithm (recurrent backpropagation), and two stochastic gradient calculation algorithms (Boltzmann machines and weight perturbation). Finally, we show that this technique can be used at the heart of many iterative techniques for computing various properties of H, obviating any need to calculate the full Hessian."
            },
            "slug": "Fast-Exact-Multiplication-by-the-Hessian-Pearlmutter",
            "title": {
                "fragments": [],
                "text": "Fast Exact Multiplication by the Hessian"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work derives a technique that directly calculates Hv, where v is an arbitrary vector, and shows that this technique can be used at the heart of many iterative techniques for computing various properties of H, obviating any need to calculate the full Hessian."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145749654"
                        ],
                        "name": "M. Rattray",
                        "slug": "M.-Rattray",
                        "structuredName": {
                            "firstName": "Magnus",
                            "lastName": "Rattray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rattray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2506116"
                        ],
                        "name": "D. Saad",
                        "slug": "D.-Saad",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Saad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Saad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ld not get stalled near such plateaus. In Park et al. (2000) such plateaus are found near singularities of the functional manifold, providing a nice framework to study them (as is done for example in Rattray et al. (1998) where they hypothesize that such singularities behave like repellors for the dynamics of natural gradient descent). An argument can also be made in favour of U at plateaus. If a plateau at exists fo"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121666312,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41e6adf581dd8dfdf259a7e25c0c7158d50a4293",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural gradient descent is an on-line variable-metric optimization algorithm which utilizes an underlying Riemannian parameter space. We analyze the dynamics of natural gradient descent beyond the asymptotic regime by employing an exact statistical mechanics description of learning in two-layer feed-forward neural networks. For a realizable learning scenario we find significant improvements over standard gradient descent for both the transient and asymptotic stages of learning, with a slower power law increase in learning time as task complexity grows."
            },
            "slug": "Natural-gradient-descent-for-on-line-learning-Rattray-Saad",
            "title": {
                "fragments": [],
                "text": "Natural gradient descent for on-line learning"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This work analyzes the dynamics of natural gradient descent beyond the asymptotic regime by employing an exact statistical mechanics description of learning in two-layer feed-forward neural networks and finds significant improvements over standard gradient descent for both the transient and asymPTotic stages of learning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739396"
                        ],
                        "name": "N. Schraudolph",
                        "slug": "N.-Schraudolph",
                        "structuredName": {
                            "firstName": "Nicol",
                            "lastName": "Schraudolph",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Schraudolph"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 34
                            }
                        ],
                        "text": "While Heskes (2000) precedes both Schraudolph (2002); Martens (2010) both Hessian Free and Krylov Subspace Descent are introduced as purely approximations to second order methods."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 151
                            }
                        ],
                        "text": "Hessian-Free as well as Krylov Subspace Descent rely on the extended Gauss-Newton approximation of the Hessian, GN, instead of the actual Hessian (see Schraudolph (2002)):\nGN = 1\nn \u2211 i\n[( \u2202r\n\u2202\u03b8\n)T \u22022 log p(t(i)|x(i))\n\u2202r2\n( \u2202r\n\u2202\u03b8 )] = Ex\u223cq\u0303 [ JTr ( Et\u223cq\u0303(t|x) [HL\u25e6r] ) Jr ]\n(9)\nThe last step of eq."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 125
                            }
                        ],
                        "text": "One particularly interesting pipeline to scale up such algorithms was originally proposed in Pearlmutter (1994) \u2013 finetuned in Schraudolph (2002) \u2013 and represents the backbone behind both HessianFree optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 151
                            }
                        ],
                        "text": "Hessian-Free as well as Krylov Subspace Descent rely on the extended Gauss-Newton approximation of the Hessian, GN, instead of the actual Hessian (see Schraudolph (2002)):"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11017566,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ffa94bba647817fa5e8f8d3250fc977435b5ca76",
            "isKey": true,
            "numCitedBy": 275,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a generic method for iteratively approximating various second-order gradient steps-Newton, Gauss-Newton, Levenberg-Marquardt, and natural gradient-in linear time per iteration, using special curvature matrix-vector products that can be computed in O(n). Two recent acceleration techniques for on-line learning, matrix momentum and stochastic meta-descent (SMD), implement this approach. Since both were originally derived by very different routes, this offers fresh insight into their operation, resulting in further improvements to SMD."
            },
            "slug": "Fast-Curvature-Matrix-Vector-Products-for-Gradient-Schraudolph",
            "title": {
                "fragments": [],
                "text": "Fast Curvature Matrix-Vector Products for Second-Order Gradient Descent"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "A generic method for iteratively approximating various second-order gradient steps-Newton, Gauss- newton, Levenberg-Marquardt, and natural gradient-in linear time per iteration, using special curvature matrix-vector products that can be computed in O(n)."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145197867"
                        ],
                        "name": "Jan Peters",
                        "slug": "Jan-Peters",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Peters",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan Peters"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144575699"
                        ],
                        "name": "S. Vijayakumar",
                        "slug": "S.-Vijayakumar",
                        "structuredName": {
                            "firstName": "Sethu",
                            "lastName": "Vijayakumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vijayakumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745219"
                        ],
                        "name": "S. Schaal",
                        "slug": "S.-Schaal",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Schaal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Schaal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 104
                            }
                        ],
                        "text": "The algorithm has also been successfully applied in the reinforcement learning community (Kakade, 2001; Peters and Schaal, 2008) and for stochastic search (Sun et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 89
                            }
                        ],
                        "text": "The algorithm has also been successfully applied in the reinforcement learning community (Kakade, 2001; Peters and Schaal, 2008) and for stochastic search (Sun et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2914735,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f1a391bab223fc2609717316bec30ae36f8ea448",
            "isKey": false,
            "numCitedBy": 833,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Natural-Actor-Critic-Peters-Vijayakumar",
            "title": {
                "fragments": [],
                "text": "Natural Actor-Critic"
            },
            "venue": {
                "fragments": [],
                "text": "ECML"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2425018"
                        ],
                        "name": "S. Rifai",
                        "slug": "S.-Rifai",
                        "structuredName": {
                            "firstName": "Salah",
                            "lastName": "Rifai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Rifai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760871"
                        ],
                        "name": "Aaron C. Courville",
                        "slug": "Aaron-C.-Courville",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Courville",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron C. Courville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145467703"
                        ],
                        "name": "Pascal Vincent",
                        "slug": "Pascal-Vincent",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Vincent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153583218"
                        ],
                        "name": "Mehdi Mirza",
                        "slug": "Mehdi-Mirza",
                        "structuredName": {
                            "firstName": "Mehdi",
                            "lastName": "Mirza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mehdi Mirza"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ore generalization-friendly metric G. Figure 2 describes the results on the Toronto Face Dataset (TFD), where using unlabeled data results in 83.04% accuracy vs 81.13% without. State of the art is 85%Rifai et al. (2012), though this result is obtained by a larger model that is pre-trained. Hyper-parameters were validated using a grid-search (more details in the Appendix). As you can see from the plot, it suggests th"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16389299,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9c431f58565f874f76a024add2aa80717ec5cf5",
            "isKey": false,
            "numCitedBy": 180,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a semi-supervised approach to solve the task of emotion recognition in 2D face images using recent ideas in deep learning for handling the factors of variation present in data. An emotion classification algorithm should be both robust to (1) remaining variations due to the pose of the face in the image after centering and alignment, (2) the identity or morphology of the face. In order to achieve this invariance, we propose to learn a hierarchy of features in which we gradually filter the factors of variation arising from both (1) and (2). We address (1) by using a multi-scale contractive convolutional network (CCNET) in order to obtain invariance to translations of the facial traits in the image. Using the feature representation produced by the CCNET, we train a Contractive Discriminative Analysis (CDA) feature extractor, a novel variant of the Contractive Auto-Encoder (CAE), designed to learn a representation separating out the emotion-related factors from the others (which mostly capture the subject identity, and what is left of pose after the CCNET). This system beats the state-of-the-art on a recently proposed dataset for facial expression recognition, the Toronto Face Database, moving the state-of-art accuracy from 82.4% to 85.0%, while the CCNET and CDA improve accuracy of a standard CAE by 8%."
            },
            "slug": "Disentangling-Factors-of-Variation-for-Facial-Rifai-Bengio",
            "title": {
                "fragments": [],
                "text": "Disentangling Factors of Variation for Facial Expression Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "A semi-supervised approach to solve the task of emotion recognition in 2D face images using recent ideas in deep learning for handling the factors of variation present in data, beating the state-of-the-art on a recently proposed dataset for facial expression recognition."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 65
                            }
                        ],
                        "text": "HessianFree optimization (Martens, 2010; Sutskever et al., 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), natural gradient descent (Amari, 1997; Park et al., 2000), TONGA (Le Roux et al., 2008; Roux and Fitzgibbon, 2010) are just a few of such recently proposed\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 25
                            }
                        ],
                        "text": "HessianFree optimization (Martens, 2010; Sutskever et al., 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), natural gradient descent (Amari, 1997; Park et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16595612,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0ced8ba22674b3b948c22deb0f43df93c82f87f",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the use of Hessian Free optimization for learning deep autoencoders. One of the critical components in that algorithm is the choice of the preconditioner. We argue in this paper that the Jacobi preconditioner leads to faster optimization and we show how it can be accurately and efficiently estimated using a randomized algorithm."
            },
            "slug": "Improved-Preconditioner-for-Hessian-Free-Chapelle",
            "title": {
                "fragments": [],
                "text": "Improved Preconditioner for Hessian Free Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is argued in this paper that the Jacobi preconditioner leads to faster optimization and it is shown how it can be accurately and efficiently estimated using a randomized algorithm."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739396"
                        ],
                        "name": "N. Schraudolph",
                        "slug": "N.-Schraudolph",
                        "structuredName": {
                            "firstName": "Nicol",
                            "lastName": "Schraudolph",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Schraudolph"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "fferentiate themselves by the approximations they use in order to scale up. One particularly interesting pipeline to scale up such algorithm was originally proposed in Pearlmutter (1994), \ufb01netuned in Schraudolph (2001) and represents the backbone behind both Hessian-Free optimization (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012). The core idea behind it is to make use of the forward (renamed"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 52818164,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ec10a1d477470617e815cc069ec4df9a943b9947",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The Gauss-Newton approximation of the Hessian guarantees positive semi-definiteness while retaining more second-order information than the Fisher information.We extend it from nonlinear least squares to all differentiable objectives such that positive semi-definiteness is maintained for the standard loss functions in neural network regression and classification. We give efficient algorithms for computing the product of extended Gauss-Newton and Fisher information matrices with arbitrary vectors, using techniques similar to but even cheaper than the fast Hessian-vector product [1]. The stability of SMD [2,3,4,5], a learning rate adaptation method that uses curvature matrix-vector products, improves when the extended Gauss-Newton matrix is substituted for the Hessian."
            },
            "slug": "Fast-Curvature-Matrix-Vector-Products-Schraudolph",
            "title": {
                "fragments": [],
                "text": "Fast Curvature Matrix-Vector Products"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "The Gauss-Newton approximation of the Hessian is extended from nonlinear least squares to all differentiable objectives such that positive semi-definiteness is maintained for the standard loss functions in neural network regression and classification."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693668"
                        ],
                        "name": "K. Fukumizu",
                        "slug": "K.-Fukumizu",
                        "structuredName": {
                            "firstName": "Kenji",
                            "lastName": "Fukumizu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukumizu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11198551,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "27b89885140da973ee04469c032ebeb02f921053",
            "isKey": false,
            "numCitedBy": 183,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Local-minima-and-plateaus-in-hierarchical-of-Fukumizu-Amari",
            "title": {
                "fragments": [],
                "text": "Local minima and plateaus in hierarchical structures of multilayer perceptrons"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116961006"
                        ],
                        "name": "Yi Sun",
                        "slug": "Yi-Sun",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688276"
                        ],
                        "name": "Daan Wierstra",
                        "slug": "Daan-Wierstra",
                        "structuredName": {
                            "firstName": "Daan",
                            "lastName": "Wierstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daan Wierstra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725157"
                        ],
                        "name": "T. Schaul",
                        "slug": "T.-Schaul",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Schaul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Schaul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 156
                            }
                        ],
                        "text": "The algorithm has also been successfully applied in the reinforcement learning community (Kakade, 2001; Peters and Schaal, 2008) and for stochastic search (Sun et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4642182,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c980686b001f55b1e514efaccb6cfbe1a8726db8",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "To optimize unknown 'fitness' functions, we present Natural Evolution Strategies, a novel algorithm that constitutes a principled alternative to standard stochastic search methods. It maintains a multinormal distribution on the set of solution candidates. The Natural Gradient is used to update the distribution's parameters in the direction of higher expected fitness, by efficiently calculating the inverse of the exact Fisher information matrix whereas previous methods had to use approximations. Other novel aspects of our method include optimal fitness baselines and importance mixing, a procedure adjusting batches with minimal numbers of fitness evaluations. The algorithm yields competitive results on a number of benchmarks."
            },
            "slug": "Stochastic-search-using-the-natural-gradient-Sun-Wierstra",
            "title": {
                "fragments": [],
                "text": "Stochastic search using the natural gradient"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The Natural Gradient is used to update the distribution's parameters in the direction of higher expected fitness, by efficiently calculating the inverse of the exact Fisher information matrix whereas previous methods had to use approximations."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2096070"
                        ],
                        "name": "J. Shewchuk",
                        "slug": "J.-Shewchuk",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Shewchuk",
                            "middleNames": [
                                "Richard"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shewchuk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 124
                            }
                        ],
                        "text": "Natural conjugate gradient, the manifold version of nonlinear conjugate gradient, is defined following the same intuitions (Shewchuck, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6491967,
            "fieldsOfStudy": [
                "Chemistry"
            ],
            "id": "70000f7791ab8519429ce939bc897738a05939c3",
            "isKey": false,
            "numCitedBy": 2496,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The Conjugate Gradient Method is the most prominent iterative method for solving sparse systems of linear equations. Unfortunately, many textbook treatments of the topic are written so that even their own authors would be mystified, if they bothered to read their own writing. For this reason, an understanding of the method has been reserved for the elite brilliant few who have painstakingly decoded the mumblings of their forebears. Nevertheless, the Conjugate Gradient Method is a composite of simple, elegant ideas that almost anyone can understand. Of course, a reader as intelligent as yourself will learn them almost effortlessly. The idea of quadratic forms is introduced and used to derive the methods of Steepest Descent, Conjugate Directions, and Conjugate Gradients. Eigenvectors are explained and used to examine the convergence of the Jacobi Method, Steepest Descent, and Conjugate Gradients. Other topics include preconditioning and the nonlinear Conjugate Gradient Method. I have taken pains to make this article easy to read. Sixty-two illustrations are provided. Dense prose is avoided. Concepts are explained in several different ways. Most equations are coupled with an intuitive interpretation."
            },
            "slug": "An-Introduction-to-the-Conjugate-Gradient-Method-Shewchuk",
            "title": {
                "fragments": [],
                "text": "An Introduction to the Conjugate Gradient Method Without the Agonizing Pain"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2685825"
                        ],
                        "name": "K. Kurata",
                        "slug": "K.-Kurata",
                        "structuredName": {
                            "firstName": "Koji",
                            "lastName": "Kurata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kurata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145930529"
                        ],
                        "name": "H. Nagaoka",
                        "slug": "H.-Nagaoka",
                        "structuredName": {
                            "firstName": "Hiroshi",
                            "lastName": "Nagaoka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Nagaoka"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 137
                            }
                        ],
                        "text": "Natural gradient can be traced back to Amari\u2019s work on information geometry (Amari, 1985) and its application to various neural networks (Amari et al., 1992; Amari, 1997), though a more in depth introduction can be found in Amari (1998); Park et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 137
                            }
                        ],
                        "text": "Natural gradient can be traced back to Amari\u2019s work on information geometry (Amari, 1985) and its application to various neural networks (Amari et al., 1992; Amari, 1997), though a more in depth introduction can be found in Amari (1998); Park et al. (2000); Arnold et al. (2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10534535,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4e13bcc9abc8520e29db1a2064213f297078b370",
            "isKey": false,
            "numCitedBy": 212,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "A Boltzmann machine is a network of stochastic neurons. The set of all the Boltzmann machines with a fixed topology forms a geometric manifold of high dimension, where modifiable synaptic weights of connections play the role of a coordinate system to specify networks. A learning trajectory, for example, is a curve in this manifold. It is important to study the geometry of the neural manifold, rather than the behavior of a single network, in order to know the capabilities and limitations of neural networks of a fixed topology. Using the new theory of information geometry, a natural invariant Riemannian metric and a dual pair of affine connections on the Boltzmann neural network manifold are established. The meaning of geometrical structures is elucidated from the stochastic and the statistical point of view. This leads to a natural modification of the Boltzmann machine learning rule."
            },
            "slug": "Information-geometry-of-Boltzmann-machines-Amari-Kurata",
            "title": {
                "fragments": [],
                "text": "Information geometry of Boltzmann machines"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Using the new theory of information geometry, a natural invariant Riemannian metric and a dual pair of affine connections on the Boltzmann neural network manifold are established and the meaning of geometrical structures is elucidated from the stochastic and the statistical point of view."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2051036"
                        ],
                        "name": "Hyeyoung Park",
                        "slug": "Hyeyoung-Park",
                        "structuredName": {
                            "firstName": "Hyeyoung",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hyeyoung Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47407861"
                        ],
                        "name": "T. Ozeki",
                        "slug": "T.-Ozeki",
                        "structuredName": {
                            "firstName": "Tomoko",
                            "lastName": "Ozeki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ozeki"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "rmulation of the training objective. In Park et al. (2000) such plateaux are seen as singularities of the functional manifold, providing a nice framework to study them (as is done for example in ichi Amari et al. (2001), Fukumizu and ichi Amari (2000)). An argument that can be made in favor of Uat plateaux is the following. If a plateau at exists for most possible inputs x, than the covariance matrix will have a sm"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1136306,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2108977deddb4ee708e438475c801e1504e7c76f",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Singularities are ubiquitous in the parameter space of hierarchical models such as multilayer perceptrons. At singularities, the Fisher information matrix degenerates, and the Cramer-Rao paradigm does no more hold, implying that the classical model selection theory such as AIC and MDL cannot be applied. It is important to study the relation between the generalization error and the training error at singularities. The present paper demonstrates a method of analyzing these errors both for the maximum likelihood estimator and the Bayesian predictive distribution in terms of Gaussian random fields, by using simple models."
            },
            "slug": "Geometrical-Singularities-in-the-Neuromanifold-of-Amari-Park",
            "title": {
                "fragments": [],
                "text": "Geometrical Singularities in the Neuromanifold of Multilayer Perceptrons"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A method of analyzing these errors both for the maximum likelihood estimator and the Bayesian predictive distribution in terms of Gaussian random fields, by using simple models."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725157"
                        ],
                        "name": "T. Schaul",
                        "slug": "T.-Schaul",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Schaul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Schaul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 4
                            }
                        ],
                        "text": "See Schaul (2012), where the convergence properties of natural gradient (in a specific case) are studied."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 58
                            }
                        ],
                        "text": "The approximations we use are meaningful only around \u03b8, in Schaul (2012) it is shown that taking large steps might harm convergence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4651555,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1444be72a3808be8b89222093d0f43d4233eb3f4",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "This theoretical investigation gives the first proof of convergence for (radial) natural evolution strategies, on d-dimensional sphere functions, and establishes the conditions on hyper-parameters, as a function of d. For the limit case of large population sizes we show asymptotic linear convergence, and in the limit of small learning rates we give a full analytic characterization of the algorithm dynamics, decomposed into transient and asymptotic phases. Finally, we show why omitting the natural gradient is catastrophic."
            },
            "slug": "Natural-evolution-strategies-converge-on-sphere-Schaul",
            "title": {
                "fragments": [],
                "text": "Natural evolution strategies converge on sphere functions"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This theoretical investigation gives the first proof of convergence for (radial) natural evolution strategies, on d-dimensional sphere functions, and establishes the conditions on hyper-parameters, as a function of d."
            },
            "venue": {
                "fragments": [],
                "text": "GECCO '12"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3227028"
                        ],
                        "name": "Fr\u00e9d\u00e9ric Bastien",
                        "slug": "Fr\u00e9d\u00e9ric-Bastien",
                        "structuredName": {
                            "firstName": "Fr\u00e9d\u00e9ric",
                            "lastName": "Bastien",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fr\u00e9d\u00e9ric Bastien"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3087941"
                        ],
                        "name": "Pascal Lamblin",
                        "slug": "Pascal-Lamblin",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Lamblin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Lamblin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1996134"
                        ],
                        "name": "Razvan Pascanu",
                        "slug": "Razvan-Pascanu",
                        "structuredName": {
                            "firstName": "Razvan",
                            "lastName": "Pascanu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Razvan Pascanu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32837403"
                        ],
                        "name": "J. Bergstra",
                        "slug": "J.-Bergstra",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bergstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bergstra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153440022"
                        ],
                        "name": "Ian J. Goodfellow",
                        "slug": "Ian-J.-Goodfellow",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Goodfellow",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian J. Goodfellow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47944877"
                        ],
                        "name": "Arnaud Bergeron",
                        "slug": "Arnaud-Bergeron",
                        "structuredName": {
                            "firstName": "Arnaud",
                            "lastName": "Bergeron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Arnaud Bergeron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065828537"
                        ],
                        "name": "Nicolas Bouchard",
                        "slug": "Nicolas-Bouchard",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Bouchard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Bouchard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1393680089"
                        ],
                        "name": "David Warde-Farley",
                        "slug": "David-Warde-Farley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Warde-Farley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Warde-Farley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 73
                            }
                        ],
                        "text": "We would like to thank the developers of Theano (Bergstra et al., 2010b; Bastien et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 81
                            }
                        ],
                        "text": "Both Minres-QLP as well as linear conjugate gradient can be found implemented in Theano at https://github.com/pascanur/theano optimize."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 48
                            }
                        ],
                        "text": "We would like to thank the developers of Theano (Bergstra et al., 2010b; Bastien et al., 2012) and Guillaume Desjardins and Yann Dauphin for their insightful comments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 12
                            }
                        ],
                        "text": "We used the Theano library (Bergstra et al., 2010a) which allows for a flexible implementation of the pipeline, that can automatically generate the computational graph of the metric times some vector for different models:\nimport theano.tensor as TT # \u2018params\u2018 is the list of Theano variables containing the parameters # \u2018vs\u2018 is the list of Theano variable representing the vector \u2018v\u2018 # with whom we want to multiply the metric # \u2018Gvs\u2018 is the list of Theano expressions representing the product # between the metric and \u2018vs\u2018\n# \u2018out_smx\u2018 is the output of the model with softmax units"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 110
                            }
                        ],
                        "text": "All methods except SGD use the truncated Newton pipeline The benchmark is run on a GTX 580 Nvidia card, using Theano (Bergstra et al., 2010a) for cuda kernels."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8180128,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "855d0f722d75cc56a66a00ede18ace96bafee6bd",
            "isKey": false,
            "numCitedBy": 1374,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Theano is a linear algebra compiler that optimizes a user's symbolically-specified mathematical computations to produce efficient low-level implementations. In this paper, we present new features and efficiency improvements to Theano, and benchmarks demonstrating Theano's performance relative to Torch7, a recently introduced machine learning library, and to RNNLM, a C++ library targeted at recurrent neural networks."
            },
            "slug": "Theano:-new-features-and-speed-improvements-Bastien-Lamblin",
            "title": {
                "fragments": [],
                "text": "Theano: new features and speed improvements"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "New features and efficiency improvements to Theano are presented, and benchmarks demonstrating Theano's performance relative to Torch7, a recently introduced machine learning library, and to RNNLM, a C++ library targeted at recurrent neural networks."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108646075"
                        ],
                        "name": "Sou-Cheng T. Choi",
                        "slug": "Sou-Cheng-T.-Choi",
                        "structuredName": {
                            "firstName": "Sou-Cheng",
                            "lastName": "Choi",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sou-Cheng T. Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080288"
                        ],
                        "name": "C. Paige",
                        "slug": "C.-Paige",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Paige",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Paige"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145621255"
                        ],
                        "name": "M. Saunders",
                        "slug": "M.-Saunders",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Saunders",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Saunders"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 100
                            }
                        ],
                        "text": "In order to better deal with singular and ill-conditioned matrices we use the MinRes-QLP algorithm (Choi et al., 2011) instead of linear conjugate gradient for certain experiments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8105948,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f50bd8b4e6f36e395ca583bd84229d4aa1c3801f",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "CG, SYMMLQ, and MINRES are Krylov subspace methods for solving symmetric systems of linear equations. When these methods are applied to an incompatible system (that is, a singular symmetric least-squares problem), CG could break down and SYMMLQ's solution could explode, while MINRES would give a least-squares solution but not necessarily the minimum-length (pseudoinverse) solution. This understanding motivates us to design a MINRES-like algorithm to compute minimum-length solutions to singular symmetric systems. MINRES uses QR factors of the tridiagonal matrix from the Lanczos process (where $R$ is upper-tridiagonal). MINRES-QLP uses a QLP decomposition (where rotations on the right reduce $R$ to lower-tridiagonal form). On ill-conditioned systems (singular or not), MINRES-QLP can give more accurate solutions than MINRES. We derive preconditioned MINRES-QLP, new stopping rules, and better estimates of the solution and residual norms, the matrix norm, and the condition number."
            },
            "slug": "MINRES-QLP:-A-Krylov-Subspace-Method-for-Indefinite-Choi-Paige",
            "title": {
                "fragments": [],
                "text": "MINRES-QLP: A Krylov Subspace Method for Indefinite or Singular Symmetric Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work derives preconditioned MINRES-QLP, new stopping rules, and better estimates of the solution and residual norms, the matrix norm, and the condition number."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Sci. Comput."
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113861474"
                        ],
                        "name": "C. Bishop",
                        "slug": "C.-Bishop",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bishop"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "1 Adapting natural gradient descent for neural networks In order to use natural gradient descent for deterministic neural networks we rely on their probabilistic interpretation (see Bishop (2006), chapter 3."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60688891,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "932a106c21a1db1e1876459c1521d27fd152caac",
            "isKey": false,
            "numCitedBy": 8461,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Looking for competent reading resources? We have pattern recognition and machine learning information science and statistics to read, not only read, but also download them or even check out online. Locate this fantastic book writtern by by now, simply here, yeah just here. Obtain the reports in the kinds of txt, zip, kindle, word, ppt, pdf, as well as rar. Once again, never ever miss to review online and download this book in our site right here. Click the link."
            },
            "slug": "Pattern-Recognition-and-Machine-Learning-Science-Bishop",
            "title": {
                "fragments": [],
                "text": "Pattern Recognition and Machine Learning (Information Science and Statistics)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157892296"
                        ],
                        "name": "D. K. Smith",
                        "slug": "D.-K.-Smith",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Smith",
                            "middleNames": [
                                "K.",
                                "Skip"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. K. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 59
                            }
                        ],
                        "text": "These products are used within a truncated-Newton approach (Nocedal and Wright, 2000) which considers the exact Hessian and only inverts it approximately without the need for explicitly storing the matrix in memory, as opposed to other approaches which perform a more crude approximation of the Hessian (or Fisher) matrix (either diagonal or block-diagonal)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 60
                            }
                        ],
                        "text": "These products are used within a truncated-Newton approach (Nocedal and Wright, 2000) which considers the exact Hessian and only inverts it approximately without the need for explicitly storing the matrix in memory, as opposed to other approaches which perform a more crude approximation of the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 189864167,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "bf86896c23300a46b7fc76298e365984c0b05105",
            "isKey": false,
            "numCitedBy": 10989,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "no exception. MRP II and JIT=TQC in purchasing and supplier education are covered in Chapter 15. Without proper education MRP II and JIT=TQC will not be successful and will not generate their true bene\u00aets. Suppliers are key to the success of MRP II and JIT=TQC. They therefore need to understand these disciplines. Purchasing in the 21st century is going to be marked by continuous changes, by who can gain the competitive edge \u00aerst, who will be the most \u0304exible and who will build the best supplier relationships. This will only be achieved by following the process as described in Schorr in a step by step fashion. An organization must however be willing to, as Schorr states in Chapter 16, `create the spark, ignite change'! Only then can it happen! If you really want to know something about purchasing then this is the book to read. It is most de\u00aenitely relevant and more importantly up to date. It will certainly be a handy reference book for a course on purchasing."
            },
            "slug": "Numerical-Optimization-Smith",
            "title": {
                "fragments": [],
                "text": "Numerical Optimization"
            },
            "venue": {
                "fragments": [],
                "text": "J. Oper. Res. Soc."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 279,
                                "start": 39
                            }
                        ],
                        "text": "Natural gradient can be traced back to Amari\u2019s work on information geometry (Amari, 1985) and its application to various neural networks (Amari et al., 1992; Amari, 1997), though a more in depth introduction can be found in Amari (1998); Park et al. (2000); Arnold et al. (2011). The algorithm has also been successfully applied in the reinforcement learning community (Kakade, 2001; Peters and Schaal, 2008) and for stochastic search (Sun et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 257,
                                "start": 39
                            }
                        ],
                        "text": "Natural gradient can be traced back to Amari\u2019s work on information geometry (Amari, 1985) and its application to various neural networks (Amari et al., 1992; Amari, 1997), though a more in depth introduction can be found in Amari (1998); Park et al. (2000); Arnold et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 76
                            }
                        ],
                        "text": "Natural gradient can be traced back to Amari\u2019s work on information geometry (Amari, 1985) and its application to various neural networks (Amari et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 39
                            }
                        ],
                        "text": "Natural gradient can be traced back to Amari\u2019s work on information geometry (Amari, 1985) and its application to various neural networks (Amari et al., 1992; Amari, 1997), though a more in depth introduction can be found in Amari (1998); Park et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 66
                            }
                        ],
                        "text": "We evaluate natural gradient, an algorithm originally proposed in Amari (1997), for learning deep models."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 76
                            }
                        ],
                        "text": "Natural gradient can be traced back to Amari\u2019s work on information geometry (Amari, 1985) and its application to various neural networks (Amari et al., 1992; Amari, 1997), though a more in depth introduction can be found in Amari (1998); Park et al. (2000); Arnold et al. (2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 116956004,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "420322994c59e9081786b46b31e2c82a9753e23a",
            "isKey": true,
            "numCitedBy": 1490,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Differential-geometrical-methods-in-statistics-Amari",
            "title": {
                "fragments": [],
                "text": "Differential-geometrical methods in statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725666"
                        ],
                        "name": "L. Tun\u00e7el",
                        "slug": "L.-Tun\u00e7el",
                        "structuredName": {
                            "firstName": "Levent",
                            "lastName": "Tun\u00e7el",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Tun\u00e7el"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1104,
                                "start": 0
                            }
                        ],
                        "text": "Absil et al. (2008) describes how second order methods can be generalized to the manifold case. In Honkela et al. (2008, 2010) a similar idea is proposed in the context of variational inference and specific assumptions on the form of p(\u03b8) are made. Gonzalez and Dorronsoro (2006) is more similar in spirit with this work, though their approach is defined for the diagonal form of the Fisher Information Matrix and differs in how they propose to compute a new conjugate direction. Natural conjugate gradient, the manifold version of nonlinear conjugate gradient, is defined following the same intuitions (Shewchuck, 1994). The only problematic part of the original algorithm is how to obtain a new conjugate direction given the previous search direction and the current natural gradient. The problem arises from the fact that the two vectors belong to different spaces. The local geometry around the point where the previous search direction dt\u22121 was computed is defined by Gt\u22121, while the geometry around the new direction is defined by Gt, where, in principle, Gt\u22121 6= Gt. Following Absil et al. (2008) we would need to \u201ctransport\u201d dt\u22121 into the space of \u2207Nt , an expensive operation, before we can compute a new direction using a standard formula like Polak-Riebiere."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 280,
                                "start": 0
                            }
                        ],
                        "text": "Absil et al. (2008) describes how second order methods can be generalized to the manifold case. In Honkela et al. (2008, 2010) a similar idea is proposed in the context of variational inference and specific assumptions on the form of p(\u03b8) are made. Gonzalez and Dorronsoro (2006) is more similar in spirit with this work, though their approach is defined for the diagonal form of the Fisher Information Matrix and differs in how they propose to compute a new conjugate direction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 10
                            }
                        ],
                        "text": "Following Absil et al. (2008) we would need to \u201ctransport\u201d dt\u22121 into the space of \u2207Nt , an expensive operation, before we can compute a new direction using a standard formula like Polak-Riebiere."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Absil et al. (2008) describes how second order methods can be generalized to the manifold case."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6103282,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f7b4c27abb76dff4c017849049541f3fc91e77be",
            "isKey": true,
            "numCitedBy": 1347,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Optimization-algorithms-on-matrix-manifolds-Tun\u00e7el",
            "title": {
                "fragments": [],
                "text": "Optimization algorithms on matrix manifolds"
            },
            "venue": {
                "fragments": [],
                "text": "Math. Comput."
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 49
                            }
                        ],
                        "text": "We would like to thank the developers of Theano (Bergstra et al., 2010b; Bastien et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 81
                            }
                        ],
                        "text": "Both Minres-QLP as well as linear conjugate gradient can be found implemented in Theano at https://github.com/pascanur/theano optimize."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 48
                            }
                        ],
                        "text": "We would like to thank the developers of Theano (Bergstra et al., 2010b; Bastien et al., 2012) and Guillaume Desjardins and Yann Dauphin for their insightful comments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 28
                            }
                        ],
                        "text": "We used the Theano library (Bergstra et al., 2010a) which allows for a flexible implementation of the pipeline, that can automatically generate the computational graph of the metric times some vector for different models:\nimport theano.tensor as TT # \u2018params\u2018 is the list of Theano variables\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 12
                            }
                        ],
                        "text": "We used the Theano library (Bergstra et al., 2010a) which allows for a flexible implementation of the pipeline, that can automatically generate the computational graph of the metric times some vector for different models:\nimport theano.tensor as TT # \u2018params\u2018 is the list of Theano variables containing the parameters # \u2018vs\u2018 is the list of Theano variable representing the vector \u2018v\u2018 # with whom we want to multiply the metric # \u2018Gvs\u2018 is the list of Theano expressions representing the product # between the metric and \u2018vs\u2018\n# \u2018out_smx\u2018 is the output of the model with softmax units"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 118
                            }
                        ],
                        "text": "All methods except SGD use the truncated Newton pipeline The benchmark is run on a GTX 580 Nvidia card, using Theano (Bergstra et al., 2010a) for cuda kernels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theano: a CPU and GPU math expression compiler"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Python for Scientific Computing Conference (SciPy). Oral Presentation."
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 201
                            }
                        ],
                        "text": "\u2026optimization (Martens, 2010; Sutskever et al., 2011; Chapelle and Erhan, 2011), Krylov Subspace Descent (Vinyals and Povey, 2012), natural gradient descent (Amari, 1997; Park et al., 2000), TONGA (Le Roux et al., 2008; Roux and Fitzgibbon, 2010) are just a few of such recently proposed algorithms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 6
                            }
                        ],
                        "text": "In Le Roux et al. (2008) one assumes that the gradients computed over different minibatches are distributed according to a Gaussian centered around the true gradient with some covariance matrix C."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 6
                            }
                        ],
                        "text": "In order to use natural gradient for deterministic neural networks we rely on their probabilistic interpretation p\u03b8(t|x)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 75
                            }
                        ],
                        "text": "While the probabilistic derivation requires the centered covariance, in Le Roux et al. (2008) it is argued that one can use the uncentered covariance U resulting in a simplified formula which is sometimes confused with the metric derived by Amari:\nU \u2248 E(x,t)\u223cq [( \u2202 log p(t|x) \u2202\u03b8 )T ( \u2202 log p(t|x)\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Topmoumoute online natural gradient algorithm. In NIPS'07"
            },
            "venue": {
                "fragments": [],
                "text": "Topmoumoute online natural gradient algorithm. In NIPS'07"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3385742"
                        ],
                        "name": "F. G\u00f6tze",
                        "slug": "F.-G\u00f6tze",
                        "structuredName": {
                            "firstName": "Friedrich",
                            "lastName": "G\u00f6tze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. G\u00f6tze"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 76
                            }
                        ],
                        "text": "Natural gradient can be traced back to Amari\u2019s work on information geometry (Amari, 1985) and its application to various neural networks (Amari et al., 1992; Amari, 1997), though a more in depth introduction can be found in Amari (1998); Park et al. (2000); Arnold et al. (2011)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 126227855,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "331b78fc26471d5c3c940b0257b4e9772b8d1631",
            "isKey": false,
            "numCitedBy": 99,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Differential-geometrical-methods-in-statistics.-in-G\u00f6tze",
            "title": {
                "fragments": [],
                "text": "Differential-geometrical methods in statistics. Lecture notes in statistics - A. Shun-ichi."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7245737"
                        ],
                        "name": "Nicolas Le Roux",
                        "slug": "Nicolas-Le-Roux",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Le Roux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Le Roux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47139824"
                        ],
                        "name": "A. Fitzgibbon",
                        "slug": "A.-Fitzgibbon",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Fitzgibbon",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fitzgibbon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61139240,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "29dceee846af4194a5e92f5fea0c53e6d38763e1",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Improving-First-and-Second-Order-Methods-by-Roux-Bengio",
            "title": {
                "fragments": [],
                "text": "Improving First and Second-Order Methods by Modeling Uncertainty"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 68
                            }
                        ],
                        "text": "2 shows the training and test error of a model trained on fold 4 of TFD, though similar results are obtained for the other folds as well."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 79
                            }
                        ],
                        "text": "We explore empirically these two hypotheses on the Toronto Face Dataset (TFD) (Susskind et al., 2010) which has a small training set and a large pool of unlabeled data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Toronto face dataset"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report UTML TR 2010-001, U. Toronto."
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theano : a CPU and GPU math expression compiler"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Python for Scientific Computing Conference ( SciPy )"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 27,
            "methodology": 23,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 44,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Revisiting-Natural-Gradient-for-Deep-Networks-Pascanu-Bengio/e8f95ccfd13689f672c39dca3eccf1c484533bcc?sort=total-citations"
}