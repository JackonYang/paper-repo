{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792458"
                        ],
                        "name": "A. Belz",
                        "slug": "A.-Belz",
                        "structuredName": {
                            "firstName": "Anja",
                            "lastName": "Belz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Belz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144568312"
                        ],
                        "name": "Ehud Reiter",
                        "slug": "Ehud-Reiter",
                        "structuredName": {
                            "firstName": "Ehud",
                            "lastName": "Reiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ehud Reiter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10438447,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a70e48c119742cb69b1cdbd62e58a8a8d0d28a8e",
            "isKey": false,
            "numCitedBy": 193,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the evaluation problem in Natural Language Generation (NLG) and present results for evaluating several NLG systems with similar functionality, including a knowledge-based generator and several statistical systems. We compare evaluation results for these systems by human domain experts, human non-experts, and several automatic evaluation metrics, including NI ST, B LEU, and ROUGE. We find that NI ST scores correlate best (>0.8) with human judgments, but that all automatic metrics we examined are biased in favour of generators that select on the basis of frequency alone. We conclude that automatic evaluation of NLG systems has considerable potential, in particular where high-quality reference texts and only a small number of human evaluators are available. However, in general it is probably best for automatic evaluations to be supported by human based evaluations, or at least by studies that demonstrate that a particular metric correlates well with human judgments in a given domain."
            },
            "slug": "Comparing-Automatic-and-Human-Evaluation-of-NLG-Belz-Reiter",
            "title": {
                "fragments": [],
                "text": "Comparing Automatic and Human Evaluation of NLG Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is found that NI ST scores correlate best with human judgments, but that all automatic metrics the authors examined are biased in favour of generators that select on the basis of frequency alone."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2862682"
                        ],
                        "name": "G. Doddington",
                        "slug": "G.-Doddington",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Doddington",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Doddington"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14067706,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "417f9ce1b1cb3c98e5c2a66d586c7a2eb7438a9f",
            "isKey": false,
            "numCitedBy": 1570,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Evaluation is recognized as an extremely helpful forcing function in Human Language Technology R&D. Unfortunately, evaluation has not been a very powerful tool in machine translation (MT) research because it requires human judgments and is thus expensive and time-consuming and not easily factored into the MT research agenda. However, at the July 2001 TIDES PI meeting in Philadelphia, IBM described an automatic MT evaluation technique that can provide immediate feedback and guidance in MT research. Their idea, which they call an \"evaluation understudy\", compares MT output with expert reference translations in terms of the statistics of short sequences of words (word N-grams). The more of these N-grams that a translation shares with the reference translations, the better the translation is judged to be. The idea is elegant in its simplicity. But far more important, IBM showed a strong correlation between these automatically generated scores and human judgments of translation quality. As a result, DARPA commissioned NIST to develop an MT evaluation facility based on the IBM work. This utility is now available from NIST and serves as the primary evaluation measure for TIDES MT research."
            },
            "slug": "Automatic-evaluation-of-machine-translation-quality-Doddington",
            "title": {
                "fragments": [],
                "text": "Automatic evaluation of machine translation quality using n-gram co-occurrence statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "NIST commissioned NIST to develop an MT evaluation facility based on the IBM work, which is now available from NIST and serves as the primary evaluation measure for TIDES MT research."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145801917"
                        ],
                        "name": "C. Mellish",
                        "slug": "C.-Mellish",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Mellish",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Mellish"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "147892141"
                        ],
                        "name": "R. Dale",
                        "slug": "R.-Dale",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Dale",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dale"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207896,
            "fieldsOfStudy": [
                "Sociology"
            ],
            "id": "b4e525029d737839e328bd335f03ba1c30e9ae0c",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "What role should evaluation play in the development of natural language generation (nlg) techniques and systems? In this paper we describe what is involved in natural language generation, and survey how evaluation has figured in work in this area to date. We comment on the issues raised by this existing work and on how the problems of nlg evaluation are diVerent from the problems of evaluating work in natural language understanding. The paper is concluded by suggesting a way forward by looking more closely at the component problems that are addressed in natural language generation research; a particular text generation application is examined and the issues that are raised in assessing its performance on a variety of dimensions are looked at. \u201d 1998 Academic Press"
            },
            "slug": "Evaluation-in-the-context-of-natural-language-Mellish-Dale",
            "title": {
                "fragments": [],
                "text": "Evaluation in the context of natural language generation"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "What is involved in natural language generation, and how evaluation has figured in work in this area to date is described; a particular text generation application is examined and the issues that are raised in assessing its performance on a variety of dimensions are looked at."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Speech Lang."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113922820"
                        ],
                        "name": "E. Reiter",
                        "slug": "E.-Reiter",
                        "structuredName": {
                            "firstName": "Ehud",
                            "lastName": "Reiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Reiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145734451"
                        ],
                        "name": "R. Robertson",
                        "slug": "R.-Robertson",
                        "structuredName": {
                            "firstName": "Rohan",
                            "lastName": "Robertson",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Robertson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145869117"
                        ],
                        "name": "S. Sripada",
                        "slug": "S.-Sripada",
                        "structuredName": {
                            "firstName": "Somayajulu",
                            "lastName": "Sripada",
                            "middleNames": [
                                "Gowri"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sripada"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3056098,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41ca49797ce1393cbaed1b4373fe768594ee2eac",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural language generation (NLG) systems are computer software systems that produce texts in English and other human languages, often from non-linguistic input data. NLG systems, like most ai systems, need substantial amounts of knowledge. However, our experience in two NLG projects suggests that it is difficult to acquire correct knowledge for NLG systems; indeed, every knowledge acquisition (KA) technique we tried had significant problems. In general terms, these problems were due to the complexity, novelty, and poorly understood nature of the tasks our systems attempted, and were worsened by the fact that people write so differently. This meant in particular that corpus-based KA approaches suffered because it was impossible to assemble a sizable corpus of high-quality consistent manually written texts in our domains; and structured expert-oriented KA techniques suffered because experts disagreed and because we could not get enough information about special and unusual cases to build robust systems. We believe that such problems are likely to affect many other NLG systems as well. In the long term, we hope that new KA techniques may emerge to help NLG system builders. In the shorter term, we believe that understanding how individual KA techniques can fail, and using a mixture of different KA techniques with different strengths and weaknesses, can help developers acquire NLG knowledge that is mostly correct."
            },
            "slug": "Acquiring-Correct-Knowledge-for-Natural-Language-Reiter-Robertson",
            "title": {
                "fragments": [],
                "text": "Acquiring Correct Knowledge for Natural Language Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "Understanding how individual KA techniques can fail, and using a mixture of different KA technique with different strengths and weaknesses, can help developers acquire NLG knowledge that is mostly correct."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2081707444"
                        ],
                        "name": "Deborah A. Coughlin",
                        "slug": "Deborah-A.-Coughlin",
                        "structuredName": {
                            "firstName": "Deborah",
                            "lastName": "Coughlin",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Deborah A. Coughlin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14586836,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "f493b8a1f05fd788b1bcaecf18eb87a3f5965b35",
            "isKey": false,
            "numCitedBy": 142,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a large-scale investigation of the correlation between human judgments of machine translation quality and the automated metrics that are increasingly used to drive progress in the field. We compare the results of 124 human evaluations of machine translated sentences to the scores generated by two automatic evaluation metrics (BLEU and NIST). When datasets are held constant or file size is sufficiently large, BLEU and NIST scores closely parallel human judgments. Surprisingly, this was true even though these scores were calculated using just one human reference. We suggest that when human evaluators are forced to make decisions without sufficient context or domain expertise, they fall back on strategies that are not unlike determining n-gram precision."
            },
            "slug": "Correlating-automated-and-human-assessments-of-Coughlin",
            "title": {
                "fragments": [],
                "text": "Correlating automated and human assessments of machine translation quality"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is suggested that when human evaluators are forced to make decisions without sufficient context or domain expertise, they fall back on strategies that are not unlike determining n-gram precision."
            },
            "venue": {
                "fragments": [],
                "text": "MTSUMMIT"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690152"
                        ],
                        "name": "Amanda Stent",
                        "slug": "Amanda-Stent",
                        "structuredName": {
                            "firstName": "Amanda",
                            "lastName": "Stent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amanda Stent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2372948"
                        ],
                        "name": "M. Marge",
                        "slug": "M.-Marge",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Marge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marge"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714700"
                        ],
                        "name": "Mohit Singhai",
                        "slug": "Mohit-Singhai",
                        "structuredName": {
                            "firstName": "Mohit",
                            "lastName": "Singhai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohit Singhai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11115098,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d1beff9dfc115eacc5466a39e535177e1a6b699",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent years have seen increasing interest in automatic metrics for the evaluation of generation systems. When a system can generate syntactic variation, automatic evaluation becomes more difficult. In this paper, we compare the performance of several automatic evaluation metrics using a corpus of automatically generated paraphrases. We show that these evaluation metrics can at least partially measure adequacy (similarity in meaning), but are not good measures of fluency (syntactic correctness). We make several proposals for improving the evaluation of generation systems that produce variation."
            },
            "slug": "Evaluating-Evaluation-Methods-for-Generation-in-the-Stent-Marge",
            "title": {
                "fragments": [],
                "text": "Evaluating Evaluation Methods for Generation in the Presence of Variation"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper compares the performance of several automatic evaluation metrics using a corpus of automatically generated paraphrases and shows that these evaluation metrics can at least partially measure adequacy, but are not good measures of fluency."
            },
            "venue": {
                "fragments": [],
                "text": "CICLing"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110878863"
                        ],
                        "name": "S. Banerjee",
                        "slug": "S.-Banerjee",
                        "structuredName": {
                            "firstName": "Satanjeev",
                            "lastName": "Banerjee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Banerjee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784914"
                        ],
                        "name": "A. Lavie",
                        "slug": "A.-Lavie",
                        "structuredName": {
                            "firstName": "Alon",
                            "lastName": "Lavie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lavie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7164502,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0cd18e4400ff75b2f8b58d60ddb9b0bc12f489e7",
            "isKey": false,
            "numCitedBy": 2987,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe METEOR, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machineproduced translation and human-produced reference translations. Unigrams can be matched based on their surface forms, stemmed forms, and meanings; furthermore, METEOR can be easily extended to include more advanced matching strategies. Once all generalized unigram matches between the two strings have been found, METEOR computes a score for this matching using a combination of unigram-precision, unigram-recall, and a measure of fragmentation that is designed to directly capture how well-ordered the matched words in the machine translation are in relation to the reference. We evaluate METEOR by measuring the correlation between the metric scores and human judgments of translation quality. We compute the Pearson R correlation value between its scores and human quality assessments of the LDC TIDES 2003 Arabic-to-English and Chinese-to-English datasets. We perform segment-bysegment correlation, and show that METEOR gets an R correlation value of 0.347 on the Arabic data and 0.331 on the Chinese data. This is shown to be an improvement on using simply unigramprecision, unigram-recall and their harmonic F1 combination. We also perform experiments to show the relative contributions of the various mapping modules."
            },
            "slug": "METEOR:-An-Automatic-Metric-for-MT-Evaluation-with-Banerjee-Lavie",
            "title": {
                "fragments": [],
                "text": "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "METEOR is described, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machineproduced translation and human-produced reference translations and can be easily extended to include more advanced matching strategies."
            },
            "venue": {
                "fragments": [],
                "text": "IEEvaluation@ACL"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144568312"
                        ],
                        "name": "Ehud Reiter",
                        "slug": "Ehud-Reiter",
                        "structuredName": {
                            "firstName": "Ehud",
                            "lastName": "Reiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ehud Reiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145869117"
                        ],
                        "name": "S. Sripada",
                        "slug": "S.-Sripada",
                        "structuredName": {
                            "firstName": "Somayajulu",
                            "lastName": "Sripada",
                            "middleNames": [
                                "Gowri"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sripada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27078692"
                        ],
                        "name": "J. Hunter",
                        "slug": "J.-Hunter",
                        "structuredName": {
                            "firstName": "Jim",
                            "lastName": "Hunter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hunter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145138757"
                        ],
                        "name": "Jin Yu",
                        "slug": "Jin-Yu",
                        "structuredName": {
                            "firstName": "Jin",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jin Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2539814"
                        ],
                        "name": "I. Davy",
                        "slug": "I.-Davy",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Davy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Davy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 97
                            }
                        ],
                        "text": "Because writers do not always produce optimal texts from a reader\u2019s perspective (Oberlander 1998; Reiter et al. 2005), a metric which is a good evaluator of how likely it is that a text has been written by a human writer is not necessarily a good predictor of how effective and useful the text is\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 98
                            }
                        ],
                        "text": "For example, when we ran a human judgment-based study to test the effectiveness of SUMTIME texts (Reiter et al. 2005), we managed to recruit 72 subjects in a few weeks; in contrast it took us several months to recruit the 23 expert subjects who participated in the studies reported in this article."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 131
                            }
                        ],
                        "text": "SUMTIME generates very high-quality texts; in some cases forecast users believe SUMTIME texts are better than human-written texts (Reiter et al. 2005; see also Table 4 of this paper)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 173
                            }
                        ],
                        "text": "For example, the SUMTIME weather-forecast generator was evaluated by showing subjects both human corpus texts and computer-generated texts, and asking which they preferred (Reiter et al. 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 78
                            }
                        ],
                        "text": "In the SUMTIME domain, this intuition is supported by the SUMTIME evaluation (Reiter et al. 2005), in which forecast readers were asked to compare two forecasts, and say which was easier to read, which was more accurate, and which was overall more appropriate."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 84
                            }
                        ],
                        "text": "In particular, we based our experiments on the SUMTIME system (Sripada et al. 2004; Reiter et al. 2005) and its associated SUMTIME-METEO corpus (Sripada et al. 2003), which were developed at Aberdeen."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 104
                            }
                        ],
                        "text": "This is one of the most popular applications of NLG (Goldberg, Driedger, and Kittredge 1994; Coch 1998; Reiter et al. 2005), and several NLG weather-forecast systems have been fielded and used."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13461687,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "cd60e751a02e7104a68717c5ba29f534d9037ace",
            "isKey": false,
            "numCitedBy": 251,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Choosing-words-in-computer-generated-weather-Reiter-Sripada",
            "title": {
                "fragments": [],
                "text": "Choosing words in computer-generated weather forecasts"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "82323309"
                        ],
                        "name": "Karen Sparck Jones",
                        "slug": "Karen-Sparck-Jones",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Sparck Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karen Sparck Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2557472"
                        ],
                        "name": "J. Galliers",
                        "slug": "J.-Galliers",
                        "structuredName": {
                            "firstName": "Julia",
                            "lastName": "Galliers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Galliers"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 34479739,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "006b8c5d5b38e8a07091b5b001ece6db1d2b132e",
            "isKey": false,
            "numCitedBy": 409,
            "numCiting": 98,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nThis comprehensive state-of-the-art book is the first devoted to the important and timely issue of evaluating NLP systems. It addresses the whole area of NLP system evaluation, including aims and scope, problems and methodology. The authors provide a wide-ranging and careful analysis of evaluation concepts, reinforced with extensive illustrations; they relate systems to their environments and develop a framework for proper evaluation. The discussion of principles is completed by a detailed review of practice and strategies in the field, covering both systems for specific tasks, like translation, and core language processors. The methodology lessons drawn from the analysis and review are applied in a series of example cases. The book also refers NLP system evaluation to the neighbouring areas of information and speech processing, and addresses issues of tool and data provision for evaluation. A comprehensive bibliography and subject index are included as well as a term glossary. This monograph will be a valuable source of inspiration in research, practice, and teaching."
            },
            "slug": "Evaluating-Natural-Language-Processing-Systems:-An-Jones-Galliers",
            "title": {
                "fragments": [],
                "text": "Evaluating Natural Language Processing Systems: An Analysis and Review"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This comprehensive state-of-the-art book is the first devoted to the important and timely issue of evaluating NLP systems, and provides a wide-ranging and careful analysis of evaluation concepts, reinforced with extensive illustrations."
            },
            "venue": {
                "fragments": [],
                "text": "Lecture Notes in Computer Science"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752326"
                        ],
                        "name": "B. Dorr",
                        "slug": "B.-Dorr",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Dorr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Dorr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696402"
                        ],
                        "name": "Christof Monz",
                        "slug": "Christof-Monz",
                        "structuredName": {
                            "firstName": "Christof",
                            "lastName": "Monz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christof Monz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32101005"
                        ],
                        "name": "Stacy President",
                        "slug": "Stacy-President",
                        "structuredName": {
                            "firstName": "Stacy",
                            "lastName": "President",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stacy President"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35442155"
                        ],
                        "name": "R. Schwartz",
                        "slug": "R.-Schwartz",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3192273"
                        ],
                        "name": "David M. Zajic",
                        "slug": "David-M.-Zajic",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zajic",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Zajic"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Dorr et al. (2005) checked if ROUGE scores correlated with task effectiveness; they did not find a strong correlation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 32
                            }
                        ],
                        "text": "In the summarization community, Dorr et al. (2005) found very weak correlation between an automatic metric (ROUGE) and task performance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9008917,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "ba7a97dfeff9b81fefee2c13f5cbd3367f5671e6",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper demonstrates the usefulness of summaries in an extrinsic task of relevance judgment based on a new method for measuring agreement, Relevance-Prediction, which compares subjects\u2019 judgments on summaries with their own judgments on full text documents. We demonstrate that, because this measure is more reliable than previous gold-standard measures, we are able to make stronger statistical statements about the benefits of summarization. We found positive correlations between ROUGE scores and two different summary types, where only weak or negative correlations were found using other agreement measures. However, we show that ROUGE may be sensitive to the choice of summarization style. We discuss the importance of these results and the implications for future summarization evaluations."
            },
            "slug": "A-Methodology-for-Extrinsic-Evaluation-of-Text-Does-Dorr-Monz",
            "title": {
                "fragments": [],
                "text": "A Methodology for Extrinsic Evaluation of Text Summarization: Does ROUGE Correlate?"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper demonstrates the usefulness of summaries in an extrinsic task of relevance judgment based on a new method for measuring agreement, Relevance-Prediction, which compares subjects\u2019 judgments on summaries with their own judgments on full text documents."
            },
            "venue": {
                "fragments": [],
                "text": "IEEvaluation@ACL"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763608"
                        ],
                        "name": "Chris Callison-Burch",
                        "slug": "Chris-Callison-Burch",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Callison-Burch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Callison-Burch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2927234"
                        ],
                        "name": "C. Fordyce",
                        "slug": "C.-Fordyce",
                        "structuredName": {
                            "firstName": "Cameron",
                            "lastName": "Fordyce",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Fordyce"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755162"
                        ],
                        "name": "Philipp Koehn",
                        "slug": "Philipp-Koehn",
                        "structuredName": {
                            "firstName": "Philipp",
                            "lastName": "Koehn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philipp Koehn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696402"
                        ],
                        "name": "Christof Monz",
                        "slug": "Christof-Monz",
                        "structuredName": {
                            "firstName": "Christof",
                            "lastName": "Monz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christof Monz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152324671"
                        ],
                        "name": "J. Schroeder",
                        "slug": "J.-Schroeder",
                        "structuredName": {
                            "firstName": "Josh",
                            "lastName": "Schroeder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schroeder"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 268
                            }
                        ],
                        "text": "There is a rich literature in MT evaluation, including a number of specialist workshops on this topic; as in NLG, there is also considerable interest in using shared-task events to provide data about how well different evaluation techniques correlate with each other (Callison-Burch et al. 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26255400,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be9bca1e9b0192fc49b316f2701242b50d98d456",
            "isKey": false,
            "numCitedBy": 287,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper analyzes the translation quality of machine translation systems for 10 language pairs translating between Czech, English, French, German, Hungarian, and Spanish. We report the translation quality of over 30 diverse translation systems based on a large-scale manual evaluation involving hundreds of hours of effort. We use the human judgments of the systems to analyze automatic evaluation metrics for translation quality, and we report the strength of the correlation with human judgments at both the system-level and at the sentence-level. We validate our manual evaluation methodology by measuring intra- and inter-annotator agreement, and collecting timing information."
            },
            "slug": "Further-Meta-Evaluation-of-Machine-Translation-Callison-Burch-Fordyce",
            "title": {
                "fragments": [],
                "text": "Further Meta-Evaluation of Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "This paper analyzes the translation quality of machine translation systems for 10 language pairs translating between Czech, English, French, German, Hungarian, and Spanish and uses the human judgments of the systems to analyze automatic evaluation metrics for translation quality."
            },
            "venue": {
                "fragments": [],
                "text": "WMT@ACL"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145148360"
                        ],
                        "name": "L. Hirschman",
                        "slug": "L.-Hirschman",
                        "structuredName": {
                            "firstName": "Lynette",
                            "lastName": "Hirschman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Hirschman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 44930659,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5abf34a91a4d7dfe347be52471ff1876f11a9b4d",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract The Message Understanding Conferences (MUCs) represent one of the earliest and longest running efforts to evaluate language understanding technology. This article reviews the history of the MUCs and their evolution towards the use of common training and blind test sets, automated scoring, task decomposition into modular building blocks and tools for portability across languages and applications. Now that evaluation has become an accepted part of the developer's toolkit, it is important to understand the interplay between evaluation methods and the state of research. MUC was successful in generating excitement about text processing problems and in attracting talented researchers to the area. It also provided a functional decomposition of the information extraction problem into a series of simpler problems, thus allowing researchers to demonstrate successful systems and to spin off commercial products. However, the ultimate goal of accurate information extraction has been elusive; systems have become faster and cheaper to build, the evaluations have become harder, but overall accuracy in information extraction has improved only modestly. The MUC experience contrasts with experiences in other evaluations. For example, the spoken evaluation in the Air Travel Information System (ATIS) has shown dramatic improvement in error rate over time, but those evaluations were limited to a single domain and the metrics did not address interaction, even though real-time interactive systems were available. Looking across the history of MUC in the context of related evaluations, we can draw important lessons about the need for evaluation to evolve with the technology it evaluates, to balance costs against benefits and to weigh the divergent needs of the multiple stake-holders\u2014 developers, funders and users\u2014in order to provide continuity while also providing the next set of challenges to the research community."
            },
            "slug": "The-Evolution-of-evaluation:-Lessons-from-the-Hirschman",
            "title": {
                "fragments": [],
                "text": "The Evolution of evaluation: Lessons from the Message Understanding Conferences"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Looking across the history of MUC in the context of related evaluations, important lessons are drawn about the need for evaluation to evolve with the technology it evaluates, to balance costs against benefits and to weigh the divergent needs of the multiple stake-holders\u2014 developers, funders and users."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Speech Lang."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781574"
                        ],
                        "name": "Chin-Yew Lin",
                        "slug": "Chin-Yew-Lin",
                        "structuredName": {
                            "firstName": "Chin-Yew",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chin-Yew Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144547315"
                        ],
                        "name": "E. Hovy",
                        "slug": "E.-Hovy",
                        "structuredName": {
                            "firstName": "Eduard",
                            "lastName": "Hovy",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Hovy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16292125,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c63bb976dc0d3a897f3b0920170a4c573ef904c6",
            "isKey": false,
            "numCitedBy": 1628,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries. The results show that automatic evaluation using unigram co-occurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results."
            },
            "slug": "Automatic-Evaluation-of-Summaries-Using-N-gram-Lin-Hovy",
            "title": {
                "fragments": [],
                "text": "Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The results show that automatic evaluation using unigram co-occurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1390051006"
                        ],
                        "name": "I. Langkilde-Geary",
                        "slug": "I.-Langkilde-Geary",
                        "structuredName": {
                            "firstName": "Irene",
                            "lastName": "Langkilde-Geary",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Langkilde-Geary"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14381340,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7c878514565a57b82966640f21d21afdb3c82d2e",
            "isKey": false,
            "numCitedBy": 169,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a general-purpose sentence generation system that can achieve both broad scale coverage and high quality while aiming to be suitable for a variety of generation tasks. We measure the coverage and correctness empirically using a section of the Penn Treebank corpus as a test set. We also describe novel features that help make the generator flexible and easier to use for a variety of tasks. To our knowledge, this is the first empirical measurement of coverage reported in the literature, and the highest reported measurements of correctness."
            },
            "slug": "An-Empirical-Verification-of-Coverage-and-for-a-Langkilde-Geary",
            "title": {
                "fragments": [],
                "text": "An Empirical Verification of Coverage and Correctness for a General-Purpose Sentence Generator"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "This paper describes a general-purpose sentence generation system that can achieve both broad scale coverage and high quality while aiming to be suitable for a variety of generation tasks, and reports the first empirical measurement of coverage reported in the literature, and the highest reported measurements of correctness."
            },
            "venue": {
                "fragments": [],
                "text": "INLG"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3323275"
                        ],
                        "name": "Kishore Papineni",
                        "slug": "Kishore-Papineni",
                        "structuredName": {
                            "firstName": "Kishore",
                            "lastName": "Papineni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kishore Papineni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781292"
                        ],
                        "name": "S. Roukos",
                        "slug": "S.-Roukos",
                        "structuredName": {
                            "firstName": "Salim",
                            "lastName": "Roukos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roukos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144582029"
                        ],
                        "name": "T. Ward",
                        "slug": "T.-Ward",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Ward",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ward"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2587983"
                        ],
                        "name": "Wei-Jing Zhu",
                        "slug": "Wei-Jing-Zhu",
                        "structuredName": {
                            "firstName": "Wei-Jing",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Jing Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 235
                            }
                        ],
                        "text": "\u2026even when evaluating linguistic quality, current automatic metrics should be used with caution, as a supplement rather than a replacement for human evaluation; similar comments have been made about the use of automatic metrics in MT (Papineni et al. 2002; Callison-Burch, Osborne, and Koehn 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Papineni et al. (2002) in fact found that BLEU scores were more highly correlated with human judgments from monolingual subjects than human judgments from bilingual subjects."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 11
                            }
                        ],
                        "text": "evaluation (Papineni et al. 2002), but it is now routinely used as the main technique for evaluating research contributions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 216
                            }
                        ],
                        "text": "In recent years there has been growing interest in evaluating NLG texts by comparing them to a corpus of human-written reference texts, using automatic metrics such as string-edit distance, tree similarity, or BLEU (Papineni et al. 2002); this is another type of intrinsic evaluation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 360,
                                "start": 216
                            }
                        ],
                        "text": "In recent years there has been growing interest in evaluating NLG texts by comparing them to a corpus of human-written reference texts, using automatic metrics such as string-edit distance, tree similarity, or BLEU (Papineni et al. 2002); this is another type of intrinsic evaluation. Such evaluations have been used by Bangalore, Rambow, and Whittaker (2000) and Marciniak and Strube (2004), for example."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 149
                            }
                        ],
                        "text": "\u00a9 2009 Association for Computational Linguistics\nand automatic evaluation metrics in machine translation and document summarization (Doddington 2002; Papineni et al. 2002; Lin and Hovy 2003), much less is known about how well automatic metrics correlate with human judgments in NLG."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1046,
                                "start": 12
                            }
                        ],
                        "text": "evaluation (Papineni et al. 2002), but it is now routinely used as the main technique for evaluating research contributions. It is accepted and indeed the norm for an article on MT in Computational Linguistics to report evaluations that are solely based on automatic corpus-based metrics; this is not the case in NLG, where human evaluations are expected at least in high-prestige venues. We are not aware of any studies in MT that have tried to correlate BLEU-like metrics with the results of task-effectiveness studies. Although a number of studies have analyzed the correlation between BLEU-type metrics and human judgments, most of these have used human judgments from NIST MT evaluations. Human judgments in most of these evaluations were solicited from monolingual subjects who were asked to compare the output of MT systems to a single reference translation, without any context; also in many of these studies the subjects were asked to assess individual sentences or even phrases, not complete texts (Doddington 2002). As Coughlin (2003) and others have pointed out, it is not clear that human judgments solicited in this way would match the judgments of bilingual subjects who were shown complete source and MT texts, and asked to evaluate the quality of the translation in a specific real-world context."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 100
                            }
                        ],
                        "text": "BLEU was first proposed as a supplement (the U in BLEU stands for \u201cunderstudy\u201d) for human evaluation (Papineni et al. 2002), but it is now routinely used as the main technique for evaluating research contributions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 108
                            }
                        ],
                        "text": "We tested five automatic corpus-based metrics: two variants of the BLEU metric used in machine translation (Papineni et al. 2002); two variants of the ROUGE metric used in document summarization (Lin and Hovy 2003); and a simple sting-edit distance metric (as a baseline)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 66
                            }
                        ],
                        "text": "Many NLG researchers are impressed by the BLEU evaluation metric (Papineni et al. 2002) in Machine Translation (MT), which has allowed MT researchers to quickly and cheaply evaluate the impact of new ideas, algorithms, and data sets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 423,
                                "start": 216
                            }
                        ],
                        "text": "In recent years there has been growing interest in evaluating NLG texts by comparing them to a corpus of human-written reference texts, using automatic metrics such as string-edit distance, tree similarity, or BLEU (Papineni et al. 2002); this is another type of intrinsic evaluation. Such evaluations have been used by Bangalore, Rambow, and Whittaker (2000) and Marciniak and Strube (2004), for example. Langkilde (2002) evaluated an NLG system by parsing texts from a corpus, feeding the parser output to her NLG system, and then comparing the generated texts to the original corpus texts."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 83
                            }
                        ],
                        "text": "and automatic evaluation metrics in machine translation and document summarization (Doddington 2002; Papineni et al. 2002; Lin and Hovy 2003), much less is known about how well automatic metrics correlate with human judgments in NLG."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1790,
                                "start": 216
                            }
                        ],
                        "text": "In recent years there has been growing interest in evaluating NLG texts by comparing them to a corpus of human-written reference texts, using automatic metrics such as string-edit distance, tree similarity, or BLEU (Papineni et al. 2002); this is another type of intrinsic evaluation. Such evaluations have been used by Bangalore, Rambow, and Whittaker (2000) and Marciniak and Strube (2004), for example. Langkilde (2002) evaluated an NLG system by parsing texts from a corpus, feeding the parser output to her NLG system, and then comparing the generated texts to the original corpus texts. Similar \u201ccorpus regeneration\u201d evaluations have since been used by a number of other researchers (Callaway 2003; Zhong and Stent 2005; Cahill and van Genabith 2006). Corpus-based evaluation has been especially popular in the evaluation of surface realizers. This may be because the most important attribute of many realizers is grammatical coverage and robust handling of special and unusual cases, and corpus-based techniques are well suited to evaluating this. Also, the range of acceptable outputs can be smaller in realizer evaluations because content, microplanning, and (in some cases) lexical choices do not vary; this means there is less concern about reference texts not adequately covering the solution space. Automatic corpus-based evaluations are appealing in NLG, as in other areas of NLP, because they are relatively cheap and quick to do if a corpus is available, do not require support from domain experts, and are repeatable. However, their use in NLG is controversial, at least when evaluating systems as a whole instead of just surface realizers, because many people are concerned that the results of such evaluations may not be meaningful. For example Reiter and Sripada (2002) point out that corpus texts are often not of high enough quality to form good reference texts; and Scott and Moore (2007) express concern that metrics will not be able to evaluate many important linguistic properties such as information structure."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11080756,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d7da009f457917aa381619facfa5ffae9329a6e9",
            "isKey": true,
            "numCitedBy": 16627,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations."
            },
            "slug": "Bleu:-a-Method-for-Automatic-Evaluation-of-Machine-Papineni-Roukos",
            "title": {
                "fragments": [],
                "text": "Bleu: a Method for Automatic Evaluation of Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work proposes a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792458"
                        ],
                        "name": "A. Belz",
                        "slug": "A.-Belz",
                        "structuredName": {
                            "firstName": "Anja",
                            "lastName": "Belz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Belz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6760864,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a6b7cf5e1a3e069338498d1c17aa7d46c1ac7e9",
            "isKey": false,
            "numCitedBy": 180,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Two important recent trends in natural language generation are (i) probabilistic techniques and (ii) comprehensive approaches that move away from traditional strictly modular and sequential models. This paper reports experiments in which pcru \u2013 a generation framework that combines probabilistic generation methodology with a comprehensive model of the generation space \u2013 was used to semi-automatically create five different versions of a weather forecast generator. The generators were evaluated in terms of output quality, development time and computational efficiency against (i) human forecasters, (ii) a traditional handcrafted pipelined nlg system and (iii) a halogen-style statistical generator. The most striking result is that despite acquiring all decision-making abilities automatically, the best pcru generators produce outputs of high enough quality to be scored more highly by human judges than forecasts written by experts."
            },
            "slug": "Automatic-generation-of-weather-forecast-texts-Belz",
            "title": {
                "fragments": [],
                "text": "Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Pcru \u2013 a generation framework that combines probabilistic generation methodology with a comprehensive model of the generation space \u2013 was used to semi-automatically create five different versions of a weather forecast generator, resulting in outputs of high enough quality to be scored more highly by human judges than forecasts written by experts."
            },
            "venue": {
                "fragments": [],
                "text": "Natural Language Engineering"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792458"
                        ],
                        "name": "A. Belz",
                        "slug": "A.-Belz",
                        "structuredName": {
                            "firstName": "Anja",
                            "lastName": "Belz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Belz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700894"
                        ],
                        "name": "Albert Gatt",
                        "slug": "Albert-Gatt",
                        "structuredName": {
                            "firstName": "Albert",
                            "lastName": "Gatt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Albert Gatt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 12
                            }
                        ],
                        "text": "For example Belz and Gatt (2008) analyzed correlations between several automatic evaluation metrics and task performance in a referring-expression generation task; they found that there was no significant correlation between any of the automatic metrics they looked at (which included specialized\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8396538,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4448f14bb23723b9866c04b6e713c0188d326426",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present research in which we apply (i) the kind of intrinsic evaluation metrics that are characteristic of current comparative HLT evaluation, and (ii) extrinsic, human task-performance evaluations more in keeping with NLG traditions, to 15 systems implementing a language generation task. We analyse the evaluation results and find that there are no significant correlations between intrinsic and extrinsic evaluation measures for this task."
            },
            "slug": "Intrinsic-vs.-Extrinsic-Evaluation-Measures-for-Belz-Gatt",
            "title": {
                "fragments": [],
                "text": "Intrinsic vs. Extrinsic Evaluation Measures for Referring Expression Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "It is found that there are no significant correlations between intrinsic and extrinsic evaluation measures for this task, and the kind of intrinsic evaluation metrics characteristic of current comparative HLT evaluation are applied."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145869117"
                        ],
                        "name": "S. Sripada",
                        "slug": "S.-Sripada",
                        "structuredName": {
                            "firstName": "Somayajulu",
                            "lastName": "Sripada",
                            "middleNames": [
                                "Gowri"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sripada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144568312"
                        ],
                        "name": "Ehud Reiter",
                        "slug": "Ehud-Reiter",
                        "structuredName": {
                            "firstName": "Ehud",
                            "lastName": "Reiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ehud Reiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27078692"
                        ],
                        "name": "J. Hunter",
                        "slug": "J.-Hunter",
                        "structuredName": {
                            "firstName": "Jim",
                            "lastName": "Hunter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hunter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145138757"
                        ],
                        "name": "Jin Yu",
                        "slug": "Jin-Yu",
                        "structuredName": {
                            "firstName": "Jin",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jin Yu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 91
                            }
                        ],
                        "text": "Independently of the SUMTIME Project, we created a range of statistical generators for the SUMTIME-METEO domain using pCRU generation (probabilistic context-free representational underspecification) (Belz 2008)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 78
                            }
                        ],
                        "text": "The SUMTIME project also created a corpus and data set, called SUMTIME-METEO (Sripada et al. 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 124
                            }
                        ],
                        "text": "For Experiment 1, which focused on content-to-text, we asked three meteorologists5 (who had not contributed to the original SUMTIME-METEO corpus) to rewrite the corpus texts for the 21 dates used in Experiment 1 (each meteorologist rewrote all 21 corpus texts), correcting and improving them as they saw fit; examples are shown in Table 1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 50
                            }
                        ],
                        "text": "We also included the corresponding texts from the SUMTIME-METEO corpus."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 65
                            }
                        ],
                        "text": "We used a randomly selected subset of 21 forecast dates from the SUMTIME-METEO corpus."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 30
                            }
                        ],
                        "text": "3.1.2 pCRU Generators for the SUMTIME-METEO Domain."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 46
                            }
                        ],
                        "text": "2005) and its associated SUMTIME-METEO corpus (Sripada et al. 2003), which were developed at Aberdeen."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 145
                            }
                        ],
                        "text": "In particular, we based our experiments on the SUMTIME system (Sripada et al. 2004; Reiter et al. 2005) and its associated SUMTIME-METEO corpus (Sripada et al. 2003), which were developed at Aberdeen."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 26
                            }
                        ],
                        "text": "3.1.1 SUMTIME Systems and SUMTIME-METEO Corpus."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5651665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a27afcbe78a83644ba4d06e89c693a585f7a37bd",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe SUMTIME-METEO, a parallel corpus of naturally occurring weather forecast texts and their corresponding forecast data; data that the human authors inspected while writing the forecast texts. We have analysed the corpus to acquire knowledge needed to build a text generator for automatically producing textual weather forecasts from numerical weather prediction data. Although parallel corpora are commonly used for the development and evaluation of machine translation technology, it is fairly novel in the text generation community. Our analyses of the corpus, in some cases, produced ambiguous results that are not useful and reflected inconsistencies in the underlying corpus. Despite the internal inconsistencies, the text-data parallel corpus was helpful in generating initial hypotheses, which were then tested with knowledge from other sources. We also describe how we have used the corpus for evaluating our prototype forecast text generator."
            },
            "slug": "Exploiting-a-parallel-TEXT-DATA-corpus-Sripada-Reiter",
            "title": {
                "fragments": [],
                "text": "Exploiting a parallel TEXT - DATA corpus"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A parallel corpus of naturally occurring weather forecast texts and their corresponding forecast data; data that the human authors inspected while writing the forecast texts is described, to acquire knowledge needed to build a text generator for automatically producing textual weather forecasts from numerical weather prediction data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145280410"
                        ],
                        "name": "Sandra Williams",
                        "slug": "Sandra-Williams",
                        "structuredName": {
                            "firstName": "Sandra",
                            "lastName": "Williams",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sandra Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144568312"
                        ],
                        "name": "Ehud Reiter",
                        "slug": "Ehud-Reiter",
                        "structuredName": {
                            "firstName": "Ehud",
                            "lastName": "Reiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ehud Reiter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5803171,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "994923a4e0dd3714160d6534933726befbe97b0e",
            "isKey": false,
            "numCitedBy": 60,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract We describe SkillSum, a Natural Language Generation (NLG) system that generates a personalised feedback report for someone who has just completed a screening assessment of their basic literacy and numeracy skills. Because many SkillSum users have limited literacy, the generated reports must be easily comprehended by people with limited reading skills; this is the most novel aspect of SkillSum, and the focus of this paper. We used two approaches to maximise readability. First, for determining content and structure (document planning), we did not explicitly model readability, but rather followed a pragmatic approach of repeatedly revising content and structure following pilot experiments and interviews with domain experts. Second, for choosing linguistic expressions (microplanning), we attempted to formulate explicitly the choices that enhanced readability, using a constraints approach and preference rules; our constraints were based on corpus analysis and our preference rules were based on psycholinguistic findings. Evaluation of the SkillSum system was twofold: it compared the usefulness of NLG technology to that of canned text output, and it assessed the effectiveness of the readability model. Results showed that NLG was more effective than canned text at enhancing users' knowledge of their skills, and also suggested that the empirical \u2018revise based on experiments and interviews\u2019 approach made a substantial contribution to readability as well as our explicit psycholinguistically inspired models of readability choices."
            },
            "slug": "Generating-basic-skills-reports-for-low-skilled-Williams-Reiter",
            "title": {
                "fragments": [],
                "text": "Generating basic skills reports for low-skilled readers*"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Evaluation of the SkillSum system showed that NLG was more effective than canned text at enhancing users' knowledge of their skills, and it suggested that the empirical \u2018revise based on experiments and interviews\u2019 approach made a substantial contribution to readability."
            },
            "venue": {
                "fragments": [],
                "text": "Natural Language Engineering"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1825424"
                        ],
                        "name": "G. Carenini",
                        "slug": "G.-Carenini",
                        "structuredName": {
                            "firstName": "Giuseppe",
                            "lastName": "Carenini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Carenini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47147237"
                        ],
                        "name": "Johanna D. Moore",
                        "slug": "Johanna-D.-Moore",
                        "structuredName": {
                            "firstName": "Johanna",
                            "lastName": "Moore",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Johanna D. Moore"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2692050,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e842932a274ca5f9d43788895c367cac2ba346e",
            "isKey": false,
            "numCitedBy": 143,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generating-and-evaluating-evaluative-arguments-Carenini-Moore",
            "title": {
                "fragments": [],
                "text": "Generating and evaluating evaluative arguments"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144768924"
                        ],
                        "name": "J. Coch",
                        "slug": "J.-Coch",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Coch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Coch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6755060,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "592a5fcb1f9664ff37f6b54c242198f47a89a3d1",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We believe that it is important from the practical point of view to use natural language generation (NLG) in real world applications. The key benefits are outlined as follows: \u2022 Higher quality of generated texts \u2022 Cost effective maintenance and adaptability \u2022 Usability\u2022(including acceptability by the users). Generally, it takes \"exotic\" (linguists, knowledge engineers) manpower to maintain, and adapt a NLG system. To avoid this problem, we have developed a Knowledge Administration \u2022 station, which is usable by the target population (in our project, weather forecasters). On the other hand, the system is designed to help the forecasters and not replace them. It is able to adapt to each forecaster's style and manage enhancements\u2022 they wish to bring to their texts. With that in mind, the Interactive Generation environment was designed to allow forecasters to modify generated texts in their native language, and then generate weather forecasts in several foreign languages based on those modifications. Interactive\u2022 Generation is a viable alternative to Automatic Translation."
            },
            "slug": "System-Demonstration-Interactive-Generation-and-in-Coch",
            "title": {
                "fragments": [],
                "text": "System Demonstration Interactive Generation and Knowledge Administration in MultiMeteo"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The Interactive Generation environment was designed to allow forecasters to modify generated texts in their native language, and then generate weather forecasts in several foreign languages based on those modifications."
            },
            "venue": {
                "fragments": [],
                "text": "INLG"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35979649"
                        ],
                        "name": "M. Meulen",
                        "slug": "M.-Meulen",
                        "structuredName": {
                            "firstName": "Marian",
                            "lastName": "Meulen",
                            "middleNames": [
                                "van",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Meulen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2249605"
                        ],
                        "name": "R. Logie",
                        "slug": "R.-Logie",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Logie",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Logie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3222547"
                        ],
                        "name": "Y. Freer",
                        "slug": "Y.-Freer",
                        "structuredName": {
                            "firstName": "Yvonne",
                            "lastName": "Freer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39713458"
                        ],
                        "name": "C. Sykes",
                        "slug": "C.-Sykes",
                        "structuredName": {
                            "firstName": "Cindy",
                            "lastName": "Sykes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Sykes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145924900"
                        ],
                        "name": "N. McIntosh",
                        "slug": "N.-McIntosh",
                        "structuredName": {
                            "firstName": "Neil",
                            "lastName": "McIntosh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. McIntosh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27078692"
                        ],
                        "name": "J. Hunter",
                        "slug": "J.-Hunter",
                        "structuredName": {
                            "firstName": "Jim",
                            "lastName": "Hunter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hunter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1072883,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "12c001d2db858795ace898397ec8a963f82c2578",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Volunteer staff from a Neonatal Intensive Care Unit (NICU) were presented with sets of anonymised physiological data recorded over approximately 45\u2009minute periods from former patients. Staff were asked to select medical/nursing actions appropriate for each of the patients whose data were displayed. Data were shown in one of three conditions (a) as multiple line graphs similar to those commonly shown on the ward, or as textual descriptions generated by (b) expert medical/nursing staff or (c) computerised natural language generation (NLG). An overall advantage was found for the human generated text, but NLG resulted in decisions that were at least as good as those for the graphical displays with which staff were familiar. It is suggested that NLG might offer a viable automated approach to removing noise and artefacts in real, complex and dynamic data sets, thereby reducing visual complexity and mental workload, and enhancing decision-making particularly for inexperienced staff. Copyright \u00a9 2008 John Wiley & Sons, Ltd."
            },
            "slug": "When-a-graph-is-poorer-than-100-words:-A-comparison-Meulen-Logie",
            "title": {
                "fragments": [],
                "text": "When a graph is poorer than 100 words: A comparison of computerised natural language generation, human generated descriptions and graphical displays in neonatal intensive care"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is suggested that NLG might offer a viable automated approach to removing noise and artefacts in real, complex and dynamic data sets, thereby reducing visual complexity and mental workload, and enhancing decision-making particularly for inexperienced staff."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717955"
                        ],
                        "name": "James C. Lester",
                        "slug": "James-C.-Lester",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Lester",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James C. Lester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47363174"
                        ],
                        "name": "B. Porter",
                        "slug": "B.-Porter",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Porter",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Porter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5582351,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57b27e9dfa94a2214d906fa67aa98da122907388",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": "To explain complex phenomena, an explanation system must be able to select information from a formal representation of domain knowledge, organize the selected information into multisentential discourse plans, and realize the discourse plans in text. Although recent years have witnessed significant progress in the development of sophisticated computational mechanisms for explanation, empirical results have been limited. This paper reports on a seven-year effort to empirically study explanation generation from semantically rich, large-scale knowledge bases. In particular, it describes KNIGHT, a robust explanation system that constructs multisentential and multi-paragraph explanations from the Biology Knowledge Base, a large-scale knowledge base in the domain of botanical anatomy, physiology, and development. We introduce the Two-Panel evaluation methodology and describe how KNIGHT's performance was assessed with this methodology in the most extensive empirical evaluation conducted on an explanation system. In this evaluation, KNIGHT scored within \"half a grade\" of domain experts, and its performance exceeded that of one of the domain experts."
            },
            "slug": "Developing-and-Empirically-Evaluating-Robust-The-Lester-Porter",
            "title": {
                "fragments": [],
                "text": "Developing and Empirically Evaluating Robust Explanation Generators: The KNIGHT Experiments"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper describes KNIGHT, a robust explanation system that constructs multisentential and multi-paragraph explanations from the Biology Knowledge Base, a large-scale knowledge base in the domain of botanical anatomy, physiology, and development, and introduces the Two-Panel evaluation methodology."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763608"
                        ],
                        "name": "Chris Callison-Burch",
                        "slug": "Chris-Callison-Burch",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Callison-Burch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Callison-Burch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057788"
                        ],
                        "name": "M. Osborne",
                        "slug": "M.-Osborne",
                        "structuredName": {
                            "firstName": "Miles",
                            "lastName": "Osborne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Osborne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755162"
                        ],
                        "name": "Philipp Koehn",
                        "slug": "Philipp-Koehn",
                        "structuredName": {
                            "firstName": "Philipp",
                            "lastName": "Koehn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philipp Koehn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7647892,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "0a1f4cc5e1d7ccdce98c65545bbcccc23a6c16e7",
            "isKey": false,
            "numCitedBy": 720,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We argue that the machine translation community is overly reliant on the Bleu machine translation evaluation metric. We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality, and give two significant counterexamples to Bleu\u2019s correlation with human judgments of quality. This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores."
            },
            "slug": "Re-evaluating-the-Role-of-Bleu-in-Machine-Research-Callison-Burch-Osborne",
            "title": {
                "fragments": [],
                "text": "Re-evaluating the Role of Bleu in Machine Translation Research"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "It is shown that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality, and two significant counterexamples to Bleu\u2019s correlation with human judgments of quality are given."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35313721"
                        ],
                        "name": "S. Bangalore",
                        "slug": "S.-Bangalore",
                        "structuredName": {
                            "firstName": "Srinivas",
                            "lastName": "Bangalore",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bangalore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702447"
                        ],
                        "name": "Owen Rambow",
                        "slug": "Owen-Rambow",
                        "structuredName": {
                            "firstName": "Owen",
                            "lastName": "Rambow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Owen Rambow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143965889"
                        ],
                        "name": "S. Whittaker",
                        "slug": "S.-Whittaker",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Whittaker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Whittaker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17640147,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "70f68fa66713f9c9f5c56f593fa2b3ab6460ba15",
            "isKey": false,
            "numCitedBy": 137,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Certain generation applications may profit from the use of stochastic methods. In developing stochastic methods, it is crucial to be able to quickly assess the relative merits of different approaches or models. In this paper, we present several types of intrinsic (system internal) metrics which we have used for baseline quantitative assessment. This quantitative assessment should then be augmented to a fuller evaluation that examines qualitative aspects. To this end, we describe an experiment that tests correlation between the quantitative metrics and human qualitative judgment. The experiment confirms that intrinsic metrics cannot replace human evaluation, but some correlate significantly with human judgments of quality and understandability and can be used for evaluation during development."
            },
            "slug": "Evaluation-Metrics-for-Generation-Bangalore-Rambow",
            "title": {
                "fragments": [],
                "text": "Evaluation Metrics for Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is confirmed that intrinsic metrics cannot replace human evaluation, but some correlate significantly with human judgments of quality and understandability and can be used for evaluation during development."
            },
            "venue": {
                "fragments": [],
                "text": "INLG"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39055450"
                        ],
                        "name": "M. D. Harris",
                        "slug": "M.-D.-Harris",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Harris",
                            "middleNames": [
                                "Dee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. D. Harris"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 19472917,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "92a28b636581de2733ecf66a4bc92f66a227cfdc",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural language generation technology is mature enough for implementing an NLG system in a commercial environment, but the circumstances differ significantly from building a research system. This paper describes the challenges and rewards of building a commercial NLG component for an electronic medical records system. While the resulting NLG system has been successfully completed, the path to that success could have been somewhat smoother knowing the issues in advance."
            },
            "slug": "Building-a-Large-scale-Commercial-NLG-System-for-an-Harris",
            "title": {
                "fragments": [],
                "text": "Building a Large-scale Commercial NLG System for an EMR"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The challenges and rewards of building a commercial NLG component for an electronic medical records system and the resulting NLG system has been successfully completed."
            },
            "venue": {
                "fragments": [],
                "text": "INLG"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1390051006"
                        ],
                        "name": "I. Langkilde-Geary",
                        "slug": "I.-Langkilde-Geary",
                        "structuredName": {
                            "firstName": "Irene",
                            "lastName": "Langkilde-Geary",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Langkilde-Geary"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152971314"
                        ],
                        "name": "Kevin Knight",
                        "slug": "Kevin-Knight",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Knight",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Knight"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2680971,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0886bd3d1b4fd46928a295a36b5230c4352f699b",
            "isKey": false,
            "numCitedBy": 421,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe novel aspects of a new natural language generator called Nitrogen. This generator has a highly flexible input representation that allows a spectrum of input from syntactic to semantic depth, and shifts the burden of many linguistic decisions to the statistical post-processor. The generation algorithm is compositional, making it efficient, yet it also handles non-compositional aspects of language. Nitrogen's design makes it robust and scalable, operating with lexicons and knowledge bases of one hundred thousand entities."
            },
            "slug": "Generation-that-Exploits-Corpus-Based-Statistical-Langkilde-Geary-Knight",
            "title": {
                "fragments": [],
                "text": "Generation that Exploits Corpus-Based Statistical Knowledge"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Novel aspects of a new natural language generator called Nitrogen are described, which has a highly flexible input representation that allows a spectrum of input from syntactic to semantic depth, and shifts the burden of many linguistic decisions to the statistical post-processor."
            },
            "venue": {
                "fragments": [],
                "text": "COLING-ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2002316"
                        ],
                        "name": "F. Och",
                        "slug": "F.-Och",
                        "structuredName": {
                            "firstName": "Franz",
                            "lastName": "Och",
                            "middleNames": [
                                "Josef"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Och"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5474833,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f12451245667a85d0ee225a80880fc93c71cc8b",
            "isKey": false,
            "numCitedBy": 3304,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria. A general problem of this approach is that there is only a loose relation to the final translation quality on unseen text. In this paper, we analyze various training criteria which directly optimize translation quality. These training criteria make use of recently proposed automatic evaluation metrics. We describe a new algorithm for efficient training an unsmoothed error count. We show that significantly better results can often be obtained if the final evaluation criterion is taken directly into account as part of the training procedure."
            },
            "slug": "Minimum-Error-Rate-Training-in-Statistical-Machine-Och",
            "title": {
                "fragments": [],
                "text": "Minimum Error Rate Training in Statistical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that significantly better results can often be obtained if the final evaluation criterion is taken directly into account as part of the training procedure."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696645"
                        ],
                        "name": "Nizar Habash",
                        "slug": "Nizar-Habash",
                        "structuredName": {
                            "firstName": "Nizar",
                            "lastName": "Habash",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nizar Habash"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11691,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8f7091f9e6d0d17e7968a049c13924b2d2f492b",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the use of a statistical structural N-gram model in the natural language generation component of a Spanish-English generation-heavy hybrid machine translation system. A structural N-gram model captures the relationship between words in a dependency representation without taking into account the overall structure at the phrase level. The model is used together with other components in the system for lexical and structural selection. An evaluation of the machine translation system shows that the use of structural N-grams decreases runtime by 60% with no loss in translation quality."
            },
            "slug": "The-Use-of-a-Structural-N-gram-Language-Model-in-Habash",
            "title": {
                "fragments": [],
                "text": "The Use of a Structural N-gram Language Model in Generation-Heavy Hybrid Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A statistical structural N-gram model is used in the natural language generation component of a Spanish-English generation-heavy hybrid machine translation system for lexical and structural selection and shows that it decreases runtime by 60% with no loss in translation quality."
            },
            "venue": {
                "fragments": [],
                "text": "INLG"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700894"
                        ],
                        "name": "Albert Gatt",
                        "slug": "Albert-Gatt",
                        "structuredName": {
                            "firstName": "Albert",
                            "lastName": "Gatt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Albert Gatt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792458"
                        ],
                        "name": "A. Belz",
                        "slug": "A.-Belz",
                        "structuredName": {
                            "firstName": "Anja",
                            "lastName": "Belz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Belz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726297"
                        ],
                        "name": "Eric Kow",
                        "slug": "Eric-Kow",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Kow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Kow"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 272
                            }
                        ],
                        "text": "\u2026occurred in the recent Generation Challenges evaluations of referring expression generation, which measured the correlations between human assessments of language quality and adequacy of content with task-performance measures (referent identification time and accuracy) (Gatt, Belz, and Kow 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 150
                            }
                        ],
                        "text": "\u2026experiments in which participants were presented with generated referring expressions and asked to identify the target referent (Belz and Gatt 2007; Gatt, Belz, and Kow 2008, 2009); these were carried out in conjunction with shared-task events organized under the Generation Challenges initiative\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 262036,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "899e806a782c0377e12719d051f98969729c65ee",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "The GREC Task at REG '08 required participating systems to select coreference chains to the main subject of short encyclopaedic texts collected from Wikipedia. Three teams submitted a total of 6 systems, and we additionally created four baseline systems. Systems were tested automatically using a range of existing intrinsic metrics. We also evaluated systems extrinsically by applying coreference resolution tools to the outputs and measuring the success of the tools. In addition, systems were tested in a reading/comprehension experiment involving human subjects. This report describes the GREC Task and the evaluation methods, gives brief descriptions of the participating systems, and presents the evaluation results."
            },
            "slug": "The-TUNA-REG-Challenge-2009:-Overview-and-Results-Gatt-Belz",
            "title": {
                "fragments": [],
                "text": "The TUNA-REG Challenge 2009: Overview and Evaluation Results"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "The GREC Task at REG '08 required participating systems to select coreference chains to the main subject of short encyclopaedic texts collected from Wikipedia, and the evaluation methods were described and described."
            },
            "venue": {
                "fragments": [],
                "text": "ENLG"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745798"
                        ],
                        "name": "Barbara Maria Di Eugenio",
                        "slug": "Barbara-Maria-Di-Eugenio",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Di Eugenio",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barbara Maria Di Eugenio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143742138"
                        ],
                        "name": "Michael Glass",
                        "slug": "Michael-Glass",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Glass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Glass"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3306661"
                        ],
                        "name": "Michael J. Trolio",
                        "slug": "Michael-J.-Trolio",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Trolio",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Trolio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18142048,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e53e117b7995f6acab7f4f8b24bc39cddc1116f",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We added a sentence planning component to an existing ITS that teaches students how to troubleshoot mechanical systems. We evaluated the original version of the system and the enhanced one via a user study in which we collected performance, learning and usability metrics. We show that on the whole the enhanced system is better than the original one. We discuss how to use the binomial cumulative distribution to assess cumulative effects."
            },
            "slug": "The-DIAG-experiments:-Natural-Language-Generation-Eugenio-Glass",
            "title": {
                "fragments": [],
                "text": "The DIAG experiments: Natural Language Generation for Intelligent Tutoring Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A sentence planning component to an existing ITS that teaches students how to troubleshoot mechanical systems is added and it is shown that on the whole the enhanced system is better than the original one."
            },
            "venue": {
                "fragments": [],
                "text": "INLG"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145557710"
                        ],
                        "name": "A. Cahill",
                        "slug": "A.-Cahill",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Cahill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cahill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7519068"
                        ],
                        "name": "Josef van Genabith",
                        "slug": "Josef-van-Genabith",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "van Genabith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josef van Genabith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2824242,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "334200f861944bd6e48595669882d64bea0b4adc",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel PCFG-based architecture for robust probabilistic generation based on wide-coverage LFG approximations (Cahill et al., 2004) automatically extracted from treebanks, maximising the probability of a tree given an f-structure. We evaluate our approach using string-based evaluation. We currently achieve coverage of 95.26%, a BLEU score of 0.7227 and string accuracy of 0.7476 on the Penn-II WSJ Section 23 sentences of length \u226420."
            },
            "slug": "Robust-PCFG-Based-Generation-Using-Automatically-Cahill-Genabith",
            "title": {
                "fragments": [],
                "text": "Robust PCFG-Based Generation Using Automatically Acquired LFG Approximations"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A novel PCFG-based architecture for robust probabilistic generation based on wide-coverage LFG approximations (Cahill et al., 2004) automatically extracted from treebanks, maximising the probability of a tree given an f-structure is presented."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792458"
                        ],
                        "name": "A. Belz",
                        "slug": "A.-Belz",
                        "structuredName": {
                            "firstName": "Anja",
                            "lastName": "Belz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Belz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700894"
                        ],
                        "name": "Albert Gatt",
                        "slug": "Albert-Gatt",
                        "structuredName": {
                            "firstName": "Albert",
                            "lastName": "Gatt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Albert Gatt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1546028,
            "fieldsOfStudy": [
                "Psychology",
                "Computer Science"
            ],
            "id": "dd81aab2d438f5dab82dfc8993a8c8f7db538921",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ") Challenge wasthe \ufb01rst shared-task evaluation challenge inthe \ufb01eld of Natural Language Generation.Six teams submitted a total of 22 systems.All submitted systems were tested automat-ically for minimality, uniqueness and \u2018hu-manlikeness\u2019. In addition, the output of 15systems was tested in a task-based exper-iment where subjects were asked to iden-tify referents, and the speed and accuracy ofidenti\ufb01cation was measured. This report de-scribes the"
            },
            "slug": "The-attribute-selection-for-GRE-challenge:-overview-Belz-Gatt",
            "title": {
                "fragments": [],
                "text": "The attribute selection for GRE challenge: overview and evaluation results"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": ") Challenge was the first shared-task evaluation challenge of Natural Language Generation and all submitted systems were tested automat-ically for minimality, uniqueness and \u2018hu-manlikeness\u2019."
            },
            "venue": {
                "fragments": [],
                "text": "MTSUMMIT"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730318"
                        ],
                        "name": "F. Portet",
                        "slug": "F.-Portet",
                        "structuredName": {
                            "firstName": "Fran\u00e7ois",
                            "lastName": "Portet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Portet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144568312"
                        ],
                        "name": "Ehud Reiter",
                        "slug": "Ehud-Reiter",
                        "structuredName": {
                            "firstName": "Ehud",
                            "lastName": "Reiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ehud Reiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27078692"
                        ],
                        "name": "J. Hunter",
                        "slug": "J.-Hunter",
                        "structuredName": {
                            "firstName": "Jim",
                            "lastName": "Hunter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hunter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145869117"
                        ],
                        "name": "S. Sripada",
                        "slug": "S.-Sripada",
                        "structuredName": {
                            "firstName": "Somayajulu",
                            "lastName": "Sripada",
                            "middleNames": [
                                "Gowri"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sripada"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 47
                            }
                        ],
                        "text": "For example, the BabyTalk project at Aberdeen (Portet et al. 2009), which is attempting to create a set of NLG systems which can generate textual summaries of clinical data about babies in a neonatal intensive care unit (NICU), is a collaboration between medical researchers, psychologists, computer\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 136
                            }
                        ],
                        "text": "Although a number of previous studies have analyzed correlations between human judgments\n\u2217 Department of Computing Science, University of Aberdeen, UK."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 5
                            }
                        ],
                        "text": "BT45 (Portet et al. 2009) (which is one of the BabyTalk systems) was evaluated for its decision-support effectiveness, using the \u201cpsychologist\u201d methodology described earlier (van der Meulen et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "We carried out an off-ward task-based evaluation of BT45 using the \u201cpsychologist\u201d methodology (van der Meulen et al. 2009), and we would like to think that the results of this evaluation would correlate with the results of a medical effectiveness evaluation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 142
                            }
                        ],
                        "text": "An Investigation into the Validity of Some Metrics for Automatically Evaluating Natural Language Generation Systems\nEhud Reiter\u2217\nUniversity of Aberdeen\nAnja Belz\u2217\u2217\nUniversity of Brighton\nThere is growing interest in using automatically computed corpus-based evaluation metrics to evaluate Natural Language Generation (NLG) systems, because these are often considerably cheaper than the human-based evaluations which have traditionally been used in NLG."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 6
                            }
                        ],
                        "text": "BT45 (Portet et al. 2009) (which is one of the BabyTalk systems) was evaluated for its decision-support effectiveness, using the \u201cpsychologist\u201d methodology described earlier (van der Meulen et al. 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 110
                            }
                        ],
                        "text": "The STOP evaluation cost UK\u00a375,000, and required 20 months to design, carry out, and analyze; the SKILLSUM and BT45 evaluations (which are perhaps more typical) cost about UK\u00a320,000 over six months."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 46
                            }
                        ],
                        "text": "For example, the BabyTalk project at Aberdeen (Portet et al. 2009), which is attempting to create a set of NLG systems which can generate textual summaries of clinical data about babies in a neonatal intensive care unit (NICU), is a collaboration between medical researchers, psychologists, computer scientists, and a commercial software house."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 191
                            }
                        ],
                        "text": "In particular, we based our experiments on the SUMTIME system (Sripada et al. 2004; Reiter et al. 2005) and its associated SUMTIME-METEO corpus (Sripada et al. 2003), which were developed at Aberdeen."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 262
                            }
                        ],
                        "text": "In this respect the practice in computational linguistics is perhaps closer to psychology, where (for example) multiple hypothesis corrections are less common than they are in medicine; indeed a textbook on statistics for psychologists used at the University of Aberdeen does not even mention the topic."
                    },
                    "intents": []
                }
            ],
            "corpusId": 36379,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9bf6bb989136bfed355a0b0433329b58dd4ca52b",
            "isKey": false,
            "numCitedBy": 193,
            "numCiting": 139,
            "paperAbstract": {
                "fragments": [],
                "text": "Intensive care is becoming increasingly complex. If mistakes are to be avoided, there is a need for the large amount of clinical data to be presented effectively to the medical staff. Although the most common approach is to present the data graphically, it has been shown that textual summarisation can lead to improved decision making. As the first step in the BabyTalk project, a prototype is being developed which will generate a textual summary of 45 minutes of continuous physiological signals and discrete events (e.g.: equipment settings and drug administration). Its architecture brings together techniques from the different areas of signal analysis, medical reasoning, and natural language generation. Although the current system is still being improved, it is powerful enough to generate meaningful texts containing the most relevant information. This prototype will be extended to summarize several hours of data and to include clinical interpretation."
            },
            "slug": "Automatic-Generation-of-Textual-Summaries-from-Care-Portet-Reiter",
            "title": {
                "fragments": [],
                "text": "Automatic Generation of Textual Summaries from Neonatal Intensive Care Data"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A prototype is being developed which will generate a textual summary of 45 minutes of continuous physiological signals and discrete events, which brings together techniques from the different areas of signal analysis, medical reasoning, and natural language generation."
            },
            "venue": {
                "fragments": [],
                "text": "AIME"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2122755"
                        ],
                        "name": "H. Dang",
                        "slug": "H.-Dang",
                        "structuredName": {
                            "firstName": "Hoa",
                            "lastName": "Dang",
                            "middleNames": [
                                "Trang"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Dang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15907275,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "81cee0ddf91776053db3f838d1009606aacca7ec",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The Document Understanding Conference (DUC) 2005 evaluation had a single user-oriented, question-focused summarization task, which was to synthesize from a set of 25--50 documents a well-organized, fluent answer to a complex question. The evaluation shows that the best summarization systems have difficulty extracting relevant sentences in response to complex questions (as opposed to representative sentences that might be appropriate to a generic summary). The relatively generous allowance of 250 words for each answer also reveals how difficult it is for current summarization systems to produce fluent text from multiple documents."
            },
            "slug": "DUC-2005:-Evaluation-of-Question-Focused-Systems-Dang",
            "title": {
                "fragments": [],
                "text": "DUC 2005: Evaluation of Question-Focused Summarization Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The evaluation shows that the best summarization systems have difficulty extracting relevant sentences in response to complex questions (as opposed to representative sentences that might be appropriate to a generic summary)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145869117"
                        ],
                        "name": "S. Sripada",
                        "slug": "S.-Sripada",
                        "structuredName": {
                            "firstName": "Somayajulu",
                            "lastName": "Sripada",
                            "middleNames": [
                                "Gowri"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sripada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144568312"
                        ],
                        "name": "Ehud Reiter",
                        "slug": "Ehud-Reiter",
                        "structuredName": {
                            "firstName": "Ehud",
                            "lastName": "Reiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ehud Reiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2539814"
                        ],
                        "name": "I. Davy",
                        "slug": "I.-Davy",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Davy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Davy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39978913"
                        ],
                        "name": "Kristian Nilssen",
                        "slug": "Kristian-Nilssen",
                        "structuredName": {
                            "firstName": "Kristian",
                            "lastName": "Nilssen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristian Nilssen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 63
                            }
                        ],
                        "text": "In particular, we based our experiments on the SUMTIME system (Sripada et al. 2004; Reiter et al. 2005) and its associated SUMTIME-METEO corpus (Sripada et al. 2003), which were developed at Aberdeen."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 62
                            }
                        ],
                        "text": "In particular, we based our experiments on the SUMTIME system (Sripada et al. 2004; Reiter et al. 2005) and its associated SUMTIME-METEO corpus (Sripada et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 162
                            }
                        ],
                        "text": "The SUMTIME system has been used operationally to produce draft weather forecasts; these are post-edited by meteorologists before they are released to end users (Sripada et al. 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3006515,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b919141d83dcb3d77104b9a7914ebcf4dc8d8d09",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "SUMTIME-MOUSAM is a Natural Language Generation (NLG) system that produces textual weather forecasts for offshore oilrigs from Numerical Weather Prediction (NWP) data. It has been used for the past year by Weathernews (UK) Ltd for producing 150 draft forecasts per day, which are then post-edited by forecasters before being released to end-users. In this paper, we describe how the system works, how it is used at Weathernews and finally some lessons we learnt from building, installing and maintaining SUMTIME-MOUSAM. One important lesson has been that using NLG technology improves maintainability although the biggest maintenance work actually involved changing data formats at the I/O interfaces. We also found our system being used by forecasters in unexpected ways for understanding and editing data. We conclude that the success of a technology owes as much to its functional superiority as to its suitability to the various stakeholders such as developers and users."
            },
            "slug": "Lessons-from-Deploying-NLG-Technology-for-Marine-Sripada-Reiter",
            "title": {
                "fragments": [],
                "text": "Lessons from Deploying NLG Technology for Marine Weather Forecast Text Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "It is concluded that the success of a technology owes as much to its functional superiority as to its suitability to the various stakeholders such as developers and users."
            },
            "venue": {
                "fragments": [],
                "text": "ECAI"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700894"
                        ],
                        "name": "Albert Gatt",
                        "slug": "Albert-Gatt",
                        "structuredName": {
                            "firstName": "Albert",
                            "lastName": "Gatt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Albert Gatt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792458"
                        ],
                        "name": "A. Belz",
                        "slug": "A.-Belz",
                        "structuredName": {
                            "firstName": "Anja",
                            "lastName": "Belz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Belz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726297"
                        ],
                        "name": "Eric Kow",
                        "slug": "Eric-Kow",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Kow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Kow"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9935917,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d4584ae905c7748d1926b1a7b784f7c6a5eabef9",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The TUNA Challenge was a set of three shared tasks at REG\u201908, all of which used data from the TUNA Corpus. The three tasks covered attribute selection for referring expressions (TUNA-AS), realisation (TUNA-R) and end-toend referring expression generation (TUNAREG). 8 teams submitted a total of 33 systems to the three tasks, with an additional submission to the Open Track. The evaluation used a range of automatically computed measures. In addition, an evaluation experiment was carried out using the peer outputs for the TUNAREG task. This report describes each task and the evaluation methods used, and presents the evaluation results."
            },
            "slug": "The-TUNA-Challenge-2008:-Overview-and-Evaluation-Gatt-Belz",
            "title": {
                "fragments": [],
                "text": "The TUNA Challenge 2008: Overview and Evaluation Results"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "The TUNA Challenge was a set of three shared tasks at REG\u201908 that covered attribute selection for referring expressions, realisation and end-toend referring expression generation, and the evaluation used a range of automatically computed measures."
            },
            "venue": {
                "fragments": [],
                "text": "INLG"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144568312"
                        ],
                        "name": "Ehud Reiter",
                        "slug": "Ehud-Reiter",
                        "structuredName": {
                            "firstName": "Ehud",
                            "lastName": "Reiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ehud Reiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145869117"
                        ],
                        "name": "S. Sripada",
                        "slug": "S.-Sripada",
                        "structuredName": {
                            "firstName": "Somayajulu",
                            "lastName": "Sripada",
                            "middleNames": [
                                "Gowri"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sripada"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13185513,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bf24f89f317c98f1c2ef794df0904a04ee891a18",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an empirical corpus study of the meaning and usage of time phrases in weather forecasts; this is based on a novel corpus analysis technique where we align phrases from the forecast text with data extracted from a numerical weather simulation. Previous papers have summarised this analysis and discussed the substantial variations we discovered among individual writers, which was perhaps our most surprising finding. In this paper we describe our analysis procedure and results in considerably more detail, and also discuss our current work on using parallel text-data corpora to learn the meanings of other types of words."
            },
            "slug": "Learning-the-Meaning-and-Usage-of-Time-Phrases-from-Reiter-Sripada",
            "title": {
                "fragments": [],
                "text": "Learning the Meaning and Usage of Time Phrases from a Parallel Text-Data Corpus"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "An empirical corpus study of the meaning and usage of time phrases in weather forecasts is presented, based on a novel corpus analysis technique where phrases from the forecast text are aligned with data extracted from a numerical weather simulation."
            },
            "venue": {
                "fragments": [],
                "text": "HLT-NAACL 2003"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690152"
                        ],
                        "name": "Amanda Stent",
                        "slug": "Amanda-Stent",
                        "structuredName": {
                            "firstName": "Amanda",
                            "lastName": "Stent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amanda Stent"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15337535,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4754c9e32331fcc531730e9ecb8776d05662260a",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we evaluate the feasibility of automatically acquiring surface realizers from corpora using general-purpose parsing tools and lexicons. We present a basic architecture for acquiring a generation grammar, describe a surface realizer that uses grammars developed in this way, and present a set of experiments on different corpora that highlight possible improvements in our approach."
            },
            "slug": "Building-Surface-Realizers-Automatically-from-\u2217-and-Stent",
            "title": {
                "fragments": [],
                "text": "Building Surface Realizers Automatically from Corpora \u2217 Huayan Zhong and"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A basic architecture for acquiring a generation grammar is presented, a surface realizer that uses grammars developed in this way is described, and a set of experiments on different corpora that highlight possible improvements in this approach are presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067635629"
                        ],
                        "name": "Tomasz Marciniak",
                        "slug": "Tomasz-Marciniak",
                        "structuredName": {
                            "firstName": "Tomasz",
                            "lastName": "Marciniak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomasz Marciniak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31380436"
                        ],
                        "name": "M. Strube",
                        "slug": "M.-Strube",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Strube",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Strube"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11909237,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "81346ab6d89319848e6ddec68763a431c99ae9e0",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present an application of machine learning to generating natural language route directions. We use the TAG formalism to represent the structure of the generated texts and split the generation process into a number of individual tasks which can be modeled as classification problems. To solve each of these tasks we apply corpus-trained classifiers relying on semantic and contextual features, determined for each task in a feature selection procedure."
            },
            "slug": "Classification-Based-Generation-Using-TAG-Marciniak-Strube",
            "title": {
                "fragments": [],
                "text": "Classification-Based Generation Using TAG"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "An application of machine learning to generating natural language route directions is presented and the TAG formalism is used to represent the structure of the generated texts and a number of individual tasks which can be modeled as classification problems are solved."
            },
            "venue": {
                "fragments": [],
                "text": "INLG"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3115414"
                        ],
                        "name": "A. Nenkova",
                        "slug": "A.-Nenkova",
                        "structuredName": {
                            "firstName": "Ani",
                            "lastName": "Nenkova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Nenkova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703046"
                        ],
                        "name": "R. Passonneau",
                        "slug": "R.-Passonneau",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Passonneau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Passonneau"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1046281,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "70cb232a6e391bfa49b0441a9956820c52ec32f2",
            "isKey": false,
            "numCitedBy": 609,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an empirically grounded method for evaluating content selection in summarization. It incorporates the idea that no single best model summary for a collection of documents exists. Our method quantifies the relative importance of facts to be conveyed. We argue that it is reliable, predictive and diagnostic, thus improves considerably over the shortcomings of the human evaluation method currently used in the Document Understanding Conference."
            },
            "slug": "Evaluating-Content-Selection-in-Summarization:-The-Nenkova-Passonneau",
            "title": {
                "fragments": [],
                "text": "Evaluating Content Selection in Summarization: The Pyramid Method"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is argued that the method presented is reliable, predictive and diagnostic, thus improves considerably over the shortcomings of the human evaluation method currently used in the Document Understanding Conference."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145513579"
                        ],
                        "name": "R. M. Young",
                        "slug": "R.-M.-Young",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Young",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. M. Young"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14726085,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ac3a00b7624d9f13aa63d2311570a362f2477401",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Using-Grice's-maxim-of-Quantity-to-select-the-of-Young",
            "title": {
                "fragments": [],
                "text": "Using Grice's maxim of Quantity to select the content of plan descriptions"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3093240"
                        ],
                        "name": "E. Goldberg",
                        "slug": "E.-Goldberg",
                        "structuredName": {
                            "firstName": "Eli",
                            "lastName": "Goldberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Goldberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103569282"
                        ],
                        "name": "N. Driedger",
                        "slug": "N.-Driedger",
                        "structuredName": {
                            "firstName": "Norbert",
                            "lastName": "Driedger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Driedger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2840130"
                        ],
                        "name": "R. Kittredge",
                        "slug": "R.-Kittredge",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Kittredge",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kittredge"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 9709337,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "c671abec9b5e2a7670389bc19e35ecdd7aabdfad",
            "isKey": false,
            "numCitedBy": 307,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Improved numerical weather prediction simulations have led weather services to examine how and where human forecasters add value to forecast production. The Forecast Production Assistant (FPA) was developed with that in mind. The authors discuss the Forecast Generator (FOG), the first application developed on the FPA. FOG is a bilingual report generator that produces routine and special purpose forecast directly from the FPA's graphical weather predictions. Using rules and a natural-language generator, FOG converts weather maps into forecast text. The natural-language issues involved are relevant to anyone designing a similar system.<<ETX>>"
            },
            "slug": "Using-natural-language-processing-to-produce-Goldberg-Driedger",
            "title": {
                "fragments": [],
                "text": "Using natural-language processing to produce weather forecasts"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The authors discuss the Forecast Generator (FOG), the first application developed on the FPA, which is a bilingual report generator that produces routine and special purpose forecast directly from the Fpa's graphical weather predictions."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Expert"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145765621"
                        ],
                        "name": "M. Molina",
                        "slug": "M.-Molina",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Molina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Molina"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690152"
                        ],
                        "name": "Amanda Stent",
                        "slug": "Amanda-Stent",
                        "structuredName": {
                            "firstName": "Amanda",
                            "lastName": "Stent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amanda Stent"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17843363,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d38251ff64ac64d3242fec4db06e5fc99cea8c7c",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "In this article we describe a method for automatically generating text summaries of data corresponding to traces of spatial movement in geographical areas. The method can help humans to understand large data streams, such as the amounts of GPS data recorded by a variety of sensors in mobile phones, cars, etc. We describe the knowledge representations we designed for our method and the main components of our method for generating the summaries: a discourse planner, an abstraction module and a text generator. We also present evaluation results that show the ability of our method to generate certain types of geospatial and temporal descriptions."
            },
            "slug": "A-Knowledge-Based-Method-for-Generating-Summaries-Molina-Stent",
            "title": {
                "fragments": [],
                "text": "A Knowledge-Based Method for Generating Summaries of Spatial Movement in Geographic Areas"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "A method for automatically generating text summaries of data corresponding to traces of spatial movement in geographical areas that can help humans to understand large data streams, such as the amounts of GPS data recorded by a variety of sensors in mobile phones, cars, etc."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Artif. Intell. Tools"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792458"
                        ],
                        "name": "A. Belz",
                        "slug": "A.-Belz",
                        "structuredName": {
                            "firstName": "Anja",
                            "lastName": "Belz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Belz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 37
                            }
                        ],
                        "text": "This is a major concern (as noted by Belz 2009) because we also do not know how well human ratings predict task-effectiveness; in other words, the fact that NIST scores predict human clarity ratings of NLG texts does not guarantee that NIST scores will predict task effectiveness, because we do not\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15247793,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "b35f39478b9e1eab938620db0ac3ac093b2958a0",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "A regular fixture on the mid 1990s international research seminar circuit was the billion-neuron artificial brain talk. The idea behind this project was simple: in order to create artificial intelligence, what was needed first of all was a very large artificial brain; if a big enough set of interconnected modules of neurons could be implemented, then it would be possible to evolve mammalian-level behavior with current computational- neuron technology. The talk included progress reports on the current size of the artificial brain, its structure, update rate, and power consumption, and explained how intelli- gent behavior was going to develop by mechanisms simulating biological evolution. What the talk didnt mention was what kind of functionality the team had so far managed to evolve, and so the first comment at the end of the talk was inevitably nice work, but have you actually done anything with the brain yet?1 In human language technology (HLT) research, we currently report a range of evaluation scores that measure and assess various aspects of systems, in particular the similarity of their outputs to samples of human language or to human-produced gold- standard annotations, but are we leaving ourselves open to the same question as the billion-neuron artificial brain researchers?"
            },
            "slug": "That's-Nice-What-Can-You-Do-With-It-Belz",
            "title": {
                "fragments": [],
                "text": "That's Nice What Can You Do With It?"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The talk included progress reports on the current size of the artificial brain, its structure, update rate, and power consumption, and explained how intelli- gent behavior was going to develop by mechanisms simulating biological evolution."
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35101246"
                        ],
                        "name": "Anna S. Law",
                        "slug": "Anna-S.-Law",
                        "structuredName": {
                            "firstName": "Anna",
                            "lastName": "Law",
                            "middleNames": [
                                "S."
                            ],
                            "suffix": "PhD"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anna S. Law"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3222547"
                        ],
                        "name": "Y. Freer",
                        "slug": "Y.-Freer",
                        "structuredName": {
                            "firstName": "Yvonne",
                            "lastName": "Freer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27078692"
                        ],
                        "name": "J. Hunter",
                        "slug": "J.-Hunter",
                        "structuredName": {
                            "firstName": "Jim",
                            "lastName": "Hunter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hunter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2249605"
                        ],
                        "name": "R. Logie",
                        "slug": "R.-Logie",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Logie",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Logie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145924900"
                        ],
                        "name": "N. McIntosh",
                        "slug": "N.-McIntosh",
                        "structuredName": {
                            "firstName": "Neil",
                            "lastName": "McIntosh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. McIntosh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2093190276"
                        ],
                        "name": "J. Quinn",
                        "slug": "J.-Quinn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Quinn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Quinn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 59
                            }
                        ],
                        "text": "To evaluate this, they would like to do a study similar to Law et al. (2005); that is, show medical subjects textual summaries (as well as standard graphical visualizations as a control) in a controlled \u201coff-ward\u201d context, ask them to make a treatment decision, and compare this decision against a\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "Law et al. (2005), who worked in the same domain (NICU) as BabyTalk, conducted an off-ward decision-support evaluation which compared human-written text summaries and graphical visualizations of clinical data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5569544,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "b28ab66dff602db272f6cff218ec34cf7ec3898c",
            "isKey": false,
            "numCitedBy": 101,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Objective. To compare expert-generated textual summaries of physiological data with trend graphs, in terms of their ability to support neonatal Intensive Care Unit (ICU) staff in making decisions when presented with medical scenarios. Methods. Forty neonatal ICU staff were recruited for the experiment, eight from each of five groups \u2013 junior, intermediate and senior nurses, junior and senior doctors. The participants were presented with medical scenarios on a computer screen, and asked to choose from a list of 18 possible actions those they thought were appropriate. Half of the scenarios were presented as trend graphs, while the other half were presented as passages of text. The textual summaries had been generated by two human experts and were intended to describe the physiological state of the patient over a short period of time (around 40 minutes) but not to interpret it. Results. In terms of the content of responses there was a clear advantage for the Text condition, with participants tending to choose more of the appropriate actions when the information was presented as text rather than as graphs. In terms of the speed of response there was no difference between the Graphs and Text conditions. There was no significant difference between the staff groups in terms of speed or content of responses. In contrast to the objective measures of performance, the majority of participants reported a subjective preference for the Graphs condition. Conclusions. In this experimental task, participants performed better when presented with a textual summary of the medical scenario than when it was presented as a set of trend graphs. If the necessary algorithms could be developed that would allow computers automatically to generate descriptive summaries of physiological data, this could potentially be a useful feature of decision support tools in the intensive care unit."
            },
            "slug": "A-Comparison-of-Graphical-and-Textual-Presentations-Law-Freer",
            "title": {
                "fragments": [],
                "text": "A Comparison of Graphical and Textual Presentations of Time Series Data to Support Medical Decision Making in the Neonatal Intensive Care Unit"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "In this experimental task, participants performed better when presented with a textual summary of the medical scenario than when it was presented as a set of trend graphs, which could be a useful feature of decision support tools in the intensive care unit."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Clinical Monitoring and Computing"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7999276"
                        ],
                        "name": "Hyungsub Shim",
                        "slug": "Hyungsub-Shim",
                        "structuredName": {
                            "firstName": "Hyungsub",
                            "lastName": "Shim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hyungsub Shim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2673061"
                        ],
                        "name": "T. Grabowski",
                        "slug": "T.-Grabowski",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Grabowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Grabowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 220579796,
            "fieldsOfStudy": [
                "Linguistics",
                "Psychology"
            ],
            "id": "4b230e3a46acf172b4b37ea8805fad668ccb23f5",
            "isKey": false,
            "numCitedBy": 188,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Comprehension is the aspect of cognition in which information is retrieved and consciously integrated. It occurs between the input of perception and memory, and the output of language and executive functioning. This article focuses on disorders of comprehension of linguistic information and semantic knowledge. Comprehension of speech begins with the interpretation of acoustic-phonetic input as word forms. These concepts to which the word forms are semantically associated must then be retrieved, in parallel with interpretation of word order and grammatical marking, to achieve comprehension of discourse. Neural systems for semantic memory are closely related to those for lexical processing. Disorders of language comprehension and semantic knowledge for concrete entities give insight into the relationship of these processes to neural systems."
            },
            "slug": "COMPREHENSION-Shim-Grabowski",
            "title": {
                "fragments": [],
                "text": "COMPREHENSION"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "D disorders of language comprehension and semantic knowledge for concrete entities give insight into the relationship of these processes to neural systems."
            },
            "venue": {
                "fragments": [],
                "text": "Continuum"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143833379"
                        ],
                        "name": "G. Lip",
                        "slug": "G.-Lip",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Lip",
                            "middleNames": [
                                "Y.",
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lip"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 517362,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "7613fbb515df2e6a268a3a743eff55aadf74512a",
            "isKey": false,
            "numCitedBy": 549,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Do you need to read published papers? Or are you a scientific paper non-reader or recluse? This book (very thoughtfully) starts off by asking whether you need to read this book\u2014how many textbooks actually ask you whether you need to use/read/buy it? The need for such a book is certainly there, as this excellent little book is intended to help existing readers read, and actually interpret, medical papers better. Current non-readers and scientific recluses may even be encouraged to open medical journals for once! The book provides an excellent practical and pragmatic approach to critical analysis of much of the uninspiring and unread published literature (which often makes you wonder how it got into print in the first place!). There is a systematic discussion on evidence-based medicine and a thoughtful practical section on how to search the medical literature. Despite the age of the Internet and computerised databases, even the most experienced Medline surfer often only manages to find approximately a third of the published material on a particular subject\u2014handy tips are provided to improve searches, to increase one\u2019s gain and to reduce eye strain or repetitive strain injury from a long, tiring session at the Medline computer terminal. The book describes the various sections of a published paper, including appraisal of the nature of the study and statistics for the non-statistician. It then discusses what you would hope to gain from reading a particular paper, including papers that report drug trials, diagnostic screening tests, systematic reviews and guidelines. It also has information on economic analyses and qualitative research. Finally, it gives some examples of how to implement evidence-based findings. Since much of clinical medicine is still not evidence-based, perhaps this goes some way to rectify this appalling state of affairs. I also found the Appendix with a checklist for finding, appraising and implementing evidence fairly helpful. Readers of this excellent book who are like me \u2014 struggling to understand much of the published literature and also to produce an intelligible published paper every so often\u2014this book not only helps you read a paper but tries to make you a better paper writer as well. Perhaps my understanding of evidence-based medicine will improve after reading this book, and make me a better teacher and researcher. Perhaps it will make me reject more of the papers submitted to the journal I help edit! At the cost of \u00a314.95, this book is an absolute bargain and it fulfils its advertised aim of being a compressed introduction to the usefulness and potential applications of evidence-based medicine in the clinical setting. I recommend it wholeheartedly as an obligatory read."
            },
            "slug": "How-the-Read-a-Paper:-The-Basics-of-Evidence-Based-Lip",
            "title": {
                "fragments": [],
                "text": "How the Read a Paper: The Basics of Evidence Based Medicine"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This book not only helps you read a paper but tries to make you a better paper writer as well and fulfils its advertised aim of being a compressed introduction to the usefulness and potential applications of evidence-based medicine in the clinical setting."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Human Hypertension"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728899"
                        ],
                        "name": "K. Binsted",
                        "slug": "K.-Binsted",
                        "structuredName": {
                            "firstName": "Kim",
                            "lastName": "Binsted",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Binsted"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49856585"
                        ],
                        "name": "H. Pain",
                        "slug": "H.-Pain",
                        "structuredName": {
                            "firstName": "Helen",
                            "lastName": "Pain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Pain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34739267"
                        ],
                        "name": "G. Ritchie",
                        "slug": "G.-Ritchie",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Ritchie",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Ritchie"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58499371,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "ee4f6821ff592a517eb18cdf1c7211724e1c601a",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We have developed a formal model of certain types of riddles, and implemented it in a computer program, JAPE, which generates simple punning riddles. In order to test the model, we evaluated the behaviour of the program, by having 120 children aged eight to eleven years old rate JAPE-generated texts, human-generated texts, and non-joke texts for \"jokiness\" and funniness. This confirmed that JAPE's output texts are indeed jokes, and that there is no significant difference in funniness or jokiness between JAPE\"s most comprehensible texts and published human-generated jokes."
            },
            "slug": "Children's-evaluation-of-computer-generated-punning-Binsted-Pain",
            "title": {
                "fragments": [],
                "text": "Children's evaluation of computer-generated punning riddles"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is confirmed that JAPE's output texts are indeed jokes, and that there is no significant difference in funniness or jokiness between JAPE\"s most comprehensible texts and published human-generated jokes."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145032158"
                        ],
                        "name": "J. Brooks",
                        "slug": "J.-Brooks",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Brooks",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Brooks"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 71114786,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "ba7684a2a2b46a1ed9fc5bc0d18bda2de4eb455e",
            "isKey": false,
            "numCitedBy": 1514,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Why-most-published-research-findings-are-false:-JP,-Brooks",
            "title": {
                "fragments": [],
                "text": "Why most published research findings are false: Ioannidis JP, Department of Hygiene and Epidemiology, University of Ioannina School of Medicine, Ioannina, Greece"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3263707"
                        ],
                        "name": "J. Oberlander",
                        "slug": "J.-Oberlander",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Oberlander",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Oberlander"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 80
                            }
                        ],
                        "text": "Because writers do not always produce optimal texts from a reader\u2019s perspective (Oberlander 1998; Reiter et al. 2005), a metric which is a good evaluator of how likely it is that a text has been written by a human writer is not necessarily a good predictor of how effective and useful the text is\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1893569,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cf8597ad5b474a6571aec74e5609247d3fd32c5d",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Parce que les textes sont generes pour etre lus, l'A. suggere ici que les algorithmes permettant de generer les syntagmes nominaux definis devraient etre bases sur des observations concernant la production du langage humain plutot que sur une observation stricte des maximes de Grice. Il appelle ceci la maxime de Spike Lee : Do the right thing, dans laquelle right est ce qui est humain et simple. Il montre que, lorsqu'on genere des expressions referentielles, on ne peut pas toujours dire si la meilleure chose a faire est d'imiter les preferences de ceux qui produisent le langage ou de ceux qui le percoivent, parce que ces preferences sont souvent en conflit. Il pense ainsi que, avant que soit developpe un point de vue plus sophistique sur les attentes des locuteurs et des auditeurs, les concepteurs de systemes de generation en langue naturelle devraient adherer a la maxime de Spike Lee : en depit de ses limites connues, elle produit en effet des resultats plus naturels que ceux obtenus par une stricte interpretation les maximes de Grice"
            },
            "slug": "Do-the-Right-Thing-...-but-Expect-the-Unexpected-Oberlander",
            "title": {
                "fragments": [],
                "text": "Do the Right Thing ... but Expect the Unexpected"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Les concepteurs de systemes de generation en langue naturelle devraient adherer a la maxime de Spike Lee : en depit de ses limites connues, elle produit en effet des resultats plus naturels that ceux obtenus par une stricte interpretation les maximes de Grice."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46310969"
                        ],
                        "name": "R. Kaplan",
                        "slug": "R.-Kaplan",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Kaplan",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kaplan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5797604"
                        ],
                        "name": "D. Saccuzzo",
                        "slug": "D.-Saccuzzo",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "Saccuzzo",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Saccuzzo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 142904448,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "2ad97836619e9d337a29091e1b464ee5593672d7",
            "isKey": false,
            "numCitedBy": 1285,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Easy-to-read and accessible, this book effectively communicates the excitement and dynamics of the field of psychological testing. Robert Kaplan and Dennis Saccuzzo provide students with a current analysis of the most widely used psychological tests in schools, professional training programs, business, industry, the military, and clinical settings. As students read through the book, they get a clear picture of how psychological tests are constructed, how they are used, and how an understanding of them can make a difference in their careers and everyday lives. Students will get a very real sense of how psychological tests are constructed, how they are used, and how an understanding of them can make a difference in their careers and everyday lives. Comprehensive and accurate, yet interesting and personally relevant, this book gets and keeps students' attention through the use of informal discussions and real-life examples."
            },
            "slug": "Psychological-Testing:-Principles,-Applications,-Kaplan-Saccuzzo",
            "title": {
                "fragments": [],
                "text": "Psychological Testing: Principles, Applications, and Issues"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145834279"
                        ],
                        "name": "W. Thompson",
                        "slug": "W.-Thompson",
                        "structuredName": {
                            "firstName": "Warren G.",
                            "lastName": "Thompson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Thompson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144155945"
                        ],
                        "name": "M. Lipkin",
                        "slug": "M.-Lipkin",
                        "structuredName": {
                            "firstName": "Mack",
                            "lastName": "Lipkin",
                            "middleNames": [
                                "Jr."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lipkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053478099"
                        ],
                        "name": "D. A. Gilbert",
                        "slug": "D.-A.-Gilbert",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Gilbert",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. A. Gilbert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2091336634"
                        ],
                        "name": "Richard A. Guzzo",
                        "slug": "Richard-A.-Guzzo",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Guzzo",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard A. Guzzo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069969782"
                        ],
                        "name": "Loriann Roberson",
                        "slug": "Loriann-Roberson",
                        "structuredName": {
                            "firstName": "Loriann",
                            "lastName": "Roberson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Loriann Roberson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 403668,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "id": "7cea805d2930e025ff0ee7b2f86ec3ab492c56d7",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The American Board of Internal Medicine suggests use of a standard form to rate residents on nine dimensions (such as clinical judgment and overall clinical competence) on a scale of 1 to 9. The authors examined the psychometric evidence for reliability and validity of 1,039 ratings of 85 residents by 135 attendings in a single internal medicine residency program. Of these ratings, 95.6% were from 6 to 9. Factor analysis revealed that high correlations among the nine dimensions (r ranged from 0.72 to 0.92) resulted from a single global factor accounting for 86% of the variance. The study also examined whether the form reliably distinguishes among residents scoring between 6 and 9. Agreement among attendings rating the same individual was weak (average reliability=0.64, by the method of James). The rating method fails to discriminate dimensions of clinical care and has low reliability for distinguishing among competent residents."
            },
            "slug": "Evaluating-evaluation-Thompson-Lipkin",
            "title": {
                "fragments": [],
                "text": "Evaluating evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The psychometric evidence for reliability and validity of 1,039 ratings of 85 residents by 135 attendings in a single internal medicine residency program found the rating method fails to discriminate dimensions of clinical care and has low reliability for distinguishing among competent residents."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of General Internal Medicine"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "147060071"
                        ],
                        "name": "J. Eriksson",
                        "slug": "J.-Eriksson",
                        "structuredName": {
                            "firstName": "Joy",
                            "lastName": "Eriksson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Eriksson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62778544,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "3a78c645eef6e1e8904b4b28b4fff33f6325c88a",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Vol. 24 No. 1 Articles Charles Abraham Rosen \u2013 Scientist and visionary: 1917\u20132002 PE Hart and NJ Nilsson Norman Russell Nielsen: 1941\u20132002 R Perrault Saul Amarel: 1928\u20132002 T Mitchell and CA Kulikowski In memoriam: Raymond Reiter June 12, 1939\u2013September 16, 2002 J Minker Intelligent control of a water-recovery system \u2013 Three years in the trenches P Bonasso, D Kortenkamp and C Thronesbery TAC-03 \u2013 A supply-chain trading competition N Sadeh, R Arunachalam, J Eriksson, N Finne and S Janson"
            },
            "slug": "Lessons-from-a-failure-:-Generating-tailored-Eriksson",
            "title": {
                "fragments": [],
                "text": "Lessons from a failure : Generating tailored smoking cessation letters"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144568312"
                        ],
                        "name": "Ehud Reiter",
                        "slug": "Ehud-Reiter",
                        "structuredName": {
                            "firstName": "Ehud",
                            "lastName": "Reiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ehud Reiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145869117"
                        ],
                        "name": "S. Sripada",
                        "slug": "S.-Sripada",
                        "structuredName": {
                            "firstName": "Somayajulu",
                            "lastName": "Sripada",
                            "middleNames": [
                                "Gowri"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sripada"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 586,
                                "start": 48
                            }
                        ],
                        "text": "this is an intrinsic form of evaluation (Sp\u00e4rck Jones and Galliers 1995). This methodology was first used in NLG by Lester and Porter (1997), who asked eight domain experts to each rate 15 texts on a number of different dimensions: overall quality and coherence, content, organization, writing style, and correctness. Some of the texts were humanwritten and some were computer-generated, but the judges did not know the origin of specific texts they read. Many more such evaluations have been performed since, often with fewer dimensions. For example, Binsted, Pain, and Ritchie (1997) evaluated a jokegeneration system by asking children to rate the funniness of texts on a 5-point scale; and Walker, Rambow, and Rogati (2002) evaluated the SPOT sentence-planning system by asking human subjects to rate the overall quality of generated texts on a 5-point scale."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 48
                            }
                        ],
                        "text": "this is an intrinsic form of evaluation (Sp\u00e4rck Jones and Galliers 1995). This methodology was first used in NLG by Lester and Porter (1997), who asked eight domain experts to each rate 15 texts on a number of different dimensions: overall quality and coherence, content, organization, writing style, and correctness."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 42153823,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6e862065b597f791b66a8861bc625bce7a8240d9",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "\u00a1 \u00a2\u009f\u00a3v\u00a4R\u00a3}\u00a5\u00a7\u00a6(\u00a5\u00a7 \u0308\u0090\u00a9 \u00a4a\u00a3v\u00abn\u00a6\u00ac\u00a5\u00a7 \u0308\u009f\u00ad\u00ae\u00a5\u00a7 \u0308Z \u0304R\u00a3v\u00a4R\u00a3u\u00a6@ \u0304 \u00a5\u00b0 \u0308 \u00b1\u0090\u00a6R\u00a5\u00b0 \u0308\u009f\u00ad{\u00a9 2n\u00a4R3_2 \u0301\u00a4\u03bc\u00ab \u00a5\u00a7 \u0308\u00b7\u00b6C \u0327P1 o 3U\u00a3v\u00a4R\u00a2\u0090\u00ab \u03013\u0090\u00a6 \u00bbU\u00a3u\u00a9%\u00ab.\u00b1U\u00a6\u00ac\u00a3 2.1\u20444\u00ae \u0304a\u00a2\u009f\u00a3 \u00a6R\u00b1\u0090\u00a9%\u00a9%\u00a3v\u00a6a\u00a6 2 \u03011\u20444 \u00a9%2 \u0301\u00a4a3\u009f\u00b1\u0090\u00a6\u00ac1\u20442o\u00bb\u0090\u00abn\u00a6\u00ac\u00a3u3\u20444 \u0304R\u00a3v\u00a9\u03bc\u00a2\u0090 \u0308\u009f\u00a5\u00a7\u00bfZ\u00b1\u009f\u00a3u\u00a6]\u00a5\u00a7 \u0308\u00c02 \u0301 \u0304R\u00a2\u009f\u00a3v\u00a4\u007f\u00ab \u0301\u00a4R\u00a3u\u00ab \u0301\u00a6'2 \u03011\u20444 \u00a6R3U\u00a3v\u00a3v\u00a9\u03bc\u00a2b\u00ab. \u0308\u00903\u20444 \u00c1\u00a7\u00ab \u0301 \u0308\u009f\u00ad \u0301\u00b1U\u00ab.\u00ad \u0301\u00a3K3\u009f\u00a4R2A\u00a9%\u00a3v\u00a6a\u00a6\u00ac\u00a5\u00a7 \u0308\u009f\u00ad\u0090\u00c2(\u00c3\u00c4\u00ab. \u0308P\u00c5a\u00b1\u0090\u00a6R\u00a3v\u00a6 2 \u03011\u20444 \u00a9 2n\u00a4R3_2 \u0301\u00a4\u03bc\u00ab\u00c6\u00a5\u00a7 \u0308\u0080\u00b6C \u0327P1\u00c7\u00a5\u00a7\u00c8\u00833\u009f\u00c1\u00a7\u00a5\u00a7\u00a9%\u00a5\u00c9 \u0304a\u00c1\u00b0\u00c5\u00c0\u00abn\u00a6R\u00a6R\u00b1\u009f\u00c8\u0083\u00a3\u00c4 \u0304a\u00a2\u0090\u00ab\u008f \u0304 \u0304a\u00a2\u009f\u00a3*\u00a2P\u00b1\u009f\u00c8'\u00ab. \u0308A1\u20442\u00ca\u00ab.\u00b1\u009f \u0304R\u00a2\u009f2n\u00a4R\u00a3u3\u20444 \u0304a\u00a3 \u00cbP \u0304a\u00a6\u00cc\u00a5\u00a7 \u0308\u00c4\u00ab'\u00a9 2n\u00a4R3_2 \u0301\u00a4\u03bc\u00abb\u00ab.\u00a4a\u00a3 \u00ab\u00ce\u00cd \u00ad \u03012n\u00c1\u00a73\u20444\u00ce\u00a6\u00ac \u0304a\u00ab \u0301 \u0308\u00903\u20444\u009f\u00ab.\u00a4\u03bc3\u20444N\u00cf\u00a7o>\u00a5\u00a7 \u0308\u00d02. \u0304a\u00a2\u009f\u00a3%\u00a4 \u00d1}2n\u00a4a3\u20444\u009f\u00a6{ \u0304a\u00a2\u0090\u00ab\u008f \u0304 \u0304R\u00a2\u009f\u00a3 \u00b6C \u0327P1 \u00a6R\u00c5A\u00a6@ \u0304a\u00a3%\u00c8\u00d2\u00a6R\u00a2\u009f2n\u00b1\u009f\u00c1\u00a73\u20444\u00803\u009f\u00a4a2A3\u20444A\u00b1\u0090\u00a9%\u00a3\u00ce \u0304R\u00a3 \u00cbP \u0304\u03bc\u00a6\u00c4\u00a6\u00ac\u00a5\u00a7\u00c8\u0083\u00a5\u00b0\u00c1\u00d3\u00ab.\u00a4 \u0304a2\u00d4 \u0304a\u00a2\u009f\u00a3 \u00a9 2 \u0301\u00a4a3_2 \u0301\u00a4\u03bc\u00ab \u0304R\u00a3%\u00cbP \u0304a\u00a6v\u00c2 \u00d5\u00ae2\u008f\u00d1T\u00a3%\u00d6n\u00a3%\u00a4uo 2 \u0301\u00b1\u009f\u00a4\u0083\u00a3 \u00cbA3_\u00a3 1\u20442 \u00a4a\u00a5\u00b0\u00a3v \u0308\u0090\u00a9 \u00a3:\u00d1\u00cc\u00a5\u00c9 \u0304a\u00a2\u007f\u00a6R\u00a3%\u00d6n\u00a3%\u00a4\u03bc\u00ab.\u00c1_\u00a9 2 \u0301\u00a4a3_2 \u0301\u00a4\u03bc\u00aba\u00a4a\u00ab \u0301\u00a5\u00a7\u00a6R\u00a3v\u00a6T\u00bfZ\u00b1\u009f\u00a3v\u00a6\u00ac \u0304R\u00a5\u00a72 \u0301 \u0308\u0090\u00a6 \u00ab \u0301\u00bbU2n\u00b1A \u0304 \u0304R\u00a2\u009f\u00a5\u00d3\u00a6\u00ae\u00ab \u0301\u00a6a\u00a6R\u00b1\u009f\u00c8\u00833A \u0304R\u00a5\u00a72 \u0301 \u0308CoA\u00bb_\u00a3v\u00a9v\u00ab.\u00b1\u0090\u00a6R\u00a3a\u00a2P\u00b1\u009f\u00c8'\u00ab. \u0308 \u00ab.\u00b1A1\u20442 \u0304a\u00a2\u009f2 \u0301\u00a4\u03bc\u00a6{\u00c8\u0083\u00ab \u0301\u00d7 \u0301\u00a3b\u00c8\u0083\u00a5\u00a7\u00a6\u00ac \u0304a\u00ab \u0301\u00d7 \u0301\u00a3v\u00a6+\u00ab \u0301 \u0308\u00903\u20444\u00c4\u00bb_\u00a3v\u00a9v\u00ab.\u00b1\u0090\u00a6R\u00a3'3\u20444A\u00a5\u00b0\u00d8_\u00a3v\u00a4R\u00a3v \u0308n \u0304 3_\u00a3%2n3\u009f\u00c1\u00b0\u00a3+\u00d1\u00cc\u00a4a\u00a5\u00b0 \u0304R\u00a3+3\u20444A\u00a5\u00b0\u00d8_\u00a3v\u00a4R\u00a3v \u0308n \u0304a\u00c1\u00b0\u00c5n\u00c2"
            },
            "slug": "Should-Corpora-Texts-Be-Gold-Standards-for-NLG-Reiter-Sripada",
            "title": {
                "fragments": [],
                "text": "Should Corpora Texts Be Gold Standards for NLG?"
            },
            "venue": {
                "fragments": [],
                "text": "INLG"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153166702"
                        ],
                        "name": "S. Cunningham",
                        "slug": "S.-Cunningham",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Cunningham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Cunningham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13110303"
                        ],
                        "name": "S. Deere",
                        "slug": "S.-Deere",
                        "structuredName": {
                            "firstName": "S",
                            "lastName": "Deere",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Deere"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3488442"
                        ],
                        "name": "A. Symon",
                        "slug": "A.-Symon",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Symon",
                            "middleNames": [
                                "G"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Symon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47633195"
                        ],
                        "name": "R. Elton",
                        "slug": "R.-Elton",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Elton",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Elton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145924900"
                        ],
                        "name": "N. McIntosh",
                        "slug": "N.-McIntosh",
                        "structuredName": {
                            "firstName": "Neil",
                            "lastName": "McIntosh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. McIntosh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 67
                            }
                        ],
                        "text": "To evaluate this, they ideally would like to do a study similar to Cunningham et al.\u2019s (1998) evaluation of the effectiveness of a visualization system in an intensive care unit; that is, deploy BabyTalk in a hospital, use it for half of the children in a ward, and determine if there is any difference in outcome (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13164334,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "a3bedf7838ce8b98ab10e5ef1d1a1dbdd65929dc",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "OBJECTIVE\nTo assess whether the provision of computerized physiologic trend data could improve outcome in newborn infants requiring intensive care.\n\n\nDESIGN\nRandomized, controlled trial, with subsidiary questionnaire studies.\n\n\nSETTING\nTertiary neonatal intensive care unit with 12 intensive care cots.\n\n\nPATIENTS\nAll infants admitted between January 1991 and September 1993 who were < or =32 wks gestation or >32 wks gestation, and ventilated for >4 hrs or asphyxiated.\n\n\nINTERVENTIONS\nRandomization to one of four groups for first 7 days of life: A) no display of trend data; B) continuous display of trend data; C1) alternating 24-hr display of trend data, starting with display in first 24 hrs; and C2) alternating 24-hr display of trend data, starting with no display in first 24 hrs.\n\n\nMEASUREMENTS AND MAIN RESULTS\nThe short-term effects of monitoring on patient outcome was judged by volume of colloid given, number of blood gases taken, and by measurement taken from cranial Doppler ultrasound. Medium-term measures included time ventilated, time given supplemental oxygen, death, time to death or discharge, and cranial ultrasound at discharge. Long-term outcome was assessed by neurodevelopmental status at age 1 to 4 yrs of age. Staff and parent questionnaires assessed their respective attitudes to the introduction of this technology. None of the patient outcome measures, short-, medium-, or long-term, demonstrated any significant benefit from the provision of computerized physiologic trend monitoring. Staff questionnaires demonstrated an acceptance of the system and an improved understanding of neonatal physiology as a result of computerized physiologic trends. Parent questionnaires demonstrated increased anxiety caused by the system in 11% of parents, although only 1% of parents continued to have concerns if the system were able to help their child.\n\n\nCONCLUSIONS\nA randomized, controlled trial was unable to demonstrate any benefit to patients resulting from the introduction of a computerized physiologic trend monitoring system. Benefits of the system have been recognized, however, in subsidiary studies, staff education, and research studies."
            },
            "slug": "A-randomized,-controlled-trial-of-computerized-in-Cunningham-Deere",
            "title": {
                "fragments": [],
                "text": "A randomized, controlled trial of computerized physiologic trend monitoring in an intensive care unit."
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "None of the patient outcome measures, short-, medium-, or long-term, demonstrated any significant benefit from the provision of computerized physiologic trend monitoring, and benefits of the system have been recognized, however, in subsidiary studies, staff education, and research studies."
            },
            "venue": {
                "fragments": [],
                "text": "Critical care medicine"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701876"
                        ],
                        "name": "P. Maes",
                        "slug": "P.-Maes",
                        "structuredName": {
                            "firstName": "Pattie",
                            "lastName": "Maes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Maes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4842002,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "5182f637bd029d28d03c08f3afbfce949f56fcfa",
            "isKey": false,
            "numCitedBy": 497,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "The next few weeks are critical for nurses no doubt anxious to discover what pay award they can expect in April. The government has piled on the pressure for public sector pay awards to be at or below 2 per cent. This figure has already been exceeded by teachers, who were awarded 2.5 per cent, and members of the armed forces, who will receive 3 per cent, both from April."
            },
            "slug": "Do-the-right-thing.-Maes",
            "title": {
                "fragments": [],
                "text": "Do the right thing."
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "The next few weeks are critical for nurses no doubt anxious to discover what pay award they can expect in April, as the government has piled on the pressure for public sector pay awards to be at or below 2%."
            },
            "venue": {
                "fragments": [],
                "text": "Nursing standard (Royal College of Nursing (Great Britain) : 1987)"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 175
                            }
                        ],
                        "text": "The quality of texts generated by NLG systems has been evaluated in many different ways in the past, most of which can be classified as evaluations based on task performance, human judgments and ratings, or comparison to corpus texts using automatic metrics."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 59
                            }
                        ],
                        "text": "To evaluate this, they would like to do a study similar to Law et al. (2005); that is, show medical subjects textual summaries (as well as standard graphical visualizations as a control) in a controlled \u201coff-ward\u201d context, ask them to make a treatment decision, and compare this decision against a\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "Law et al. (2005), who worked in the same domain (NICU) as BabyTalk, conducted an off-ward decision-support evaluation which compared human-written text summaries and graphical visualizations of clinical data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 140
                            }
                        ],
                        "text": "To determine if any of the differences were statistically significant, we used SPSS\u2019s General Linear Model (GLM), with rating as the dependent variable, and generator, subject, and forecast date as independent variables; we used a post hoc Tukey HSD test to identify significant differences between\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generating textual summaries of graphical time series data to support medical decision making in the neonatal intensive care unit"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Clinical Monitoring and Computing"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145441750"
                        ],
                        "name": "J. Ioannidis",
                        "slug": "J.-Ioannidis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Ioannidis",
                            "middleNames": [
                                "P.",
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ioannidis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 72948349,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "3685e6f7e8817e91249332fc005951af1274044c",
            "isKey": false,
            "numCitedBy": 303,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Contradicted-and-Initially-Stronger-Effects-in-Ioannidis",
            "title": {
                "fragments": [],
                "text": "Contradicted and Initially Stronger Effects in Highly Cited Clinical Research"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29747026"
                        ],
                        "name": "M. Crawford",
                        "slug": "M.-Crawford",
                        "structuredName": {
                            "firstName": "Mabel",
                            "lastName": "Crawford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Crawford"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 71648382,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "b283b91cb101c324c39a6dac656eefa486068a91",
            "isKey": false,
            "numCitedBy": 235,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Art-of-Readable-Writing-Crawford",
            "title": {
                "fragments": [],
                "text": "The Art of Readable Writing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2358515"
                        ],
                        "name": "Gabriel Murray",
                        "slug": "Gabriel-Murray",
                        "structuredName": {
                            "firstName": "Gabriel",
                            "lastName": "Murray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gabriel Murray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145086187"
                        ],
                        "name": "S. Renals",
                        "slug": "S.-Renals",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Renals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Renals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694130"
                        ],
                        "name": "J. Carletta",
                        "slug": "J.-Carletta",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Carletta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Carletta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47147237"
                        ],
                        "name": "Johanna D. Moore",
                        "slug": "Johanna-D.-Moore",
                        "structuredName": {
                            "firstName": "Johanna",
                            "lastName": "Moore",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Johanna D. Moore"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 144
                            }
                        ],
                        "text": "\u2026on similarly strict statistical analyses; in particular always use two-tailed p-values, always apply multiple hypothesis corrections, always discard post hoc findings (unless they are from tests specifically designed for post hoc analysis, such as Tukey HSD), and always use nonparametric\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Dorr et al. (2005) checked if ROUGE scores correlated with task effectiveness; they did not find a strong correlation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 32
                            }
                        ],
                        "text": "In the summarization community, Dorr et al. (2005) found very weak correlation between an automatic metric (ROUGE) and task performance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 195944134,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4774432f02ef4c5285952dd8c7daff0852c3a601",
            "isKey": false,
            "numCitedBy": 323,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Proceedings-of-the-ACL-Workshop-on-Intrinsic-and-Murray-Renals",
            "title": {
                "fragments": [],
                "text": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization"
            },
            "venue": {
                "fragments": [],
                "text": "ACL 2005"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145107462"
                        ],
                        "name": "Stuart J. Russell",
                        "slug": "Stuart-J.-Russell",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Russell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stuart J. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2358562"
                        ],
                        "name": "Eric Wefald",
                        "slug": "Eric-Wefald",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Wefald",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Wefald"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60656080,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd8f37182a2d5ee00ce69a874dd82be287578fe7",
            "isKey": false,
            "numCitedBy": 432,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Do-the-right-thing-Russell-Wefald",
            "title": {
                "fragments": [],
                "text": "Do the right thing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730221"
                        ],
                        "name": "Charles B. Callaway",
                        "slug": "Charles-B.-Callaway",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Callaway",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles B. Callaway"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61447416,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a4ec0bab1bd869d23fb37d2953ea9759990ecf4",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "After many successes, statistical approaches that have been popular in the parsing community are now making headway into Natural Language Generation (NLG). These systems are aimed mainly at surface realization, and promise the same advantages that make statistics valuable for parsing: robustness, wide coverage and domain independence. A recent experiment aimed to empirically verify the linguistic coverage for such a statistical surface realization component by generating transformed sentences from the Penn TreeBank corpus. This article presents the empirical results of a similar experiment to evaluate the coverage of a purely symbolic surface realizer. We present the problems facing a symbolic approach on the same task, describe the results of its evaluation, and contrast them with the results of the statistical method to help quantitatively determine the level of coverage currently obtained by NLG surface realizers."
            },
            "slug": "Evaluating-Coverage-for-Large-Symbolic-NLG-Grammars-Callaway",
            "title": {
                "fragments": [],
                "text": "Evaluating Coverage for Large Symbolic NLG Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The problems facing a symbolic approach on the same task are presented, the results of its evaluation are described, and the results are described to help quantitatively determine the level of coverage currently obtained by NLG surface realizers."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2086330293"
                        ],
                        "name": "Jlfnm Fpoli",
                        "slug": "Jlfnm-Fpoli",
                        "structuredName": {
                            "firstName": "Jlfnm",
                            "lastName": "Fpoli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jlfnm Fpoli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2097726113"
                        ],
                        "name": "Qsr Fti O6J",
                        "slug": "Qsr-Fti-O6J",
                        "structuredName": {
                            "firstName": "Qsr",
                            "lastName": "O6J",
                            "middleNames": [
                                "Fti"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qsr Fti O6J"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082812658"
                        ],
                        "name": "Eitm Fujlf",
                        "slug": "Eitm-Fujlf",
                        "structuredName": {
                            "firstName": "Eitm",
                            "lastName": "Fujlf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eitm Fujlf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083759414"
                        ],
                        "name": "Fpvwixqyi Olz",
                        "slug": "Fpvwixqyi-Olz",
                        "structuredName": {
                            "firstName": "Fpvwixqyi",
                            "lastName": "Olz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fpvwixqyi Olz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083948800"
                        ],
                        "name": "F. Yvgpolfpi",
                        "slug": "F.-Yvgpolfpi",
                        "structuredName": {
                            "firstName": "F",
                            "lastName": "Yvgpolfpi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Yvgpolfpi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085503561"
                        ],
                        "name": "H. Ypi",
                        "slug": "H.-Ypi",
                        "structuredName": {
                            "firstName": "H",
                            "lastName": "Ypi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ypi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2103430354"
                        ],
                        "name": "QYQnR hGKLYQgYoPFpJ3R",
                        "slug": "QYQnR-hGKLYQgYoPFpJ3R",
                        "structuredName": {
                            "firstName": "QYQnR",
                            "lastName": "hGKLYQgYoPFpJ3R",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "QYQnR hGKLYQgYoPFpJ3R"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15733381,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "d884f735cd9e62feb20d7f78995838ecfff735e0",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Training-a-Sentence-Planner-for-Spoken-Dialogue-Fpoli-O6J",
            "title": {
                "fragments": [],
                "text": "Training a Sentence Planner for Spoken Dialogue Using Boosting"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 37
                            }
                        ],
                        "text": "This is a major concern (as noted by Belz 2009) because we also do not know how well human ratings predict task-effectiveness; in other words, the fact that NIST scores predict human clarity ratings of NLG texts does not guarantee that NIST scores will predict task effectiveness, because we do not\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "That's nice ... what do you do with it? Computational Linguistics"
            },
            "venue": {
                "fragments": [],
                "text": "That's nice ... what do you do with it? Computational Linguistics"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 272
                            }
                        ],
                        "text": "\u2026occurred in the recent Generation Challenges evaluations of referring expression generation, which measured the correlations between human assessments of language quality and adequacy of content with task-performance measures (referent identification time and accuracy) (Gatt, Belz, and Kow 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 150
                            }
                        ],
                        "text": "\u2026experiments in which participants were presented with generated referring expressions and asked to identify the target referent (Belz and Gatt 2007; Gatt, Belz, and Kow 2008, 2009); these were carried out in conjunction with shared-task events organized under the Generation Challenges initiative\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The TUNA-REG Challenge Reiter and Belz Validity of Some Metrics for NLG Evaluation 2009: Overview and evaluation results"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 12th European Workshop on Natural Language Generation (ENLG'09)"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 149
                            }
                        ],
                        "text": "\u2026ratings (Binsted, Pain, and Ritchie 1997), although one can perform a task-based evaluation of the educational impact of humor generation software (Black et al. 2007), or (more speculatively) perhaps evaluate the psychological impact of a joke by monitoring facial expressions and laughter (which\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 16
                            }
                        ],
                        "text": "Experts and non-experts also agreed about relative rankings, except that experts rank pCRU-greedy second and the corpus texts third, whereas the non-experts have these the other way around."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Helen Pain, and Ruli Manurung Evaluation of joke-creation software with children with complex communication needs"
            },
            "venue": {
                "fragments": [],
                "text": "Communication Matters"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "That\u2019s nice ... what do you do with it"
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics,"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 59
                            }
                        ],
                        "text": "To evaluate this, they would like to do a study similar to Law et al. (2005); that is, show medical subjects textual summaries (as well as standard graphical visualizations as a control) in a controlled \u201coff-ward\u201d context, ask them to make a treatment decision, and compare this decision against a\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 59
                            }
                        ],
                        "text": "To evaluate this, they would like to do a study similar to Law et al. (2005); that is, show medical subjects textual summaries (as well as standard graphical visualizations as a control) in a controlled \u201coff-ward\u201d context, ask them to make a treatment decision, and compare this decision against a gold standard."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "Law et al. (2005), who worked in the same domain (NICU) as BabyTalk, conducted an off-ward decision-support evaluation which compared human-written text summaries and graphical visualizations of clinical data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generating textual summaries of graphical time series data to support medical decision making in the neonatal"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 55
                            }
                        ],
                        "text": "of the educational impact of humor generation software (Black et al. 2007), or (more speculatively) perhaps evaluate the psychological impact of a joke by monitoring facial"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 149
                            }
                        ],
                        "text": "\u2026ratings (Binsted, Pain, and Ritchie 1997), although one can perform a task-based evaluation of the educational impact of humor generation software (Black et al. 2007), or (more speculatively) perhaps evaluate the psychological impact of a joke by monitoring facial expressions and laughter (which\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Evaluation of joke-creation software with children with complex communication needs"
            },
            "venue": {
                "fragments": [],
                "text": "Communication Matters, 21:23\u201328."
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generating textual summaries of graphical time series data to support medical decision making in the neonatal intensive care unit"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Clinical Monitoring and Computing"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Intrinsic vs"
            },
            "venue": {
                "fragments": [],
                "text": "extrinsic evaluation measures for referring expression generation. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL\u201908),"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Developing and empirically evaluating"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 664,
                                "start": 134
                            }
                        ],
                        "text": "Task-based evaluations involve directly measuring the impact of generated texts on end users; these are extrinsic evaluations (Sp\u00e4rck Jones and Galliers 1995), and typically involve techniques from psychology or from an application domain such as medicine. One of the first task-based evaluations of an NLG system was done by Young (1999), who generated instructional texts using four different algorithms, asked subjects to carry out the instructions, and then measured how many mistakes they made. Although task performance is the most common measure used in task-based evaluations in NLG, other measures can also be used. For example, Carenini and Moore (2006) evaluated the impact of persuasive texts (in a house-selling context) by seeing how users ranked houses in a hot list; and Di Eugenio, Glass, and Trolio (2002) evaluated the impact of adding an NLG component to an intelligent tutoring system by measuring learning gain."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 339,
                                "start": 134
                            }
                        ],
                        "text": "Task-based evaluations involve directly measuring the impact of generated texts on end users; these are extrinsic evaluations (Sp\u00e4rck Jones and Galliers 1995), and typically involve techniques from psychology or from an application domain such as medicine. One of the first task-based evaluations of an NLG system was done by Young (1999), who generated instructional texts using four different algorithms, asked subjects to carry out the instructions, and then measured how many mistakes they made."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An NLG evaluation competition? Eight reasons to be cautious"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Workshop on Shared Tasks and Comparative Evaluation in Natural Language Generation,"
            },
            "year": 2007
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 17,
            "methodology": 12
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 74,
        "totalPages": 8
    },
    "page_url": "https://www.semanticscholar.org/paper/An-Investigation-into-the-Validity-of-Some-Metrics-Reiter-Belz/175468ba0a7242f259a4d7b81f3d82951313de61?sort=total-citations"
}