{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691804"
                        ],
                        "name": "D. Ballard",
                        "slug": "D.-Ballard",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Ballard",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ballard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "An improvement on this strictly bottom-up scheme is described by Ballard [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 38968420,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aa109a5c8440332a05ac538d98c4f93d25500c81",
            "isKey": false,
            "numCitedBy": 281,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "In the development of large-scale knowledge networks much recent progress has been inspired by connections to neurobiology. An important component of any \"neural\" network is an accompanying learning algorithm. Such an algorithm, to be biologically plausible, must work for very large numbers of units. Studies of large-scale systems have so far been restricted to systems Without internal units (units With no direct connections to the input or output). Internal units are crucial to such systems as they are the means by which a system can encode high-order regularities (or invariants) that are Implicit in its inputs and outputs. Computer simulations of learning using internal units have been restricted to small-scale systems. This paper describes away of coupling autoassociative learning modules Into hierarchies that should greatly improve the performance of learning algorithms in large-scale systems. The Idea has been tested experimentally with positive results."
            },
            "slug": "Modular-Learning-in-Neural-Networks-Ballard",
            "title": {
                "fragments": [],
                "text": "Modular Learning in Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper describes away of coupling autoassociative learning modules Into hierarchies that should greatly improve the performance of learning algorithms in large-scale systems."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 205001834,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "052b1d8ce63b07fec3de9dbb583772d860b7c769",
            "isKey": false,
            "numCitedBy": 20330,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1."
            },
            "slug": "Learning-representations-by-back-propagating-errors-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning representations by back-propagating errors"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "Back-propagation repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector, which helps to represent important features of the task domain."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2546518"
                        ],
                        "name": "D. Plaut",
                        "slug": "D.-Plaut",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Plaut",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Plaut"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 47
                            }
                        ],
                        "text": "Various other methods have also been suggested (Amari, 1967: Parker, 1987; Plaut and Hinton, 1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14299616,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b59ebc8abee160f1cb34a992a6b07d1c9f7bafb",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-sets-of-filters-using-back-propagation-Plaut-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning sets of filters using back-propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691804"
                        ],
                        "name": "D. Ballard",
                        "slug": "D.-Ballard",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Ballard",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ballard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 53550263,
            "fieldsOfStudy": [
                "Biology",
                "Computer Science",
                "Psychology"
            ],
            "id": "902a9f6fd1b17f0d0bc9de54cccd4baa179a7845",
            "isKey": false,
            "numCitedBy": 342,
            "numCiting": 237,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract The cerebral cortex is a rich and diverse structure that is the basis of intelligent behavior. One of the deepest mysteries of the function of cortex is that neural processing times are only about one hundred times as fast as the fastest response times for complex behavior. At the very least, this would seem to indicate that the cortex does massive amounts of parallel computation. This paper explores the hypothesis that an important part of the cortex can be modeled as a connectionist computer that is especially suited for parallel problem solving. The connectionist computer uses a special representation, termed value unit encoding, that represents small subsets of parameters in a way that allows parallel access to many different parameter values. This computer can be thought of as computing hierarchies of sensorimotor invariants. The neural substrate can be interpreted as a commitment to data structures and algorithms that compute invariants fast enough to explain the behavioral response times. A detailed consideration of this model has several implications for the underlying anatomy and physiology."
            },
            "slug": "Cortical-connections-and-parallel-processing:-and-Ballard",
            "title": {
                "fragments": [],
                "text": "Cortical connections and parallel processing: Structure and function"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The hypothesis that an important part of the cortex can be modeled as a connectionist computer that is especially suited for parallel problem solving is explored."
            },
            "venue": {
                "fragments": [],
                "text": "Behavioral and Brain Sciences"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577641"
                        ],
                        "name": "R. Linsker",
                        "slug": "R.-Linsker",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Linsker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Linsker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15600894,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "727690d076c9a73f77a6c5d863b40fdc14da0ba1",
            "isKey": false,
            "numCitedBy": 258,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "This is the second paper in a series of three that explores the emergence of several prominent features of the functional architecture of visual cortex, in a \"modular self-adaptive network\" containing several layers of cells with parallel feedforward connections whose strengths develop according to a Hebb-type correlation-rewarding rule. In the present paper I show that orientation-selective cells, similar to the \"simple\" cortical cells of Hubel and Wiesel [Hubel, D. H. & Wiesel, T. N. (1962) J. Physiol. 160, 106-154], emerge in such a network. No orientation preference is specified to the system at any stage, the orientation-selective cell layer emerges even in the absence of environmental input to the system, and none of the basic developmental rules is specific to visual processing."
            },
            "slug": "From-basic-network-principles-to-neural-emergence-Linsker",
            "title": {
                "fragments": [],
                "text": "From basic network principles to neural architecture: emergence of orientation-selective cells."
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "It is shown that orientation-selective cells, similar to the \"simple\" cortical cells of Hubel and Wiesel, emerge in a \"modular self-adaptive network\" containing several layers of cells with parallel feedforward connections whose strengths develop according to a Hebb-type correlation-rewarding rule."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 47
                            }
                        ],
                        "text": "A network with three hidden layers was used by Hinton (1986) to learn the family relationships between 24 different people."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 768,
                                "start": 99
                            }
                        ],
                        "text": "Back-propagation: A multilayer LMS procedure The \"back-propagation\" learning procedure (Rumelhart, Hinton and Williams, 1986a, 1986b) is a generalization of the LMS procedure that works for networks which have layers of hidden units between the input and output units. These multilayer networks can compute much more complicated functions than networks that lack hidden units, but the learning is generally much slower because it must explore the space of possible ways of using the hidden units. There are now many examples in which backpropagation constructs interesting internal representations in the hidden units, and these representations allow the network to generalize in sensible ways. Variants of the procedure were discovered independently by Le Cun (1985) and Parker (1985)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 33
                            }
                        ],
                        "text": "Examples are given in Rumelhart, Hinton and Williams (1986b). Alternatively, it can be used to store a set of patterns by constructing a point attractor for each"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 11
                            }
                        ],
                        "text": "Rumelhart, Hinton, and Williams (1986a) show how the back propagation procedure can be applied to iterative networks in which there are no limitations on the connectivity."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 227369,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3e6bea2649298c68d17b9421fc7dd19eeacc935e",
            "isKey": false,
            "numCitedBy": 205,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "One major goal of research on massively parallel networks of neuron-like processing elements is to discover efficient methods for recognizing patterns. Another goal is to discover general learning procedures that allow networks to construct the internal representations that are required for complex tasks. This paper describes a recently developed procedure that can learn to perform a recognition task. The network is trained on examples in which the input vector represents an instance of a pattern in a particular position and the required output vector represents its name. After prolonged training, the network develops canonical internal representations of the patterns and it uses these canonical representations to identify familiar patterns in novel positions."
            },
            "slug": "Learning-Translation-Invariant-Recognition-in-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning Translation Invariant Recognition in Massively Parallel Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper describes a recently developed procedure that can learn to perform a recognition task and uses canonical internal representations of the patterns to identify familiar patterns in novel positions."
            },
            "venue": {
                "fragments": [],
                "text": "PARLE"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308302"
                        ],
                        "name": "D. Ackley",
                        "slug": "D.-Ackley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Ackley",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ackley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12174018,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657",
            "isKey": false,
            "numCitedBy": 3393,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Learning-Algorithm-for-Boltzmann-Machines-Ackley-Hinton",
            "title": {
                "fragments": [],
                "text": "A Learning Algorithm for Boltzmann Machines"
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 195
                            }
                        ],
                        "text": "A second approach, which seems to be feasible for self-supervised backpropagation, is to use a method called \"recirculation\" that approximates gradient descent and is more biologically plausible [41]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16002922,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97f7d20e1e82347d78cef335218692207b29d23f",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a new learning procedure for networks that contain groups of nonlinear units arranged in a closed loop. The aim of the learning is to discover codes that allow the activity vectors in a \"visible\" group to be represented by activity vectors in a \"hidden\" group. One way to test whether a code is an accurate representation is to try to reconstruct the visible vector from the hidden vector. The difference between the original and the reconstructed visible vectors is called the reconstruction error, and the learning procedure aims to minimize this error. The learning procedure has two passes. On the first pass, the original visible vector is passed around the loop, and on the second pass an average of the original vector and the reconstructed vector is passed around the loop. The learning procedure changes each weight by an amount proportional to the product of the \"presynaptic\" activity and the difference in the post-synaptic activity on the two passes. This procedure is much simpler to implement than methods like back-propagation. Simulations in simple networks show that it usually converges rapidly on a good set of codes, and analysis shows that in certain restricted cases it performs gradient descent in the squared reconstruction error."
            },
            "slug": "Learning-Representations-by-Recirculation-Hinton-McClelland",
            "title": {
                "fragments": [],
                "text": "Learning Representations by Recirculation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Simulations in simple networks show that the learning procedure usually converges rapidly on a good set of codes, and analysis shows that in certain restricted cases it performs gradient descent in the squared reconstruction error."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763321"
                        ],
                        "name": "E. Saund",
                        "slug": "E.-Saund",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Saund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Saund"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 94
                            }
                        ],
                        "text": "A variation of it has been used to extract the underlying degrees of freedom of simple shapes (Saund, 1986)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6986935,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "667eaa4874c687cdddee3e8ce6b6bc2749a79a53",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "A method is presented for using connectionist networks of simple computing elements to discover a particular type of constraint in multidimensional data. Suppose that some data source provides samples consisting of n-dimensional feature-vectors, but that this data all happens to lie on an m-dimensional surface embedded in the n-dimensional feature space. Then occurrences of data can be more concisely described by specifying an m-dimensional location on the embedded surface than by reciting all n components of the feature vector. The recoding of data in such a way is a form of abstraction. This paper describes a method for performing this type of abstraction in connectionist networks of simple computing elements. We present a scheme for representing the values of continuous (scalar) variables in subsets of units. The backpropagation weight updating method for training connectionist networks is extended by the use of auxiliary pressure in order to coax hidden units into the prescribed representation for scalar-valued variables."
            },
            "slug": "Abstraction-and-Representation-of-Continuous-in-Saund",
            "title": {
                "fragments": [],
                "text": "Abstraction and Representation of Continuous Variables in Connectionist Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The backpropagation weight updating method for training connectionist networks is extended by the use of auxiliary pressure in order to coax hidden units into the prescribed representation for scalar-valued variables."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1995280"
                        ],
                        "name": "G. Edelman",
                        "slug": "G.-Edelman",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Edelman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Edelman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2084582"
                        ],
                        "name": "G. Reeke",
                        "slug": "G.-Reeke",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Reeke",
                            "middleNames": [
                                "N."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Reeke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 68
                            }
                        ],
                        "text": "There are many other interesting procedures which have been omitted [24, 26, 34, 35, 47, 54, 94]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 34922640,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca7425bdbfea6b1a2f9492452eaafe3972745712",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Two parallel sets of selective networks composed of intercommunicating neuron-like elements have been connected to produce a new kind of automaton capable of limited recognition of two-dimensional patterns. Salient features of this automaton are (i) preestablished unchanging connectivity, (ii) preassigned connection strengths that are selectively altered according to experience, (iii) local feature detection in one network with simultaneous global feature correlation in the other, and (iv) reentrant interactions between the two networks to generate a new function, associative memory. No forced learning, explicit semantic rules, or a priori instructions are used."
            },
            "slug": "Selective-networks-capable-of-representative-and-Edelman-Reeke",
            "title": {
                "fragments": [],
                "text": "Selective networks capable of representative transformations, limited generalizations, and associative memory."
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Two parallel sets of selective networks composed of intercommunicating neuron-like elements have been connected to produce a new kind of automaton capable of limited recognition of two-dimensional patterns."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 784288,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "98b4d4e24aab57ab4e1124ff8106909050645cfa",
            "isKey": false,
            "numCitedBy": 14027,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices."
            },
            "slug": "Neural-networks-and-physical-systems-with-emergent-Hopfield",
            "title": {
                "fragments": [],
                "text": "Neural networks and physical systems with emergent collective computational abilities."
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A model of a system having a large number of simple equivalent components, based on aspects of neurobiology but readily adapted to integrated circuits, produces a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144157926"
                        ],
                        "name": "T. Hogg",
                        "slug": "T.-Hogg",
                        "structuredName": {
                            "firstName": "Tad",
                            "lastName": "Hogg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hogg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794321"
                        ],
                        "name": "B. Huberman",
                        "slug": "B.-Huberman",
                        "structuredName": {
                            "firstName": "Bernardo",
                            "lastName": "Huberman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Huberman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 68
                            }
                        ],
                        "text": "There are many other interesting procedures which have been omitted (Grossberg 1980; Volper and Hampson, 1986; Hampson and Volper, 1987; Hogg and Huberman, 1984; Kerszberg and Bergman, 1986; Edelman and Reeke, 1982)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 35217180,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "78e8e76f0e362811e09fd1a5ef8a941cde4f31e7",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We experimentally examine the consequences of the hypothesis that the brain operates reliably, even though individual components may intermittently fail, by computing with dynamical attractors. Specifically, such a mechanism exploits dynamic collective behavior of a system with attractive fixed points in its phase space. In contrast to the usual methods of reliable computation involving a large number of redundant elements, this technique of self-repair only requires collective computation with a few units, and it is amenable to quantitative investigation. Experiments on parallel computing arrays show that this mechanism leads naturally to rapid self-repair, adaptation to the environment, recognition and discrimination of fuzzy inputs, and conditional learning, properties that are commonly associated with biological computation."
            },
            "slug": "Understanding-biological-computation:-reliable-and-Hogg-Huberman",
            "title": {
                "fragments": [],
                "text": "Understanding biological computation: reliable learning and recognition."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Experiments on parallel computing arrays show that this mechanism leads naturally to rapid self-repair, adaptation to the environment, recognition and discrimination of fuzzy inputs, and conditional learning, properties that are commonly associated with biological computation."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577641"
                        ],
                        "name": "R. Linsker",
                        "slug": "R.-Linsker",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Linsker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Linsker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5175050,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "06f5a18780d7332ed68a9c786e1c597b27a8e0f6",
            "isKey": false,
            "numCitedBy": 266,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Orientation-selective cells--cells that are selectively responsive to bars and edges at particular orientations--are a salient feature of the architecture of mammalian visual cortex. In the previous paper of this series, I showed that such cells emerge spontaneously during the development of a simple multilayered network having local but initially random feedforward connections that mature, one layer at a time, according to a simple development rule (of Hebb type). In this paper, I show that, in the presence of lateral connections between developing orientation cells, these cells self-organize into banded patterns of cells of similar orientation. These patterns are similar to the \"orientation columns\" found in mammalian visual cortex. No orientation preference is specified to the system at any stage, none of the basic developmental rules is specific to visual processing, and the results emerge even in the absence of visual input to the system (as has been observed in macaque monkey)."
            },
            "slug": "From-basic-network-principles-to-neural-emergence-Linsker",
            "title": {
                "fragments": [],
                "text": "From basic network principles to neural architecture: emergence of orientation columns."
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is shown that, in the presence of lateral connections between developing orientation cells, these cells self-organize into banded patterns of cells of similar orientation, similar to the \"orientation columns\" found in mammalian visual cortex."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577641"
                        ],
                        "name": "R. Linsker",
                        "slug": "R.-Linsker",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Linsker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Linsker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7708166,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "f8fba8a2a6f1209f4e09cede01c8b2c64a656aea",
            "isKey": false,
            "numCitedBy": 377,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The functional architecture of mammalian visual cortex has been elucidated in impressive detail by experimental work of the past 20-25 years. The origin of many of the salient features of this architecture, however, has remained unexplained. This paper is the first of three (the others will appear in subsequent issues of these Proceedings) that address the origin and organization of feature-analyzing (spatial-opponent and orientation-selective) cells in simple systems governed by biologically plausible development rules. I analyze the progressive maturation of a system composed of a few layers of cells, with connections that develop according to a simple set of rules (including Hebb-type modification). To understand the prenatal origin of orientation-selective cells in certain primates, I consider the case in which there is no external input, with the first layer exhibiting random spontaneous electrical activity. No orientation preference is specified to the system at any stage, and none of the basic developmental rules is specific to visual processing. Here I introduce the theory of \"modular self-adaptive networks,\" of which this system is an example, and explicitly demonstrate the emergence of a layer of spatial-opponent cells. This sets the stage for the emergence, in succeeding layers, of an orientation-selective cell population."
            },
            "slug": "From-basic-network-principles-to-neural-emergence-Linsker",
            "title": {
                "fragments": [],
                "text": "From basic network principles to neural architecture: emergence of spatial-opponent cells."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper is the first of three that address the origin and organization of feature-analyzing cells in simple systems governed by biologically plausible development rules, and introduces the theory of \"modular self-adaptive networks,\" of which this system is an example, and explicitly demonstrates the emergence of a layer of spatial-opponent cells."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144932833"
                        ],
                        "name": "J. S. Judd",
                        "slug": "J.-S.-Judd",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Judd",
                            "middleNames": [
                                "Stephen"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. S. Judd"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 56909682,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ed6f076369825e36254c2554824544f840663768",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "WE FORMALIZE A NOTION OF LEARNING IN CONNECTIONIST NETWORKS THAT CHARAC- TERIZES THE TRAINING OF FEED-FORWARD NETWORKS. CONSIDERING DIFFERENT FAM- ILIES OF NODE FUNCTIONS, WE PROVE THE LEARNING PROBLEM NP-COMPLETE AND THUS DEMONSTRATE THAT IS HAS NO EFFICIENT GENERAL SOLUTION. ONE FAMILY OF NODE FUNCTIONS STUDIED IS THE SET OF LOGISTIC-LINEAR FUNCTIONS, AS USED BY THE POPULAR BACK-PROPOGATION ALGORITHM. SEVERAL IMPLICATIONS OF THE THEOREM ARE DISCUSSED, INCLUDING WHY THIS RESULT IS ACTUALLY HELPFUL FOR CONNECTION IST LEARNING RESEARCH."
            },
            "slug": "Complexity-of-Connectionist-Learning-with-Various-Judd",
            "title": {
                "fragments": [],
                "text": "Complexity of Connectionist Learning with Various Node Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is proved the learning problem NP-COMPLETE has no effective general solution, and the set of logistic-linear FUNCTIONS, as used by the popular back-PROPOGATION ALGORITHM, is considered."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2165973"
                        ],
                        "name": "J. Feldman",
                        "slug": "J.-Feldman",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Feldman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Feldman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61076685,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "865787016949fefd4f0a31862a76db18077f2cf3",
            "isKey": false,
            "numCitedBy": 133,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : The neural encoding of memory is a problem of great interest and importance. Trlalditional proposals have taken one of two extreme views: The one-concept, one-neuron, punctate view and the full distributed, holographic alternative. Major advances in the behavioral, biological and computational sciences have greatly increased our understanding of the question and its potential answers. There is now good reason to reject both extreme views, but a compact encoding that derives from the punctate model appears to fit well with all the facts. Much of the work espousing holographic models is reinterpreted as studying system properties of neural networks and, as such, considered to be of great importance. Some suggestions for directions of further research are discussed."
            },
            "slug": "Neural-Representation-of-Conceptual-Knowledge.-Feldman",
            "title": {
                "fragments": [],
                "text": "Neural Representation of Conceptual Knowledge."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "There is now good reason to reject both extreme views, but a compact encoding that derives from the punctate model appears to fit well with all the facts."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 218
                            }
                        ],
                        "text": "The short-term knowledge of the network is normally encoded by the states of the units, but some models also have fast-changing temporary weights or thresholds that can be used to encode temporary contexts or bindings [44, 96]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16710884,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7257eacd80458e70c74494eb1b6759b52ff21399",
            "isKey": false,
            "numCitedBy": 214,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Connectionist models usually have a single weight on each connection. Some interesting new properties emerge if each connection has two weights: A slowly changing, plastic weight which stores long-term knowledge and a fast-changing, elastic weight which stores temporary knowledge and spontaneously decays towards zero. If a network learns a set of associations and then these associations are \"blurred\" by subsequent learning, all the original associations can be \"deblurred\" by rehearsing on just a few of them. The rehearsal allows the fast weights to take on values that temporarily cancel out the changes in the slow weights caused by the subsequent learning."
            },
            "slug": "Using-fast-weights-to-deblur-old-memories-Hinton",
            "title": {
                "fragments": [],
                "text": "Using fast weights to deblur old memories"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "All the original associations of a network can be \"deblurred\" by rehearsing on just a few of them by allowing the fast weights to take on values that temporarily cancel out the changes in the slow weights caused by the subsequent learning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144826602"
                        ],
                        "name": "C. Anderson",
                        "slug": "C.-Anderson",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Anderson",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Anderson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1522994,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8a7acaf6469c06ae5876d92f013184db5897bb13",
            "isKey": false,
            "numCitedBy": 3237,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem. The task is to balance a pole that is hinged to a movable cart by applying forces to the cart's base. It is argued that the learning problems faced by adaptive elements that are components of adaptive networks are at least as difficult as this version of the pole-balancing problem. The learning system consists of a single associative search element (ASE) and a single adaptive critic element (ACE). In the course of learning to balance the pole, the ASE constructs associations between input and output by searching under the influence of reinforcement feedback, and the ACE constructs a more informative evaluation function than reinforcement feedback alone can provide. The differences between this approach and other attempts to solve problems using neurolike elements are discussed, as is the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences."
            },
            "slug": "Neuronlike-adaptive-elements-that-can-solve-control-Barto-Sutton",
            "title": {
                "fragments": [],
                "text": "Neuronlike adaptive elements that can solve difficult learning control problems"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem and the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man, and Cybernetics"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895771"
                        ],
                        "name": "D. Zipser",
                        "slug": "D.-Zipser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zipser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zipser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18100780,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb4bad84a2fd896edfa4f5c22061b2913fec500d",
            "isKey": false,
            "numCitedBy": 1445,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Feature-discovery-by-competitive-learning-Rumelhart-Zipser",
            "title": {
                "fragments": [],
                "text": "Feature discovery by competitive learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700974"
                        ],
                        "name": "Barak A. Pearlmutter",
                        "slug": "Barak-A.-Pearlmutter",
                        "structuredName": {
                            "firstName": "Barak",
                            "lastName": "Pearlmutter",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barak A. Pearlmutter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 118
                            }
                        ],
                        "text": "The derivative of this mutual information is relatively easy to compute and so it can be maximized by gradient ascent (Pearlmutter and Hinton, 1986)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14878957,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "17808fdaa56ca0227cea4990ecf36f5a1bc9bb58",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Hill climbing is used to maximize an information theoretic measure of the difference \nbetween the actual behavior of a unit and the behavior that would be predicted by a \nstatistician who knew the first order statistics of the inputs but believed them to be \nindependent. This causes the unit to detect higher order correlations among its inputs. \nInitial simulations are presented, and seem encouraging. We describe an extension of the \nbasic idea which makes it resemble competitive learning and which causes members of a \npopulation of these units to differentiate, each extracting different structure from the input."
            },
            "slug": "G-maximization:-An-unsupervised-learning-procedure-Pearlmutter-Hinton",
            "title": {
                "fragments": [],
                "text": "G-maximization: An unsupervised learning procedure for discovering regularities"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An extension of the basic idea which makes it resemble competitive learning and which causes members of a population of these units to differentiate, each extracting different structure from the input is described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2165973"
                        ],
                        "name": "J. Feldman",
                        "slug": "J.-Feldman",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Feldman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Feldman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691804"
                        ],
                        "name": "D. Ballard",
                        "slug": "D.-Ballard",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Ballard",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ballard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 74
                            }
                        ],
                        "text": "They grew out of work on early visual processing and associative memories [28, 40, 79]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62225923,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ab4aec5e0714b352e6c90d063fe830cbc70912bc",
            "isKey": false,
            "numCitedBy": 736,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Much of the progress in the fields constituting cognitive science has been based upon the use of explicit information processing models, almost exclusively patterned after conventional serial computers. An extension of these ideas to massively parallel, connectionist models appears to offer a number of advantages. After a preliminary discussion, this paper introduces a general connectionist model and considers how it might be used in cognitive science. Among the issues addressed are: stability and noise-sensitivity, distributed decision-making, time and sequence problems, and the representation of complex concepts."
            },
            "slug": "Connectionist-Models-and-Their-Properties-Feldman-Ballard",
            "title": {
                "fragments": [],
                "text": "Connectionist models and their properties"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516142"
                        ],
                        "name": "D. Tank",
                        "slug": "D.-Tank",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Tank",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tank"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 32465715,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e69606729837aa1d0168c47f812cbccaba09dc83",
            "isKey": false,
            "numCitedBy": 344,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "An analog model neural network that can solve a general problem of recognizing patterns in a time-dependent signal is presented. The networks use a patterned set of delays to collectively focus stimulus sequence information to a neural state at a future time. The computational capabilities of the circuit are demonstrated on tasks somewhat similar to those necessary for the recognition of words in a continuous stream of speech. The network architecture can be understood from consideration of an energy function that is being minimized as the circuit computes. Neurobiological mechanisms are known for the generation of appropriate delays."
            },
            "slug": "Neural-computation-by-concentrating-information-in-Tank-Hopfield",
            "title": {
                "fragments": [],
                "text": "Neural computation by concentrating information in time."
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "An analog model neural network that can solve a general problem of recognizing patterns in a time-dependent signal is presented and can be understood from consideration of an energy function that is being minimized as the circuit computes."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15291527,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff2c2e3e83d1e8828695484728393c76ee07a101",
            "isKey": false,
            "numCitedBy": 15715,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The fundamental principles, basic mechanisms, and formal analyses involved in the development of parallel distributed processing (PDP) systems are presented in individual chapters contributed by leading experts. Topics examined include distributed representations, PDP models and general issues in cognitive science, feature discovery by competitive learning, the foundations of harmony theory, learning and relearning in Boltzmann machines, and learning internal representations by error propagation. Consideration is given to linear algebra in PDP, the logic of additive functions, resource requirements of standard and programmable nets, and the P3 parallel-network simulating system."
            },
            "slug": "Parallel-distributed-processing:-explorations-in-of-Rumelhart-McClelland",
            "title": {
                "fragments": [],
                "text": "Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3140752"
                        ],
                        "name": "Charles R. Rosenberg",
                        "slug": "Charles-R.-Rosenberg",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Rosenberg",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles R. Rosenberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 0
                            }
                        ],
                        "text": "Sejnowski and Rosenberg (1987) have shown that a network with one hidden layer can be trained to pronounce letters surprisingly well."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12926318,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de996c32045df6f7b404dda2a753b6a9becf3c08",
            "isKey": false,
            "numCitedBy": 1885,
            "numCiting": 229,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes NETtalk, a class of massively-parallel network systems that learn to convert English text to speech. The memory representations for pronunciations are learned by practice and are shared among many processing units. The performance of NETtalk has some similarities with observed human performance. (i) The learning follows a power law. (ii) The more words the network learns, the better it is at generalizing and correctly pronouncing new words, (iii) The performance of the network degrades very slowly as connections in the network are damaged: no single link or processing unit is essential. (iv) Relearning after damage is much faster than learning during the original training. (v) Distributed or spaced practice is more effective for long-term retention than massed practice. Network models can be constructed that have the same performance and learning characteristics on a particular task, but differ completely at the levels of synaptic strengths and single-unit responses. However, hierarchical clustering techniques applied to NETtalk reveal that these different networks have similar internal representations of letter-to-sound correspondences within groups of processing units. This suggests that invariant internal representations may be found in assemblies of neurons intermediate in size between highly localized and completely distributed representations."
            },
            "slug": "Parallel-Networks-that-Learn-to-Pronounce-English-Sejnowski-Rosenberg",
            "title": {
                "fragments": [],
                "text": "Parallel Networks that Learn to Pronounce English Text"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "H hierarchical clustering techniques applied to NETtalk reveal that these different networks have similar internal representations of letter-to-sound correspondences within groups of processing units, which suggests that invariant internal representations may be found in assemblies of neurons intermediate in size between highly localized and completely distributed representations."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 50027191,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3106e66537a0c8f53278e553bcb38f0b0992ec0e",
            "isKey": false,
            "numCitedBy": 1241,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a \u0302 network of simple computing elements and some entities to be represented, the most straightforward scheme is to use one computing element for each entity. This is called a local representation. It is easy to understand and easy to implement because the structure of the physical network mirrors the structure of the knowledge it contains. This report describes a different type of representation that is less familiar and harder to think about than local representations. Each entity is represented by a pattern of activity distributed over many computing elements, and each computing element is involved in representing many different entities. The strength of this more complicated kind of representation does not lie in its notational convenience or its ease of implementation in a conventional computer, but rather in the efficiency with which it makes use of the processing abilities of networks of simple, neuron-like computing elements. Every representational scheme has its good and bad points. Distributed representations are no exception. Some desirable properties like content-addressable memory and automatic generalization arise very naturally from the use of patterns of activity as representations. Other properties, like the ability to temporarily store a large set of arbitrary associations, are much harder to achieve. The best psychological evidence for distributed representations is the degree to which their strengths and weaknesses match those of the human mind. ^This research was supported by a grant from the System Development Foundation. I thank Jim Anderson, Dave Ackley Dana Ballard, Francis Crick, Scott Fahlman, Jerry Feldman, Christopher Longuet-Higgins, Don Norman, Terry Sejnowski, and Tim Shallice for helpful discussions. Jay McClelland and Dave Rumelhart helped me refine and rewrite many of the ideas presented here A substantially revised version of this report will appear as a chapter by Hinton, McClelland and Rumelhart in Parallel Distributed Processing: Explorations in the micro-structure of cognition, edited by McClelland and Rumelhart)"
            },
            "slug": "Distributed-Representations-Hinton-McClelland",
            "title": {
                "fragments": [],
                "text": "Distributed Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This report describes a different type of representation that is less familiar and harder to think about than local representations, which makes use of the processing abilities of networks of simple, neuron-like computing elements."
            },
            "venue": {
                "fragments": [],
                "text": "The Philosophy of Artificial Intelligence"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2246319"
                        ],
                        "name": "E. Bienenstock",
                        "slug": "E.-Bienenstock",
                        "structuredName": {
                            "firstName": "Elie",
                            "lastName": "Bienenstock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bienenstock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8884630"
                        ],
                        "name": "L. Cooper",
                        "slug": "L.-Cooper",
                        "structuredName": {
                            "firstName": "Leon",
                            "lastName": "Cooper",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Cooper"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2094648"
                        ],
                        "name": "P. Munro",
                        "slug": "P.-Munro",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Munro",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Munro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1607496,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "9f22cf81654dd50b95e65b86b1125cfe6803a67b",
            "isKey": false,
            "numCitedBy": 2695,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "The development of stimulus selectivity in the primary sensory cortex of higher vertebrates is considered in a general mathematical framework. A synaptic evolution scheme of a new kind is proposed in which incoming patterns rather than converging afferents compete. The change in the efficacy of a given synapse depends not only on instantaneous pre- and postsynaptic activities but also on a slowly varying time-averaged value of the postsynaptic activity. Assuming an appropriate nonlinear form for this dependence, development of selectivity is obtained under quite general conditions on the sensory environment. One does not require nonlinearity of the neuron's integrative power nor does one need to assume any particular form for intracortical circuitry. This is first illustrated in simple cases, e.g., when the environment consists of only two different stimuli presented alternately in a random manner. The following formal statement then holds: the state of the system converges with probability 1 to points of maximum selectivity in the state space. We next consider the problem of early development of orientation selectivity and binocular interaction in primary visual cortex. Giving the environment an appropriate form, we obtain orientation tuning curves and ocular dominance comparable to what is observed in normally reared adult cats or monkeys. Simulations with binocular input and various types of normal or altered environments show good agreement with the relevant experimental data. Experiments are suggested that could test our theory further."
            },
            "slug": "Theory-for-the-development-of-neuron-selectivity:-Bienenstock-Cooper",
            "title": {
                "fragments": [],
                "text": "Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "The development of stimulus selectivity in the primary sensory cortex of higher vertebrates is considered in a general mathematical framework and a synaptic evolution scheme of a new kind is proposed in which incoming patterns rather than converging afferents compete."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of neuroscience : the official journal of the Society for Neuroscience"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145559845"
                        ],
                        "name": "P. Anandan",
                        "slug": "P.-Anandan",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Anandan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Anandan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "Barto and Anandan [14] describe a learning procedure of this kind called \"associative reward-penalty\" or AR."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5915714,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0120eefaf05bfad5293e87f56d2e787c05f78cf7",
            "isKey": false,
            "numCitedBy": 328,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "A class of learning tasks is described that combines aspects of learning automation tasks and supervised learning pattern-classification tasks. These tasks are called associative reinforcement learning tasks. An algorithm is presented, called the associative reward-penalty, or AR-P algorithm for which a form of optimal performance is proved. This algorithm simultaneously generalizes a class of stochastic learning automata and a class of supervised learning pattern-classification methods related to the Robbins-Monro stochastic approximation procedure. The relevance of this hybrid algorithm is discussed with respect to the collective behaviour of learning automata and the behaviour of networks of pattern-classifying adaptive elements. Simulation results are presented that illustrate the associative reinforcement learning task and the performance of the AR-P algorithm as compared with that of several existing algorithms."
            },
            "slug": "Pattern-recognizing-stochastic-learning-automata-Barto-Anandan",
            "title": {
                "fragments": [],
                "text": "Pattern-recognizing stochastic learning automata"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A class of learning tasks is described that combines aspects of learning automation tasks and supervised learning pattern-classification tasks, called associative reinforcement learning tasks, and an algorithm is presented, called the associative reward-penalty, or AR-P algorithm, for which a form of optimal performance is proved."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man, and Cybernetics"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3160228"
                        ],
                        "name": "K. Fukushima",
                        "slug": "K.-Fukushima",
                        "structuredName": {
                            "firstName": "Kunihiko",
                            "lastName": "Fukushima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukushima"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3126340"
                        ],
                        "name": "S. Miyake",
                        "slug": "S.-Miyake",
                        "structuredName": {
                            "firstName": "Sei",
                            "lastName": "Miyake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Miyake"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2357880,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9aec973227713cd45f156090d82a3056cca8060f",
            "isKey": false,
            "numCitedBy": 697,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neocognitron:-A-new-algorithm-for-pattern-tolerant-Fukushima-Miyake",
            "title": {
                "fragments": [],
                "text": "Neocognitron: A new algorithm for pattern recognition tolerant of deformations and shifts in position"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741124"
                        ],
                        "name": "L. Valiant",
                        "slug": "L.-Valiant",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Valiant",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Valiant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 59712,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10ddb646feddc12337b5a755c72e153e37088c02",
            "isKey": false,
            "numCitedBy": 4184,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learnt using it in a reasonable (polynomial) number of steps. We find that inherent algorithmic complexity appears to set serious limits to the range of concepts that can be so learnt. The methodology and results suggest concrete principles for designing realistic learning systems."
            },
            "slug": "A-theory-of-the-learnable-Valiant",
            "title": {
                "fragments": [],
                "text": "A theory of the learnable"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper regards learning as the phenomenon of knowledge acquisition in the absence of explicit programming, and gives a precise methodology for studying this phenomenon from a computational viewpoint."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '84"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "93006261"
                        ],
                        "name": "D. I. Feinstein",
                        "slug": "D.-I.-Feinstein",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Feinstein",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. I. Feinstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50760571"
                        ],
                        "name": "R. Palmer",
                        "slug": "R.-Palmer",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Palmer",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Palmer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 4269710,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology",
                "Psychology"
            ],
            "id": "786b660e60069ecf01673ade33e287d6adb022b7",
            "isKey": false,
            "numCitedBy": 398,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Crick and Mitchison1 have presented a hypothesis for the functional role of dream sleep involving an \u2018unlearning\u2019 process. We have independently carried out mathematical and computer modelling of learning and \u2018unlearning\u2019 in a collective neural network of 30\u20131,000 neurones. The model network has a content-addressable memory or \u2018associative memory\u2019 which allows it to learn and store many memories. A particular memory can be evoked in its entirety when the network is stimulated by any adequate-sized subpart of the information of that memory2. But different memories of the same size are not equally easy to recall. Also, when memories are learned, spurious memories are also created and can also be evoked. Applying an \u2018unlearning\u2019 process, similar to the learning processes but with a reversed sign and starting from a noise input, enhances the performance of the network in accessing real memories and in minimizing spurious ones. Although our model was not motivated by higher nervous function, our system displays behaviours which are strikingly parallel to those needed for the hypothesized role of \u2018unlearning\u2019 in rapid eye movement (REM) sleep."
            },
            "slug": "\u2018Unlearning\u2019-has-a-stabilizing-effect-in-collective-Hopfield-Feinstein",
            "title": {
                "fragments": [],
                "text": "\u2018Unlearning\u2019 has a stabilizing effect in collective memories"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Although the model was not motivated by higher nervous function, the system displays behaviours which are strikingly parallel to those needed for the hypothesized role of \u2018unlearning\u2019 in rapid eye movement (REM) sleep."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47022813"
                        ],
                        "name": "J.A. Anderson",
                        "slug": "J.A.-Anderson",
                        "structuredName": {
                            "firstName": "J.A.",
                            "lastName": "Anderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J.A. Anderson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60643864,
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "id": "a275138be4d3133ba2e9d1a7a014e0afcbb5d546",
            "isKey": false,
            "numCitedBy": 854,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Contents: G.E. Hinton, J.A. Anderson, Introduction to the Updated Edition. D.E. Rumelhart, D.A. Norman, Introduction. J.A. Anderson, G.E. Hinton, Models of Information Processing in the Brain. J.A. Feldman, A Connectionist Model of Visual Memory. D. Willshaw, Holography, Associative Memory, and Inductive Generalization. T. Kohonen, E. Oja, P. Lehtio, Storage and Processing of Information in Distributed Associative Memory Systems. S.E. Fahlman, Representing Implicit Knowledge. G.E. Hinton, Implementing Semantic Networks in Parallel Hardware. T.J. Sejnowski, Skeleton Filters in the Brain. J.A. Anderson, M.C. Mozer, Categorization and Selective Neurons. S. Geman, Notes on a Self-Organizing Machine. R. Ratcliff, Parallel-Processing Mechanisms and Processing of Organized Information in Human Memory."
            },
            "slug": "Parallel-Models-of-Associative-Memory-Hinton-Anderson",
            "title": {
                "fragments": [],
                "text": "Parallel Models of Associative Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This chapter discusses G.E. Hinton's models of Information Processing in the Brain, Implementing Semantic Networks in Parallel Hardware, and R. Ratcliff's Parallel-Processing Mechanisms and Processing of Organized Information in Human Memory."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699108"
                        ],
                        "name": "G. Tesauro",
                        "slug": "G.-Tesauro",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Tesauro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Tesauro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13265971,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "948e0898db672a86841b10567afe3a15355531b1",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We st ud y th e amount of ti me needed to learn a fixed t rain\u00ad ing se t in the \"back-pro pagation\" proced ure for learning in multi-layer ne ural network models. The task chosen was 32-bit parity, a hi gh\u00ad order fu nct ion for wh ich memor iza ti on o f specific in p u t- out put pairs is necessary. For small t raining sets, the learning time is consistent with a ~-power law depen dence on the nu mber of patterns in the t ra ining set. For lar ger training set s, t he learn ing t ime dive rges at a critical t ra ining set size which appears to be related to the st orage capacity of t he network."
            },
            "slug": "Scaling-Relationships-in-Back-Propagation-Learning:-Tesauro",
            "title": {
                "fragments": [],
                "text": "Scaling Relationships in Back-Propagation Learning: Dependence on Training Set Size"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The task chosen was 32-bit parity, a challenge for learning in multi-layer ne ural network models where the learning time is consistent with a ~-power law depen dence on the nu mber of patterns in the t ra ining set."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680968"
                        ],
                        "name": "R. Prager",
                        "slug": "R.-Prager",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Prager",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Prager"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068002150"
                        ],
                        "name": "T. D. Harrison",
                        "slug": "T.-D.-Harrison",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Harrison",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. D. Harrison"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1998157"
                        ],
                        "name": "F. Fallside",
                        "slug": "F.-Fallside",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Fallside",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Fallside"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62211969,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "87d79c0c5255bce9dacaf4dab07d00c682200f2e",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Boltzmann-machines-for-speech-recognition-Prager-Harrison",
            "title": {
                "fragments": [],
                "text": "Boltzmann machines for speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145430701"
                        ],
                        "name": "J. Pearl",
                        "slug": "J.-Pearl",
                        "structuredName": {
                            "firstName": "Judea",
                            "lastName": "Pearl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pearl"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Pearl (1986) has shown that if the interactions between a set of probabilistic variables are constrained to form a tree structure, there are efficient parallel methods for estimating the interactions between \"hidden\" variables."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13723620,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb0419bccc2244ed33c9c42341f342511262daa3",
            "isKey": false,
            "numCitedBy": 2147,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Belief networks are directed acyclic graphs in which the nodes represent propositions (or variables), the arcs signify direct dependencies between the linked propositions, and the strengths of these dependencies are quantified by conditional probabilities. A network of this sort can be used to represent the generic knowledge of a domain expert, and it turns into a computational architecture if the links are used not merely for storing factual knowledge but also for directing and activating the data flow in the computations which manipulate this knowledge. The first part of the paper deals with the task of fusing and propagating the impacts of new information through the networks in such a way that, when equilibrium is reached, each proposition will be assigned a measure of belief consistent with the axioms of probability theory. It is shown that if the network is singly connected (e.g. tree-structured), then probabilities can be updated by local propagation in an isomorphic network of parallel and autonomous processors and that the impact of new information can be imparted to all propositions in time proportional to the longest path in the network. The second part of the paper deals with the problem of finding a tree-structured representation for a collection of probabilistically coupled propositions using auxiliary (dummy) variables, colloquially called \"hidden causes.\" It is shown that if such a tree-structured representation exists, then it is possible to uniquely uncover the topology of the tree by observing pairwise dependencies among the available propositions (i.e., the leaves of the tree). The entire tree structure, including the strengths of all internal relationships, can be reconstructed in time proportional to n log n, where n is the number of leaves."
            },
            "slug": "Fusion,-Propagation,-and-Structuring-in-Belief-Pearl",
            "title": {
                "fragments": [],
                "text": "Fusion, Propagation, and Structuring in Belief Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that if the network is singly connected (e.g. tree-structured), then probabilities can be updated by local propagation in an isomorphic network of parallel and autonomous processors and that the impact of new information can be imparted to all propositions in time proportional to the longest path in the network."
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 68
                            }
                        ],
                        "text": "There are many variations of competitive learning in the literature [4, 29, 33, 95] and there is not space here to review them all."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 34813152,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b635d109012bf6d0a8bc5e49b6c641137ac92c44",
            "isKey": false,
            "numCitedBy": 162,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "A field theory is proposed as a mathematical method for analyzing learning and self-organizing nerve nets and systems in a unified manner. It is shown by the use of the theory that a nerve net has an ability for automatically forming categorizers or signal detecting cells for the signals which the net receives from its environment. Moreover, when the set of signals has a topological structure, the detectors are arranged in the nerve system (or field) to preserve the topology, so that the topographical structure is introduced in the nerve system by self-organization."
            },
            "slug": "Field-theory-of-self-organizing-neural-nets-Amari",
            "title": {
                "fragments": [],
                "text": "Field theory of self-organizing neural nets"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "A field theory is proposed as a mathematical method for analyzing learning and self-organizing nerve nets and systems in a unified manner and shows by the use of the theory that a nerve net has an ability for automatically forming categorizers or signal detecting cells for the signals which the net receives from its environment."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man, and Cybernetics"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682174"
                        ],
                        "name": "S. Grossberg",
                        "slug": "S.-Grossberg",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Grossberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Grossberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 68
                            }
                        ],
                        "text": "There are many other interesting procedures which have been omitted [24, 26, 34, 35, 47, 54, 94]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14488376,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "c83e6a46aa91ad6aa42f2d20102407e2c372faee",
            "isKey": false,
            "numCitedBy": 467,
            "numCiting": 162,
            "paperAbstract": {
                "fragments": [],
                "text": "This article indicates how competition between afferent data and learned feedback expectancies can stabilize a developing code by buffering committed populations of detectors against continual erosion by new environmental demands. Tille gating phenomena that result lead to dynamically maintained critical peri(Jlds, and to attentional phenomena such as overshadowing in the adult. The fuillctional unit of cognitive coding is suggested to be an adaptive resonance, or amplification and ,prolongation of neural activity, that occurs when afferent data and efferent expectancies reach consensus through a matching process. The resonant state embodies the perceptual event, or attentional focus, and its amplified and sustained activities are capable of driving slow changes of long-term memor:r\"' Mismatch between afferent data and efferent expectancies yields a global sulppression of activity and triggers a reset of short-term memory, as well as raJ~id parallel search and hypothesis testing for uncommitted cells. These mechanisms help to explain and predict, as manifestations of the unified theme of stable code development, positive and negative aftereffects, the McCollough effect, spatial frequency adaptation, monocular rivalry, binocular rivalry and hysteresis, pattern completion, and Gestalt switching; analgesia, partial reinforcement acquisition effect, conditioned reinforcers, underaroused versus overaroused depression; the contingent negative variation, P300, and pontoge]lliculo-occipital waves; olfactory coding, corticogeniculate feedback, matching of proprioceptive and terminal motor maps, and cerebral dominance. The psychophysiological mechanisms that unify these effects are inherently nonlinear and parallel and are inequivalent to the computer, probabilistic, and linear models currently in use."
            },
            "slug": "How-does-a-brain-build-a-cognitive-code-Grossberg",
            "title": {
                "fragments": [],
                "text": "How does a brain build a cognitive code?"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This article indicates how competition between afferent data and learned feedback expectancies can stabilize a developing code by buffering committed populations of detectors against continual erosion by new environmental demands."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691804"
                        ],
                        "name": "D. Ballard",
                        "slug": "D.-Ballard",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Ballard",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ballard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4318006,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "badb0516cf0ca095c4370a41795323e40e43372c",
            "isKey": false,
            "numCitedBy": 344,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The functional abilities and parallel architecture of the human visual system are a rich source of ideas about visual processing. Any visual task that we can perform quickly and effortlessly is likely to have a computational solution using a parallel algorithm. Recently, several such parallel algorithms have been found that exploit information implicit in an image to compute intrinsic properties of surfaces, such as surface orientation, reflectance and depth. These algorithms require a computational architecture that has similarities to that of visual cortex in primates."
            },
            "slug": "Parallel-visual-computation-Ballard-Hinton",
            "title": {
                "fragments": [],
                "text": "Parallel visual computation"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The functional abilities and parallel architecture of the human visual system are a rich source of ideas about visual processing and several parallel algorithms have been found that exploit information implicit in an image to compute intrinsic properties of surfaces, such as surface orientation, reflectance and depth."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308302"
                        ],
                        "name": "D. Ackley",
                        "slug": "D.-Ackley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Ackley",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ackley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 224
                            }
                        ],
                        "text": "It is possible to combine genetic learning with gradient descent (or hill climbing) to get a hybrid learning procedure called \"iterated genetic hill climbing\" or \" IGH\" that works better than either learning procedure alone [1, 17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 7
                            }
                        ],
                        "text": "Ackley [1] shows that a stochastic variation of IGH can be implemented in a connectionist network that is trying to learn which output vector produces a high enough payoff to satisfy some external criterion."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 124451641,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d9133c9374d07c5f99ae5499fddf3fd7f439356e",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In the \"black box function optimization\" problem, a search strategy is required to find an extremal point of a function without knowing the structure of the function or the range of possible function values. Solving such problems efficiently requires two abilities. On the one hand, a strategy must be capable of learning while searching: It must gather global information about the space and concentrate the search in the most promising regions. On the other hand, a strategy must be capable of sustained exploration: If a search of the most promising region does not uncover a satisfactory point, the strategy must redirect its efforts into other regions of the space. \nThis dissertation describes a connectionist learning machine that produces a search strategy called stochastic iterated genetic hillclimbing (SIGH). Viewed over a short period of time, SIGH displays a coarse-to-fine searching strategy, like simulated annealing and genetic algorithms. However, in SIGH the convergence process is reversible. The connectionist implementation makes it possible to diverge the search after it has converged, and to recover coarse-grained information about the space that was suppressed during convergence. The successful optimization of a complex function by SIGH usually involves a series of such converge/diverge cycles. \nSIGH can be viewed as a generalization of a genetic algorithm and a stochastic hillclimbing algorithm, in which genetic search discovers starting points for subsequent hillclimbing, and hillclimbing biases the population for subsequent genetic search. Several search stratgies--including SIGH, hillclimbers, genetic algorithms, and simulated annealing--are tested on a set of illustrative functions and on a series of graph partitioning problems. SIGH is competitive with genetic algorithms and simulated annealing in most cases, and markedly superior in a function where the uphill directions usually lead away from the global maximum. In that case, SIGH's ability to pass information from one coarse-to-fine search to the next is crucial. Combinations of genetic and hillclimbing techniques can offer dramatic performance improvements over either technique alone."
            },
            "slug": "Stochastic-iterated-genetic-hillclimbing-Ackley",
            "title": {
                "fragments": [],
                "text": "Stochastic iterated genetic hillclimbing"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This dissertation describes a connectionist learning machine that produces a search strategy called stochastic iterated genetic hillclimbing (SIGH), which is competitive with genetic algorithms and simulated annealing in most cases, and markedly superior in a function where the uphill directions usually lead away from the global maximum."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40227361"
                        ],
                        "name": "D. Willshaw",
                        "slug": "D.-Willshaw",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Willshaw",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Willshaw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46714951"
                        ],
                        "name": "C. von der Malsburg",
                        "slug": "C.-von-der-Malsburg",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "von der Malsburg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. von der Malsburg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 39857778,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "87e3c0c31d5c7abb293840ca3511985f0aaeb875",
            "isKey": false,
            "numCitedBy": 252,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper examines the idea that ordered patterns of nerve connections are set up by means of markers carried by the individual cells. The case of the ordered retinotectal projection in amphibia and fishes is discussed in great detail. It is suggested that retinotectal mappings are the result of two mechanisms acting in concert. One mechanism induces a set of retinal markers into the tectum. By this means, an initially haphazard pattern of synapses is transformed into a continuous or piece-wise continuous projection. The other mechanism places the individual pieces of the map in the correct orientation. The machinery necessary for this inductive scheme has been expressed in terms of a set of differential equations, which have been solved numerically for a number of cases. Straightforward assumptions are made as to how markers are distributed in the retina; how they are induced into the tectum; and how the induced markers bring about alterations in the pattern of synaptic contacts. A detailed physiological interpretation of the model is given. The inductive mechanism has been formulated at the level of the individual synaptic interactions. Therefore, it is possible to specify, in a given situation, not only the nature of the end state of the mapping but also how the mapping develops over time. The role of the modes of growth of retina and tectum in shaping the developing projection becomes clear. Since, on this model, the tectum is initially devoid of markers, there is an important difference between the development and the regeneration of ordered mappings. In the development of duplicate maps from various types of compound-eyes, it is suggested that the tectum, rather than the retina, contains an abnormal distribution of markers. An important parameter in these experiments, and also in the regeneration experiments where part-duplication has been found, is the range of interaction amongst the retinal cells. It is suggested that the results of many of the regeneration experiments (including apparently contradictory ones) are manifestations of a conflict between the two alternative ways of specifying the orientation of the map: through the information carried by the markers previously induced into the tectum and through the orientation mechanism itself."
            },
            "slug": "A-marker-induction-mechanism-for-the-establishment-Willshaw-Malsburg",
            "title": {
                "fragments": [],
                "text": "A marker induction mechanism for the establishment of ordered neural mappings: its application to the retinotectal problem."
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This paper examines the idea that ordered patterns of nerve connections are set up by means of markers carried by the individual cells, and suggests that the results of many of the regeneration experiments are manifestations of a conflict between the two alternative ways of specifying the orientation of the map."
            },
            "venue": {
                "fragments": [],
                "text": "Philosophical transactions of the Royal Society of London. Series B, Biological sciences"
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785805"
                        ],
                        "name": "M. Thathachar",
                        "slug": "M.-Thathachar",
                        "structuredName": {
                            "firstName": "Mandayam",
                            "lastName": "Thathachar",
                            "middleNames": [
                                "A.",
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Thathachar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143637056"
                        ],
                        "name": "P. Sastry",
                        "slug": "P.-Sastry",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Sastry",
                            "middleNames": [
                                "Shanti"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Sastry"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6530684,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7b9a65336d3d80c8def61a2645cc1ebc0661056",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of learning correct decision rules to minimize the probability of misclassification is a long-standing problem of supervised learning in pattern recognition. The problem of learning such optimal discriminant functions is considered for the class of problems where the statistical properties of the pattern classes are completely unknown. The problem is posed as a game with common payoff played by a team of mutually cooperating learning automata. This essentially results in a probabilistic search through the space of classifiers. The approach is inherently capable of learning discriminant functions that are nonlinear in their parameters also. A learning algorithm is presented for the team and convergence is established. It is proved that the team can obtain the optimal classifier to an arbitrary approximation. Simulation results with a few examples are presented where the team learns the optimal classifier."
            },
            "slug": "Learning-Optimal-Discriminant-Functions-through-a-Thathachar-Sastry",
            "title": {
                "fragments": [],
                "text": "Learning Optimal Discriminant Functions through a Cooperative Game of Automata"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is proved that the team can obtain the optimal classifier to an arbitrary approximation when posed as a game with common payoff played by a team of mutually cooperating learning automata."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man, and Cybernetics"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40396597"
                        ],
                        "name": "Toshiyuki Hanazawa",
                        "slug": "Toshiyuki-Hanazawa",
                        "structuredName": {
                            "firstName": "Toshiyuki",
                            "lastName": "Hanazawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Toshiyuki Hanazawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9243990"
                        ],
                        "name": "K. Shikano",
                        "slug": "K.-Shikano",
                        "structuredName": {
                            "firstName": "Kiyohiro",
                            "lastName": "Shikano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Shikano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49464494"
                        ],
                        "name": "Kevin J. Lang",
                        "slug": "Kevin-J.-Lang",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Lang",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin J. Lang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9563026,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd62c9976534a6a2096a38244f6cbb03635a127e",
            "isKey": false,
            "numCitedBy": 2786,
            "numCiting": 92,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: (1) using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation; and (2) the time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independently of position in time and therefore not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes B, D, and G in varying phonetic contexts was chosen. For comparison, several discrete hidden Markov models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5% correct while the rate obtained by the best of the HMMs was only 93.7%. >"
            },
            "slug": "Phoneme-recognition-using-time-delay-neural-Waibel-Hanazawa",
            "title": {
                "fragments": [],
                "text": "Phoneme recognition using time-delay neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "94059053"
                        ],
                        "name": "H. Simon",
                        "slug": "H.-Simon",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Simon",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Simon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 160
                            }
                        ],
                        "text": "One of the best and commonest ways of fighting complexity is to introduce a modular, hierarchical structure in which different modules are only loosely coupled (Simon, 1969)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62586095,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "724e671b43ba51113e92c68f05301c686ad2fc1c",
            "isKey": false,
            "numCitedBy": 8151,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Continuing his exploration of the organization of complexity and the science of design, this new edition of Herbert Simon's classic work on artificial intelligence adds a chapter that sorts out the current themes and tools -- chaos, adaptive systems, genetic algorithms -- for analyzing complexity and complex systems. There are updates throughout the book as well. These take into account important advances in cognitive psychology and the science of design while confirming and extending the book's basic thesis: that a physical symbol system has the necessary and sufficient means for intelligent action. The chapter \"Economic Reality\" has also been revised to reflect a change in emphasis in Simon's thinking about the respective roles of organizations and markets in economic systems."
            },
            "slug": "The-Sciences-of-the-Artificial-Simon",
            "title": {
                "fragments": [],
                "text": "The Sciences of the Artificial"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87253279"
                        ],
                        "name": "J. Sejnowski",
                        "slug": "J.-Sejnowski",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Sejnowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 63
                            }
                        ],
                        "text": "These issues are examined in detail by Hummel and Zucker [52], Hinton and Sejnowski [45], Geman and Geman [31], Hopfield and Tank [51] and Marroquin [65]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 337,
                                "start": 333
                            }
                        ],
                        "text": "There are, of course, many complexities: Under what conditions will the network settle to a stable solution? Will this solution be the optimal one? How long will it take to settle? What is the precise relationship between weights and probabilities? These issues are examined in detail by Hummel and Zucker [52], Hinton and Sejnowski [45], Geman and Geman [31], Hopfield and Tank [51] and Marroquin [65]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10379672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1718965f492d4e9fe1d98a3fb83efe671a4aed2c",
            "isKey": false,
            "numCitedBy": 557,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "When a vision system creates an interpretation of some input datn, it assigns truth values or probabilities to intcrnal hypothcses about the world. We present a non-dctcrministic method for assigning truth values that avoids many of the problcms encountered by existing relaxation methods. Instead of rcprcscnting probabilitics with realnumbers, we usc a more dircct encoding in which thc probability \\ associated with a hypotlmis is rcprcscntcd by the probability hat it is in one of two states, true or false. Wc give a particular nondeterministic operator, based on statistical mechanics, for updating the truth values of hypothcses. The operator ensures that the probability of discovering a particular combination of hypothcscs is a simplc function of how good that combination is. Wc show that thcrc is a simple relationship bctween this operator and Bayesian inference, and we describe a learning rule which allows a parallel system to converge on a set ofweights that optimizes its perccptt~al inferences."
            },
            "slug": "OPTIMAL-PERCEPTUAL-INFERENCE-Hinton-Sejnowski",
            "title": {
                "fragments": [],
                "text": "OPTIMAL PERCEPTUAL INFERENCE"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A particular nondeterministic operator is given, based on statistical mechanics, for updating the truth values of hypothcses, and a learning rule is described which allows a parallel system to converge on a set ofweights that optimizes its perccptt~al inferences."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802785"
                        ],
                        "name": "S. Nowlan",
                        "slug": "S.-Nowlan",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Nowlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nowlan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "The evolutionary search for good architectures may actually be guided by learning [43]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2937027,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "f9197ff9fdabd2b78bfe0602365011c6699b0d66",
            "isKey": false,
            "numCitedBy": 1187,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The assumption that acquired character istics are not in\u00ad herited is ofte n taken to imply t hat t he adaptations t hat an organism learns dur ing its lifeti me cannot guide t he course of evolut ion . This infere nce is incor rec t (2). Learni ng alt ers the shape of t he search space in which evolu tio n operates and thereby pro vides good evolut ion ar y paths towa rds sets of co-adapted alleles. We demonst r at e t hat th is effect allows learning organisms to evolve much faster than their 000 \u00ad learning equivalents, even though the characteris tics acquired by t he phenotype are not communicated to the genotype."
            },
            "slug": "How-Learning-Can-Guide-Evolution-Hinton-Nowlan",
            "title": {
                "fragments": [],
                "text": "How Learning Can Guide Evolution"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30489560"
                        ],
                        "name": "R. Hummel",
                        "slug": "R.-Hummel",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Hummel",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hummel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698824"
                        ],
                        "name": "S. Zucker",
                        "slug": "S.-Zucker",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Zucker",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Zucker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 38740,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4fc956c8d8b41c1e9c499cad0cf882debf45ca64",
            "isKey": false,
            "numCitedBy": 981,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "A large class of problems can be formulated in terms of the assignment of labels to objects. Frequently, processes are needed which reduce ambiguity and noise, and select the best label among several possible choices. Relaxation labeling processes are just such a class of algorithms. They are based on the parallel use of local constraints between labels. This paper develops a theory to characterize the goal of relaxation labeling. The theory is founded on a definition of con-sistency in labelings, extending the notion of constraint satisfaction. In certain restricted circumstances, an explicit functional exists that can be maximized to guide the search for consistent labelings. This functional is used to derive a new relaxation labeling operator. When the restrictions are not satisfied, the theory relies on variational cal-culus. It is shown that the problem of finding consistent labelings is equivalent to solving a variational inequality. A procedure nearly identical to the relaxation operator derived under restricted circum-stances serves in the more general setting. Further, a local convergence result is established for this operator. The standard relaxation labeling formulas are shown to approximate our new operator, which leads us to conjecture that successful applications of the standard methods are explainable by the theory developed here. Observations about con-vergence and generalizations to higher order compatibility relations are described."
            },
            "slug": "On-the-Foundations-of-Relaxation-Labeling-Processes-Hummel-Zucker",
            "title": {
                "fragments": [],
                "text": "On the Foundations of Relaxation Labeling Processes"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that the problem of finding consistent labelings is equivalent to solving a variational inequality, and a procedure nearly identical to the relaxation operator derived under restricted circum-stances serves in the more general setting."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1960915"
                        ],
                        "name": "G. Weisbuch",
                        "slug": "G.-Weisbuch",
                        "structuredName": {
                            "firstName": "G\u00e9rard",
                            "lastName": "Weisbuch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Weisbuch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399006281"
                        ],
                        "name": "F. Fogelman-Souli\u00e9",
                        "slug": "F.-Fogelman-Souli\u00e9",
                        "structuredName": {
                            "firstName": "Fran\u00e7oise",
                            "lastName": "Fogelman-Souli\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Fogelman-Souli\u00e9"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 45
                            }
                        ],
                        "text": "It can be improved by introducing thtasholds (Weisbuch and Fogelman-Soulie, 1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16076015,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "20bc25da33314d4dc157419f1321f962c59cca3f",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Networks of threshold automata are random dynamical systems with a large number of attractors, which J. Hopfield proposed to use as associative memories. We establish the scaling laws relating the maximum number of \u00abuseful\u00bb attractors and the radius of the attraction basin to the number of automata. A by-product of our analysis is a better choice for thresholds which doubles the performances in terms of the maximum number of \u00abuseful\u00bb attractors Les reseaux d'automates a seuil sont des systemes dynamiques a structure aleatoire semblables aux verres de spins dont J. Hopfield a propose l'application comme memoires associatives. Nous etablissons les lois d'echelles reliant le nombre maximum d'attracteurs utiles et la distance d'attraction, au nombre des automates du reseau. Notre approche permet aussi un meilleur choix des seuils, ce qui double les performances du reseau en nombre d'attracteurs."
            },
            "slug": "Scaling-laws-for-the-attractors-of-Hopfield-Weisbuch-Fogelman-Souli\u00e9",
            "title": {
                "fragments": [],
                "text": "Scaling laws for the attractors of Hopfield networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 47
                            }
                        ],
                        "text": "Various other methods have also been suggested [5, 71, 75]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 31220579,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1339348aeef592802288d9d929a085cb3ae61c4b",
            "isKey": false,
            "numCitedBy": 451,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes error-correction adjustment procedures for determining the weight vector of linear pattern classifiers under general pattern distribution. It is mainly aimed at clarifying theoretically the performance of adaptive pattern classifiers. In the case where the loss depends on the distance between a pattern vector and a decision boundary and where the average risk function is unimodal, it is proved that, by the procedures proposed here, the weight vector converges to the optimal one even under nonseparable pattern distributions. The speed and the accuracy of convergence are analyzed, and it is shown that there is an important tradeoff between speed and accuracy of convergence. Dynamical behaviors, when the probability distributions of patterns are changing, are also shown. The theory is generalized and made applicable to the case with general discriminant functions, including piecewise-linear discriminant functions."
            },
            "slug": "A-Theory-of-Adaptive-Pattern-Classifiers-Amari",
            "title": {
                "fragments": [],
                "text": "A Theory of Adaptive Pattern Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "It is proved that, by the procedures proposed here, the weight vector converges to the optimal one even under nonseparable pattern distributions, and there is an important tradeoff between speed and accuracy of convergence."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Electron. Comput."
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144404817"
                        ],
                        "name": "J. Holland",
                        "slug": "J.-Holland",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Holland",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Holland"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 313,
                                "start": 288
                            }
                        ],
                        "text": "There are, of course, many complexities: Under what conditions will the network settle to a stable solution? Will this solution be the optimal one? How long will it take to settle? What is the precise relationship between weights and probabilities? These issues are examined in detail by Hummel and Zucker (1983), Hinton and Sejnowski (1983), Gernert and Geman (1984), Hopfield and Tank (1985) and Marroquin (1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 27
                            }
                        ],
                        "text": "Holland and his co-workers (Holland, 1975; Gretenstette, 1985) have investigated a class of learning procedures which they call \"genetic algorithms\" because they are explicitly inspired by an analogy with evolution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 58781161,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b4279db68b16e20fbc56f9d41980a950191d30a",
            "isKey": false,
            "numCitedBy": 38739,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Name of founding work in the area. Adaptation is key to survival and evolution. Evolution implicitly optimizes organisims. AI wants to mimic biological optimization { Survival of the ttest { Exploration and exploitation { Niche nding { Robust across changing environments (Mammals v. Dinos) { Self-regulation,-repair and-reproduction 2 Artiicial Inteligence Some deenitions { \"Making computers do what they do in the movies\" { \"Making computers do what humans (currently) do best\" { \"Giving computers common sense; letting them make simple deci-sions\" (do as I want, not what I say) { \"Anything too new to be pidgeonholed\" Adaptation and modiication is root of intelligence Some (Non-GA) branches of AI: { Expert Systems (Rule based deduction)"
            },
            "slug": "Adaptation-in-natural-and-artificial-systems-Holland",
            "title": {
                "fragments": [],
                "text": "Adaptation in natural and artificial systems"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "Names of founding work in the area of Adaptation and modiication, which aims to mimic biological optimization, and some (Non-GA) branches of AI."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688681"
                        ],
                        "name": "T. Kohonen",
                        "slug": "T.-Kohonen",
                        "structuredName": {
                            "firstName": "Teuvo",
                            "lastName": "Kohonen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kohonen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Kohonen (1977) shows that the weight matrix, W, that minimizes this error measure can be expressed analytically as: us = BA*"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 126
                            }
                        ],
                        "text": "allowed to be active at a time and physically adjacent pairs of hidden units inhibit each other less than more distant pairs (Kohonen, 1982; Amari, 1983). A model with similarities to competitive learning has been t'sed by Willshaw and Von der Malsburg (1979) to explain the formation of topographic maps between the retina and the tectum."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60862320,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "f3a217c11175f2cf904b2f7f6378b7ade176f2d0",
            "isKey": false,
            "numCitedBy": 596,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Why should wait for some days to get or receive the associative memory a system theoretical approach book that you order? Why should you take it if you can get the faster one? You can find the same book that you order right here. This is it the book that you can receive directly after purchasing. This associative memory a system theoretical approach is well known book in the world, of course many people will try to own it. Why don't you become the first? Still confused with the way?"
            },
            "slug": "Associative-memory.-A-system-theoretical-approach-Kohonen",
            "title": {
                "fragments": [],
                "text": "Associative memory. A system-theoretical approach"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This associative memory a system theoretical approach is well known book in the world, of course many people will try to own it and this is it the book that you can receive directly after purchasing."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2630782"
                        ],
                        "name": "J. Marroqu\u00edn",
                        "slug": "J.-Marroqu\u00edn",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Marroqu\u00edn",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Marroqu\u00edn"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 36917179,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "64548b6a6176baf6093f25057cd892c340b1828e",
            "isKey": false,
            "numCitedBy": 181,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : In this thesis we study the general problem of reconstructing a function, defined on a finite lattice, from a set of incomplete, noisy and/or ambiguous observations. The goal of this work is to demonstrate the generality and practical value of a probabilistic (in particular, Bayesian) approach to this problem, particularly in the context of Computer Vision. In this approach, the prior knowledge about the solution is expressed in the form of a Gibbsian probability distribution on the space of all possible functions, so that the reconstruction task in formulated as an estimation problem. Keywords: Inverse problems; Computer vision; Surface interpolation; Image restoration; Markov random fields; Optimal estimation; Simulated annealing."
            },
            "slug": "Probabilistic-solution-of-inverse-problems-Marroqu\u00edn",
            "title": {
                "fragments": [],
                "text": "Probabilistic solution of inverse problems"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The goal of this work is to demonstrate the generality and practical value of a probabilistic approach to this problem, particularly in the context of Computer Vision, with a Gibbsian probability distribution on the space of all possible functions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053227062"
                        ],
                        "name": "Richard Durbin",
                        "slug": "Richard-Durbin",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Durbin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard Durbin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40227361"
                        ],
                        "name": "D. Willshaw",
                        "slug": "D.-Willshaw",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Willshaw",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Willshaw"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 4321691,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a94be030ccd68f3a5a3bf9245137fe114c549819",
            "isKey": false,
            "numCitedBy": 855,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The travelling salesman problem1 is a classical problem in the field of combinatorial optimization, concerned with efficient methods for maximizing or minimizing a function of many independent variables. Given the positions of N cities, which in the simplest case lie in the plane, what is the shortest closed tour in which each city can be visited once? We describe how a parallel analogue algorithm, derived from a formal model2\u20133 for the establishment of topographically ordered projections in the brain4\u201310, can be applied to the travelling salesman problem1,11,12. Using an iterative procedure, a circular closed path is gradually elongated non-uniformly until it eventually passes sufficiently near to all the cities to define a tour. This produces shorter tour lengths than another recent parallel analogue algorithm13, scales well with the size of the problem, and is naturally extendable to a large class of optimization problems involving topographic mappings between geometrical structures14."
            },
            "slug": "An-analogue-approach-to-the-travelling-salesman-an-Durbin-Willshaw",
            "title": {
                "fragments": [],
                "text": "An analogue approach to the travelling salesman problem using an elastic net method"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work describes how a parallel analogue algorithm, derived from a formal model for the establishment of topographically ordered projections in the brain, can be applied to the travelling salesman problem, and produces shorter tour lengths than another recent parallel analogue algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 342,
                                "start": 314
                            }
                        ],
                        "text": "There are, of course, many complexities: Under what conditions will the network settle to a stable solution? Will this solution be the optimal one? How long will it take to settle? What is the precise relationship between weights and probabilities? These issues are examined in detail by Hummel and Zucker (1983), Hinton and Sejnowski (1983), Gernert and Geman (1984), Hopfield and Tank (1985) and Marroquin (1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 39
                            }
                        ],
                        "text": "Boltzmann machines A Boltzmann machine (Ackley, Hinton, and Sejnowski, 1985; Hinton and Sejnowski, 1986) is a generalization of a Hopfield net (see section 4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 58779360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8592e46a5435d18bba70557846f47290b34c1aa5",
            "isKey": false,
            "numCitedBy": 1338,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Relaxation Searches, Easy and Hard Learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, An Example of the Effects of Damage, Conclusion, Acknowledgments, Appendix: Derivation of the Learning Algorithm, References"
            },
            "slug": "Learning-and-relearning-in-Boltzmann-machines-Hinton-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Learning and relearning in Boltzmann machines"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "This chapter contains sections titled: Relaxation Searches, Easy and Hard learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, and an Example of the Effects of Damage."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707642"
                        ],
                        "name": "D. Geman",
                        "slug": "D.-Geman",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 90
                            }
                        ],
                        "text": "These issues are examined in detail by Hummel and Zucker [52], Hinton and Sejnowski [45], Geman and Geman [31], Hopfield and Tank [51] and Marroquin [65]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 359,
                                "start": 355
                            }
                        ],
                        "text": "There are, of course, many complexities: Under what conditions will the network settle to a stable solution? Will this solution be the optimal one? How long will it take to settle? What is the precise relationship between weights and probabilities? These issues are examined in detail by Hummel and Zucker [52], Hinton and Sejnowski [45], Geman and Geman [31], Hopfield and Tank [51] and Marroquin [65]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5837272,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "459b30a9a960080f3b313e41886b1aa0e51e882c",
            "isKey": false,
            "numCitedBy": 18705,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios."
            },
            "slug": "Stochastic-Relaxation,-Gibbs-Distributions,-and-the-Geman-Geman",
            "title": {
                "fragments": [],
                "text": "Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "The analogy between images and statistical mechanics systems is made and the analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations, creating a highly parallel ``relaxation'' algorithm for MAP estimation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075137204"
                        ],
                        "name": "R. Brady",
                        "slug": "R.-Brady",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Brady",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Brady"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 150
                            }
                        ],
                        "text": "climbing) to get a hybrid learning procedure called \"iterated genetic hill climbing\" or \" IGH\" that works better than either learning procedure alone [1, 17]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 4316295,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "fee79cb35c9bdcb327a035b44e6fc75a43e1d428",
            "isKey": false,
            "numCitedBy": 201,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Several problems, in particular the \u2018travelling salesman\u2019 problem1 wherein one seeks the shortest route encompassing a randomly distributed group of cities, have been optimized by repeated random alteration (mutation) of a trial solution followed by selection of the cheaper (fitter) solution. Most non-trivial problems have complicated fitness functions, and optimization tends to become stuck in local fitness maxima. A recently introduced strategy to escape (simulated annealing) involves accepting unfavourable mutations with finite probability1\u20133. Independently, there has been interest in genetic strategies which overcome the problem of fitness maxima in biological evolution4\u20136, and several authors have applied biological elements to optimization7,8. Here we use computer algorithms to investigate new strategies for the 64-city travelling salesman problem, which combine conventional optimization or \u2018quenching\u2019 with biological elements, namely having a population of trial solutions, helping weaker individuals to survive, and an analogue of sexual crossing-over of genes. The new strategies were faster and gave better results than simulated annealing."
            },
            "slug": "Optimization-strategies-gleaned-from-biological-Brady",
            "title": {
                "fragments": [],
                "text": "Optimization strategies gleaned from biological evolution"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Computer algorithms are used to investigate new strategies for the 64-city travelling salesman problem, which combine conventional optimization or \u2018quenching\u2019 with biological elements, namely having a population of trial solutions, helping weaker individuals to survive, and an analogue of sexual crossing-over of genes."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145847131"
                        ],
                        "name": "S. Kirkpatrick",
                        "slug": "S.-Kirkpatrick",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Kirkpatrick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kirkpatrick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5882723"
                        ],
                        "name": "C. D. Gelatt",
                        "slug": "C.-D.-Gelatt",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Gelatt",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. D. Gelatt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "88645967"
                        ],
                        "name": "M. Vecchi",
                        "slug": "M.-Vecchi",
                        "structuredName": {
                            "firstName": "Michelle",
                            "lastName": "Vecchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Vecchi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 205939,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "dd5061631a4d11fa394f4421700ebf7e78dcbc59",
            "isKey": false,
            "numCitedBy": 39632,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods."
            },
            "slug": "Optimization-by-Simulated-Annealing-Kirkpatrick-Gelatt",
            "title": {
                "fragments": [],
                "text": "Optimization by Simulated Annealing"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2911738"
                        ],
                        "name": "D. Angluin",
                        "slug": "D.-Angluin",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Angluin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Angluin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152471016"
                        ],
                        "name": "Carl H. Smith",
                        "slug": "Carl-H.-Smith",
                        "structuredName": {
                            "firstName": "Carl H.",
                            "lastName": "Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carl H. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3209224,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6524a6b8e6091c0a11d665180a5ad94bbf1d3b4",
            "isKey": false,
            "numCitedBy": 900,
            "numCiting": 190,
            "paperAbstract": {
                "fragments": [],
                "text": "There has been a great deal of theoretical and experimental work in computer science on inductive inference systems, that is, systems that try to infer general rules from examples. However, a complete and applicable theory of such systems is still a distant goal. This survey highlights and explains the main ideas that have been developed in the study of inductive inference, with special emphasis on the relations between the general theory and the specific algorithms and implementations. 154 references."
            },
            "slug": "Inductive-Inference:-Theory-and-Methods-Angluin-Smith",
            "title": {
                "fragments": [],
                "text": "Inductive Inference: Theory and Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This survey highlights and explains the main ideas that have been developed in the study of inductive inference, with special emphasis on the relations between the general theory and the specific algorithms and implementations."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62245742,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "111fd833a4ae576cfdbb27d87d2f8fc0640af355",
            "isKey": false,
            "numCitedBy": 19355,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-internal-representations-by-error-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning internal representations by error propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2696176"
                        ],
                        "name": "L. Bahl",
                        "slug": "L.-Bahl",
                        "structuredName": {
                            "firstName": "Lalit",
                            "lastName": "Bahl",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474650"
                        ],
                        "name": "R. Mercer",
                        "slug": "R.-Mercer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Mercer",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mercer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 59
                            }
                        ],
                        "text": "The best existing techniques, such as hidden Markov models [9], are significantly worse than people, and an improvement in the quality of recognition would be of great"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14789841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58",
            "isKey": false,
            "numCitedBy": 1403,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Speech recognition is formulated as a problem of maximum likelihood decoding. This formulation requires statistical models of the speech production process. In this paper, we describe a number of statistical models for use in speech recognition. We give special attention to determining the parameters for such models from sparse data. We also describe two decoding methods, one appropriate for constrained artificial languages and one appropriate for more realistic decoding tasks. To illustrate the usefulness of the methods described, we review a number of decoding results that have been obtained with them."
            },
            "slug": "A-Maximum-Likelihood-Approach-to-Continuous-Speech-Bahl-Jelinek",
            "title": {
                "fragments": [],
                "text": "A Maximum Likelihood Approach to Continuous Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper describes a number of statistical models for use in speech recognition, with special attention to determining the parameters for such models from sparse data, and describes two decoding methods appropriate for constrained artificial languages and one appropriate for more realistic decoding tasks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1847175"
                        ],
                        "name": "M. Minsky",
                        "slug": "M.-Minsky",
                        "structuredName": {
                            "firstName": "Marvin",
                            "lastName": "Minsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Minsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 272,
                                "start": 244
                            }
                        ],
                        "text": "Some preliminary work has been done on the representation of inheritance hierarchies and the representation of frame-like structures in which a whole object is composed of a number of parts each of which plays a different role within the whole (Minsky, 1977; Hinton, 1981)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7612202,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "4e52208addbaaa7ba9c2d47e6c0961f029b0d880",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper is based on a theory being developed in collaboration with Seymour Papert [Note 1] in which we view the mind as an organized society of intercommunicating \"agents\". Each such agent is, by itself, very simple. The subject of this paper is how that simplicity affects communication between different parts of a single mind and, indirectly, how it may affect interpersonal communications."
            },
            "slug": "Plain-Talk-about-Neurodevelopmental-Epistemology-Minsky",
            "title": {
                "fragments": [],
                "text": "Plain Talk about Neurodevelopmental Epistemology"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "This paper is based on a theory being developed in collaboration with Seymour Papert in which the mind is viewed as an organized society of intercommunicating \"agents\", each such agent is, by itself, very simple."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4113648"
                        ],
                        "name": "F. Crick",
                        "slug": "F.-Crick",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Crick",
                            "middleNames": [
                                "H.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Crick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145666307"
                        ],
                        "name": "G. Mitchison",
                        "slug": "G.-Mitchison",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Mitchison",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Mitchison"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 41500914,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "736ad2a6e6fa37e6b0fdf0cadcb788e248adfa06",
            "isKey": false,
            "numCitedBy": 819,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose that the function of dream sleep (more properly rapid-eye movement or REM sleep) is to remove certain undesirable modes of interaction in networks of cells in the cerebral cortex. We postulate that this is done in REM sleep by a reverse learning mechanism (see also p. 158), so that the trace in the brain of the unconscious dream is weakened, rather than strengthened, by the dream."
            },
            "slug": "The-function-of-dream-sleep-Crick-Mitchison",
            "title": {
                "fragments": [],
                "text": "The function of dream sleep"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "It is proposed that the function of dream sleep is to remove certain undesirable modes of interaction in networks of cells in the cerebral cortex by a reverse learning mechanism, so that the trace in the brain of the unconscious dream is weakened, rather than strengthened, by the dream."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 120
                            }
                        ],
                        "text": "The measure is actually the G measure described in (11) and the proof is an adaptation of the proof of the EM procedure [22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": false,
            "numCitedBy": 48399,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060224464"
                        ],
                        "name": "Y. L. Cun",
                        "slug": "Y.-L.-Cun",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Cun",
                            "middleNames": [
                                "Le"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. L. Cun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403028455"
                        ],
                        "name": "Fran\u00e7oise Fogelman-Souli\u00e9",
                        "slug": "Fran\u00e7oise-Fogelman-Souli\u00e9",
                        "structuredName": {
                            "firstName": "Fran\u00e7oise",
                            "lastName": "Fogelman-Souli\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fran\u00e7oise Fogelman-Souli\u00e9"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 195643413,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "47c7e23da6bdd9ac91dae8751af25696fea2c332",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Etude des modeles d'apprentissage simples et application aux memoires associatives. Methodes d'apprentissage pour reseaux a cellules cachees. Algorithme de retro propagation et ses variantes. Applications diverses (associative, reconnaissance de caracteres, diagnostic medical) et logiciel de simulation"
            },
            "slug": "Mod\u00e8les-connexionnistes-de-l'apprentissage-Cun-Fogelman-Souli\u00e9",
            "title": {
                "fragments": [],
                "text": "Mod\u00e8les connexionnistes de l'apprentissage"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1252,
                                "start": 155
                            }
                        ],
                        "text": "1;) in addition to the normal problem of deciding how to assign credit to decisions about hidden variables, there is a temporal credit assignment problem (Sutton, 1984). In the iterative version of back-propagation (section 6.4), temporal credit assignment is performed by explicitly computing the effect of each activity level on the eventual outcome. In reinforcement learning procedures, temporal credit assignment is typically performed by learning to associate \"secondary\" reinforcement values with the states that are intermediate in time between the action and the external reinforcement. One important idea is to make the reinforcement value of an intermediate state regress towards the weighted average of the reinforcement values of its successors, where the weightings reflect the conditional probabilities of the successors. In the limit, this causes the reinforcement value of each state to be equal to the expected reinforcement of its successor, and hence equal to the expected final reinforcement.12 Sutton (1987) explains why, in a stochastic system, it is typically more efficient to regress towards the reinforcement value of the next state rather than the reinforcement value of the final outcome. Barto, Sutton and Anderson (1983) have demonstrated the usefulness of thin tyre of procedure for learning with delayed reinforcement."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1030,
                                "start": 155
                            }
                        ],
                        "text": "1;) in addition to the normal problem of deciding how to assign credit to decisions about hidden variables, there is a temporal credit assignment problem (Sutton, 1984). In the iterative version of back-propagation (section 6.4), temporal credit assignment is performed by explicitly computing the effect of each activity level on the eventual outcome. In reinforcement learning procedures, temporal credit assignment is typically performed by learning to associate \"secondary\" reinforcement values with the states that are intermediate in time between the action and the external reinforcement. One important idea is to make the reinforcement value of an intermediate state regress towards the weighted average of the reinforcement values of its successors, where the weightings reflect the conditional probabilities of the successors. In the limit, this causes the reinforcement value of each state to be equal to the expected reinforcement of its successor, and hence equal to the expected final reinforcement.12 Sutton (1987) explains why, in a stochastic system, it is typically more efficient to regress towards the reinforcement value of the next state rather than the reinforcement value of the final outcome."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 154
                            }
                        ],
                        "text": "1;) in addition to the normal problem of deciding how to assign credit to decisions about hidden variables, there is a temporal credit assignment problem (Sutton, 1984)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60564875,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "22069cd4504656d3bb85748a4d43be7a4d7d5545",
            "isKey": true,
            "numCitedBy": 862,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Temporal-credit-assignment-in-reinforcement-Sutton",
            "title": {
                "fragments": [],
                "text": "Temporal credit assignment in reinforcement learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 101
                            }
                        ],
                        "text": "To demonstrate the ability of backpropagation to discover important underlying features of a domain, Hinton [38] used a multi-layer network to learn the G.E. HINTON Christopher = Penelope Andrew = Christine I I I t 1 I Margaret = Arthur Victode = James Jennifer = Charles i I I Colin Charlotte Roberto = Maria Pierro = Francesca t 1 I I I I Gina = Emilio Lucia = Marco Angela = Tomaso 1 I 1"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 48
                            }
                        ],
                        "text": "Theoretically, the network could oscillate, but Hinton [37] and Anderson and Mozer [7] showed that iterative retrieval normally works well."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 55
                            }
                        ],
                        "text": "Some preliminary work has been done by Minsky [67] and Hinton [37] on the representation of inheritance hierarchies and the representation of frame-like structures in which a whole object is composed of a number of parts each of which plays a different role within the whole."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 108
                            }
                        ],
                        "text": "To demonstrate the ability of backpropagation to discover important underlying features of a domain, Hinton [38] used a multi-layer network to learn the"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 63
                            }
                        ],
                        "text": "These issues are examined in detail by Hummel and Zucker [52], Hinton and Sejnowski [45], Geman and Geman [31], Hopfield and Tank [51] and Marroquin [65]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 53796860,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ade4934db522fe6d634ff6f48887da46eedb4d1",
            "isKey": false,
            "numCitedBy": 903,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-distributed-representations-of-concepts.-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning distributed representations of concepts."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682174"
                        ],
                        "name": "S. Grossberg",
                        "slug": "S.-Grossberg",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Grossberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Grossberg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 202239408,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "fbf3020ba4fd2b2267def6209259c83d788403d4",
            "isKey": false,
            "numCitedBy": 1069,
            "numCiting": 92,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "How-does-a-brain-build-a-cognitive-code-Grossberg",
            "title": {
                "fragments": [],
                "text": "How does a brain build a cognitive code"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 63077747,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d007ed936c51a700d8c65d1bbfae7acc83783c31",
            "isKey": false,
            "numCitedBy": 228,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Une-procedure-d'apprentissage-pour-reseau-a-seuil-LeCun",
            "title": {
                "fragments": [],
                "text": "Une procedure d'apprentissage pour reseau a seuil asymmetrique (A learning scheme for asymmetric threshold networks)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2319833"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 207975157,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "56623a496727d5c71491850e04512ddf4152b487",
            "isKey": false,
            "numCitedBy": 4468,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Beyond-Regression-:-\"New-Tools-for-Prediction-and-Werbos",
            "title": {
                "fragments": [],
                "text": "Beyond Regression : \"New Tools for Prediction and Analysis in the Behavioral Sciences"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1913418"
                        ],
                        "name": "B. Widrow",
                        "slug": "B.-Widrow",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Widrow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Widrow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1976925"
                        ],
                        "name": "M. Hoff",
                        "slug": "M.-Hoff",
                        "structuredName": {
                            "firstName": "Marcian",
                            "lastName": "Hoff",
                            "middleNames": [
                                "E."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hoff"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60830585,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e14b2ff9dc2234df94fc24d89fc25e797d0e9e7",
            "isKey": false,
            "numCitedBy": 2623,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adaptive-switching-circuits-Widrow-Hoff",
            "title": {
                "fragments": [],
                "text": "Adaptive switching circuits"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2577641"
                        ],
                        "name": "R. Linsker",
                        "slug": "R.-Linsker",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Linsker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Linsker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60832176,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "baf10f3ca8b02e5345c1d5f58b94d87c0c421c9c",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Computer-simulation-in-brain-science:-Development-a-Linsker",
            "title": {
                "fragments": [],
                "text": "Computer simulation in brain science: Development of feature-analyzing cells and their columnar organization in a layered self-adaptive network"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "118969901"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59859558,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d453386011ef21285fa81fb4f87fdf811c6ad7a",
            "isKey": false,
            "numCitedBy": 522,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-internal-representations-by-errors-Rumelhart",
            "title": {
                "fragments": [],
                "text": "Learning internal representations by back-propagating errors"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2553065"
                        ],
                        "name": "M. Kerszberg",
                        "slug": "M.-Kerszberg",
                        "structuredName": {
                            "firstName": "Michel",
                            "lastName": "Kerszberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kerszberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144113993"
                        ],
                        "name": "A. Bergman",
                        "slug": "A.-Bergman",
                        "structuredName": {
                            "firstName": "Aviv",
                            "lastName": "Bergman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bergman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59696707,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c63dde14e562710c9288608458edce7c502588aa",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Computer-simulation-in-brain-science:-The-evolution-Kerszberg-Bergman",
            "title": {
                "fragments": [],
                "text": "Computer simulation in brain science: The evolution of data processing abilities in competing automata"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144246983"
                        ],
                        "name": "H. Barlow",
                        "slug": "H.-Barlow",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Barlow",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Barlow"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 72
                            }
                        ],
                        "text": "In a local representation, each concept is represented by a single unit [13, 27]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 39831555,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "f7c6c62c0fc12bd65511429bc861430cfb52eccf",
            "isKey": false,
            "numCitedBy": 662,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Single-units-and-sensation:-a-neuron-doctrine-for-Barlow",
            "title": {
                "fragments": [],
                "text": "Single units and sensation: a neuron doctrine for perceptual psychology?"
            },
            "venue": {
                "fragments": [],
                "text": "Perception"
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055557361"
                        ],
                        "name": "C. Peterson",
                        "slug": "C.-Peterson",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Peterson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Peterson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110869996"
                        ],
                        "name": "James R. Anderson",
                        "slug": "James-R.-Anderson",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Anderson",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James R. Anderson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 3851750,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "607802a4067cb7738bac85d3ca3386f859e637b9",
            "isKey": false,
            "numCitedBy": 500,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Mean-Field-Theory-Learning-Algorithm-for-Neural-Peterson-Anderson",
            "title": {
                "fragments": [],
                "text": "A Mean Field Theory Learning Algorithm for Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750924"
                        ],
                        "name": "Demetri Terzopoulos",
                        "slug": "Demetri-Terzopoulos",
                        "structuredName": {
                            "firstName": "Demetri",
                            "lastName": "Terzopoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Demetri Terzopoulos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 90
                            }
                        ],
                        "text": "It resembles the use of multi-resolution techniques to speed up search in computer vision (Terzopoulos, 1984)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 26705393,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "efadad5b0d52daa3b31b26f0b988bda73ec0e923",
            "isKey": false,
            "numCitedBy": 117,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multiresolution-computation-of-visible-surface-Terzopoulos",
            "title": {
                "fragments": [],
                "text": "Multiresolution computation of visible-surface representations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47804608"
                        ],
                        "name": "R. Bek",
                        "slug": "R.-Bek",
                        "structuredName": {
                            "firstName": "Roman",
                            "lastName": "Bek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bek"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 20520403,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "a09d930fea5161406f735acf02e703e7824811dc",
            "isKey": false,
            "numCitedBy": 720,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Discourse-on-one-way-in-which-a-quantum-mechanics-Bek",
            "title": {
                "fragments": [],
                "text": "Discourse on one way in which a quantum-mechanics language on the classical logical base can be built up"
            },
            "venue": {
                "fragments": [],
                "text": "Kybernetika"
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699108"
                        ],
                        "name": "G. Tesauro",
                        "slug": "G.-Tesauro",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Tesauro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Tesauro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2924193"
                        ],
                        "name": "B. Janssens",
                        "slug": "B.-Janssens",
                        "structuredName": {
                            "firstName": "Bob",
                            "lastName": "Janssens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Janssens"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 2
                            }
                        ],
                        "text": "10Tesauro (1987) reports a case in which the number of weight updates is roughly proportional to the number of training cases (it is actually a 4/3 power law)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 33431719,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4579229a90b9fa18234fc64a10e2d2312ef6a64f",
            "isKey": false,
            "numCitedBy": 113,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Scaling-Relationships-in-Back-propagation-Learning-Tesauro-Janssens",
            "title": {
                "fragments": [],
                "text": "Scaling Relationships in Back-propagation Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055401600"
                        ],
                        "name": "J. W. Bakker",
                        "slug": "J.-W.-Bakker",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Bakker",
                            "middleNames": [
                                "W.",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. W. Bakker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52127672"
                        ],
                        "name": "A. J. Nijman",
                        "slug": "A.-J.-Nijman",
                        "structuredName": {
                            "firstName": "Aloysius",
                            "lastName": "Nijman",
                            "middleNames": [
                                "Jozef"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Nijman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761192"
                        ],
                        "name": "P. Treleaven",
                        "slug": "P.-Treleaven",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Treleaven",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Treleaven"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 26691219,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3611ce7d3b78e1f9964c9f49c2643142c711356a",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "PARLE-Parallel-Architectures-and-Languages-Europe-Bakker-Nijman",
            "title": {
                "fragments": [],
                "text": "PARLE Parallel Architectures and Languages Europe"
            },
            "venue": {
                "fragments": [],
                "text": "Lecture Notes in Computer Science"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144323963"
                        ],
                        "name": "S. Alexander",
                        "slug": "S.-Alexander",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Alexander",
                            "middleNames": [
                                "Thomas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Alexander"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12374910,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7d78b005150b873a1b72423cdc045267e03daa7",
            "isKey": false,
            "numCitedBy": 3370,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adaptive-Signal-Processing-Alexander",
            "title": {
                "fragments": [],
                "text": "Adaptive Signal Processing"
            },
            "venue": {
                "fragments": [],
                "text": "Texts and Monographs in Computer Science"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 72
                            }
                        ],
                        "text": "These problems can be alleviated by using a process called \"unlearning\" [20, 50]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The function of dream"
            },
            "venue": {
                "fragments": [],
                "text": "sleep, Nature"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 68
                            }
                        ],
                        "text": "There are many variations of competitive learning in the literature [4, 29, 33, 95] and there is not space here to review them all."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A self-organizing multilayered neural network, Biol"
            },
            "venue": {
                "fragments": [],
                "text": "Cybern. 20"
            },
            "year": 1975
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "This form of backpropagation has been used successfully to compress images [19] and to compress speech waves [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discovering the hidden structure of speech"
            },
            "venue": {
                "fragments": [],
                "text": "Discovering the hidden structure of speech"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A marker induction mechanism for the establishment of ordered neural mapping: Its application to the retino-tectal connections"
            },
            "venue": {
                "fragments": [],
                "text": "Philos. Trans. Roy. Soc. Lond. B"
            },
            "year": 1979
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Perception as an optimization process"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE Computer Society Conf. on Computer Vision and Pattern Recognition. Miami,"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 23
                            }
                        ],
                        "text": "As Alspector and Allen [3] have demonstrated, the speed of one particular learning procedure can be increased by a factor of about a million if we combine these techniques."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 20
                            }
                        ],
                        "text": "Alspector and Allen [3] are fabricating a chip which will run about 1 million times as fast as a simulation on a VAX."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A neuromorphic VLSI learning system, in: E Loseleben (Ed.), Advanced Research in VLSI"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 191
                            }
                        ],
                        "text": "If the second derivatives can be computed or estimated they can be used to pick a direction for the weight change vector that yields faster convergence than the direction of steepest descent (Parker, 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Second order back-propagation: An optimal adaptive algorithm for any adaptive network Unpublished manuscript)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Principles of Neurodynamics (Spartan Books"
            },
            "venue": {
                "fragments": [],
                "text": "Principles of Neurodynamics (Spartan Books"
            },
            "year": 1962
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 194
                            }
                        ],
                        "text": "In a distributed representation, the kinds of concepts that we have words for are represented by patterns of activity distributed over many units, and each unit takes part in many such patterns [42]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Distributed representations, in: D.E. Rumelhart, J.L. McClelland and the PDP Research Group (Eds.), Parallel Distributed Processing: Explorations in the Microstructure of Cognition, I: Foundations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 107
                            }
                        ],
                        "text": "After sufficient experience, the network can correctly identify familiar, simple shapes in novel positions [39]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning translation invariant recognition in a massively parallel network, in: PARLE: Parallel Architectures and Languages Europe 1 (Springer, Berlin"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reinforcement learning in connectionist networks: A mathematical analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Tech. Rept"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory of neural-analog reinforcement systems and its application to the brain-model problem"
            },
            "venue": {
                "fragments": [],
                "text": "Theory of neural-analog reinforcement systems and its application to the brain-model problem"
            },
            "year": 1954
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning to predict by the method of temporal differences"
            },
            "venue": {
                "fragments": [],
                "text": "GTE Laboratories"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 235
                            }
                        ],
                        "text": "One approach that has been used in studying the induction of grammars is to define a hypothesis space of possible grammars, and to show that with enough training cases the system will converge on the correct grammar with probability 1 [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory and methods, Comput"
            },
            "venue": {
                "fragments": [],
                "text": "Surv. 15"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 633,
                                "start": 236
                            }
                        ],
                        "text": "It is possible to combine genetic leaming with gradient descent (or hill-climbing) to get a hybrid learning procedure called \"iterated genetic hill-climbing\" or \"IGH\" that works better than either learning procedure alone (Brady, 1985; Ackley, 1987). IGH is as a form of multiple-restart hill-climbing in which the starting points, instead of being chosen at random, are chosen by \"mating\" previously discovered local optima. Alternatively, it can be viewed as genetic learning in which each new individual is allowed to perform hill-climbing in the fitness function before being evaluated and added to the population. Ackley (1987) shows that a stochastic variation of IGH can be implemented in a connectionist network thai is trying to learn which output vector produces a high enough payoff to satisfy some external criterion."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 222
                            }
                        ],
                        "text": "It is possible to combine genetic leaming with gradient descent (or hill-climbing) to get a hybrid learning procedure called \"iterated genetic hill-climbing\" or \"IGH\" that works better than either learning procedure alone (Brady, 1985; Ackley, 1987)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stochastic Iterated Genetic Hill-climbing PhD Thesis)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A \"complexity level\" analysis of vision"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings First International Conference on Computer Vision"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptive switching circuits, in: IRE WESCON Conv"
            },
            "venue": {
                "fragments": [],
                "text": "Record"
            },
            "year": 1960
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 54
                            }
                        ],
                        "text": "Achieving global optimality by reinforcement learning Thatachar and Sastry (1985) use a different mapping between automata and connectionist networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learningoptimal discriminant functions through a cooperative game of automata Technical Report EE/64/1985)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 3
                            }
                        ],
                        "text": "As Alspector and Allen (1987) have demonstrated, the speed of one particular learning procedure can be increased by a factor of about a million if we combine these techniques."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 0
                            }
                        ],
                        "text": "Alspector and Allen (1987) have designed a chip which will run about 1 million times as fast as a simulation on a VAX, and they intend to fabricate it."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A neuromorphic VLSI learning system"
            },
            "venue": {
                "fragments": [],
                "text": "Advanced Research in VLSI: Proceedings of the 1987 Stanford Conference"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 111
                            }
                        ],
                        "text": "Each connection is treated as a stochastic switch that has a certain probability of being closed at any moment (Minsky, 1954)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 0
                            }
                        ],
                        "text": "Minsky and Papert (1969) give a clear analysis of the limitations on what mappings can be computed by three-layered nets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory of neural-analog reinforcement systems and its application to the brainmodel problem Ph.D. Dissertation)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1954
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 83
                            }
                        ],
                        "text": "Theoretically, the network could oscillate, but Hinton [37] and Anderson and Mozer [7] showed that iterative retrieval normally works well."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Categorization and selective neurons"
            },
            "venue": {
                "fragments": [],
                "text": "Parallel Models of Associative Memory"
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 250
                            }
                        ],
                        "text": "Recently, it has been shown that a variation of this model can be interpreted as performing steepest descent in an error function and can be applied to a range of optimization problems that involve topographic mappings between geometrical structures [23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The elastic net method: An analogue approach to the travelling salesman problem, Nature"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 72
                            }
                        ],
                        "text": "In a local representation, each concept is represented by a single unit [13, 27]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural representation of conceptual knowledge, Tech. Rept. TR189"
            },
            "venue": {
                "fragments": [],
                "text": "Department of Computer Science,"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 64
                            }
                        ],
                        "text": "Theoretically, the network could oscillate, but Hinton [37] and Anderson and Mozer [7] showed that iterative retrieval normally works well."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "Barto, Sutton and Anderson [15] have demonstrated the usefulness of this type of procedure for learning with delayed reinforcement."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neuronlike elements that solve difficult learning control problems"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 72
                            }
                        ],
                        "text": "In a local representation, each concept is represented by a single unit (Barlow, 1972; Feldman, 1986)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural representation of conceptual knowledge Technical Report TR189)"
            },
            "venue": {
                "fragments": [],
                "text": "Department of Computer Science,"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 127
                            }
                        ],
                        "text": "Typical examples of this kind of learning are described by Cooper, Liberman and Oja [18] and by Bienenstock, Cooper, and Munro [16]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Orientation specificity and binocular interaction in visual cortex, J"
            },
            "venue": {
                "fragments": [],
                "text": "NeuroscL 2"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the acquisition of the past tense in English"
            },
            "venue": {
                "fragments": [],
                "text": "Parallel Distributed Processing: Explorations in the Microstructure of Cognition, II: Applications"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Second order back-propagation: An optimal adaptive algorithm for any adaptive network"
            },
            "venue": {
                "fragments": [],
                "text": "Second order back-propagation: An optimal adaptive algorithm for any adaptive network"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "Fukushima and Miyake [30] have demonstrated that a version of competitive learning can be used to allow a multi-layer network to recognize simple two-dimensional shapes in a number of different positions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A new algorithm for pattern recognition tolerant of deformations and shifts in position, Pattern Recogn"
            },
            "venue": {
                "fragments": [],
                "text": "15"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Clustering, taxonomy, and topological maps of patterns"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Sixth International Conference on Pattern Recognition. Silver Spring, MD, IEEE Computer Society Press. Le Cun, Y"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 300,
                                "start": 296
                            }
                        ],
                        "text": "Even so, there are some important choices about whether to represent a physical quantity (like the depth at a point in the image) by the state of a single continuous unit, or by the activities in a set of units each of which indicates its confidence that the depth lies within a certain interval [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Structure and function, Behav"
            },
            "venue": {
                "fragments": [],
                "text": "Brain Sci. 9"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Clustering, taxonomy, and topological maps of patterns"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Sixth International Conference on Pattern Recognition"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A dual back-propagation scheme for scalar reinforcement learning"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth Annual Conference of the Cognitive Science Society"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 100
                            }
                        ],
                        "text": "The \"on-line\" version, which requires less memory, updates the weights after each input-output case (Widrow and Hoff, 1960)6."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptive switching circuits. IRE WESCON Cony"
            },
            "venue": {
                "fragments": [],
                "text": "Phil. Trans. Proc. R. Soc"
            },
            "year": 1960
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 72
                            }
                        ],
                        "text": "These problems can be alleviated by using a process called \"unlearning\" (Hopfield, Feinstein and Palmer, 1983; Crick and Mitchison, 1983)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The funcfpn of dream"
            },
            "venue": {
                "fragments": [],
                "text": "sleep. Nature,"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 68
                            }
                        ],
                        "text": "There are many variations of competitive learning in the literature [4, 29, 33, 95] and there is not space here to review them all."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Parallel development and coding of neural feature detectors, Biol"
            },
            "venue": {
                "fragments": [],
                "text": "Cybern. 23"
            },
            "year": 1976
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Competitive learning, Cognitive Sci"
            },
            "venue": {
                "fragments": [],
                "text": "Competitive learning, Cognitive Sci"
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "ModUles connexionnistes de l'apprentissage"
            },
            "venue": {
                "fragments": [],
                "text": "ModUles connexionnistes de l'apprentissage"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Connectionist speech recognition, Thesis proposal"
            },
            "venue": {
                "fragments": [],
                "text": "Connectionist speech recognition, Thesis proposal"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 194
                            }
                        ],
                        "text": "Also, if the distributed representation allows the weights to capture important underlying regularities in the task domain, it can lead to much better generalization than a local representation (Rumelhart and McClelland, 1986; Rumelhart, Hinton and Williams, 1986a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": " the PDP research group (Eds.), Parallel distributed processing: Explorations in the microstructure of cognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The elastic net method: An analogue approach to the travelling salesman problem"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "This form of backpropagation has been used successfully to compress images [19] and to compress speech waves [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning internal representations from gray-scale images: An example of extensional programming"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth Annual Conference of the Cognitive Science Society"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 127
                            }
                        ],
                        "text": "This is called a \"Hebbian\" learning rule because the weight modification depends on both presynaptic and postsynaptic activity [36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Organization of Behavior (Wiley"
            },
            "venue": {
                "fragments": [],
                "text": "New York,"
            },
            "year": 1949
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discovering the hidden structure of speech (Tech. Rep.)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 43,
            "methodology": 16
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 122,
        "totalPages": 13
    },
    "page_url": "https://www.semanticscholar.org/paper/Connectionist-Learning-Procedures-Hinton/a57c6d627ffc667ae3547073876c35d6420accff?sort=total-citations"
}