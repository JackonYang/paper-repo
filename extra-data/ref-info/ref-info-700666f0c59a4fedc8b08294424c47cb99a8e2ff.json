{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740213"
                        ],
                        "name": "Pedro M. Domingos",
                        "slug": "Pedro-M.-Domingos",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Domingos",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pedro M. Domingos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694780"
                        ],
                        "name": "M. Pazzani",
                        "slug": "M.-Pazzani",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Pazzani",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pazzani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7413266,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "217a0ecf9795721f9f3661f5562a5b1afd4a3b59",
            "isKey": false,
            "numCitedBy": 873,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The simple Bayesian classi er (SBC) is commonly thought to assume that attributes are independent given the class, but this is apparently contradicted by the surprisingly good performance it exhibits in many domains that contain clear attribute dependences. No explanation for this has been proposed so far. In this paper we show that the SBC does not in fact assume attribute independence, and can be optimal even when this assumption is violated by a wide margin. The key to this nding lies in the distinction between classi cation and probability estimation: correct classi cation can be achieved even when the probability estimates used contain large errors. We show that the previously-assumed region of optimality of the SBC is a second-order in nitesimal fraction of the actual one. This is followed by the derivation of several necessary and several su cient conditions for the optimality of the SBC. For example, the SBC is optimal for learning arbitrary conjunctions and disjunctions, even though they violate the independence assumption. The paper also reports empirical evidence of the SBC's competitive performance in domains containing substantial degrees of attribute dependence. 1 THE SIMPLE BAYESIAN"
            },
            "slug": "Beyond-Independence:-Conditions-for-the-Optimality-Domingos-Pazzani",
            "title": {
                "fragments": [],
                "text": "Beyond Independence: Conditions for the Optimality of the Simple Bayesian Classifier"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "It is shown that the simple Bayesian classi er (SBC) does not in fact assume attribute independence, and can be optimal even when this assumption is violated by a wide margin, and the previously-assumed region of optimality is a second-order in nitesimal fraction of the actual one."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50785579"
                        ],
                        "name": "N. Friedman",
                        "slug": "N.-Friedman",
                        "structuredName": {
                            "firstName": "Nir",
                            "lastName": "Friedman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Friedman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144527211"
                        ],
                        "name": "D. Geiger",
                        "slug": "D.-Geiger",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Geiger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geiger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770007"
                        ],
                        "name": "M. Goldszmidt",
                        "slug": "M.-Goldszmidt",
                        "structuredName": {
                            "firstName": "Mois\u00e9s",
                            "lastName": "Goldszmidt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Goldszmidt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 930676,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c76ac0a39577760d4aaf6fce98327543ec64a560",
            "isKey": false,
            "numCitedBy": 4537,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work in supervised learning has shown that a surprisingly simple Bayesian classifier with strong assumptions of independence among features, called naive Bayes, is competitive with state-of-the-art classifiers such as C4.5. This fact raises the question of whether a classifier with less restrictive assumptions can perform even better. In this paper we evaluate approaches for inducing classifiers from data, based on the theory of learning Bayesian networks. These networks are factored representations of probability distributions that generalize the naive Bayesian classifier and explicitly represent statements about independence. Among these approaches we single out a method we call Tree Augmented Naive Bayes (TAN), which outperforms naive Bayes, yet at the same time maintains the computational simplicity (no search involved) and robustness that characterize naive Bayes. We experimentally tested these approaches, using problems from the University of California at Irvine repository, and compared them to C4.5, naive Bayes, and wrapper methods for feature selection."
            },
            "slug": "Bayesian-Network-Classifiers-Friedman-Geiger",
            "title": {
                "fragments": [],
                "text": "Bayesian Network Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Tree Augmented Naive Bayes (TAN) is single out, which outperforms naive Bayes, yet at the same time maintains the computational simplicity and robustness that characterize naive Baye."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764547"
                        ],
                        "name": "M. Sahami",
                        "slug": "M.-Sahami",
                        "structuredName": {
                            "firstName": "Mehran",
                            "lastName": "Sahami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sahami"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Sahami (1996) proposed a related scheme, and, in a similar spirit, Singh and Provan (1995, 1996) obtained good results by forming Bayesian networks using only a subset of the attributes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5680462,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d136cd362fb9d38cec1b6dbbf41c3d693c2cec1",
            "isKey": false,
            "numCitedBy": 428,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a framework for characterizing Bayesian classification methods. This framework can be thought of as a spectrum of allowable dependence in a given probabilistic model with the Naive Bayes algorithm at the most restrictive end and the learning of full Bayesian networks at the most general extreme. While much work has been carried out along the two ends of this spectrum, there has been surprising little done along the middle. We analyze the assumptions made as one moves along this spectrum and show the tradeoffs between model accuracy and learning speed which become critical to consider in a variety of data mining domains. We then present a general induction algorithm that allows for traversal of this spectrum depending on the available computational power for carrying out induction and show its application in a number of domains with different properties."
            },
            "slug": "Learning-Limited-Dependence-Bayesian-Classifiers-Sahami",
            "title": {
                "fragments": [],
                "text": "Learning Limited Dependence Bayesian Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A framework for characterizing Bayesian classification methods is presented and a general induction algorithm is presented that allows for traversal of this spectrum depending on the available computational power for carrying out induction and its application in a number of domains with different properties."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713919"
                        ],
                        "name": "P. Langley",
                        "slug": "P.-Langley",
                        "structuredName": {
                            "firstName": "Pat",
                            "lastName": "Langley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Langley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2706719"
                        ],
                        "name": "Wayne Iba",
                        "slug": "Wayne-Iba",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Iba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wayne Iba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123928894"
                        ],
                        "name": "K. Thompson",
                        "slug": "K.-Thompson",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Thompson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Thompson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 0
                            }
                        ],
                        "text": "Langley, Iba, and Thompson (1992) compared the Bayesian classifier with a decision tree learner, and found it was more accurate in four of the five data sets used."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 22
                            }
                        ],
                        "text": "On the other hand, as Langley et al. (1992) point out, the Bayesian classifier has the advantage of noise tolerance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 140
                            }
                        ],
                        "text": "The Bayesian classifier\u2019s average-case behavior for insufficient samples (i.e., samples not including all possible examples) was analyzed by Langley et al. (1992), who plotted sample cases and found the rate of convergence to 100% accuracy to be quite rapid.7 Comparing Langley et al.\u2019s results with\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 15383317,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1925bacaa10b4ec83a0509132091bb79243b41b6",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present an average-case analysis of the Bayesian classiier, a simple probabilistic induction algorithm that fares remarkably well on many learning tasks. Our analysis assumes a monotone conjunctive target concept, Boolean attributes that are independent of each other and that follow a single distribution, and the absence of attribute noise. We rst calculate the probability that the algorithm will induce an arbitrary pair of concept descriptions ; we then use this expression to compute the probability of correct classiication over the space of instances. The analysis takes into account the number of training instances, the number of relevant and irrelevant attributes, the distribution of these attributes, and the level of class noise. In addition, we explore the behavioral implications of the analysis by presenting predicted learning curves for a number of artiicial domains. We also give experimental results on these domains as a check on our reasoning. Finally, we discuss some unresolved questions about the behavior of Bayesian classiiers and outline directions for future research. we nd the current format more desirable. We have not submitted the paper to any other conference or journal."
            },
            "slug": "An-Analysis-of-Bayesian-Classiiers-Langley-Iba",
            "title": {
                "fragments": [],
                "text": "An Analysis of Bayesian Classiiers"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "An average-case analysis of the Bayesian classiier is presented, a simple probabilistic induction algorithm that fares remarkably well on many learning tasks and explores the behavioral implications by presenting predicted learning curves for a number of artiicial domains."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 109
                            }
                        ],
                        "text": "Recently, several authors (Kong & Dietterich, 1995; Kohavi & Wolpert, 1996; Tibshirani, 1996; Breiman, 1996; Friedman, 1996) have proposed similar bias-variance decompositions for zero-one loss functions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 15
                            }
                        ],
                        "text": "In particular, Friedman (1996) has shown, using normal approximations to the class probabilities, that the bias-variance interaction now takes a very different form."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 91
                            }
                        ],
                        "text": "It is well known that squared error loss can be decomposed into three additive components (Friedman, 1996): the intrinsic error due to noise in the sample, the statistical bias (systematic component of the approximation error, or error for an infinite sample) and the variance (component of the\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18543237,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "15252afe259894bd3d0f306f29eca5e90ab05eac",
            "isKey": false,
            "numCitedBy": 869,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The classification problem is considered in which an outputvariable y assumes discrete values with respectiveprobabilities that depend upon the simultaneous values of a set of input variablesx = {x_1,....,x_n}. At issue is how error in the estimates of theseprobabilities affects classification error when the estimates are used ina classification rule. These effects are seen to be somewhat counterintuitive in both their strength and nature. In particular the bias andvariance components of the estimation error combine to influenceclassification in a very different way than with squared error on theprobabilities themselves. Certain types of (very high) bias can becanceled by low variance to produce accurate classification. This candramatically mitigate the effect of the bias associated with some simpleestimators like \u201cnaive\u201d Bayes, and the bias induced by thecurse-of-dimensionality on nearest-neighbor procedures. This helps explainwhy such simple methods are often competitive with and sometimes superiorto more sophisticated ones for classification, and why\u201cbagging/aggregating\u201d classifiers can often improveaccuracy. These results also suggest simple modifications to theseprocedures that can (sometimes dramatically) further improve theirclassification performance."
            },
            "slug": "On-Bias,-Variance,-0/1\u2014Loss,-and-the-Friedman",
            "title": {
                "fragments": [],
                "text": "On Bias, Variance, 0/1\u2014Loss, and the Curse-of-Dimensionality"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work candramatically mitigate the effect of the bias associated with some simpleestimators like \u201cnaive\u201d Bayes, and the bias induced by the curse-of-dimensionality on nearest-neighbor procedures."
            },
            "venue": {
                "fragments": [],
                "text": "Data Mining and Knowledge Discovery"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713919"
                        ],
                        "name": "P. Langley",
                        "slug": "P.-Langley",
                        "structuredName": {
                            "firstName": "Pat",
                            "lastName": "Langley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Langley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2706719"
                        ],
                        "name": "Wayne Iba",
                        "slug": "Wayne-Iba",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Iba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wayne Iba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123928894"
                        ],
                        "name": "K. Thompson",
                        "slug": "K.-Thompson",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Thompson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Thompson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "On the other hand, as  Langley et al. (1992)  point out, the Bayesian classifier has the advantage of noise tolerance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 22
                            }
                        ],
                        "text": "On the other hand, as Langley et al. (1992) point out, the Bayesian classifier has the advantage of noise tolerance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 0
                            }
                        ],
                        "text": "Langley, Iba, and Thompson (1992) compared the Bayesian classifier with a decision tree learner, and found it was more accurate in four of the five data sets used."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 140
                            }
                        ],
                        "text": "The Bayesian classifier\u2019s average-case behavior for insufficient samples (i.e., samples not including all possible examples) was analyzed by Langley et al. (1992), who plotted sample cases and found the rate of convergence to 100% accuracy to be quite rapid.7 Comparing Langley et al.\u2019s results with\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Langley, Iba, and Thompson (1992)  compared the Bayesian classifier with a decision tree learner, and found it was more accurate in four of the five data sets used."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The Bayesian classifier\u2019s average-case behavior for insufficient samples (i.e., samples not including all possible examples) was analyzed by  Langley et al. (1992) , who plotted sample cases and found the rate of convergence to 100% accuracy to be quite rapid.7 Comparing Langley et al.\u2019s results with Pazzani and Sarrett\u2019s (1990) average-case formulas for the classical wholist algorithm for learning conjunctions shows that the latter ..."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 21634132,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5e40ea249dfad6d8d133b7917ca031c0b32410a5",
            "isKey": true,
            "numCitedBy": 1356,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present an average-case analysis of the Bayesian classifier, a simple induction algorithm that fares remarkably well on many learning tasks. Our analysis assumes a monotone conjunctive target concept, and independent, noise-free Boolean attributes. We calculate the probability that the algorithm will induce an arbitrary pair of concept descriptions and then use this to compute the probability of correct classification over the instance space. The analysis takes into account the number of training instances, the number of attributes, the distribution of these attributes, and the level of class noise. We also explore the behavioral implications of the analysis by presenting predicted learning curves for artificial domains, and give experimental results on these domains as a check on our reasoning."
            },
            "slug": "An-Analysis-of-Bayesian-Classifiers-Langley-Iba",
            "title": {
                "fragments": [],
                "text": "An Analysis of Bayesian Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "An average-case analysis of the Bayesian classifier, a simple induction algorithm that fares remarkably well on many learning tasks, and explores the behavioral implications of the analysis by presenting predicted learning curves for artificial domains."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46802499"
                        ],
                        "name": "M. Ben-Bassat",
                        "slug": "M.-Ben-Bassat",
                        "structuredName": {
                            "firstName": "Moshe",
                            "lastName": "Ben-Bassat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ben-Bassat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3293334"
                        ],
                        "name": "K. Klove",
                        "slug": "K.-Klove",
                        "structuredName": {
                            "firstName": "Karin",
                            "lastName": "Klove",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Klove"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2422343"
                        ],
                        "name": "M. Weil",
                        "slug": "M.-Weil",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Weil",
                            "middleNames": [
                                "Harry"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Weil"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 9
                            }
                        ],
                        "text": "Overall, Ben-Bassat, Klove, and Weil (1980) have shown that the Bayesian classifier is quite robust with respect to errors in probability estimates due to small sample size; this is not surprising, since it can be attributed to the same factors that make it robust with respect to violations of the\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15060273,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "34d3cc43929c49bd4bb5f8fff9ea987f11ffe993",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "The sensitivity of Bayesian pattern recognition models to multiplicative deviations in the prior and conditional probabilities is investigated for the two-class case. Explicit formulas are obtained for the factor K by which the computed posterior probabilities should be divided in order to eliminate the deviation effect. Numerical results for the case of binary features indicate that the Bayesian model tolerates large deviations in the prior and conditional probabilities. In fact, the a priori ratio and the likelihood ratio may deviate within a range of 65-135 percent and still produce posterior probabilities in accurate proximity of at most \u00b10.10. The main implication is that Bayesian systems which are based on limited data or subjective probabilities are expected to have a high percentage of correct classification despite the fact that the prior and conditional probabilities they use may deviate rather significantly from the true values."
            },
            "slug": "Sensitivity-Analysis-in-Bayesian-Classification-Ben-Bassat-Klove",
            "title": {
                "fragments": [],
                "text": "Sensitivity Analysis in Bayesian Classification Models: Multiplicative Deviations"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "The sensitivity of Bayesian pattern recognition models to multiplicative deviations in the prior and conditional probabilities is investigated for the two-class case and results indicate that Bayesian systems which are based on limited data or subjective probabilities are expected to have a high percentage of correct classification despite the fact that the priorand conditional probabilities they use may deviate rather significantly from the true values."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694780"
                        ],
                        "name": "M. Pazzani",
                        "slug": "M.-Pazzani",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Pazzani",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pazzani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 87
                            }
                        ],
                        "text": "In Table 4, Accuracy Once shows results for the backward stepwise joining algorithm of Pazzani (1996), forming at most one Cartesian product as determined by the highest accuracy using leave-one-out cross validation on the training set; Entropy Once is the same algorithm except it creates at most\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In Table 4, Accuracy Once shows results for the backward stepwise joining algorithm of  Pazzani (1996) , forming at most one Cartesian product as determined by the highest accuracy using leave-one-out cross validation on the training set; Entropy Onceis the same algorithm except it creates at most one Cartesian product with the two attributes that have"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Pazzani (1996)  proposed joining attributes instead of attribute values."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 10
                            }
                        ],
                        "text": "Following Pazzani (1996), the first measure was estimated accuracy, as determined by leave-one-out cross validation on the training set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Pazzani (1996) proposed joining attributes instead of attribute values."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17119891,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "09eec628ca1d377b4b1878f912411ebba5e82651",
            "isKey": false,
            "numCitedBy": 228,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Naive Bayesian classifiers which make independence assumptions perform remarkably well on some data sets but poorly on others. We explore ways to improve the Bayesian classifier by searching for dependencies among attributes. We propose and evaluate two algorithms for detecting dependencies among attributes and show that the backward sequential elimination and joining algorithm provides the most improvement over the naive Bayesian classifier. The domains on which the most improvement occurs are those domains on which the naive Bayesian classifier is significantly less accurate than a decision tree learner. This suggests that the attributes used in some common databases are not independent conditioned on the class and that the violations of the independence assumption that affect the accuracy of the classifier can be detected from training data."
            },
            "slug": "Searching-for-Dependencies-in-Bayesian-Classifiers-Pazzani",
            "title": {
                "fragments": [],
                "text": "Searching for Dependencies in Bayesian Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that the backward sequential elimination and joining algorithm provides the most improvement over the naive Bayesian classifier and that the violations of the independence assumption that affect the accuracy of the classifier can be detected from training data."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713919"
                        ],
                        "name": "P. Langley",
                        "slug": "P.-Langley",
                        "structuredName": {
                            "firstName": "Pat",
                            "lastName": "Langley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Langley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47829236"
                        ],
                        "name": "S. Sage",
                        "slug": "S.-Sage",
                        "structuredName": {
                            "firstName": "Stephanie",
                            "lastName": "Sage",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sage"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Langley and Sage (1994) argued that, when two attributes are correlated, it might be better to delete one attribute than to assume the two are conditionally independent."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5075598,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d63c7b3b86276a6085e8ff7a104a3fd8864b8c02",
            "isKey": false,
            "numCitedBy": 783,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Induction-of-Selective-Bayesian-Classifiers-Langley-Sage",
            "title": {
                "fragments": [],
                "text": "Induction of Selective Bayesian Classifiers"
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37838196"
                        ],
                        "name": "G. Provan",
                        "slug": "G.-Provan",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Provan",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Provan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 67
                            }
                        ],
                        "text": "Sahami (1996) proposed a related scheme, and, in a similar spirit, Singh and Provan (1995, 1996) obtained good results by forming Bayesian networks using only a subset of the attributes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18894798,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a1c44a5f503880aea901de1eff1a2e6ad39ec0b7",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a computation-ally eecient method for inducing selective Bayesian network classiiers. Our approach is to use information-theoretic metrics to ef-ciently select a subset of attributes from which to learn the classiier. We explore three conditional, information-theoretic met-rics that are extensions of metrics used extensively in decision tree learning, namely Quin-lan's gain and gain ratio metrics and Man-taras's distance metric. We experimentally show that the algorithms based on gain ratio and distance metric learn selective Bayesian networks that have predictive accuracies as good as or better than those learned by existing selective Bayesian network induction approaches (K2-AS), but at a signiicantly lower computational cost. We prove that the subset-selection phase of these information-based algorithms has polynomial complexity, as compared to the worst-case exponential time complexity of the corresponding phase in K2-AS."
            },
            "slug": "Eecient-Learning-of-Selective-Bayesian-Network-Provan",
            "title": {
                "fragments": [],
                "text": "Eecient Learning of Selective Bayesian Network Classiiers"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is proved that the subset-selection phase of these information-based algorithms has polynomial complexity, as compared to the worst-case exponential time complexity of the corresponding phase in K2-AS."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110447955"
                        ],
                        "name": "Moninder Singh",
                        "slug": "Moninder-Singh",
                        "structuredName": {
                            "firstName": "Moninder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Moninder Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37838196"
                        ],
                        "name": "G. Provan",
                        "slug": "G.-Provan",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Provan",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Provan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 67
                            }
                        ],
                        "text": "Sahami (1996) proposed a related scheme, and, in a similar spirit, Singh and Provan (1995, 1996) obtained good results by forming Bayesian networks using only a subset of the attributes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15214010,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "048497a422f330721c7606272b46ff83ae843ea3",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Comparison-of-Induction-Algorithms-for-Selective-Singh-Provan",
            "title": {
                "fragments": [],
                "text": "A Comparison of Induction Algorithms for Selective and non-Selective Bayesian Classifiers"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726733"
                        ],
                        "name": "Ron Kohavi",
                        "slug": "Ron-Kohavi",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Kohavi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ron Kohavi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 19
                            }
                        ],
                        "text": "In a similar vein, Kohavi (1996) formed decision trees with Bayesian classifiers at the nodes, and showed that it tended to outperform either approach alone, especially on large data sets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 8
                            }
                        ],
                        "text": "Indeed, Kohavi (1996) has observed that the Bayesian classifier tends to outperform C4.5 on smaller data sets (hundreds to thousands of examples), and conversely for larger ones (thousands to tens of thousands)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8314975,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "34ae1e95775cfec793441c9f588a68c0020f21e5",
            "isKey": false,
            "numCitedBy": 1511,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Naive-Bayes induction algorithms were previously shown to be surprisingly accurate on many classification tasks even when the conditional independence assumption on which they are based is violated. However, most studies were done on small databases. We show that in some larger databases, the accuracy of Naive-Bayes does not scale up as well as decision trees. We then propose a new algorithm, NBTree, which induces a hybrid of decision-tree classifiers and Naive-Bayes classifiers: the decision-tree nodes contain univariate splits as regular decision-trees, but the leaves contain Naive-Bayesian classifiers. The approach retains the interpretability of Naive-Bayes and decision trees, while resulting in classifiers that frequently outperform both constituents, especially in the larger databases tested."
            },
            "slug": "Scaling-Up-the-Accuracy-of-Naive-Bayes-Classifiers:-Kohavi",
            "title": {
                "fragments": [],
                "text": "Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A new algorithm, NBTree, is proposed, which induces a hybrid of decision-tree classifiers and Naive-Bayes classifiers: the decision-Tree nodes contain univariate splits as regular decision-trees, but the leaves contain Na\u00efve-Bayesian classifiers."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34929449"
                        ],
                        "name": "George H. John",
                        "slug": "George-H.-John",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "John",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George H. John"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713919"
                        ],
                        "name": "P. Langley",
                        "slug": "P.-Langley",
                        "structuredName": {
                            "firstName": "Pat",
                            "lastName": "Langley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Langley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "John and Langley (1995) showed that the Bayesian classifier\u2019s performance can be much improved if the traditional treatment of numeric attributes, which assumes Gaussian distributions, is replaced by kernel density estimation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 24
                            }
                        ],
                        "text": "Then one possible set of discriminant functions is\nfi(E) = P (Ci) a \u220f\nj=1\nP (Aj =vjk|Ci)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 667586,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "7d4642da036febe5174da1390521444ef405c864",
            "isKey": false,
            "numCitedBy": 3389,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "When modeling a probability distribution with a Bayesian network, we are faced with the problem of how to handle continuous variables. Most previous work has either solved the problem by discretizing, or assumed that the data are generated by a single Gaussian. In this paper we abandon the normality assumption and instead use statistical methods for nonparametric density estimation. For a naive Bayesian classifier, we present experimental results on a variety of natural and artificial domains, comparing two methods of density estimation: assuming normality and modeling each conditional distribution with a single Gaussian; and using nonparametric kernel density estimation. We observe large reductions in error on several natural and artificial data sets, which suggests that kernel estimation is a useful tool for learning Bayesian models."
            },
            "slug": "Estimating-Continuous-Distributions-in-Bayesian-John-Langley",
            "title": {
                "fragments": [],
                "text": "Estimating Continuous Distributions in Bayesian Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper abandon the normality assumption and instead use statistical methods for nonparametric density estimation for kernel estimation, which suggests that kernel estimation is a useful tool for learning Bayesian models."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713919"
                        ],
                        "name": "P. Langley",
                        "slug": "P.-Langley",
                        "structuredName": {
                            "firstName": "Pat",
                            "lastName": "Langley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Langley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Langley (1993) proposed the use of \u201crecursive Bayesian classifiers\u201d to address this limitation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 37
                            }
                        ],
                        "text": "This is described in more detail in the next section."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18387451,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7b47df505a1a00f2e7a464321f6033bd88f0b2b1",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we review the induction of simple Bayesian classifiers, note some of their drawbacks, and describe a recursive algorithm that constructs a hierarchy of probabilistic concept descriptions. We posit that this approach should outperform the simpler scheme in domains that involve disjunctive concepts, since they violate the independence assumption on which the latter relies. To test this hypothesis, we report experimental studies with both natural and artificial domains. The results are mixed, but they are encouraging enough to recommend closer examination of recursive Bayesian classifiers in future work."
            },
            "slug": "Induction-of-Recursive-Bayesian-Classifiers-Langley",
            "title": {
                "fragments": [],
                "text": "Induction of Recursive Bayesian Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A recursive algorithm is described that should outperform the simpler scheme in domains that involve disjunctive concepts, since they violate the independence assumption on which the latter relies."
            },
            "venue": {
                "fragments": [],
                "text": "ECML"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4362150"
                        ],
                        "name": "E. Russek",
                        "slug": "E.-Russek",
                        "structuredName": {
                            "firstName": "Estelle",
                            "lastName": "Russek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Russek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11827568"
                        ],
                        "name": "R. Kronmal",
                        "slug": "R.-Kronmal",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Kronmal",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kronmal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46805979"
                        ],
                        "name": "L. Fisher",
                        "slug": "L.-Fisher",
                        "structuredName": {
                            "firstName": "Lloyd",
                            "lastName": "Fisher",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Fisher"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Other authors have verified by Monte Carlo simulation that \u201cchoosing a simple method of discrimination is often beneficial even if the underlying model assumptions are wrong\u201d (Flury, Schmid, & Narayanan (1994) for quadratic discriminant functions;  Russek, Kronmal, & Fisher (1983)  for the Bayesian classifier vs. multivariate Gaussian models)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8365049,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d0ef657ae02606f8c1950ba0cffb48a3320ddc87",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-effect-of-assuming-independence-in-applying-to-Russek-Kronmal",
            "title": {
                "fragments": [],
                "text": "The effect of assuming independence in applying Bayes' theorem to risk estimation and classification in diagnosis."
            },
            "venue": {
                "fragments": [],
                "text": "Computers and biomedical research, an international journal"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102278866"
                        ],
                        "name": "Bernhard W. Flury",
                        "slug": "Bernhard-W.-Flury",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Flury",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bernhard W. Flury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057824321"
                        ],
                        "name": "Martin J. Schmid",
                        "slug": "Martin-J.-Schmid",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Schmid",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin J. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47735288"
                        ],
                        "name": "A. Narayanan",
                        "slug": "A.-Narayanan",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Narayanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Narayanan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122690109,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1cad0c0e1368fffafcfbc3ab83e141d64f485138",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "In multivariate discrimination of several normal populations, the optimal classification procedure is based on quadratic discriminant functions. We compare expected error rates of the quadratic classification procedure if the covariance matrices are estimated under the following four models: (i) arbitrary covariance matrices, (ii) common principal components, (iii) proportional covariance matrices, and (iv) identical covariance matrices. Using Monte Carlo simulation to estimate expected error rates, we study the performance of the four discrimination procedures for five different parameter setups corresponding to \u201cstandard\u201d situations that have been used in the literature. The procedures are examined for sample sizes ranging from 10 to 60, and for two to four groups. Our results quantify the extent to which a parsimonious method reduces error rates, and demonstrate that choosing a simple method of discrimination is often beneficial even if the underlying model assumptions are wrong."
            },
            "slug": "Error-rates-in-quadratic-discrimination-with-on-the-Flury-Schmid",
            "title": {
                "fragments": [],
                "text": "Error rates in quadratic discrimination with constraints on the covariance matrices"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 94
                            }
                        ],
                        "text": "Recently, several authors (Kong & Dietterich, 1995; Kohavi & Wolpert, 1996; Tibshirani, 1996; Breiman, 1996; Friedman, 1996) have proposed similar bias-variance decompositions for zero-one loss functions."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7732239,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d414438926b73bde0313948d8b074cb5360a0e6f",
            "isKey": false,
            "numCitedBy": 637,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work has shown that combining multiple versions of unstable classifiers such as trees or neural nets results in reduced test set error. To study this, the concepts of bias and variance of a classifier are defined. Unstable classifiers can have universally low bias. Their problem is high variance. Combining multiple versions is a variance reducing device. One of the most effective is bagging (Breiman [1996a]) Here, modified training sets are formed by resampling from the original training set, classifiers constructed using these training sets and then combined by voting. Freund and Schapire [1995,1996] propose an algorithm the basis of which is to adaptively resample and combine (hence the acronym-arcing) so that the weights in the resampling are increased for those cases most often missclassified and the combining is done by weighted voting. Arcing is more sucessful than bagging in variance reduction. We explore two arcing algorithms, compare them to each other and to bagging, and try to understand how arcing works."
            },
            "slug": "Bias,-Variance-,-And-Arcing-Classifiers-Breiman",
            "title": {
                "fragments": [],
                "text": "Bias, Variance , And Arcing Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This work explores two arcing algorithms, compares them to each other and to bagging, and tries to understand how arcing works, which is more sucessful than bagging in variance reduction."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110447955"
                        ],
                        "name": "Moninder Singh",
                        "slug": "Moninder-Singh",
                        "structuredName": {
                            "firstName": "Moninder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Moninder Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37838196"
                        ],
                        "name": "G. Provan",
                        "slug": "G.-Provan",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Provan",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Provan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 205272,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23b13dc758bc0bf70e783a617085283d824216b1",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we empirically evaluate algorithms for learning four types of Bayesian network (BN) classifiers \u2013 Na\u00efve-Bayes, tree augmented Na\u00efve-Bayes (TANs), BN augmented Na\u00efve-Bayes (BANs) and general BNs (GBNs), where the GBNs and BANs are learned using two variants of a conditionalindependence based BN-learning algorithm. Based on their performance, we then define a new type of classifier. Experimental results show the resulting classifiers, learned using the proposed learning algorithms, are competitive with (or superior to) the best classifiers, based on both Bayesian networks and other formalisms, and that the computational time for learning and using these classifiers is relatively small. These results argue that BN classifiers deserve more attention in machine learning and data mining communities."
            },
            "slug": "Efficient-Learning-of-Selective-Bayesian-Network-Singh-Provan",
            "title": {
                "fragments": [],
                "text": "Efficient Learning of Selective Bayesian Network Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Experimental results show the resulting classifiers are competitive with (or superior to) the best classifiers, based on both Bayesian networks and other formalisms, and that the computational time for learning and using these classifiers is relatively small."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143986204"
                        ],
                        "name": "I. Kononenko",
                        "slug": "I.-Kononenko",
                        "structuredName": {
                            "firstName": "Igor",
                            "lastName": "Kononenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Kononenko"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 0
                            }
                        ],
                        "text": "Kononenko (1991) proposed successively joining dependent attribute values, using a statistical test to judge whether two attribute values are significantly dependent."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 187
                            }
                        ],
                        "text": "This section empirically tests this claim by comparing Pazzani\u2019s (1996) extension with one that differs from it solely by using the method for attribute dependence detection described in (Kononenko, 1991) and (Wan & Wong, 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 22
                            }
                        ],
                        "text": "Confidence levels for the observed differences in accuracy between the (discretized) Bayesian classifier and the other algorithms, according to a one-tailed paired t test, are also reported.3\nThe results are summarized in Table 2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 44
                            }
                        ],
                        "text": "Rather than using a statistical test, as in Kononenko (1991), Pazzani\u2019s algorithm used cross-validation to estimate the accuracy of a classifier with each possible join, and made the single change that most improved accuracy."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 149
                            }
                        ],
                        "text": "\u2026attributes Am and An and the class variable C, a possible measure of the degree of pairwise dependence between Am and An given C (Wan & Wong, 1989; Kononenko, 1991) is\nD(Am, An|C) = H(Am|C) + H(An|C) \u2212H(AmAn|C), (4)\nwhere AmAn represents the Cartesian product of attributes Am and An (i.e., a\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1590400,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "343115f687957110e56dfdf430a65ee4490a77ab",
            "isKey": true,
            "numCitedBy": 374,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "In the paper the algorithm of the 'naive' Bayesian classifier (that assumes the independence of attributes) is extended to detect the dependencies between attributes. The idea is to optimize the tradeoff between the 'non-naivety' and the reliability of approximations of probabilities. Experiments in four medical diagnostic problems are described. In two domains where by the experts opinion the attributes are in fact independent the semi- naive Bayesian classifier achieved the same classification accuracy as naive Bayes. In two other domains the semi-naive Bayesian classifier slightly outperformed the naive Bayesian classifier."
            },
            "slug": "Semi-Naive-Bayesian-Classifier-Kononenko",
            "title": {
                "fragments": [],
                "text": "Semi-Naive Bayesian Classifier"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "The algorithm of the 'naive' Bayesian classifier (that assumes the independence of attributes) is extended to detect the dependencies between attributes to optimize the tradeoff between the 'non-naivety' and the reliability of approximations of probabilities."
            },
            "venue": {
                "fragments": [],
                "text": "EWSL"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 82
                            }
                        ],
                        "text": "Let the Vapnik-Chervonenkis dimension, or VC dimensionfor short, be defined as in (Haussler, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 151
                            }
                        ],
                        "text": "Proof: This result follows immediately from Theorem 4 and the fact that, givena attributes, the VC dimension of linear discriminant functions is O(a) (Haussler, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 150
                            }
                        ],
                        "text": "Proof: This result follows immediately from Theorem 4 and the fact that, given aattributes, the VC dimension of linear discriminant functions is O(a) (Haussler, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 84
                            }
                        ],
                        "text": "Let the Vapnik-Chervonenkis dimension, or VC dimension for short, be defined as in\n(Haussler, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 27204621,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3b814ad3055d6bfd7828effdbfbf1372646b7c22",
            "isKey": true,
            "numCitedBy": 551,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Quantifying-Inductive-Bias:-AI-Learning-Algorithms-Haussler",
            "title": {
                "fragments": [],
                "text": "Quantifying Inductive Bias: AI Learning Algorithms and Valiant's Learning Framework"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726733"
                        ],
                        "name": "Ron Kohavi",
                        "slug": "Ron-Kohavi",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Kohavi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ron Kohavi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 8
                            }
                        ],
                        "text": "Indeed, Kohavi (1996) has observed that the Bayesian classifier tends to outperform C4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 281
                            }
                        ],
                        "text": "\u2026directly from the definition of global optimality, and the fact that there exist m-of-n concepts for which the Bayesian classifier makes errors, even when the examples are noise-free (i.e., an example always has the same class) and the Bayes rate is therefore zero (e.g., 3-of-7, Kohavi, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 10
                            }
                        ],
                        "text": "Following Kohavi, Becker, and Sommerfield (1997), the Laplace correction was used with f = 1/n, where n is the number of examples in the training set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 19
                            }
                        ],
                        "text": "In a similar vein, Kohavi (1996) formed decision trees with Bayesian classifiers at the nodes, and showed that it tended to outperform either approach alone, especially on large data sets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60538272,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "49f6fe73703ecad9271b23697da8902fe49348b4",
            "isKey": true,
            "numCitedBy": 349,
            "numCiting": 360,
            "paperAbstract": {
                "fragments": [],
                "text": "In this doctoral dissertation, we study three basic problems in machine learning and two new hypothesis spaces with corresponding learning algorithms. The problems we investigate are: accuracy estimation, feature subset selection, and parameter tuning. The latter two problems are related and are studied under the wrapper approach. The hypothesis spaces we investigate are: decision tables with a default majority rule (DTMs) and oblivious read-once decision graphs (OODGs). For accuracy estimation, we investigate cross-validation and the~.632 bootstrap. We show examples where they fail and conduct a large scale study comparing them. We conclude that repeated runs of five-fold cross-validation give a good tradeoff between bias and variance for the problem of model selection used in later chapters. We define the wrapper approach and use it for feature subset selection and parameter tuning. We relate definitions of feature relevancy to the set of optimal features, which is defined with respect to both a concept and an induction algorithm. The wrapper approach requires a search space, operators, a search engine, and an evaluation function. We investigate all of them in detail and introduce compound operators for feature subset selection. Finally, we abstract the search problem into search with probabilistic estimates. We introduce decision tables with a default majority rule (DTMs) to test the conjecture that feature subset selection is a very powerful bias. The accuracy of induced DTMs is surprisingly powerful, and we concluded that this bias is extremely important for many real-world datasets. We show that the resulting decision tables are very small and can be succinctly displayed. We study properties of oblivious read-once decision graphs (OODGs) and show that they do not suffer from some inherent limitations of decision trees. We describe a a general framework for constructing OODGs bottom-up and specialize it using the wrapper approach. We show that the graphs produced are use less features than C4.5, the state-of-the-art decision tree induction algorithm, and are usually easier for humans to comprehend."
            },
            "slug": "Wrappers-for-Performance-Enhancements-and-Oblivious-Kohavi",
            "title": {
                "fragments": [],
                "text": "Wrappers for Performance Enhancements and Oblivious Decision Graphs."
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This doctoral dissertation concludes that repeated runs of five-fold cross-validation give a good tradeoff between bias and variance for the problem of model selection used in later chapters."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065335246"
                        ],
                        "name": "James Dougherty",
                        "slug": "James-Dougherty",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Dougherty",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Dougherty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726733"
                        ],
                        "name": "Ron Kohavi",
                        "slug": "Ron-Kohavi",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Kohavi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ron Kohavi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764547"
                        ],
                        "name": "M. Sahami",
                        "slug": "M.-Sahami",
                        "structuredName": {
                            "firstName": "Mehran",
                            "lastName": "Sahami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sahami"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 9
                            }
                        ],
                        "text": "Although Dougherty et al. (1995) found this approach to\nbe slightly less accurate than a more informed one, it has the advantage of simplicity, and is sufficient for verifying that the Bayesian classifier performs as well as, or better than, other learners."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 187
                            }
                        ],
                        "text": "\u2026of the discretized and Gaussian versions also confirm the advantage of discretization, although on this larger ensemble of data sets the difference is less pronounced than that found by Dougherty et al. (1995), and the Gaussian version also does quite well compared to the non-Bayesian learners."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 0
                            }
                        ],
                        "text": "Dougherty, Kohavi, and Sahami (1995) reached similar conclusions by instead discretizing numeric attributes, and found the Bayesian classifier with discretization slightly outperformed a decision-tree learner in 16 data sets, on average."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 85
                            }
                        ],
                        "text": "Definition 1 Let C(E) be the actual class of example E, and let CX(E) be the class assigned to it by classifier X ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2527609,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be3cdcd1e0e668d209c7ac30cbf083854e80046e",
            "isKey": true,
            "numCitedBy": 2150,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Supervised-and-Unsupervised-Discretization-of-Dougherty-Kohavi",
            "title": {
                "fragments": [],
                "text": "Supervised and Unsupervised Discretization of Continuous Features"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726733"
                        ],
                        "name": "Ron Kohavi",
                        "slug": "Ron-Kohavi",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Kohavi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ron Kohavi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696678"
                        ],
                        "name": "D. Wolpert",
                        "slug": "D.-Wolpert",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wolpert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wolpert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 61
                            }
                        ],
                        "text": "Recently, several authors (Kong & Dietterich, 1995; Kohavi & Wolpert, 1996; Tibshirani, 1996; Breiman, 1996; Friedman, 1996) have proposed similar bias-variance decompositions for zero-one loss functions."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14229903,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "faf746427dc80d8a90c0e716fe7a301aed6b6414",
            "isKey": false,
            "numCitedBy": 719,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a bias variance decomposition of expected misclassi cation rate the most commonly used loss function in supervised classi cation learning The bias variance decomposition for quadratic loss functions is well known and serves as an important tool for analyzing learning algorithms yet no decomposition was o ered for the more commonly used zero one misclassi cation loss functions until the recent work of Kong Dietterich and Breiman Their decomposition su ers from some ma jor shortcomings though e g potentially negative variance which our decomposition avoids We show that in practice the naive frequency based estimation of the decompo sition terms is by itself biased and show how to correct for this bias We illustrate the decomposition on various algorithms and datasets from the UCI repository"
            },
            "slug": "Bias-Plus-Variance-Decomposition-for-Zero-One-Loss-Kohavi-Wolpert",
            "title": {
                "fragments": [],
                "text": "Bias Plus Variance Decomposition for Zero-One Loss Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that in practice the naive frequency based estimation of the decompo sition terms is by itself biased and how to correct for this bias is correct."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144609463"
                        ],
                        "name": "R. S. Cost",
                        "slug": "R.-S.-Cost",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Cost",
                            "middleNames": [
                                "Scott"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. S. Cost"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744109"
                        ],
                        "name": "S. Salzberg",
                        "slug": "S.-Salzberg",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Salzberg",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Salzberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 203
                            }
                        ],
                        "text": "The learners used were state-of-the art representatives of three major approaches to classification learning: decision tree induction (C4.5 release 8, Quinlan, 1993), instance-based learning (PEBLS 2.1, Cost & Salzberg, 1993) and rule induction (CN2 version 6.1, Clark & Boswell, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The learners used were state-of-the art representatives of three major approaches to classification learning: decision tree induction (C4.5 release 8, Quinlan, 1993), instance-based learning (PEBLS 2.1,  Cost & Salzberg, 1993 ) and rule induction (CN2 version 6.1, Clark & Boswell, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15994243,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "949a68c4d0513574a4a504ce6f867b1330f8a947",
            "isKey": true,
            "numCitedBy": 422,
            "numCiting": 71,
            "paperAbstract": {
                "fragments": [],
                "text": "In the past, nearest neighbor algorithms for learning from examples have worked best in domains in which all features had numeric values. In such domains, the examples can be treated as points and distance metrics can use standard definitions. In symbolic domains, a more sophisticated treatment of the feature space is required. We introduce a nearest neighbor algorithm for learning in domains with symbolic features. Our algorithm calculates distance tables that allow it to produce real-valued distances between instances, and attaches weights to the instances to further modify the structure of feature space. We show that this technique produces excellent classification accuracy on three problems that have been studied by machine learning researchers: predicting protein secondary structure, identifying DNA promoter sequences, and pronouncing English text. Direct experimental comparisons with the other learning algorithms show that our nearest neighbor algorithm is comparable or superior in all three domains. In addition, our algorithm has advantages in training speed, simplicity, and perspicuity. We conclude that experimental evidence favors the use and continued development of nearest neighbor algorithms for domains such as the ones studied here."
            },
            "slug": "A-Weighted-Nearest-Neighbor-Algorithm-for-Learning-Cost-Salzberg",
            "title": {
                "fragments": [],
                "text": "A Weighted Nearest Neighbor Algorithm for Learning with Symbolic Features"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A nearest neighbor algorithm for learning in domains with symbolic features, which produces excellent classification accuracy on three problems that have been studied by machine learning researchers: predicting protein secondary structure, identifying DNA promoter sequences, and pronouncing English text."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341779"
                        ],
                        "name": "J. R. Quinlan",
                        "slug": "J.-R.-Quinlan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Quinlan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The learners used were state-of-the art representatives of three major approaches to classification learning: decision tree induction (C4.5 release 8,  Quinlan, 1993 ), instance-based learning (PEBLS 2.1, Cost & Salzberg, 1993) and rule induction (CN2 version 6.1, Clark & Boswell, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5262555,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "807c1f19047f96083e13614f7ce20f2ac98c239a",
            "isKey": true,
            "numCitedBy": 21898,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nClassifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on ID3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available on a 3.5-inch floppy diskette for a Sun workstation. \n \nC4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies. \n \nThis book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses."
            },
            "slug": "C4.5:-Programs-for-Machine-Learning-Quinlan",
            "title": {
                "fragments": [],
                "text": "C4.5: Programs for Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A complete guide to the C4.5 system as implemented in C for the UNIX environment, which starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694780"
                        ],
                        "name": "M. Pazzani",
                        "slug": "M.-Pazzani",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Pazzani",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pazzani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2454477"
                        ],
                        "name": "Wendy Sarrett",
                        "slug": "Wendy-Sarrett",
                        "structuredName": {
                            "firstName": "Wendy",
                            "lastName": "Sarrett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wendy Sarrett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 678113,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f029b035d2637c380eab13a394d279e9b2f9ef31",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an approach to modeling the average case behavior of learning algorithms. Our motivation is to predict the expected accuracy of learning algorithms as a function of the number of training examples. We apply this framework to a purely empirical learning algorithm, (the one-sided algorithm for pure conjunctive concepts), and to an algorithm that combines empirical and explanation-based learning. The model is used to gain insight into the behavior of these algorithms on a series of problems. Finally, we evaluate how well the average case model performs when the training examples violate the assumptions of the model."
            },
            "slug": "A-framework-for-average-case-analysis-of-learning-Pazzani-Sarrett",
            "title": {
                "fragments": [],
                "text": "A framework for average case analysis of conjunctive learning algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "An approach to modeling the average case behavior of learning algorithms as a function of the number of training examples is presented and an algorithm that combines empirical and explanation-based learning is applied."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694780"
                        ],
                        "name": "M. Pazzani",
                        "slug": "M.-Pazzani",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Pazzani",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pazzani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694959"
                        ],
                        "name": "Jack Muramatsu",
                        "slug": "Jack-Muramatsu",
                        "structuredName": {
                            "firstName": "Jack",
                            "lastName": "Muramatsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jack Muramatsu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691741"
                        ],
                        "name": "Daniel Billsus",
                        "slug": "Daniel-Billsus",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Billsus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Billsus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Pazzani, Muramatsu, and Billsus (1996)  compared several learners on a suite of information filtering tasks, and found that the Bayesian classifier was the most accurate one overall."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 0
                            }
                        ],
                        "text": "Pazzani, Muramatsu, and Billsus (1996) compared several learners on a suite of information filtering tasks, and found that the Bayesian classifier was the most accurate one overall."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8341815,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e26eee1ffe5d7ed9280f4e5af602e3a4585cbaf",
            "isKey": false,
            "numCitedBy": 790,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe Syskill & Webert, a software agent that learns to rate pages on the World Wide Web (WWW), deciding what pages might interest a user. The user rates explored pages on a three point scale, and Syskill & Webert learns a user profile by analyzing the information on each page. The user profile can be used in two ways. First, it can be used to suggest which links a user would be interested in exploring. Second, it can be used to construct a LYCOS query to find pages that would interest a user. We compare six different algorithms from machine learning and information retrieval on this task. We find that the naive Bayesian classifier offers several advantages over other learning algorithms on this task. Furthermore, we find that an initial portion of a web page is sufficient for making predictions on its interestingness substantially reducing the amount of network transmission required to make predictions."
            },
            "slug": "Syskill-&-Webert:-Identifying-Interesting-Web-Sites-Pazzani-Muramatsu",
            "title": {
                "fragments": [],
                "text": "Syskill & Webert: Identifying Interesting Web Sites"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The naive Bayesian classifier offers several advantages over other learning algorithms on this task and an initial portion of a web page is sufficient for making predictions on its interestingness substantially reducing the amount of network transmission required to make predictions."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI, Vol. 1"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2913246"
                        ],
                        "name": "E. B. Kong",
                        "slug": "E.-B.-Kong",
                        "structuredName": {
                            "firstName": "Eun",
                            "lastName": "Kong",
                            "middleNames": [
                                "Bae"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. B. Kong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 27
                            }
                        ],
                        "text": "Recently, several authors (Kong & Dietterich, 1995; Kohavi & Wolpert, 1996; Tibshirani, 1996; Breiman, 1996; Friedman, 1996) have proposed similar bias-variance decompositions for zero-one loss functions."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17043461,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25744dbb4294fe7abb2d9b1b0d39006482ebb4ab",
            "isKey": false,
            "numCitedBy": 443,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Error-Correcting-Output-Coding-Corrects-Bias-and-Kong-Dietterich",
            "title": {
                "fragments": [],
                "text": "Error-Correcting Output Coding Corrects Bias and Variance"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761784"
                        ],
                        "name": "R. Tibshirani",
                        "slug": "R.-Tibshirani",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Tibshirani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tibshirani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 76
                            }
                        ],
                        "text": "Recently, several authors (Kong & Dietterich, 1995; Kohavi & Wolpert, 1996; Tibshirani, 1996; Breiman, 1996; Friedman, 1996) have proposed similar bias-variance decompositions for zero-one loss functions."
                    },
                    "intents": []
                }
            ],
            "corpusId": 118798332,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9253f3e13bca7e845e60394d85ddaec0d4cfc6d6",
            "isKey": false,
            "numCitedBy": 149,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the notions of bias and variance for classiication rules. Following Efron (1978) we develop a decomposition of prediction error into its natural components. Then we derive bootstrap estimates of these components and illustrate how they can be used to describe the error behaviour of a classiier in practice. In the process we also obtain a bootstrap estimate of the error of a \\bagged\" classiier."
            },
            "slug": "Bias,-Variance-and-Prediction-Error-for-Rules-Tibshirani",
            "title": {
                "fragments": [],
                "text": "Bias, Variance and Prediction Error for Classification Rules"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A decomposition of prediction error into its natural components is developed and a bootstrap estimate of the error of a \\bagged\" classiier is obtained."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47610596"
                        ],
                        "name": "Doug Fisher",
                        "slug": "Doug-Fisher",
                        "structuredName": {
                            "firstName": "Doug",
                            "lastName": "Fisher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Doug Fisher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2679586"
                        ],
                        "name": "H. Lenz",
                        "slug": "H.-Lenz",
                        "structuredName": {
                            "firstName": "Hans-Joachim",
                            "lastName": "Lenz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Lenz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 10
                            }
                        ],
                        "text": "Following Pazzani (1996), the first measure was estimated accuracy, as determined by leave-one-out cross validation on the training set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Pazzani (1996) proposed joining attributes instead of attribute values."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 87
                            }
                        ],
                        "text": "In Table 4, Accuracy Once shows results for the backward stepwise joining algorithm of Pazzani (1996), forming at most one Cartesian product as determined by the highest accuracy using leave-one-out cross validation on the training set; Entropy Once is the same algorithm except it creates at most\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60202025,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba3a7f7b4dd4933568606fe6d1910778a5cb7af0",
            "isKey": false,
            "numCitedBy": 229,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nThis volume contains a revised collection of papers originally presented at the Fifth International Workshop on Artificial Intelligence and Statistics in 1995. The topics represented in this volume are diverse, and include natural language application causality and graphical models, classification, learning, knowledge discovery, and exploratory data analysis. The chapters illustrate the rich possibilities for interdisciplinary study at the interface of artificial intelligence and statistics. The chapters vary in the background that they assume, but moderate familiarity with techniques of artificial intelligence and statistics is desirable in most cases."
            },
            "slug": "Learning-from-Data:-Artificial-Intelligence-and-V-Fisher-Lenz",
            "title": {
                "fragments": [],
                "text": "Learning from Data: Artificial Intelligence and Statistics V"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This volume contains a revised collection of papers originally presented at the Fifth International Workshop on Artificial Intelligence and Statistics in 1995, and illustrates the rich possibilities for interdisciplinary study at the interface of artificial intelligence and statistics."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48323507"
                        ],
                        "name": "Peter Clark",
                        "slug": "Peter-Clark",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2427357"
                        ],
                        "name": "R. Boswell",
                        "slug": "R.-Boswell",
                        "structuredName": {
                            "firstName": "Robin",
                            "lastName": "Boswell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Boswell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 263
                            }
                        ],
                        "text": "The learners used were state-of-the art representatives of three major approaches to classification learning: decision tree induction (C4.5 release 8, Quinlan, 1993), instance-based learning (PEBLS 2.1, Cost & Salzberg, 1993) and rule induction (CN2 version 6.1, Clark & Boswell, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 0
                            }
                        ],
                        "text": "Clark and Niblett (1989) compared it with two rule learners and a decision-tree learner, and found that it did surprisingly well."
                    },
                    "intents": []
                }
            ],
            "corpusId": 46265723,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c1ff999b23053a9ea1ad2cd4d7023295e7566382",
            "isKey": true,
            "numCitedBy": 1003,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The CN2 algorithm induces an ordered list of classification rules from examples using entropy as its search heuristic. In this short paper, we describe two improvements to this algorithm. Firstly, we present the use of the Laplacian error estimate as an alternative evaluation function and secondly, we show how unordered as well as ordered rules can be generated. We experimentally demonstrate significantly improved performances resulting from these changes, thus enhancing the usefulness of CN2 as an inductive tool. Comparisons with Quinlan's C4.5 are also made."
            },
            "slug": "Rule-Induction-with-CN2:-Some-Recent-Improvements-Clark-Boswell",
            "title": {
                "fragments": [],
                "text": "Rule Induction with CN2: Some Recent Improvements"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Improvements to the CN2 algorithm are described, including the use of the Laplacian error estimate as an alternative evaluation function and it is shown how unordered as well as ordered rules can be generated."
            },
            "venue": {
                "fragments": [],
                "text": "EWSL"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 104
                            }
                        ],
                        "text": "Then, by taking the logarithm of Equation 2, the Bayesian classifier is equivalent to a linear machine (Duda & Hart, 1973) whose discriminant function for class Ci is logP (Ci)+ \u2211\nj,k logP (Aj = vjk|Ci) bjk (i.e., the weight of each Boolean feature is the log-probability of the corresponding\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 105
                            }
                        ],
                        "text": "The simple Bayesian classifier is limited in expressiveness in that it can only create linear frontiers (Duda & Hart, 1973)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 200
                            }
                        ],
                        "text": "Since in nominal domains the basic Bayesian classifier cannot learn some linearly separable concepts, in these domains its range of optimality is a subset of the perceptron\u2019s, or of a linear machine\u2019s (Duda & Hart, 1973)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 105
                            }
                        ],
                        "text": "This article shows that, although the Bayesian classifier\u2019s probability estimates are only optimal under quadratic loss if the independence assumption holds, the classifier itself can be optimal under zero-one loss (misclassification rate) even when this assumption is violated by a wide margin."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 285,
                                "start": 268
                            }
                        ],
                        "text": "In numeric domains, the Bayesian classifier is not restricted to linearly separable problems; for example, if classes are normally distributed, nonlinear boundaries and multiple disconnected regions can arise, and the Bayesian classifier is able to identify them (see Duda & Hart, 1973)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 197
                            }
                        ],
                        "text": "This definition reduces to Equation 6 when one class has probability 1 given E.\nDefinition 2 The Bayes rate for an example is the lowest zero-one loss achievable by any classifier on that example (Duda & Hart, 1973)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 178
                            }
                        ],
                        "text": "Many classifiers can be viewed as computing a set of discriminant functions of the example, one for each class, and assigning the example to the class whose function is maximum (Duda & Hart, 1973)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 131
                            }
                        ],
                        "text": "Evidence of the Bayesian classifier\u2019s surprising practical value has also led to attempts to extend it by increasing its tolerance of attribute independence in various ways, but the success of these attempts has been uneven."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 167
                            }
                        ],
                        "text": "If P (Ci|E) is the probability that example E is of class Ci, zero-one loss is minimized if, and only if, E is assigned to the class Ck for which P (Ck|E) is maximum (Duda & Hart, 1973)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 124
                            }
                        ],
                        "text": "Then the following result is an immediate extension to the general nominal case of a well-known one for Boolean attributes (Duda & Hart, 1973)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12946615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b07ce649d6f6eb636872527104b0209d3edc8188",
            "isKey": true,
            "numCitedBy": 16927,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "slug": "Pattern-classification-and-scene-analysis-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Pattern classification and scene analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "venue": {
                "fragments": [],
                "text": "A Wiley-Interscience publication"
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48323507"
                        ],
                        "name": "Peter Clark",
                        "slug": "Peter-Clark",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2353436"
                        ],
                        "name": "T. Niblett",
                        "slug": "T.-Niblett",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Niblett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Niblett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Clark and Niblett (1989)  compared it with two rule learners and a decision-tree learner, and found that it did surprisingly well."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Clark and Niblett (1989) compared it with two rule learners and a decision-tree learner, and found that it did surprisingly well."
                    },
                    "intents": []
                }
            ],
            "corpusId": 4408326,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7ec5f9694bc3d061b376256320eacb8ec3566b77",
            "isKey": false,
            "numCitedBy": 1257,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Systems for inducing concept descriptions from examples are valuable tools for assisting in the task of knowledge acquisition for expert systems. This paper presents a description and empirical evaluation of a new induction system, CN2, designed for the efficient induction of simple, comprehensible production rules in domains where problems of poor description language and/or noise may be present. Implementations of the CN2, ID3, and AQ algorithms are compared on three medical classification tasks."
            },
            "slug": "The-CN2-Induction-Algorithm-Clark-Niblett",
            "title": {
                "fragments": [],
                "text": "The CN2 Induction Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A description and empirical evaluation of a new induction system, CN2, designed for the efficient induction of simple, comprehensible production rules in domains where problems of poor description language and/or noise may be present."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145298005"
                        ],
                        "name": "Catherine Blake",
                        "slug": "Catherine-Blake",
                        "structuredName": {
                            "firstName": "Catherine",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Catherine Blake"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 10
                            }
                        ],
                        "text": "Clark and Niblett (1989) compared it with two rule learners and a decision-tree learner, and found that it did surprisingly well. Cestnik (1990) reached similar conclusions."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 10
                            }
                        ],
                        "text": "Clark and Niblett (1989) compared it with two rule learners and a decision-tree learner, and found that it did surprisingly well."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 10
                            }
                        ],
                        "text": "Clark and Niblett (1989) compared it with two rule learners and a decision-tree learner, and found that it did surprisingly well. Cestnik (1990) reached similar conclusions. Kononenko (1990) reported that, in addition, at least one class of users (doctors) finds the Bayesian classifier\u2019s representation quite intuitive and easy to understand, something which is often a significant concern in machine learning."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 62622768,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e068be31ded63600aea068eacd12931efd2a1029",
            "isKey": false,
            "numCitedBy": 13446,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "UCI-Repository-of-machine-learning-databases-Blake",
            "title": {
                "fragments": [],
                "text": "UCI Repository of machine learning databases"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713687"
                        ],
                        "name": "M. Kub\u00e1t",
                        "slug": "M.-Kub\u00e1t",
                        "structuredName": {
                            "firstName": "Miroslav",
                            "lastName": "Kub\u00e1t",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kub\u00e1t"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080192"
                        ],
                        "name": "D. Flotzinger",
                        "slug": "D.-Flotzinger",
                        "structuredName": {
                            "firstName": "Doris",
                            "lastName": "Flotzinger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Flotzinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29840515"
                        ],
                        "name": "G. Pfurtscheller",
                        "slug": "G.-Pfurtscheller",
                        "structuredName": {
                            "firstName": "Gert",
                            "lastName": "Pfurtscheller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Pfurtscheller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 9
                            }
                        ],
                        "text": "John and Langley (1995) showed that the Bayesian classifier\u2019s performance can be much improved if the traditional treatment of numeric attributes, which assumes Gaussian distributions, is replaced by kernel density estimation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Langley (1993) proposed the use of \u201crecursive Bayesian classifiers\u201d to address this limitation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 23
                            }
                        ],
                        "text": "In a related approach, Kubat, Flotzinger, and Pfurtscheller (1993) found that using a decision-tree learner to select features for use in the Bayesian classifier gave good results in the domain of EEG signal classification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 38701317,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9b058d820c6aa084c14daa68c08836c08b5be8bf",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The objective of this paper is to draw the attention of the ML-researchers to the domain of data analysis. The issue is illustrated by an attractive case study\u2014automatic classification of non-averaged EEG-signals. We applied several approaches and obtained best results from a combination of an ID3-like program with Bayesian learning."
            },
            "slug": "Discovering-Patterns-in-EEG-Signals:-Comparative-of-Kub\u00e1t-Flotzinger",
            "title": {
                "fragments": [],
                "text": "Discovering Patterns in EEG-Signals: Comparative Study of a Few Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "The objective of this paper is to draw the attention of the ML-researchers to the domain of data analysis by an attractive case study\u2014automatic classification of non-averaged EEG-signals."
            },
            "venue": {
                "fragments": [],
                "text": "ECML"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726733"
                        ],
                        "name": "Ron Kohavi",
                        "slug": "Ron-Kohavi",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Kohavi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ron Kohavi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38725127"
                        ],
                        "name": "B. Becker",
                        "slug": "B.-Becker",
                        "structuredName": {
                            "firstName": "Barry",
                            "lastName": "Becker",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Becker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40307399"
                        ],
                        "name": "D. Sommerfield",
                        "slug": "D.-Sommerfield",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Sommerfield",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sommerfield"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 10
                            }
                        ],
                        "text": "Following Kohavi, Becker, and Sommerfield (1997), the Laplace correction was used with f = 1/n, where n is the number of examples in the training set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 53749419,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba5bd9458b0bc495c2ccb38a69a4e2f3e17c2088",
            "isKey": false,
            "numCitedBy": 166,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Improving-simple-Bayes-Kohavi-Becker",
            "title": {
                "fragments": [],
                "text": "Improving simple Bayes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2476128"
                        ],
                        "name": "B. Cestnik",
                        "slug": "B.-Cestnik",
                        "structuredName": {
                            "firstName": "Bojan",
                            "lastName": "Cestnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Cestnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Cestnik (1990) reached similar conclusions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 63
                            }
                        ],
                        "text": "The results of previous authors could be a fluke, due to unusual characteristics of the data sets used (especially since, in several cases, the number of data sets used was relatively small)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 20779819,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ad58594194155af8b48eaf3ab9525a1fafd24e58",
            "isKey": false,
            "numCitedBy": 580,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Estimating-Probabilities:-A-Crucial-Task-in-Machine-Cestnik",
            "title": {
                "fragments": [],
                "text": "Estimating Probabilities: A Crucial Task in Machine Learning"
            },
            "venue": {
                "fragments": [],
                "text": "ECAI"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2353436"
                        ],
                        "name": "T. Niblett",
                        "slug": "T.-Niblett",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Niblett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Niblett"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 138
                            }
                        ],
                        "text": "A principled solution to this problem is to incorporate a small-sample correction into all probabilities, such as the Laplace correction (Niblett, 1987)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 43140291,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d9414d6f32edefe583ed98300f97e1d355584a43",
            "isKey": false,
            "numCitedBy": 168,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Constructing-Decision-Trees-in-Noisy-Domains-Niblett",
            "title": {
                "fragments": [],
                "text": "Constructing Decision Trees in Noisy Domains"
            },
            "venue": {
                "fragments": [],
                "text": "EWSL"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The eeect of assuming independence in applying Bayes' theorem to risk estimation and classiication in diagnosis"
            },
            "venue": {
                "fragments": [],
                "text": "Computers and Biomedical Research"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 94
                            }
                        ],
                        "text": "Recently, several authors (Kong & Dietterich, 1995; Kohavi & Wolpert, 1996; Tibshirani, 1996; Breiman, 1996; Friedman, 1996) have proposed similar bias-variance decompositions for zero-one loss functions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 94
                            }
                        ],
                        "text": "Recently, several authors (Kong & Dietterich, 1995; Kohavi & Wolpert, 1996; Tibshirani, 1996; Breiman, 1996; Friedman, 1996) have proposed similar bias-variance decompositions for zero-one loss functions. In particular, Friedman (1996) has shown, using normal approximations to the class probabilities, that the bias-variance interaction now takes a very different form."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bias, variance and arcing classifiers (Technical Report 460)"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics Department,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 136
                            }
                        ],
                        "text": "Given attributes Am and An and the class variable C, a possible measure of the degree of pairwise dependence between Am and An given C (Wan & Wong, 1989; Kononenko, 1991) is\nD(Am, An|C) = H(Am|C) + H(An|C) \u2212H(AmAn|C), (4)\nwhere AmAn represents the Cartesian product of attributes Am and An (i.e., a\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 4
                            }
                        ],
                        "text": "Confidence levels for the observed differences in accuracy between the (discretized) Bayesian classifier and the other algorithms, according to a one-tailed paired t test, are also reported.3\nThe results are summarized in Table 2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 209
                            }
                        ],
                        "text": "This section empirically tests this claim by comparing Pazzani\u2019s (1996) extension with one that differs from it solely by using the method for attribute dependence detection described in (Kononenko, 1991) and (Wan & Wong, 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A measure for concept dissimilarity and its applications in machine learning"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the International Conference on Computing and Information (pp. 267{273)"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 0
                            }
                        ],
                        "text": "Friedman, Geiger, and Goldszmidt (1997) compared the simple Bayesian classifier with Bayesian networks, a much more powerful representation that has the Bayesian classifier as a special case, and found that the latter approach tended to produce no improvements, and sometimes led to large reductions\u2026"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian network classiiers"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 4
                            }
                        ],
                        "text": "See Dietterich (1996) for more on this issue."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical tests for comparing supervised classification learning algorithms (technical report)"
            },
            "venue": {
                "fragments": [],
                "text": "Statistical tests for comparing supervised classification learning algorithms (technical report)"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 94
                            }
                        ],
                        "text": "Recently, several authors (Kong & Dietterich, 1995; Kohavi & Wolpert, 1996; Tibshirani, 1996; Breiman, 1996; Friedman, 1996) have proposed similar bias-variance decompositions for zero-one loss functions."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bias, variance and arcing classifiers (Technical Report 460) Statistics Department"
            },
            "venue": {
                "fragments": [],
                "text": "Bias, variance and arcing classifiers (Technical Report 460) Statistics Department"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 10
                            }
                        ],
                        "text": "Following Kohavi, Becker, and Sommerfield (1997), the Laplace correction was used with f = 1/n, where n is the number of examples in the training set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improving simple Bayes (technical report). Data Mining and Visualization Group"
            },
            "venue": {
                "fragments": [],
                "text": "Silicon Graphics Inc"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 932,
                                "start": 99
                            }
                        ],
                        "text": "The fourth line shows the confidence levels obtained by applying the more sensitive Wilcoxon test (DeGroot, 1986) to the 28 average accuracy differences obtained, and results in high confidence that the Bayesian classifier is more accurate than each of the other learners. The fifth line shows the average accuracy across all data sets, and again the Bayesian classifier performs the best. The last line shows the average rank of each algorithm, computed for each domain by assigning rank 1 to the most accurate algorithm, rank 2 to the second best, and so on. The Bayesian classifier is the best-ranked of all algorithms, indicating that when it does not win it still tends to be one of the best. The comparative results of the discretized and Gaussian versions also confirm the advantage of discretization, although on this larger ensemble of data sets the difference is less pronounced than that found by Dougherty et al. (1995), and the Gaussian version also does quite well compared to the non-Bayesian learners."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1986).Probability and statistics(2nd ed.)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 4
                            }
                        ],
                        "text": "See Dietterich (1996) for more on this issue."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Statistical tests for comparing supervised classiication learning algorithms (technical report)"
            },
            "venue": {
                "fragments": [],
                "text": "Statistical tests for comparing supervised classiication learning algorithms (technical report)"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 142
                            }
                        ],
                        "text": "Given attributes Am and An and the class variable C, a possible measure of the degree of pairwise dependence between Am and An given C (Wan & Wong, 1989; Kononenko, 1991) is\nD(Am, An|C) = H(Am|C) + H(An|C) \u2212H(AmAn|C), (4)\nwhere AmAn represents the Cartesian product of attributes Am and An (i.e., a\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 215
                            }
                        ],
                        "text": "This section empirically tests this claim by comparing Pazzani\u2019s (1996) extension with one that differs from it solely by using the method for attribute dependence detection described in (Kononenko, 1991) and (Wan & Wong, 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A measure for concept dissimilarity and its applications in machine learning"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the International Conference on Computing and Information"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 76
                            }
                        ],
                        "text": "Recently, several authors (Kong & Dietterich, 1995; Kohavi & Wolpert, 1996; Tibshirani, 1996; Breiman, 1996; Friedman, 1996) have proposed similar bias-variance decompositions for zero-one loss functions."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bias, variance and prediction error for classiication rules (technical report )"
            },
            "venue": {
                "fragments": [],
                "text": "Bias, variance and prediction error for classiication rules (technical report )"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 10
                            }
                        ],
                        "text": "Following Kohavi, Becker, and Sommerfield (1997), the Laplace correction was used with f = 1/n, where n is the number of examples in the training set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improving simple Bayes ( technical report )"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 94
                            }
                        ],
                        "text": "Recently, several authors (Kong & Dietterich, 1995; Kohavi & Wolpert, 1996; Tibshirani, 1996; Breiman, 1996; Friedman, 1996) have proposed similar bias-variance decompositions for zero-one loss functions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 94
                            }
                        ],
                        "text": "Recently, several authors (Kong & Dietterich, 1995; Kohavi & Wolpert, 1996; Tibshirani, 1996; Breiman, 1996; Friedman, 1996) have proposed similar bias-variance decompositions for zero-one loss functions. In particular, Friedman (1996) has shown, using normal approximations to the class probabilities, that the bias-variance interaction now takes a very different form."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1996).Bias, variance and arcing classifiers"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 94
                            }
                        ],
                        "text": "Recently, several authors (Kong & Dietterich, 1995; Kohavi & Wolpert, 1996; Tibshirani, 1996; Breiman, 1996; Friedman, 1996) have proposed similar bias-variance decompositions for zero-one loss functions."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bias, variance and arcing classiiers (Technical Report 460) Statistics Department"
            },
            "venue": {
                "fragments": [],
                "text": "Bias, variance and arcing classiiers (Technical Report 460) Statistics Department"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 9
                            }
                        ],
                        "text": "Overall, Ben-Bassat, Klove, and Weil (1980) have shown that the Bayesian classifier is quite robust with respect to errors in probability estimates due to small sample size; this is not surprising, since it can be attributed to the same factors that make it robust with respect to violations of the\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sensitivity analysis in Bayesian classification"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 99
                            }
                        ],
                        "text": "The fourth line shows the confidence levels obtained by applying the more sensitive Wilcoxon test (DeGroot, 1986) to the 28 average accuracy differences obtained, and results in high confidence that the Bayesian classifier is more accurate than each of the other learners."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 932,
                                "start": 99
                            }
                        ],
                        "text": "The fourth line shows the confidence levels obtained by applying the more sensitive Wilcoxon test (DeGroot, 1986) to the 28 average accuracy differences obtained, and results in high confidence that the Bayesian classifier is more accurate than each of the other learners. The fifth line shows the average accuracy across all data sets, and again the Bayesian classifier performs the best. The last line shows the average rank of each algorithm, computed for each domain by assigning rank 1 to the most accurate algorithm, rank 2 to the second best, and so on. The Bayesian classifier is the best-ranked of all algorithms, indicating that when it does not win it still tends to be one of the best. The comparative results of the discretized and Gaussian versions also confirm the advantage of discretization, although on this larger ensemble of data sets the difference is less pronounced than that found by Dougherty et al. (1995), and the Gaussian version also does quite well compared to the non-Bayesian learners."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Probability and statistics (2nd ed.)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 10
                            }
                        ],
                        "text": "Following Pazzani (1996), the first measure was estimated accuracy, as determined by leave-one-out cross validation on the training set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Pazzani (1996) proposed joining attributes instead of attribute values."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 87
                            }
                        ],
                        "text": "In Table 4, Accuracy Once shows results for the backward stepwise joining algorithm of Pazzani (1996), forming at most one Cartesian product as determined by the highest accuracy using leave-one-out cross validation on the training set; Entropy Once is the same algorithm except it creates at most\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Searching for dependencies in Bayesian classiiers Learning from data: Artiicial intelligence and statistics V (pp. 239{248)"
            },
            "venue": {
                "fragments": [],
                "text": "Searching for dependencies in Bayesian classiiers Learning from data: Artiicial intelligence and statistics V (pp. 239{248)"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 10
                            }
                        ],
                        "text": "Following Kohavi, Becker, and Sommerfield (1997), the Laplace correction was used with f = 1/n, where n is the number of examples in the training set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improving simple Bayes ( technical report ) . Data Mining and Visualization Group , Silicon Graphics Inc . , Mountain View , CA"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 10
                            }
                        ],
                        "text": "Following Kohavi, Becker, and Sommerfield (1997), the Laplace correction was used with f = 1/n, where n is the number of examples in the training set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improving simple Bayes (technical report). Data Mining and Visualization Group, Silicon Graphics Inc., Mountain View, CA. ftp://starry.stanford.edu/pub/ronnyk/impSBC.ps.Z"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 67
                            }
                        ],
                        "text": "Many classifiers can be viewed as computing a set of discriminant functions of the example, one for each class, and assigning the example to the class whose function is maximum (Duda & Hart, 1973)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 0
                            }
                        ],
                        "text": "Kononenko (1990) reported that, in addition, at least one class of users (doctors) finds the Bayesian classifier\u2019s representation quite intuitive and easy to understand, something which is often a significant concern in machine learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Comparison of inductive and naive Bayesian learning approaches to automatic knowledge acquisition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1995).Wrappers for performance enhancement and oblivious decision graphs"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 30,
            "methodology": 21,
            "result": 5
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 59,
        "totalPages": 6
    },
    "page_url": "https://www.semanticscholar.org/paper/On-the-Optimality-of-the-Simple-Bayesian-Classifier-Domingos-Pazzani/700666f0c59a4fedc8b08294424c47cb99a8e2ff?sort=total-citations"
}