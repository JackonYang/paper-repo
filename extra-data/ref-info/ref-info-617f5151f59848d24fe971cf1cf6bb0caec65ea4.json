{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108812052"
                        ],
                        "name": "Xiaohui Zhao",
                        "slug": "Xiaohui-Zhao",
                        "structuredName": {
                            "firstName": "Xiaohui",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaohui Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109569771"
                        ],
                        "name": "Zhuo Wu",
                        "slug": "Zhuo-Wu",
                        "structuredName": {
                            "firstName": "Zhuo",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhuo Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108004518"
                        ],
                        "name": "Xiaoguang Wang",
                        "slug": "Xiaoguang-Wang",
                        "structuredName": {
                            "firstName": "Xiaoguang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoguang Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[9, 23, 30, 39, 61] work on recognized texts and their positions (see Fig."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Under the traditional routines, most existing methods [9, 23, 30, 39, 40, 45, 54, 61] design frameworks in multiple stages of text reading (usually including detection and recognition) and information extraction independently in order."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Some works[9, 23, 39, 61] took the layout into consideration, and worked on the reconstructed character or word segmentation of the document."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 88518818,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "024aea24cc4d0949d6fe1482591d2429a9d8bbeb",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Extracting key information from documents, such as receipts or invoices, and preserving the interested texts to structured data is crucial in the document-intensive streamline processes of office automation in areas that includes but not limited to accounting, financial, and taxation areas. To avoid designing expert rules for each specific type of document, some published works attempt to tackle the problem by learning a model to explore the semantic context in text sequences based on the Named Entity Recognition (NER) method in the NLP field. In this paper, we propose to harness the effective information from both semantic meaning and spatial distribution of texts in documents. Specifically, our proposed model, Convolutional Universal Text Information Extractor (CUTIE), applies convolutional neural networks on gridded texts where texts are embedded as features with semantical connotations. We further explore the effect of employing different structures of convolutional neural network and propose a fast and portable structure. We demonstrate the effectiveness of the proposed method on a dataset with up to $4,484$ labelled receipts, without any pre-training or post-processing, achieving state of the art performance that is much better than the NER based methods in terms of either speed and accuracy. Experimental results also demonstrate that the proposed CUTIE model being able to achieve good performance with a much smaller amount of training data."
            },
            "slug": "CUTIE:-Learning-to-Understand-Documents-with-Text-Zhao-Wu",
            "title": {
                "fragments": [],
                "text": "CUTIE: Learning to Understand Documents with Convolutional Universal Text Information Extractor"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The proposed model, Convolutional Universal Text Information Extractor (CUTIE), applies convolutional neural networks on gridded texts where texts are embedded as features with semantical connotations and aims to harness the effective information from both semantic meaning and spatial distribution of texts in documents."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067170682"
                        ],
                        "name": "Rasmus Berg Palm",
                        "slug": "Rasmus-Berg-Palm",
                        "structuredName": {
                            "firstName": "Rasmus",
                            "lastName": "Palm",
                            "middleNames": [
                                "Berg"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rasmus Berg Palm"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805808"
                        ],
                        "name": "Florian Laws",
                        "slug": "Florian-Laws",
                        "structuredName": {
                            "firstName": "Florian",
                            "lastName": "Laws",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Florian Laws"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724252"
                        ],
                        "name": "O. Winther",
                        "slug": "O.-Winther",
                        "structuredName": {
                            "firstName": "Ole",
                            "lastName": "Winther",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Winther"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 56176213,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "12dde50826698bfb45b45d931c2b94ba0c576b27",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Document information extraction tasks performed by humans create data consisting of a PDF or document image input, and extracted string outputs. This end-to-end data is naturally consumed and produced when performing the task because it is valuable in and of itself. It is naturally available, at no additional cost. Unfortunately, state-of-the-art word classification methods for information extraction cannot use this data, instead requiring word-level labels which are expensive to create and consequently not available for many real life tasks. In this paper we propose the Attend, Copy, Parse architecture, a deep neural network model that can be trained directly on end-to-end data, bypassing the need for word-level labels. We evaluate the proposed architecture on a large diverse set of invoices, and outperform a state-of-the-art production system based on word classification. We believe our proposed architecture can be used on many real life information extraction tasks where word classification cannot be used due to a lack of the required word-level labels."
            },
            "slug": "Attend,-Copy,-Parse-End-to-end-Information-from-Palm-Laws",
            "title": {
                "fragments": [],
                "text": "Attend, Copy, Parse End-to-end Information Extraction from Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper proposes the Attend, Copy, Parse architecture, a deep neural network model that can be trained directly on end-to-end data, bypassing the need for word-level labels, and evaluates the proposed architecture on a large diverse set of invoices, which outperform a state-of-the-art production system based on word classification."
            },
            "venue": {
                "fragments": [],
                "text": "2019 International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110655544"
                        ],
                        "name": "He Guo",
                        "slug": "He-Guo",
                        "structuredName": {
                            "firstName": "He",
                            "lastName": "Guo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "He Guo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3343260"
                        ],
                        "name": "Xiameng Qin",
                        "slug": "Xiameng-Qin",
                        "structuredName": {
                            "firstName": "Xiameng",
                            "lastName": "Qin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiameng Qin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1962338347"
                        ],
                        "name": "Jiaming Liu",
                        "slug": "Jiaming-Liu",
                        "structuredName": {
                            "firstName": "Jiaming",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiaming Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1912505"
                        ],
                        "name": "Junyu Han",
                        "slug": "Junyu-Han",
                        "structuredName": {
                            "firstName": "Junyu",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junyu Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2272123"
                        ],
                        "name": "Jingtuo Liu",
                        "slug": "Jingtuo-Liu",
                        "structuredName": {
                            "firstName": "Jingtuo",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jingtuo Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12081764"
                        ],
                        "name": "Errui Ding",
                        "slug": "Errui-Ding",
                        "structuredName": {
                            "firstName": "Errui",
                            "lastName": "Ding",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Errui Ding"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[14] proposed an entity-aware attention text extraction network to extract entities from VRDs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 47
                            }
                        ],
                        "text": "Two related concurrent works were presented in [4, 14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 202712641,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c5efdc2d4084e47f7b436c97bf81b60fef8f6bdd",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Extracting Text of Interest (ToI) from images is a crucial part of many OCR applications, such as entity recognition of cards, invoices, and receipts. Most of the existing works employ complicated engineering pipeline, which contains OCR and structure information extraction, to fulfill this task. This paper proposes an Entity-aware Attention Text Extraction Network called EATEN, which is an end-to-end trainable system to extract the ToIs without any post-processing. In the proposed framework, each entity is parsed by its corresponding entity-aware decoder, respectively. Moreover, we innovatively introduce a state transition mechanism which further improves the robustness of visual ToI extraction. In consideration of the absence of public benchmarks, we construct a dataset of almost 0.6 million images in three real-world scenarios (train ticket, passport and business card), which is publicly available at https://github.com/beacandler/EATEN. To the best of our knowledge, EATEN is the first single shot method to extract entities from images. Extensive experiments on these benchmarks demonstrate the state-of-the-art performance of EATEN."
            },
            "slug": "EATEN:-Entity-Aware-Attention-for-Single-Shot-Text-Guo-Qin",
            "title": {
                "fragments": [],
                "text": "EATEN: Entity-Aware Attention for Single Shot Visual Text Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper proposes an Entity-aware Attention Text Extraction Network called EATEN, which is an end-to-end trainable system to extract the ToIs without any post-processing, and is the first single shot method to extract entities from images."
            },
            "venue": {
                "fragments": [],
                "text": "2019 International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3032611"
                        ],
                        "name": "Yiheng Xu",
                        "slug": "Yiheng-Xu",
                        "structuredName": {
                            "firstName": "Yiheng",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiheng Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123545597"
                        ],
                        "name": "Minghao Li",
                        "slug": "Minghao-Li",
                        "structuredName": {
                            "firstName": "Minghao",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minghao Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145500855"
                        ],
                        "name": "Lei Cui",
                        "slug": "Lei-Cui",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Cui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Cui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110003"
                        ],
                        "name": "Shaohan Huang",
                        "slug": "Shaohan-Huang",
                        "structuredName": {
                            "firstName": "Shaohan",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaohan Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49807919"
                        ],
                        "name": "Furu Wei",
                        "slug": "Furu-Wei",
                        "structuredName": {
                            "firstName": "Furu",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Furu Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92660691"
                        ],
                        "name": "Ming Zhou",
                        "slug": "Ming-Zhou",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming Zhou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Different from [54] which extracts these features from scratch, we directly reuse C = (c1, c2, ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Under the traditional routines, most existing methods [9, 23, 30, 39, 40, 45, 54, 61] design frameworks in multiple stages of text reading (usually including detection and recognition) and information extraction independently in order."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "LayoutLM [54] makes use of large pre-training data and fine-tunes on SROIE."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "2(b)), while [54, 58] further includes image embeddings (see Fig."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "85 LayoutLM[54] 95."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[30] combined texts and their positions through a Graph Convolutional Network (GCN) and [54] further integrated position and image embeddings for pre-training inspired by BERT [10]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 209515395,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3465c06c872d8c48d628c5fc2d484087719351b6",
            "isKey": true,
            "numCitedBy": 164,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Pre-training techniques have been verified successfully in a variety of NLP tasks in recent years. Despite the widespread use of pre-training models for NLP applications, they almost exclusively focus on text-level manipulation, while neglecting layout and style information that is vital for document image understanding. In this paper, we propose the LayoutLM to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents. Furthermore, we also leverage image features to incorporate words' visual information into LayoutLM. To the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for document-level pre-training. It achieves new state-of-the-art results in several downstream tasks, including form understanding (from 70.72 to 79.27), receipt understanding (from 94.02 to 95.24) and document image classification (from 93.07 to 94.42). The code and pre-trained LayoutLM models are publicly available at https://aka.ms/layoutlm."
            },
            "slug": "LayoutLM:-Pre-training-of-Text-and-Layout-for-Image-Xu-Li",
            "title": {
                "fragments": [],
                "text": "LayoutLM: Pre-training of Text and Layout for Document Image Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The LayoutLM is proposed to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110965354"
                        ],
                        "name": "Xiaojing Liu",
                        "slug": "Xiaojing-Liu",
                        "structuredName": {
                            "firstName": "Xiaojing",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojing Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112256"
                        ],
                        "name": "Feiyu Gao",
                        "slug": "Feiyu-Gao",
                        "structuredName": {
                            "firstName": "Feiyu",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Feiyu Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112207346"
                        ],
                        "name": "Qiong Zhang",
                        "slug": "Qiong-Zhang",
                        "structuredName": {
                            "firstName": "Qiong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiong Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36530509"
                        ],
                        "name": "Huasha Zhao",
                        "slug": "Huasha-Zhao",
                        "structuredName": {
                            "firstName": "Huasha",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huasha Zhao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 85528598,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04df8c70257b5280b9d303502c9d7ddf946f181b",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Visually rich documents (VRDs) are ubiquitous in daily business and life. Examples are purchase receipts, insurance policy documents, custom declaration forms and so on. In VRDs, visual and layout information is critical for document understanding, and texts in such documents cannot be serialized into the one-dimensional sequence without losing information. Classic information extraction models such as BiLSTM-CRF typically operate on text sequences and do not incorporate visual features. In this paper, we introduce a graph convolution based model to combine textual and visual information presented in VRDs. Graph embeddings are trained to summarize the context of a text segment in the document, and further combined with text embeddings for entity extraction. Extensive experiments have been conducted to show that our method outperforms BiLSTM-CRF baselines by significant margins, on two real-world datasets. Additionally, ablation studies are also performed to evaluate the effectiveness of each component of our model."
            },
            "slug": "Graph-Convolution-for-Multimodal-Information-from-Liu-Gao",
            "title": {
                "fragments": [],
                "text": "Graph Convolution for Multimodal Information Extraction from Visually Rich Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper introduces a graph convolution based model to combine textual and visual information presented in VRDs and outperforms BiLSTM-CRF baselines by significant margins, on two real-world datasets."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2048569409"
                        ],
                        "name": "Wenwen Yu",
                        "slug": "Wenwen-Yu",
                        "structuredName": {
                            "firstName": "Wenwen",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenwen Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054645836"
                        ],
                        "name": "Ning Lu",
                        "slug": "Ning-Lu",
                        "structuredName": {
                            "firstName": "Ning",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ning Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2689287"
                        ],
                        "name": "Xianbiao Qi",
                        "slug": "Xianbiao-Qi",
                        "structuredName": {
                            "firstName": "Xianbiao",
                            "lastName": "Qi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xianbiao Qi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2124651409"
                        ],
                        "name": "Ping Gong",
                        "slug": "Ping-Gong",
                        "structuredName": {
                            "firstName": "Ping",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ping Gong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069928447"
                        ],
                        "name": "Rong Xiao",
                        "slug": "Rong-Xiao",
                        "structuredName": {
                            "firstName": "Rong",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rong Xiao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 215786577,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba5ed98c4546fada5c732bced4a1c1615f1a4c16",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Computer vision with state-of-the-art deep learning models has achieved huge success in the field of Optical Character Recognition (OCR) including text detection and recognition tasks recently. However, Key Information Extraction (KIE) from documents as the downstream task of OCR, having a large number of use scenarios in real-world, remains a challenge because documents not only have textual features extracting from OCR systems but also have semantic visual features that are not fully exploited and play a critical role in KIE. Too little work has been devoted to efficiently make full use of both textual and visual features of the documents. In this paper, we introduce PICK, a framework that is effective and robust in handling complex documents layout for KIE by combining graph learning with graph convolution operation, yielding a richer semantic representation containing the textual and visual features and global layout without ambiguity. Extensive experiments on realworld datasets have been conducted to show that our method outperforms baselines methods by significant margins. Our code is available at https://github.com/wenwenyu/PICK-pytorch."
            },
            "slug": "PICK:-Processing-Key-Information-Extraction-from-Yu-Lu",
            "title": {
                "fragments": [],
                "text": "PICK: Processing Key Information Extraction from Documents using Improved Graph Learning-Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "PICK is introduced, a framework that is effective and robust in handling complex documents layout for KIE by combining graph learning with graph convolution operation, yielding a richer semantic representation containing the textual and visual features and global layout without ambiguity."
            },
            "venue": {
                "fragments": [],
                "text": "2020 25th International Conference on Pattern Recognition (ICPR)"
            },
            "year": 2021
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276155"
                        ],
                        "name": "Baoguang Shi",
                        "slug": "Baoguang-Shi",
                        "structuredName": {
                            "firstName": "Baoguang",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Baoguang Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145905113"
                        ],
                        "name": "X. Bai",
                        "slug": "X.-Bai",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Bai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 545733,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0016d247423bede7f66224044ccc08f21d49fd39",
            "isKey": false,
            "numCitedBy": 458,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Most state-of-the-art text detection methods are specific to horizontal Latin text and are not fast enough for real-time applications. We introduce Segment Linking (SegLink), an oriented text detection method. The main idea is to decompose text into two locally detectable elements, namely segments and links. A segment is an oriented box covering a part of a word or text line, A link connects two adjacent segments, indicating that they belong to the same word or text line. Both elements are detected densely at multiple scales by an end-to-end trained, fully-convolutional neural network. Final detections are produced by combining segments connected by links. Compared with previous methods, SegLink improves along the dimensions of accuracy, speed, and ease of training. It achieves an f-measure of 75.0% on the standard ICDAR 2015 Incidental (Challenge 4) benchmark, outperforming the previous best by a large margin. It runs at over 20 FPS on 512x512 images. Moreover, without modification, SegLink is able to detect long lines of non-Latin text, such as Chinese."
            },
            "slug": "Detecting-Oriented-Text-in-Natural-Images-by-Shi-Bai",
            "title": {
                "fragments": [],
                "text": "Detecting Oriented Text in Natural Images by Linking Segments"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "SegLink, an oriented text detection method to decompose text into two locally detectable elements, namely segments and links, achieves an f-measure of 75.0% on the standard ICDAR 2015 Incidental (Challenge 4) benchmark, outperforming the previous best by a large margin."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065512194"
                        ],
                        "name": "Liang Qiao",
                        "slug": "Liang-Qiao",
                        "structuredName": {
                            "firstName": "Liang",
                            "lastName": "Qiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liang Qiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51428687"
                        ],
                        "name": "Sanli Tang",
                        "slug": "Sanli-Tang",
                        "structuredName": {
                            "firstName": "Sanli",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sanli Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2398015"
                        ],
                        "name": "Zhanzhan Cheng",
                        "slug": "Zhanzhan-Cheng",
                        "structuredName": {
                            "firstName": "Zhanzhan",
                            "lastName": "Cheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhanzhan Cheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47103450"
                        ],
                        "name": "Yunlu Xu",
                        "slug": "Yunlu-Xu",
                        "structuredName": {
                            "firstName": "Yunlu",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yunlu Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1490934795"
                        ],
                        "name": "Yi Niu",
                        "slug": "Yi-Niu",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Niu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Niu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3290437"
                        ],
                        "name": "Shiliang Pu",
                        "slug": "Shiliang-Pu",
                        "structuredName": {
                            "firstName": "Shiliang",
                            "lastName": "Pu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shiliang Pu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144894849"
                        ],
                        "name": "Fei Wu",
                        "slug": "Fei-Wu",
                        "structuredName": {
                            "firstName": "Fei",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fei Wu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 211133114,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ede81c6e378d33d4861b3fab9218f045bcc33995",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Many approaches have recently been proposed to detect irregular scene text and achieved promising results. However, their localization results may not well satisfy the following text recognition part mainly because of two reasons: 1) recognizing arbitrary shaped text is still a challenging task, and 2) prevalent non-trainable pipeline strategies between text detection and text recognition will lead to suboptimal performances. To handle this incompatibility problem, in this paper we propose an end-to-end trainable text spotting approach named Text Perceptron. Concretely, Text Perceptron first employs an efficient segmentation-based text detector that learns the latent text reading order and boundary information. Then a novel Shape Transform Module (abbr. STM) is designed to transform the detected feature regions into regular morphologies without extra parameters. It unites text detection and the following recognition part into a whole framework, and helps the whole network achieve global optimization. Experiments show that our method achieves competitive performance on two standard text benchmarks, i.e., ICDAR 2013 and ICDAR 2015, and also obviously outperforms existing methods on irregular text benchmarks SCUT-CTW1500 and Total-Text."
            },
            "slug": "Text-Perceptron:-Towards-End-to-End-Text-Spotting-Qiao-Tang",
            "title": {
                "fragments": [],
                "text": "Text Perceptron: Towards End-to-End Arbitrary-Shaped Text Spotting"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This paper proposes an end-to-end trainable text spotting approach named Text Perceptron, which unites text detection and the following recognition part into a whole framework, and helps the whole network achieve global optimization."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118328320"
                        ],
                        "name": "Tong He",
                        "slug": "Tong-He",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40219976"
                        ],
                        "name": "Zhi Tian",
                        "slug": "Zhi-Tian",
                        "structuredName": {
                            "firstName": "Zhi",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi Tian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49015548"
                        ],
                        "name": "Weilin Huang",
                        "slug": "Weilin-Huang",
                        "structuredName": {
                            "firstName": "Weilin",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weilin Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780381"
                        ],
                        "name": "Chunhua Shen",
                        "slug": "Chunhua-Shen",
                        "structuredName": {
                            "firstName": "Chunhua",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunhua Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143970608"
                        ],
                        "name": "Y. Qiao",
                        "slug": "Y.-Qiao",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Qiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Qiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755278"
                        ],
                        "name": "Changming Sun",
                        "slug": "Changming-Sun",
                        "structuredName": {
                            "firstName": "Changming",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Changming Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3801827,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89642d3bacccbe543e224ea139b69986048915ef",
            "isKey": false,
            "numCitedBy": 143,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Text detection and recognition in natural images have long been considered as two separate tasks that are processed sequentially. Jointly training two tasks is non-trivial due to significant differences in learning difficulties and convergence rates. In this work, we present a conceptually simple yet efficient framework that simultaneously processes the two tasks in a united framework. Our main contributions are three-fold: (1) we propose a novel text-alignment layer that allows it to precisely compute convolutional features of a text instance in arbitrary orientation, which is the key to boost the performance; (2) a character attention mechanism is introduced by using character spatial information as explicit supervision, leading to large improvements in recognition; (3) two technologies, together with a new RNN branch for word recognition, are integrated seamlessly into a single model which is end-to-end trainable. This allows the two tasks to work collaboratively by sharing convolutional features, which is critical to identify challenging text instances. Our model obtains impressive results in end-to-end recognition on the ICDAR 2015 [19], significantly advancing the most recent results [2], with improvements of F-measure from (0.54, 0.51, 0.47) to (0.82, 0.77, 0.63), by using a strong, weak and generic lexicon respectively. Thanks to joint training, our method can also serve as a good detector by achieving a new state-of-the-art detection performance on related benchmarks. Code is available at https://github.com/tonghe90/textspotter."
            },
            "slug": "An-End-to-End-TextSpotter-with-Explicit-Alignment-He-Tian",
            "title": {
                "fragments": [],
                "text": "An End-to-End TextSpotter with Explicit Alignment and Attention"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel text-alignment layer is proposed that allows it to precisely compute convolutional features of a text instance in arbitrary orientation, which is the key to boost the performance of the model on the ICDAR 2015."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409949029"
                        ],
                        "name": "Wei Feng",
                        "slug": "Wei-Feng",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Feng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2011051"
                        ],
                        "name": "Wenhao He",
                        "slug": "Wenhao-He",
                        "structuredName": {
                            "firstName": "Wenhao",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenhao He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145820427"
                        ],
                        "name": "Fei Yin",
                        "slug": "Fei-Yin",
                        "structuredName": {
                            "firstName": "Fei",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fei Yin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2870877"
                        ],
                        "name": "Xu-Yao Zhang",
                        "slug": "Xu-Yao-Zhang",
                        "structuredName": {
                            "firstName": "Xu-Yao",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xu-Yao Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117992479"
                        ],
                        "name": "Cheng-Lin Liu",
                        "slug": "Cheng-Lin-Liu",
                        "structuredName": {
                            "firstName": "Cheng-Lin",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheng-Lin Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ing includes text detection and recognition in images, which belongs to the optical character recogtion (OCR) research field and has already been widely used in many Computer Vision (CV) applications [12, 42, 52]. In the information extraction (IE) stage, specific contents (e.g. entity, relation) are mined and processed from previously recognized plain text for diverse Natural Language Processing (NLP) tasks,"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "a stacked RNN on top of the recursive CNN [25], whose performance surpassed the state-of-the-art among diverse variations. To sufficiently exploit the complementary between detection and recognition, [3, 12, 18, 26, 31, 35, 42, 52] were proposed to jointly detect and recognize text instances in an end-to-end manner. They all achieved impressive results compared to traditional pipepline approaches due to capturing relations with"
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 207978383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b8993ad751885096166bb35753044d68a7ffe18",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Most existing text spotting methods either focus on horizontal/oriented texts or perform arbitrary shaped text spotting with character-level annotations. In this paper, we propose a novel text spotting framework to detect and recognize text of arbitrary shapes in an end-to-end manner, using only word/line-level annotations for training. Motivated from the name of TextSnake, which is only a detection model, we call the proposed text spotting framework TextDragon. In TextDragon, a text detector is designed to describe the shape of text with a series of quadrangles, which can handle text of arbitrary shapes. To extract arbitrary text regions from feature maps, we propose a new differentiable operator named RoISlide, which is the key to connect arbitrary shaped text detection and recognition. Based on the extracted features through RoISlide, a CNN and CTC based text recognizer is introduced to make the framework free from labeling the location of characters. The proposed method achieves state-of-the-art performance on two curved text benchmarks CTW1500 and Total-Text, and competitive results on the ICDAR 2015 Dataset."
            },
            "slug": "TextDragon:-An-End-to-End-Framework-for-Arbitrary-Feng-He",
            "title": {
                "fragments": [],
                "text": "TextDragon: An End-to-End Framework for Arbitrary Shaped Text Spotting"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper proposes a novel text spotting framework to detect and recognize text of arbitrary shapes in an end-to-end manner, using only word/line-level annotations for training, and achieves state-of-the-art performance on two curved text benchmarks CTW1500 and Total-Text, and competitive results on the ICDAR 2015 Dataset."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71421876"
                        ],
                        "name": "Hao Wang",
                        "slug": "Hao-Wang",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069300348"
                        ],
                        "name": "Pu Lu",
                        "slug": "Pu-Lu",
                        "structuredName": {
                            "firstName": "Pu",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pu Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1561587870"
                        ],
                        "name": "Hui Zhang",
                        "slug": "Hui-Zhang",
                        "structuredName": {
                            "firstName": "Hui",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hui Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2181925"
                        ],
                        "name": "Mingkun Yang",
                        "slug": "Mingkun-Yang",
                        "structuredName": {
                            "firstName": "Mingkun",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mingkun Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145905113"
                        ],
                        "name": "X. Bai",
                        "slug": "X.-Bai",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Bai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9510649"
                        ],
                        "name": "Yongchao Xu",
                        "slug": "Yongchao-Xu",
                        "structuredName": {
                            "firstName": "Yongchao",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yongchao Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49109969"
                        ],
                        "name": "Mengchao He",
                        "slug": "Mengchao-He",
                        "structuredName": {
                            "firstName": "Mengchao",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mengchao He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153709848"
                        ],
                        "name": "Yongpan Wang",
                        "slug": "Yongpan-Wang",
                        "structuredName": {
                            "firstName": "Yongpan",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yongpan Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109194747"
                        ],
                        "name": "Wenyu Liu",
                        "slug": "Wenyu-Liu",
                        "structuredName": {
                            "firstName": "Wenyu",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenyu Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 77
                            }
                        ],
                        "text": "To sufficiently exploit the complementary between detection and recognition, [3, 12, 18, 26, 31, 35, 42, 52] were proposed to jointly detect and recognize text instances in an end-to-end manner."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 223
                            }
                        ],
                        "text": "Specifically, text reading includes text detection and recognition in images, which belongs to the optical character recogtion (OCR) research field and has already been widely used in many Computer Vision (CV) applications [12, 42, 52]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 208202085,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6fbe116c6ac5eda6f8e4abb153ed40819792515f",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, end-to-end text spotting that aims to detect and recognize text from cluttered images simultaneously has received particularly growing interest in computer vision. Different from the existing approaches that formulate text detection as bounding box extraction or instance segmentation, we localize a set of points on the boundary of each text instance. With the representation of such boundary points, we establish a simple yet effective scheme for end-to-end text spotting, which can read the text of arbitrary shapes. Experiments on three challenging datasets, including ICDAR2015, TotalText and COCO-Text demonstrate that the proposed method consistently surpasses the state-of-the-art in both scene text detection and end-to-end text recognition tasks."
            },
            "slug": "All-You-Need-Is-Boundary:-Toward-Arbitrary-Shaped-Wang-Lu",
            "title": {
                "fragments": [],
                "text": "All You Need Is Boundary: Toward Arbitrary-Shaped Text Spotting"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "This work localizes a set of points on the boundary of each text instance to establish a simple yet effective scheme for end-to-end text spotting, which can read the text of arbitrary shapes."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2155494310"
                        ],
                        "name": "Hui Li",
                        "slug": "Hui-Li",
                        "structuredName": {
                            "firstName": "Hui",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hui Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1585288737"
                        ],
                        "name": "Peng Wang",
                        "slug": "Peng-Wang",
                        "structuredName": {
                            "firstName": "Peng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peng Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780381"
                        ],
                        "name": "Chunhua Shen",
                        "slug": "Chunhua-Shen",
                        "structuredName": {
                            "firstName": "Chunhua",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunhua Shen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "a stacked RNN on top of the recursive CNN [25], whose performance surpassed the state-of-the-art among diverse variations. To sufficiently exploit the complementary between detection and recognition, [3, 12, 18, 26, 31, 35, 42, 52] were proposed to jointly detect and recognize text instances in an end-to-end manner. They all achieved impressive results compared to traditional pipepline approaches due to capturing relations with"
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 627305,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3470684522ba013135a61fd6644a102e2f14cc7c",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work, we jointly address the problem of text detection and recognition in natural scene images based on convolutional recurrent neural networks. We propose a unified network that simultaneously localizes and recognizes text with a single forward pass, avoiding intermediate processes, such as image cropping, feature re-calculation, word separation, and character grouping. In contrast to existing approaches that consider text detection and recognition as two distinct tasks and tackle them one by one, the proposed framework settles these two tasks concurrently. The whole framework can be trained end-to-end, requiring only images, ground-truth bounding boxes and text labels. The convolutional features are calculated only once and shared by both detection and recognition, which saves processing time. Through multi-task training, the learned features become more informative and improves the overall performance. Our proposed method has achieved competitive performance on several benchmark datasets."
            },
            "slug": "Towards-End-to-End-Text-Spotting-with-Convolutional-Li-Wang",
            "title": {
                "fragments": [],
                "text": "Towards End-to-End Text Spotting with Convolutional Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A unified network that simultaneously localizes and recognizes text with a single forward pass is proposed, avoiding intermediate processes, such as image cropping, feature re-calculation, word separation, and character grouping."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10344582"
                        ],
                        "name": "Pengyuan Lyu",
                        "slug": "Pengyuan-Lyu",
                        "structuredName": {
                            "firstName": "Pengyuan",
                            "lastName": "Lyu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pengyuan Lyu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8155680"
                        ],
                        "name": "Minghui Liao",
                        "slug": "Minghui-Liao",
                        "structuredName": {
                            "firstName": "Minghui",
                            "lastName": "Liao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minghui Liao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146721"
                        ],
                        "name": "C. Yao",
                        "slug": "C.-Yao",
                        "structuredName": {
                            "firstName": "Cong",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Wenhao Wu",
                        "slug": "Wenhao-Wu",
                        "structuredName": {
                            "firstName": "Wenhao",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenhao Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145905113"
                        ],
                        "name": "X. Bai",
                        "slug": "X.-Bai",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Bai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 77
                            }
                        ],
                        "text": "To sufficiently exploit the complementary between detection and recognition, [3, 12, 18, 26, 31, 35, 42, 52] were proposed to jointly detect and recognize text instances in an end-to-end manner."
                    },
                    "intents": []
                }
            ],
            "corpusId": 49651094,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "375479213a9982ecf4363669bc36449ca11421a8",
            "isKey": false,
            "numCitedBy": 319,
            "numCiting": 99,
            "paperAbstract": {
                "fragments": [],
                "text": "Unifying text detection and text recognition in an end-to-end training fashion has become a new trend for reading text in the wild, as these two tasks are highly relevant and complementary. In this paper, we investigate the problem of scene text spotting, which aims at simultaneous text detection and recognition in natural images. An end-to-end trainable neural network named as Mask TextSpotter is presented. Different from the previous text spotters that follow the pipeline consisting of a proposal generation network and a sequence-to-sequence recognition network, Mask TextSpotter enjoys a simple and smooth end-to-end learning procedure, in which both detection and recognition can be achieved directly from two-dimensional space via semantic segmentation. Further, a spatial attention module is proposed to enhance the performance and universality. Benefiting from the proposed two-dimensional representation on both detection and recognition, it easily handles text instances of irregular shapes, for instance, curved text. We evaluate it on four English datasets and one multi-language dataset, achieving consistently superior performance over state-of-the-art methods in both detection and end-to-end text recognition tasks. Moreover, we further investigate the recognition module of our method separately, which significantly outperforms state-of-the-art methods on both regular and irregular text datasets for scene text recognition."
            },
            "slug": "Mask-TextSpotter:-An-End-to-End-Trainable-Neural-Lyu-Liao",
            "title": {
                "fragments": [],
                "text": "Mask TextSpotter: An End-to-End Trainable Neural Network for Spotting Text with Arbitrary Shapes"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The recognition module of the Mask TextSpotter method is investigated separately, which significantly outperforms state-of-the-art methods on both regular and irregular text datasets for scene text recognition."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2151060023"
                        ],
                        "name": "Xuebo Liu",
                        "slug": "Xuebo-Liu",
                        "structuredName": {
                            "firstName": "Xuebo",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuebo Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152335674"
                        ],
                        "name": "Ding Liang",
                        "slug": "Ding-Liang",
                        "structuredName": {
                            "firstName": "Ding",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ding Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111618604"
                        ],
                        "name": "Shipeng Yan",
                        "slug": "Shipeng-Yan",
                        "structuredName": {
                            "firstName": "Shipeng",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shipeng Yan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113597869"
                        ],
                        "name": "Dagui Chen",
                        "slug": "Dagui-Chen",
                        "structuredName": {
                            "firstName": "Dagui",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dagui Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143970608"
                        ],
                        "name": "Y. Qiao",
                        "slug": "Y.-Qiao",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Qiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Qiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721677"
                        ],
                        "name": "Junjie Yan",
                        "slug": "Junjie-Yan",
                        "structuredName": {
                            "firstName": "Junjie",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junjie Yan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "To sufficiently exploit the complementary between detection and recognition, [3, 12, 18, 26, 31, 35, 42, 52] were proposed to jointly detect and recognize text instances in an end-to-end manner."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9858530,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee03d4a310e551c892dd4674b0dc36c7a11b8652",
            "isKey": false,
            "numCitedBy": 321,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Incidental scene text spotting is considered one of the most difficult and valuable challenges in the document analysis community. Most existing methods treat text detection and recognition as separate tasks. In this work, we propose a unified end-to-end trainable Fast Oriented Text Spotting (FOTS) network for simultaneous detection and recognition, sharing computation and visual information among the two complementary tasks. Specifically, RoIRotate is introduced to share convolutional features between detection and recognition. Benefiting from convolution sharing strategy, our FOTS has little computation overhead compared to baseline text detection network, and the joint training method makes our method perform better than these two-stage methods. Experiments on ICDAR 2015, ICDAR 2017 MLT, and ICDAR 2013 datasets demonstrate that the proposed method outperforms state-of-the-art methods significantly, which further allows us to develop the first real-time oriented text spotting system which surpasses all previous state-of-the-art results by more than 5% on ICDAR 2015 text spotting task while keeping 22.6 fps."
            },
            "slug": "FOTS:-Fast-Oriented-Text-Spotting-with-a-Unified-Liu-Liang",
            "title": {
                "fragments": [],
                "text": "FOTS: Fast Oriented Text Spotting with a Unified Network"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work proposes a unified end-to-end trainable Fast Oriented Text Spotting (FOTS) network for simultaneous detection and recognition, sharing computation and visual information among the two complementary tasks, and introduces RoIRotate to share convolutional features between detection and Recognition."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144295318"
                        ],
                        "name": "S. Soderland",
                        "slug": "S.-Soderland",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Soderland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Soderland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ", structured, and semi-structured) follows the conventions of [22, 51]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8359747,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "22fb3b3b2bdf768dd435eedfc5ef5155d3e56b1a",
            "isKey": false,
            "numCitedBy": 1071,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "A wealth of on-line text information can be made available to automatic processing by information extraction (IE) systems. Each IE application needs a separate set of rules tuned to the domain and writing style. WHISK helps to overcome this knowledge-engineering bottleneck by learning text extraction rules automatically.WHISK is designed to handle text styles ranging from highly structured to free text, including text that is neither rigidly formatted nor composed of grammatical sentences. Such semi-structured text has largely been beyond the scope of previous systems. When used in conjunction with a syntactic analyzer and semantic tagging, WHISK can also handle extraction from free text such as news stories."
            },
            "slug": "Learning-Information-Extraction-Rules-for-and-Free-Soderland",
            "title": {
                "fragments": [],
                "text": "Learning Information Extraction Rules for Semi-Structured and Free Text"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "WHISK is designed to handle text styles ranging from highly structured to free text, including text that is neither rigidly formatted nor composed of grammatical sentences, and can also handle extraction from free text such as news stories."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144617250"
                        ],
                        "name": "Manuel Carbonell",
                        "slug": "Manuel-Carbonell",
                        "structuredName": {
                            "firstName": "Manuel",
                            "lastName": "Carbonell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manuel Carbonell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064078500"
                        ],
                        "name": "Alicia Forn'es",
                        "slug": "Alicia-Forn'es",
                        "structuredName": {
                            "firstName": "Alicia",
                            "lastName": "Forn'es",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alicia Forn'es"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41206897"
                        ],
                        "name": "M. Villegas",
                        "slug": "M.-Villegas",
                        "structuredName": {
                            "firstName": "Mauricio",
                            "lastName": "Villegas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Villegas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066073648"
                        ],
                        "name": "Josep Llad'os",
                        "slug": "Josep-Llad'os",
                        "structuredName": {
                            "firstName": "Josep",
                            "lastName": "Llad'os",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josep Llad'os"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[3] localizes, recognizes and classifies each word in the document."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 79
                            }
                        ],
                        "text": "In the information extraction module, we use convolutions of three kernel size [3, 5, 7] followed by max pooling to extract representations of text and the dimension of text\u2019s embedding vector is 256."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 47
                            }
                        ],
                        "text": "Two related concurrent works were presented in [3, 12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 209439569,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8917995cc5e23f2341670e7671d5748402ea74c4",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "TreyNet:-A-Neural-Model-for-Text-Localization,-and-Carbonell-Forn'es",
            "title": {
                "fragments": [],
                "text": "TreyNet: A Neural Model for Text Localization, Transcription and Named Entity Recognition in Full Pages"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51126615"
                        ],
                        "name": "Shangbang Long",
                        "slug": "Shangbang-Long",
                        "structuredName": {
                            "firstName": "Shangbang",
                            "lastName": "Long",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shangbang Long"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51125377"
                        ],
                        "name": "Jiaqiang Ruan",
                        "slug": "Jiaqiang-Ruan",
                        "structuredName": {
                            "firstName": "Jiaqiang",
                            "lastName": "Ruan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiaqiang Ruan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "19262604"
                        ],
                        "name": "W. Zhang",
                        "slug": "W.-Zhang",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116553662"
                        ],
                        "name": "Xin He",
                        "slug": "Xin-He",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xin He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Wenhao Wu",
                        "slug": "Wenhao-Wu",
                        "structuredName": {
                            "firstName": "Wenhao",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenhao Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146721"
                        ],
                        "name": "C. Yao",
                        "slug": "C.-Yao",
                        "structuredName": {
                            "firstName": "Cong",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Anchor-based methods [2, 17, 28, 32, 49] predict the existence of texts and regress their location offsets at pre-defined grid points of the input image, while segmentation-based methods [33, 53, 62] learn the pixel-level classification tasks to separate"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "where the Detector can be any anchor-based [17, 28, 32, 49] or segmentation-based [33, 53, 62] text detection methods."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 49570059,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8d4bbfc2d83088d649756f6bac554bd59c17f6e9",
            "isKey": false,
            "numCitedBy": 294,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Driven by deep neural networks and large scale datasets, scene text detection methods have progressed substantially over the past years, continuously refreshing the performance records on various standard benchmarks. However, limited by the representations (axis-aligned rectangles, rotated rectangles or quadrangles) adopted to describe text, existing methods may fall short when dealing with much more free-form text instances, such as curved text, which are actually very common in real-world scenarios. To tackle this problem, we propose a more flexible representation for scene text, termed as TextSnake, which is able to effectively represent text instances in horizontal, oriented and curved forms. In TextSnake, a text instance is described as a sequence of ordered, overlapping disks centered at symmetric axes, each of which is associated with potentially variable radius and orientation. Such geometry attributes are estimated via a Fully Convolutional Network (FCN) model. In experiments, the text detector based on TextSnake achieves state-of-the-art or comparable performance on Total-Text and SCUT-CTW1500, the two newly published benchmarks with special emphasis on curved text in natural images, as well as the widely-used datasets ICDAR 2015 and MSRA-TD500. Specifically, TextSnake outperforms the baseline on Total-Text by more than 40% in F-measure."
            },
            "slug": "TextSnake:-A-Flexible-Representation-for-Detecting-Long-Ruan",
            "title": {
                "fragments": [],
                "text": "TextSnake: A Flexible Representation for Detecting Text of Arbitrary Shapes"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A more flexible representation for scene text is proposed, termed as TextSnake, which is able to effectively represent text instances in horizontal, oriented and curved forms and outperforms the baseline on Total-Text by more than 40% in F-measure."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50462511"
                        ],
                        "name": "Pan He",
                        "slug": "Pan-He",
                        "structuredName": {
                            "firstName": "Pan",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pan He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49015548"
                        ],
                        "name": "Weilin Huang",
                        "slug": "Weilin-Huang",
                        "structuredName": {
                            "firstName": "Weilin",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weilin Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118328320"
                        ],
                        "name": "Tong He",
                        "slug": "Tong-He",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "22317545"
                        ],
                        "name": "Qile Zhu",
                        "slug": "Qile-Zhu",
                        "structuredName": {
                            "firstName": "Qile",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qile Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143970608"
                        ],
                        "name": "Y. Qiao",
                        "slug": "Y.-Qiao",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Qiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Qiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108672978"
                        ],
                        "name": "Xiaolin Li",
                        "slug": "Xiaolin-Li",
                        "structuredName": {
                            "firstName": "Xiaolin",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaolin Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "into two sub-tasks: text detection and text recognition. In text detection, methods are usually divided into two categories: anchor-based methods and segmentation-based methods . Anchor-based methods [2, 17, 28, 32, 49] predict the existence of texts and regress their location offsets at pre-defined grid points of the input image, while segmentation-based methods [33, 53, 62] learn the pixel-level classification tas"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "height, width and channel number of I. The text detection branch takes Ias input and predicts the locations of all possible text regions. B= Detector(I) (1) where the Detector can be any anchor-based [17, 28, 32, 49] or segmentation-based [33, 53, 62] text detection methods. Here, B= (b1,b2,...,bm)is a set of m text bounding boxes and bi = (x0,y0, x1,y1)denotes the top-left and bottom-right positions of the i-th "
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 23968407,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ffd92ef63f5dc26b025f9d546e248f585d1b0e7a",
            "isKey": false,
            "numCitedBy": 234,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel single-shot text detector that directly outputs word-level bounding boxes in a natural image. We propose an attention mechanism which roughly identifies text regions via an automatically learned attentional map. This substantially suppresses background interference in the convolutional features, which is the key to producing accurate inference of words, particularly at extremely small sizes. This results in a single model that essentially works in a coarse-to-fine manner. It departs from recent FCN-based text detectors which cascade multiple FCN models to achieve an accurate prediction. Furthermore, we develop a hierarchical inception module which efficiently aggregates multi-scale inception features. This enhances local details, and also encodes strong context information, allowing the detector to work reliably on multi-scale and multi-orientation text with single-scale images. Our text detector achieves an F-measure of 77% on the ICDAR 2015 benchmark, advancing the state-of-the-art results in [18, 28]. Demo is available at: http://sstd.whuang.org/."
            },
            "slug": "Single-Shot-Text-Detector-with-Regional-Attention-He-Huang",
            "title": {
                "fragments": [],
                "text": "Single Shot Text Detector with Regional Attention"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A novel single-shot text detector that directly outputs word-level bounding boxes in a natural image and develops a hierarchical inception module which efficiently aggregates multi-scale inception features."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276155"
                        ],
                        "name": "Baoguang Shi",
                        "slug": "Baoguang-Shi",
                        "structuredName": {
                            "firstName": "Baoguang",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Baoguang Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145905113"
                        ],
                        "name": "X. Bai",
                        "slug": "X.-Bai",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Bai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146721"
                        ],
                        "name": "C. Yao",
                        "slug": "C.-Yao",
                        "structuredName": {
                            "firstName": "Cong",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ation-based methods [33, 53, 62] learn the pixel-level classification tasks to separate text regions apart from the background. In text recognition, the mainstreaming CRNN framework was indroduced by [50], using recurrent neural networks (RNNs) [6, 19] combined with CNN-based methods for better sequential recognition of text lines. Then, the attention mechanism replaced existing CTC decoder [50] and w"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 24139,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e9149ab00236d04db23394774e716c4f1d89231",
            "isKey": false,
            "numCitedBy": 1383,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Image-based sequence recognition has been a long-standing research topic in computer vision. In this paper, we investigate the problem of scene text recognition, which is among the most important and challenging tasks in image-based sequence recognition. A novel neural network architecture, which integrates feature extraction, sequence modeling and transcription into a unified framework, is proposed. Compared with previous systems for scene text recognition, the proposed architecture possesses four distinctive properties: (1) It is end-to-end trainable, in contrast to most of the existing algorithms whose components are separately trained and tuned. (2) It naturally handles sequences in arbitrary lengths, involving no character segmentation or horizontal scale normalization. (3) It is not confined to any predefined lexicon and achieves remarkable performances in both lexicon-free and lexicon-based scene text recognition tasks. (4) It generates an effective yet much smaller model, which is more practical for real-world application scenarios. The experiments on standard benchmarks, including the IIIT-5K, Street View Text and ICDAR datasets, demonstrate the superiority of the proposed algorithm over the prior arts. Moreover, the proposed algorithm performs well in the task of image-based music score recognition, which evidently verifies the generality of it."
            },
            "slug": "An-End-to-End-Trainable-Neural-Network-for-Sequence-Shi-Bai",
            "title": {
                "fragments": [],
                "text": "An End-to-End Trainable Neural Network for Image-Based Sequence Recognition and Its Application to Scene Text Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel neural network architecture, which integrates feature extraction, sequence modeling and transcription into a unified framework, is proposed, which generates an effective yet much smaller model, which is more practical for real-world application scenarios."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150063810"
                        ],
                        "name": "Cl\u00e9ment Sage",
                        "slug": "Cl\u00e9ment-Sage",
                        "structuredName": {
                            "firstName": "Cl\u00e9ment",
                            "lastName": "Sage",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cl\u00e9ment Sage"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108521"
                        ],
                        "name": "A. Aussem",
                        "slug": "A.-Aussem",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Aussem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Aussem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2943140"
                        ],
                        "name": "H. Elghazel",
                        "slug": "H.-Elghazel",
                        "structuredName": {
                            "firstName": "Haytham",
                            "lastName": "Elghazel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Elghazel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721326"
                        ],
                        "name": "V. Eglin",
                        "slug": "V.-Eglin",
                        "structuredName": {
                            "firstName": "V\u00e9ronique",
                            "lastName": "Eglin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Eglin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "28334674"
                        ],
                        "name": "J\u00e9r\u00e9my Espinas",
                        "slug": "J\u00e9r\u00e9my-Espinas",
                        "structuredName": {
                            "firstName": "J\u00e9r\u00e9my",
                            "lastName": "Espinas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J\u00e9r\u00e9my Espinas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Under the traditional routines, most existing methods [9, 23, 30, 39, 40, 45, 54, 61] design frameworks in multiple stages of text reading (usually including detection and recognition) and information extraction independently in order."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[40, 45] adopted the idea from natural language processing and used recurrent neural networks to extract entities of interest from VRDs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In earlier explorations, the task is intrinsically downgraded into a traditional OCR procedure and the downstream IE from serialized plain text [40, 45], which completely discards the visual features and layout information from images, as shown in Fig."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 196186469,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15ef63d424a97fd340470a5904663f1b99cf46e5",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Efficiently extracting information from documents issued by their partners is crucial for companies that face huge daily document flows. Particularly, tables contain most valuable information of business documents. However, their contents are challenging to automatically parse as tables from industrial contexts may have complex and ambiguous physical structure. Bypassing their structure recognition, we propose a generic method for end-to-end table field extraction that starts with the sequence of document tokens segmented by an OCR engine and directly tags each token with one of the possible field types. Similar to the state-of-the-art methods for non-tabular field extraction, our approach resorts to a token level recurrent neural network combining spatial and textual features. We empirically assess the effectiveness of recurrent connections for our task by comparing our method with a baseline feedforward network having local context knowledge added to its inputs. We train and evaluate both approaches on a dataset of 28,570 purchase orders to retrieve the ID numbers and quantities of the ordered products. Our method outperforms the baseline with micro F1 score on unknown document layouts of 0.821 compared to 0.764."
            },
            "slug": "Recurrent-Neural-Network-Approach-for-Table-Field-Sage-Aussem",
            "title": {
                "fragments": [],
                "text": "Recurrent Neural Network Approach for Table Field Extraction in Business Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A generic method for end-to-end table field extraction that starts with the sequence of document tokens segmented by an OCR engine and directly tags each token with one of the possible field types, resorts to a token level recurrent neural network combining spatial and textual features."
            },
            "venue": {
                "fragments": [],
                "text": "2019 International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2378954"
                        ],
                        "name": "Xuezhe Ma",
                        "slug": "Xuezhe-Ma",
                        "structuredName": {
                            "firstName": "Xuezhe",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuezhe Ma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144547315"
                        ],
                        "name": "E. Hovy",
                        "slug": "E.-Hovy",
                        "structuredName": {
                            "firstName": "Eduard",
                            "lastName": "Hovy",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Hovy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 24
                            }
                        ],
                        "text": "Learning-based networks [7, 10, 24, 36, 55, 56] were firstly proposed to work on plain text documents."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "(2) NER(TR): Pipeline of text reading and LSTM-CRF."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 13
                            }
                        ],
                        "text": "As expected, NER(TR) performs excellent on such kind of documents, thanks to the inherent serializable property."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "The input embedding of LSTM-CRF[36] is set to 256, followed by a BiLSTM with 128 hidden units and CRF."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 57
                            }
                        ],
                        "text": "We re-implement three top information extraction methods [23, 30, 36]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "NER(TR) discards the layout information and serializes all the texts into onedimensional text sequence, reporting inferior performance than other methods."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 23
                            }
                        ],
                        "text": "The input embedding of LSTM-CRF[31] is set to 256, followed by a BiLSTM with 128 hidden units and CRF. (3) GCN(TR): Pipeline of text Reading and GCN."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 233
                            }
                        ],
                        "text": "In the information extraction (IE) stage, specific contents (e.g. entity, relation) are mined and processed from previously recognized plain text for diverse Natural Language Processing (NLP) tasks, such as Named Entity Recognition (NER) [22, 31, 42] and Question-Answer (QA) [1, 11, 51]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 172
                            }
                        ],
                        "text": "entity, relation) are mined and processed from previously recognized plain text for diverse Natural Language Processing (NLP) tasks, such as Named Entity Recognition (NER) [24, 36, 48] and Question-Answer (QA) [1, 13, 57]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 54
                            }
                        ],
                        "text": "The separated information extraction module Chargrid, NER and GCN run at 2.97, 22.40 and 14.58 fps, and their full pipeline Chargrid(TR), NER(TR) and GCN(TR) report speed of 1.13, 1.69 and 1.62 respectively."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 59
                            }
                        ],
                        "text": "Our model inherits the advantages of both Chargrid(TR) and NER(TR), where context features provide necessary information to tell entities apart and we perform entity extraction in the character granularity."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10489017,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4",
            "isKey": true,
            "numCitedBy": 1994,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55\\% accuracy for POS tagging and 91.21\\% F1 for NER."
            },
            "slug": "End-to-end-Sequence-Labeling-via-Bi-directional-Ma-Hovy",
            "title": {
                "fragments": [],
                "text": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel neutral network architecture is introduced that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF, thus making it applicable to a wide range of sequence labeling tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1861421"
                        ],
                        "name": "M. Busta",
                        "slug": "M.-Busta",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Busta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Busta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145532509"
                        ],
                        "name": "Luk\u00e1s Neumann",
                        "slug": "Luk\u00e1s-Neumann",
                        "structuredName": {
                            "firstName": "Luk\u00e1s",
                            "lastName": "Neumann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luk\u00e1s Neumann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564537"
                        ],
                        "name": "Jiri Matas",
                        "slug": "Jiri-Matas",
                        "structuredName": {
                            "firstName": "Jiri",
                            "lastName": "Matas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiri Matas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 79
                            }
                        ],
                        "text": "In the information extraction module, we use convolutions of three kernel size [3, 5, 7] followed by max pooling to extract representations of text."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 77
                            }
                        ],
                        "text": "To sufficiently exploit the complementary between detection and recognition, [3, 12, 18, 26, 31, 35, 42, 52] were proposed to jointly detect and recognize text instances in an end-to-end manner."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5592192,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "64ff7f81f066a26a40f52e41931a97c166db094d",
            "isKey": false,
            "numCitedBy": 172,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "A method for scene text localization and recognition is proposed. The novelties include: training of both text detection and recognition in a single end-to-end pass, the structure of the recognition CNN and the geometry of its input layer that preserves the aspect of the text and adapts its resolution to the data.,,The proposed method achieves state-of-the-art accuracy in the end-to-end text recognition on two standard datasets \u2013 ICDAR 2013 and ICDAR 2015, whilst being an order of magnitude faster than competing methods - the whole pipeline runs at 10 frames per second on an NVidia K80 GPU."
            },
            "slug": "Deep-TextSpotter:-An-End-to-End-Trainable-Scene-and-Busta-Neumann",
            "title": {
                "fragments": [],
                "text": "Deep TextSpotter: An End-to-End Trainable Scene Text Localization and Recognition Framework"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The proposed method achieves state-of-the-art accuracy in the end-to-end text recognition on two standard datasets \u2013 ICDar 2013 and ICDAR 2015, whilst being an order of magnitude faster than competing methods."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2398015"
                        ],
                        "name": "Zhanzhan Cheng",
                        "slug": "Zhanzhan-Cheng",
                        "structuredName": {
                            "firstName": "Zhanzhan",
                            "lastName": "Cheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhanzhan Cheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057969271"
                        ],
                        "name": "Fan Bai",
                        "slug": "Fan-Bai",
                        "structuredName": {
                            "firstName": "Fan",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fan Bai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47103450"
                        ],
                        "name": "Yunlu Xu",
                        "slug": "Yunlu-Xu",
                        "structuredName": {
                            "firstName": "Yunlu",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yunlu Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070795686"
                        ],
                        "name": "Gang Zheng",
                        "slug": "Gang-Zheng",
                        "structuredName": {
                            "firstName": "Gang",
                            "lastName": "Zheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gang Zheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3290437"
                        ],
                        "name": "Shiliang Pu",
                        "slug": "Shiliang-Pu",
                        "structuredName": {
                            "firstName": "Shiliang",
                            "lastName": "Pu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shiliang Pu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50730331"
                        ],
                        "name": "Shuigeng Zhou",
                        "slug": "Shuigeng-Zhou",
                        "structuredName": {
                            "firstName": "Shuigeng",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuigeng Zhou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ble texts for later sequential recognition. For each text region, its features of shape 32\u00d7256are extracted from the shared convolutional features by RoIAlign [15] and decoded by LSTM-based attention [5], where the number of hidden units is set to 256. In the information extraction module, we use convolutions of three kernel size [3,5,7]followed by max pooling to extract representations of text. The "
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 28347739,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "45524d7a40435d989579b88b70d25e4d65ac9e3c",
            "isKey": false,
            "numCitedBy": 285,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Scene text recognition has been a hot research topic in computer vision due to its various applications. The state of the art is the attention-based encoder-decoder framework that learns the mapping between input images and output sequences in a purely data-driven way. However, we observe that existing attention-based methods perform poorly on complicated and/or low-quality images. One major reason is that existing methods cannot get accurate alignments between feature areas and targets for such images. We call this phenomenon \u201cattention drift\u201d. To tackle this problem, in this paper we propose the FAN (the abbreviation of Focusing Attention Network) method that employs a focusing attention mechanism to automatically draw back the drifted attention. FAN consists of two major components: an attention network (AN) that is responsible for recognizing character targets as in the existing methods, and a focusing network (FN) that is responsible for adjusting attention by evaluating whether AN pays attention properly on the target areas in the images. Furthermore, different from the existing methods, we adopt a ResNet-based network to enrich deep representations of scene text images. Extensive experiments on various benchmarks, including the IIIT5k, SVT and ICDAR datasets, show that the FAN method substantially outperforms the existing methods."
            },
            "slug": "Focusing-Attention:-Towards-Accurate-Text-in-Images-Cheng-Bai",
            "title": {
                "fragments": [],
                "text": "Focusing Attention: Towards Accurate Text Recognition in Natural Images"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The FAN (the abbreviation of Focusing Attention Network) method is proposed that employs a focusing attention mechanism to automatically draw back the drifted attention in scene text images and substantially outperforms the existing methods."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2973998"
                        ],
                        "name": "Fedor Borisyuk",
                        "slug": "Fedor-Borisyuk",
                        "structuredName": {
                            "firstName": "Fedor",
                            "lastName": "Borisyuk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fedor Borisyuk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1821267"
                        ],
                        "name": "Albert Gordo",
                        "slug": "Albert-Gordo",
                        "structuredName": {
                            "firstName": "Albert",
                            "lastName": "Gordo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Albert Gordo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145422368"
                        ],
                        "name": "V. Sivakumar",
                        "slug": "V.-Sivakumar",
                        "structuredName": {
                            "firstName": "Viswanath",
                            "lastName": "Sivakumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Sivakumar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 50773726,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fde3ee5f9f8e217a4d6716013315614811820f21",
            "isKey": false,
            "numCitedBy": 137,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a deployed, scalable optical character recognition (OCR) system, which we call Rosetta , designed to process images uploaded daily at Facebook scale. Sharing of image content has become one of the primary ways to communicate information among internet users within social networks such as Facebook, and the understanding of such media, including its textual information, is of paramount importance to facilitate search and recommendation applications. We present modeling techniques for efficient detection and recognition of text in images and describe Rosetta 's system architecture. We perform extensive evaluation of presented technologies, explain useful practical approaches to build an OCR system at scale, and provide insightful intuitions as to why and how certain components work based on the lessons learnt during the development and deployment of the system."
            },
            "slug": "Rosetta:-Large-Scale-System-for-Text-Detection-and-Borisyuk-Gordo",
            "title": {
                "fragments": [],
                "text": "Rosetta: Large Scale System for Text Detection and Recognition in Images"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "This paper presents a deployed, scalable optical character recognition (OCR) system, which is called Rosetta, designed to process images uploaded daily at Facebook scale, and describes Rosetta 's system architecture."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144439048"
                        ],
                        "name": "Xiang Li",
                        "slug": "Xiang-Li",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiang Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71074736"
                        ],
                        "name": "Wenhai Wang",
                        "slug": "Wenhai-Wang",
                        "structuredName": {
                            "firstName": "Wenhai",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenhai Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061634523"
                        ],
                        "name": "Wenbo Hou",
                        "slug": "Wenbo-Hou",
                        "structuredName": {
                            "firstName": "Wenbo",
                            "lastName": "Hou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenbo Hou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7247659"
                        ],
                        "name": "Ruo-Ze Liu",
                        "slug": "Ruo-Ze-Liu",
                        "structuredName": {
                            "firstName": "Ruo-Ze",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ruo-Ze Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144720255"
                        ],
                        "name": "Tong Lu",
                        "slug": "Tong-Lu",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146236917"
                        ],
                        "name": "Jian Yang",
                        "slug": "Jian-Yang",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 46966180,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7b50853560e62e94f4d20426a6ddb43b8c9b314f",
            "isKey": false,
            "numCitedBy": 271,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Scene text detection has witnessed rapid progress especially with the recent development of convolutional neural networks. However, there still exists two challenges which prevent the algorithm into industry applications. On the one hand, most of the state-of-art algorithms require quadrangle bounding box which is in-accurate to locate the texts with arbitrary shape. On the other hand, two text instances which are close to each other may lead to a false detection which covers both instances. Traditionally, the segmentation-based approach can relieve the first problem but usually fail to solve the second challenge. To address these two challenges, in this paper, we propose a novel Progressive Scale Expansion Network (PSENet), which can precisely detect text instances with arbitrary shapes. More specifically, PSENet generates the different scale of kernels for each text instance, and gradually expands the minimal scale kernel to the text instance with the complete shape. Due to the fact that there are large geometrical margins among the minimal scale kernels, our method is effective to split the close text instances, making it easier to use segmentation-based methods to detect arbitrary-shaped text instances. Extensive experiments on CTW1500, Total-Text, ICDAR 2015 and ICDAR 2017 MLT validate the effectiveness of PSENet. Notably, on CTW1500, a dataset full of long curve texts, PSENet achieves a F-measure of 74.3% at 27 FPS, and our best F-measure (82.2%) outperforms state-of-art algorithms by 6.6%. The code will be released in the future."
            },
            "slug": "Shape-Robust-Text-Detection-With-Progressive-Scale-Li-Wang",
            "title": {
                "fragments": [],
                "text": "Shape Robust Text Detection With Progressive Scale Expansion Network"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A novel Progressive Scale Expansion Network (PSENet) is proposed, which can precisely detect text instances with arbitrary shapes and is effective to split the close text instances, making it easier to use segmentation-based methods to detect arbitrary-shaped text instances."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145654957"
                        ],
                        "name": "Daniel Schuster",
                        "slug": "Daniel-Schuster",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Schuster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Schuster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2956206"
                        ],
                        "name": "Klemens Muthmann",
                        "slug": "Klemens-Muthmann",
                        "structuredName": {
                            "firstName": "Klemens",
                            "lastName": "Muthmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Klemens Muthmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054613265"
                        ],
                        "name": "D. Esser",
                        "slug": "D.-Esser",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Esser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Esser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145417024"
                        ],
                        "name": "A. Schill",
                        "slug": "A.-Schill",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Schill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Schill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113458452"
                        ],
                        "name": "Michael Berger",
                        "slug": "Michael-Berger",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Berger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Berger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2906706"
                        ],
                        "name": "C. Weidling",
                        "slug": "C.-Weidling",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Weidling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Weidling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2652700"
                        ],
                        "name": "Kamil Aliyev",
                        "slug": "Kamil-Aliyev",
                        "structuredName": {
                            "firstName": "Kamil",
                            "lastName": "Aliyev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kamil Aliyev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1942133"
                        ],
                        "name": "A. Hofmeier",
                        "slug": "A.-Hofmeier",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Hofmeier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hofmeier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "elations between text reading and information extraction. 2.2 Information Extraction Information extraction has been studied for decades. Before the advent of learning based models, rule-based methods[8, 11, 21, 38, 44, 47] were proposed. Pattern matching were widely used in [21, 44] to extract one or multiple target values. Afterwards, Intellix[47] required predefined template with relevant fields annotated and SmartFi"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18934920,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "87dee6b4a5fcbab541b45a967c24030df6cee29b",
            "isKey": true,
            "numCitedBy": 49,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic information extraction from scanned business documents is especially valuable in the application domain of document archiving. But current systems for automated document processing still require a lot of configuration work that can only be done by experienced users or administrators. We present an approach for information extraction which purely builds on end-user provided training examples and intentionally omits efficient known extraction techniques like rule based extraction that require intense training and/or information extraction expertise. Our evaluation on a large corpus of business documents shows competitive results of above 85% F1-measure on 10 commonly used fields like document type, sender, receiver and date. The system is deployed and used inside the commercial document management system DocuWare."
            },
            "slug": "Intellix-End-User-Trained-Information-Extraction-Schuster-Muthmann",
            "title": {
                "fragments": [],
                "text": "Intellix -- End-User Trained Information Extraction for Document Archiving"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work presents an approach for information extraction which purely builds on end-user provided training examples and intentionally omits efficient known extraction techniques like rule based extraction that require intense training and/or information extraction expertise."
            },
            "venue": {
                "fragments": [],
                "text": "2013 12th International Conference on Document Analysis and Recognition"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153368526"
                        ],
                        "name": "Timo I. Denk",
                        "slug": "Timo-I.-Denk",
                        "structuredName": {
                            "firstName": "Timo",
                            "lastName": "Denk",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timo I. Denk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9992847"
                        ],
                        "name": "C. Reisswig",
                        "slug": "C.-Reisswig",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Reisswig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Reisswig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 54
                            }
                        ],
                        "text": "Under the traditional routines, most existing methods [7, 21, 27, 33, 34, 39, 48, 54] design frameworks in multiple stages of text reading (usually including detection and recognition) and information extraction independently in order."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 10
                            }
                        ],
                        "text": "Some works[7, 21, 33, 54] took the layout into consideration, and worked on the reconstructed character or word segmentation of the document."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 79
                            }
                        ],
                        "text": "In the information extraction module, we use convolutions of three kernel size [3, 5, 7] followed by max pooling to extract representations of text and the dimension of text\u2019s embedding vector is 256."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "[7, 21, 27, 33, 54] work on recognized texts and their positions (see Fig."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 202558968,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2fbda89395f993040b7665730c64182ade3be195",
            "isKey": true,
            "numCitedBy": 48,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "For understanding generic documents, information like font sizes, column layout, and generally the positioning of words may carry semantic information that is crucial for solving a downstream document intelligence task. Our novel BERTgrid, which is based on Chargrid by Katti et al. (2018), represents a document as a grid of contextualized word piece embedding vectors, thereby making its spatial structure and semantics accessible to the processing neural network. The contextualized embedding vectors are retrieved from a BERT language model. We use BERTgrid in combination with a fully convolutional network on a semantic instance segmentation task for extracting fields from invoices. We demonstrate its performance on tabulated line item and document header field extraction."
            },
            "slug": "BERTgrid:-Contextualized-Embedding-for-2D-Document-Denk-Reisswig",
            "title": {
                "fragments": [],
                "text": "BERTgrid: Contextualized Embedding for 2D Document Representation and Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The novel BERTgrid, which is based on Chargrid, represents a document as a grid of contextualized word piece embedding vectors, thereby making its spatial structure and semantics accessible to the processing neural network."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1390799037"
                        ],
                        "name": "Zheng Huang",
                        "slug": "Zheng-Huang",
                        "structuredName": {
                            "firstName": "Zheng",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zheng Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153819461"
                        ],
                        "name": "Kai Chen",
                        "slug": "Kai-Chen",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "73730984"
                        ],
                        "name": "Jianhua He",
                        "slug": "Jianhua-He",
                        "structuredName": {
                            "firstName": "Jianhua",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianhua He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145905113"
                        ],
                        "name": "X. Bai",
                        "slug": "X.-Bai",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Bai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694974"
                        ],
                        "name": "Dimosthenis Karatzas",
                        "slug": "Dimosthenis-Karatzas",
                        "structuredName": {
                            "firstName": "Dimosthenis",
                            "lastName": "Karatzas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dimosthenis Karatzas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771189"
                        ],
                        "name": "Shijian Lu",
                        "slug": "Shijian-Lu",
                        "structuredName": {
                            "firstName": "Shijian",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shijian Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694502"
                        ],
                        "name": "C. Jawahar",
                        "slug": "C.-Jawahar",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Jawahar",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Jawahar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 18
                            }
                        ],
                        "text": "One is the public SROIE [18] benchmark, and the other two are self-built datasets, Taxi Invoices and Resumes, respectively."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 49
                            }
                        ],
                        "text": "In consistence with [30] and the SROIE Challenge [20], we use F1-score to evaluate the performance of all experiments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 68
                            }
                        ],
                        "text": "LayoutLM [48] makes use of large pre-training data and finetunes on SROIE."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 33
                            }
                        ],
                        "text": "In consistence with [27] and the SROIE Challenge [18], we use F1-score to evaluate the performance of all experiments."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 1
                            }
                        ],
                        "text": "\u2022 SROIE [18] is a public dateset for receipt information extraction in ICDAR 2019 Chanllenge."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "One is the public SROIE [20] benchmark, and the other two are self-built datasets, Taxi Invoices and Resumes, respectively."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "SROIE Dataset: On SROIE dataset, we perform two set of experiments and the results are shown in Table 4."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 8
                            }
                        ],
                        "text": "\u2022 SROIE [20] is a public dateset for receipt information extraction in ICDAR 2019 Chanllenge."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 14
                            }
                        ],
                        "text": "The layout of SROIE dataset is variable and it has structured text."
                    },
                    "intents": []
                }
            ],
            "corpusId": 211026630,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d00cbb0c05c1dc922126fe72c1078b773d01c688",
            "isKey": true,
            "numCitedBy": 51,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The ICDAR 2019 Challenge on \"Scanned receipts OCR and key information extraction\" (SROIE) covers important aspects related to the automated analysis of scanned receipts. The SROIE tasks play a key role in many document analysis systems and hold significant commercial potential. Although a lot of work has been published over the years on administrative document analysis, the community has advanced relatively slowly, as most datasets have been kept private. One of the key contributions of SROIE to the document analysis community is to offer a first, standardized dataset of 1000 whole scanned receipt images and annotations, as well as an evaluation procedure for such tasks. The Challenge is structured around three tasks, namely Scanned Receipt Text Localization (Task 1), Scanned Receipt OCR (Task 2) and Key Information Extraction from Scanned Receipts (Task 3). The competition opened on 10th February, 2019 and closed on 5th May, 2019. We received 29, 24 and 18 valid submissions received for the three competition tasks, respectively. This report presents the competition datasets, define the tasks and the evaluation protocols, offer detailed submission statistics, as well as an analysis of the submitted performance. While the tasks of text localization and recognition seem to be relatively easy to tackle, it is interesting to observe the variety of ideas and approaches proposed for the information extraction task. According to the submissions' performance we believe there is still margin for improving information extraction performance, although the current dataset would have to grow substantially in following editions. Given the success of the SROIE competition evidenced by the wide interest generated and the healthy number of submissions from academic, research institutes and industry over different countries, we consider that the SROIE competition can evolve into a useful resource for the community, drawing further attention and promoting research and development efforts in this field."
            },
            "slug": "ICDAR2019-Competition-on-Scanned-Receipt-OCR-and-Huang-Chen",
            "title": {
                "fragments": [],
                "text": "ICDAR2019 Competition on Scanned Receipt OCR and Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The ICDAR 2019 Challenge on \"Scanned receipts OCR and key information extraction\" (SROIE) covers important aspects related to the automated analysis of scanned receipts, and is considered to evolve into a useful resource for the community, drawing further attention and promoting research and development efforts in this field."
            },
            "venue": {
                "fragments": [],
                "text": "2019 International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719068"
                        ],
                        "name": "Anoop R. Katti",
                        "slug": "Anoop-R.-Katti",
                        "structuredName": {
                            "firstName": "Anoop",
                            "lastName": "Katti",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anoop R. Katti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9992847"
                        ],
                        "name": "C. Reisswig",
                        "slug": "C.-Reisswig",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Reisswig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Reisswig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39387393"
                        ],
                        "name": "Cordula Guder",
                        "slug": "Cordula-Guder",
                        "structuredName": {
                            "firstName": "Cordula",
                            "lastName": "Guder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cordula Guder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14334250"
                        ],
                        "name": "Sebastian Brarda",
                        "slug": "Sebastian-Brarda",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Brarda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Brarda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2704747"
                        ],
                        "name": "S. Bickel",
                        "slug": "S.-Bickel",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Bickel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bickel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2216963"
                        ],
                        "name": "J. H\u00f6hne",
                        "slug": "J.-H\u00f6hne",
                        "structuredName": {
                            "firstName": "Johannes",
                            "lastName": "H\u00f6hne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H\u00f6hne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2803968"
                        ],
                        "name": "J. Faddoul",
                        "slug": "J.-Faddoul",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Faddoul",
                            "middleNames": [
                                "Baptiste"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Faddoul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For Chargrid[23], the input character embedding is set to 128 and the rest of network is identical to the paper."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We re-implement three top information extraction methods [23, 30, 36]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Under the traditional routines, most existing methods [9, 23, 30, 39, 40, 45, 54, 61] design frameworks in multiple stages of text reading (usually including detection and recognition) and information extraction independently in order."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Some works[9, 23, 39, 61] took the layout into consideration, and worked on the reconstructed character or word segmentation of the document."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[9, 23, 30, 39, 61] work on recognized texts and their positions (see Fig."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52815006,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15aae08159856cdbf0ce539357d473a04dcbb7f3",
            "isKey": true,
            "numCitedBy": 100,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a novel type of text representation that preserves the 2D layout of a document. This is achieved by encoding each document page as a two-dimensional grid of characters. Based on this representation, we present a generic document understanding pipeline for structured documents. This pipeline makes use of a fully convolutional encoder-decoder network that predicts a segmentation mask and bounding boxes. We demonstrate its capabilities on an information extraction task from invoices and show that it significantly outperforms approaches based on sequential text or document images."
            },
            "slug": "Chargrid:-Towards-Understanding-2D-Documents-Katti-Reisswig",
            "title": {
                "fragments": [],
                "text": "Chargrid: Towards Understanding 2D Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A novel type of text representation is introduced that preserves the 2D layout of a document by encoding each document page as a two-dimensional grid of characters and it is shown that it significantly outperforms approaches based on sequential text or document images."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148927556"
                        ],
                        "name": "Xinyu Zhou",
                        "slug": "Xinyu-Zhou",
                        "structuredName": {
                            "firstName": "Xinyu",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinyu Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146721"
                        ],
                        "name": "C. Yao",
                        "slug": "C.-Yao",
                        "structuredName": {
                            "firstName": "Cong",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075348632"
                        ],
                        "name": "He Wen",
                        "slug": "He-Wen",
                        "structuredName": {
                            "firstName": "He",
                            "lastName": "Wen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "He Wen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47905836"
                        ],
                        "name": "Yuzhi Wang",
                        "slug": "Yuzhi-Wang",
                        "structuredName": {
                            "firstName": "Yuzhi",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuzhi Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132667"
                        ],
                        "name": "Shuchang Zhou",
                        "slug": "Shuchang-Zhou",
                        "structuredName": {
                            "firstName": "Shuchang",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuchang Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2416953"
                        ],
                        "name": "Weiran He",
                        "slug": "Weiran-He",
                        "structuredName": {
                            "firstName": "Weiran",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weiran He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1387852255"
                        ],
                        "name": "Jiajun Liang",
                        "slug": "Jiajun-Liang",
                        "structuredName": {
                            "firstName": "Jiajun",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiajun Liang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "based methods . Anchor-based methods [15, 25, 29, 43] predict the existence of texts and regress their location offsets at pre-defined grid points of the input image, while segmentation-based methods [30, 47, 55] learn the pixel-level classification tasks to separate text regions apart from the background. In text recognition, the mainstreaming CRNN framework was indroduced by [44], using recurrent neural net"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The text detection branch takes Ias input and predicts the locations of all possible text regions. B= Detector(I) (1) where the Detector can be any anchor-based [15, 25, 29, 43] or segmentation-based [30, 47, 55] text detection methods. Here, B= (b1,b2,...,bm)is a set of m text bounding boxes and bi = (x0,y0, x1,y1)denotes the top-left and bottom-right positions of the i-th text. Given text positions B, RoIAl"
                    },
                    "intents": []
                }
            ],
            "corpusId": 706860,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f1630b4485027eb99ae59b745372ef1f3699c16",
            "isKey": false,
            "numCitedBy": 904,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous approaches for scene text detection have already achieved promising performances across various benchmarks. However, they usually fall short when dealing with challenging scenarios, even when equipped with deep neural network models, because the overall performance is determined by the interplay of multiple stages and components in the pipelines. In this work, we propose a simple yet powerful pipeline that yields fast and accurate text detection in natural scenes. The pipeline directly predicts words or text lines of arbitrary orientations and quadrilateral shapes in full images, eliminating unnecessary intermediate steps (e.g., candidate aggregation and word partitioning), with a single neural network. The simplicity of our pipeline allows concentrating efforts on designing loss functions and neural network architecture. Experiments on standard datasets including ICDAR 2015, COCO-Text and MSRA-TD500 demonstrate that the proposed algorithm significantly outperforms state-of-the-art methods in terms of both accuracy and efficiency. On the ICDAR 2015 dataset, the proposed algorithm achieves an F-score of 0.7820 at 13.2fps at 720p resolution."
            },
            "slug": "EAST:-An-Efficient-and-Accurate-Scene-Text-Detector-Zhou-Yao",
            "title": {
                "fragments": [],
                "text": "EAST: An Efficient and Accurate Scene Text Detector"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work proposes a simple yet powerful pipeline that yields fast and accurate text detection in natural scenes, and significantly outperforms state-of-the-art methods in terms of both accuracy and efficiency."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40241708"
                        ],
                        "name": "K. Shaalan",
                        "slug": "K.-Shaalan",
                        "structuredName": {
                            "firstName": "Khaled",
                            "lastName": "Shaalan",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Shaalan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "entity, relation) are mined and processed from previously recognized plain text for diverse Natural Language Processing (NLP) tasks, such as Named Entity Recognition (NER) [24, 36, 48] and Question-Answer (QA) [1, 13, 57]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18484924,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aac418d2f297d3171e5a4d9979154ed73e430922",
            "isKey": false,
            "numCitedBy": 162,
            "numCiting": 125,
            "paperAbstract": {
                "fragments": [],
                "text": "As more and more Arabic textual information becomes available through the Web in homes and businesses, via Internet and Intranet services, there is an urgent need for technologies and tools to process the relevant information. Named Entity Recognition (NER) is an Information Extraction task that has become an integral part of many other Natural Language Processing (NLP) tasks, such as Machine Translation and Information Retrieval. Arabic NER has begun to receive attention in recent years. The characteristics and peculiarities of Arabic, a member of the Semitic languages family, make dealing with NER a challenge. The performance of an Arabic NER component affects the overall performance of the NLP system in a positive manner. This article attempts to describe and detail the recent increase in interest and progress made in Arabic NER research. The importance of the NER task is demonstrated, the main characteristics of the Arabic language are highlighted, and the aspects of standardization in annotating named entities are illustrated. Moreover, the different Arabic linguistic resources are presented and the approaches used in Arabic NER field are explained. The features of common tools used in Arabic NER are described, and standard evaluation metrics are illustrated. In addition, a review of the state of the art of Arabic NER research is discussed. Finally, we present our conclusions. Throughout the presentation, illustrative examples are used for clarification."
            },
            "slug": "A-Survey-of-Arabic-Named-Entity-Recognition-and-Shaalan",
            "title": {
                "fragments": [],
                "text": "A Survey of Arabic Named Entity Recognition and Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The importance of the NER task is demonstrated, the main characteristics of the Arabic language are highlighted, and the aspects of standardization in annotating named entities are illustrated."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108353180"
                        ],
                        "name": "Yuliang Liu",
                        "slug": "Yuliang-Liu",
                        "structuredName": {
                            "firstName": "Yuliang",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuliang Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144838978"
                        ],
                        "name": "Lianwen Jin",
                        "slug": "Lianwen-Jin",
                        "structuredName": {
                            "firstName": "Lianwen",
                            "lastName": "Jin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lianwen Jin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "into two sub-tasks: text detection and text recognition. In text detection, methods are usually divided into two categories: anchor-based methods and segmentation-based methods . Anchor-based methods [2, 17, 28, 32, 49] predict the existence of texts and regress their location offsets at pre-defined grid points of the input image, while segmentation-based methods [33, 53, 62] learn the pixel-level classification tas"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "height, width and channel number of I. The text detection branch takes Ias input and predicts the locations of all possible text regions. B= Detector(I) (1) where the Detector can be any anchor-based [17, 28, 32, 49] or segmentation-based [33, 53, 62] text detection methods. Here, B= (b1,b2,...,bm)is a set of m text bounding boxes and bi = (x0,y0, x1,y1)denotes the top-left and bottom-right positions of the i-th "
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9111344,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b2293cb53ffb2d4b4a0799bd3c3b3718a2f5af2",
            "isKey": false,
            "numCitedBy": 228,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Detecting incidental scene text is a challenging task because of multi-orientation, perspective distortion, and variation of text size, color and scale. Retrospective research has only focused on using rectangular bounding box or horizontal sliding window to localize text, which may result in redundant background noise, unnecessary overlap or even information loss. To address these issues, we propose a new Convolutional Neural Networks (CNNs) based method, named Deep Matching Prior Network (DMPNet), to detect text with tighter quadrangle. First, we use quadrilateral sliding windows in several specific intermediate convolutional layers to roughly recall the text with higher overlapping area and then a shared Monte-Carlo method is proposed for fast and accurate computing of the polygonal areas. After that, we designed a sequential protocol for relative regression which can exactly predict text with compact quadrangle. Moreover, a auxiliary smooth Ln loss is also proposed for further regressing the position of text, which has better overall performance than L2 loss and smooth L1 loss in terms of robustness and stability. The effectiveness of our approach is evaluated on a public word-level, multi-oriented scene text database, ICDAR 2015 Robust Reading Competition Challenge 4 Incidental scene text localization. The performance of our method is evaluated by using F-measure and found to be 70.64%, outperforming the existing state-of-the-art method with F-measure 63.76%."
            },
            "slug": "Deep-Matching-Prior-Network:-Toward-Tighter-Text-Liu-Jin",
            "title": {
                "fragments": [],
                "text": "Deep Matching Prior Network: Toward Tighter Multi-oriented Text Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new Convolutional Neural Networks (CNNs) based method, named Deep Matching Prior Network (DMPNet), to detect text with tighter quadrangle, which has better overall performance than L2 loss and smooth L1 loss in terms of robustness and stability."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3165738"
                        ],
                        "name": "Bodhisattwa Prasad Majumder",
                        "slug": "Bodhisattwa-Prasad-Majumder",
                        "structuredName": {
                            "firstName": "Bodhisattwa",
                            "lastName": "Majumder",
                            "middleNames": [
                                "Prasad"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bodhisattwa Prasad Majumder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2406599"
                        ],
                        "name": "Navneet Potti",
                        "slug": "Navneet-Potti",
                        "structuredName": {
                            "firstName": "Navneet",
                            "lastName": "Potti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Navneet Potti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2519906"
                        ],
                        "name": "Sandeep Tata",
                        "slug": "Sandeep-Tata",
                        "structuredName": {
                            "firstName": "Sandeep",
                            "lastName": "Tata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sandeep Tata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796372"
                        ],
                        "name": "James Bradley Wendt",
                        "slug": "James-Bradley-Wendt",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Wendt",
                            "middleNames": [
                                "Bradley"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Bradley Wendt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110560772"
                        ],
                        "name": "Qi Zhao",
                        "slug": "Qi-Zhao",
                        "structuredName": {
                            "firstName": "Qi",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qi Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763978"
                        ],
                        "name": "Marc Najork",
                        "slug": "Marc-Najork",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Najork",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc Najork"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "so been utilized. [30] combined texts and their positions through a Graph Convolutional Network (GCN) and [54] further integrated position and image embeddings for pre-training inspired by BERT [10]. [37] presented a representation learning approach to extract structured information from templatic documents, which worked in pipeline of candidate generation, scoring and assignment. However, these works"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 219732851,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "58877aa9aa2d09585a4ff4881b02cb1c7ff9bc28",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel approach using representation learning for tackling the problem of extracting structured information from form-like document images. We propose an extraction system that uses knowledge of the types of the target fields to generate extraction candidates and a neural network architecture that learns a dense representation of each candidate based on neighboring words in the document. These learned representations are not only useful in solving the extraction task for unseen document templates from two different domains but are also interpretable, as we show using loss cases."
            },
            "slug": "Representation-Learning-for-Information-Extraction-Majumder-Potti",
            "title": {
                "fragments": [],
                "text": "Representation Learning for Information Extraction from Form-like Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An extraction system that uses knowledge of the types of the target fields to generate extraction candidates and a neural network architecture that learns a dense representation of each candidate based on neighboring words in the document is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8155680"
                        ],
                        "name": "Minghui Liao",
                        "slug": "Minghui-Liao",
                        "structuredName": {
                            "firstName": "Minghui",
                            "lastName": "Liao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minghui Liao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276155"
                        ],
                        "name": "Baoguang Shi",
                        "slug": "Baoguang-Shi",
                        "structuredName": {
                            "firstName": "Baoguang",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Baoguang Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145905113"
                        ],
                        "name": "X. Bai",
                        "slug": "X.-Bai",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Bai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2443233"
                        ],
                        "name": "Xinggang Wang",
                        "slug": "Xinggang-Wang",
                        "structuredName": {
                            "firstName": "Xinggang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinggang Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743698"
                        ],
                        "name": "Wenyu Liu",
                        "slug": "Wenyu-Liu",
                        "structuredName": {
                            "firstName": "Wenyu",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenyu Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "into two sub-tasks: text detection and text recognition. In text detection, methods are usually divided into two categories: anchor-based methods and segmentation-based methods . Anchor-based methods [2, 17, 28, 32, 49] predict the existence of texts and regress their location offsets at pre-defined grid points of the input image, while segmentation-based methods [33, 53, 62] learn the pixel-level classification tas"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "height, width and channel number of I. The text detection branch takes Ias input and predicts the locations of all possible text regions. B= Detector(I) (1) where the Detector can be any anchor-based [17, 28, 32, 49] or segmentation-based [33, 53, 62] text detection methods. Here, B= (b1,b2,...,bm)is a set of m text bounding boxes and bi = (x0,y0, x1,y1)denotes the top-left and bottom-right positions of the i-th "
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16796292,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2883e50279d034e4fe8416734c7393f7c7a8f2e4",
            "isKey": false,
            "numCitedBy": 540,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "\n \n This paper presents an end-to-end trainable fast scene text detector, named TextBoxes, which detects scene text with both high accuracy and efficiency in a single network forward pass, involving no post-process except for a standard non-maximum suppression. TextBoxes outperforms competing methods in terms of text localization accuracy and is much faster, taking only 0.09s per image in a fast implementation. Furthermore, combined with a text recognizer, TextBoxes significantly outperforms state-of-the-art approaches on word spotting and end-to-end text recognition tasks.\n \n"
            },
            "slug": "TextBoxes:-A-Fast-Text-Detector-with-a-Single-Deep-Liao-Shi",
            "title": {
                "fragments": [],
                "text": "TextBoxes: A Fast Text Detector with a Single Deep Neural Network"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "An end-to-end trainable fast scene text detector, named TextBoxes, which detects scene text with both high accuracy and efficiency in a single network forward pass, involving no post-process except for a standard non-maximum suppression."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6965856"
                        ],
                        "name": "Peter Anderson",
                        "slug": "Peter-Anderson",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Anderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Anderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144137069"
                        ],
                        "name": "Xiaodong He",
                        "slug": "Xiaodong-He",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31790073"
                        ],
                        "name": "Chris Buehler",
                        "slug": "Chris-Buehler",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Buehler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Buehler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2406263"
                        ],
                        "name": "Damien Teney",
                        "slug": "Damien-Teney",
                        "structuredName": {
                            "firstName": "Damien",
                            "lastName": "Teney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Damien Teney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145177220"
                        ],
                        "name": "Mark Johnson",
                        "slug": "Mark-Johnson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145273587"
                        ],
                        "name": "Stephen Gould",
                        "slug": "Stephen-Gould",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Gould",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen Gould"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39089563"
                        ],
                        "name": "Lei Zhang",
                        "slug": "Lei-Zhang",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "lation) are mined and processed from previously recognized plain text for diverse Natural Language Processing (NLP) tasks, such as Named Entity Recognition (NER) [24, 36, 48] and Question-Answer (QA) [1, 13, 57]. Note that in existing routines, the two stages are separately executed. It means that the former recognizes text from images without semantic supervision (e.g., entity name annotation) of IE stage, "
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3753452,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8",
            "isKey": false,
            "numCitedBy": 2275,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge."
            },
            "slug": "Bottom-Up-and-Top-Down-Attention-for-Image-and-Anderson-He",
            "title": {
                "fragments": [],
                "text": "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions is proposed, demonstrating the broad applicability of this approach to VQA."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2605572"
                        ],
                        "name": "S. Huffman",
                        "slug": "S.-Huffman",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Huffman",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Huffman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14690792,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8dadbf2dfebe794ad4fc5022f8bb65195c8f0d5a",
            "isKey": false,
            "numCitedBy": 193,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "A growing population of users want to extract a growing variety of information from on-line texts. Unfortunately, current information extraction systems typically require experts to hand-build dictionaries of extraction patterns for each new type of information to be extracted. This paper presents a system that can learn dictionaries of extraction patterns directly from user-provided examples of texts and events to be extracted from them. The system, called LIEP, learns patterns that recognize relationships between key constituents based on local syntax. Sets of patterns learned by LIEP for a sample extraction task perform nearly at the level of a hand-built dictionary of patterns."
            },
            "slug": "Learning-information-extraction-patterns-from-Huffman",
            "title": {
                "fragments": [],
                "text": "Learning information extraction patterns from examples"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A system that can learn dictionaries of extraction patterns directly from user-provided examples of texts and events to be extracted from them, and learns patterns that recognize relationships between key constituents based on local syntax."
            },
            "venue": {
                "fragments": [],
                "text": "Learning for Natural Language Processing"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054613265"
                        ],
                        "name": "D. Esser",
                        "slug": "D.-Esser",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Esser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Esser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145654957"
                        ],
                        "name": "Daniel Schuster",
                        "slug": "Daniel-Schuster",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Schuster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Schuster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2956206"
                        ],
                        "name": "Klemens Muthmann",
                        "slug": "Klemens-Muthmann",
                        "structuredName": {
                            "firstName": "Klemens",
                            "lastName": "Muthmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Klemens Muthmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113458452"
                        ],
                        "name": "Michael Berger",
                        "slug": "Michael-Berger",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Berger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Berger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145417024"
                        ],
                        "name": "A. Schill",
                        "slug": "A.-Schill",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Schill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Schill"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1897279,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41db13459f0344a0ca63342302484c4f6e044376",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Archiving official written documents such as invoices, reminders and account statements in business and private area gets more and more important. Creating appropriate index entries for document archives like sender's name, creation date or document number is a tedious manual work. We present a novel approach to handle automatic indexing of documents based on generic positional extraction of index terms. For this purpose we apply the knowledge of document templates stored in a common full text search index to find index positions that were successfully extracted in the past."
            },
            "slug": "Automatic-indexing-of-scanned-documents:-a-approach-Esser-Schuster",
            "title": {
                "fragments": [],
                "text": "Automatic indexing of scanned documents: a layout-based approach"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work presents a novel approach to handle automatic indexing of documents based on generic positional extraction of index terms based on document templates stored in a common full text search index to find index positions that were successfully extracted in the past."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2135128997"
                        ],
                        "name": "Akira Fukui",
                        "slug": "Akira-Fukui",
                        "structuredName": {
                            "firstName": "Akira",
                            "lastName": "Fukui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Akira Fukui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422202"
                        ],
                        "name": "Dong Huk Park",
                        "slug": "Dong-Huk-Park",
                        "structuredName": {
                            "firstName": "Dong Huk",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Huk Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422876"
                        ],
                        "name": "Daylen Yang",
                        "slug": "Daylen-Yang",
                        "structuredName": {
                            "firstName": "Daylen",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daylen Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34721166"
                        ],
                        "name": "Anna Rohrbach",
                        "slug": "Anna-Rohrbach",
                        "structuredName": {
                            "firstName": "Anna",
                            "lastName": "Rohrbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anna Rohrbach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34849128"
                        ],
                        "name": "Marcus Rohrbach",
                        "slug": "Marcus-Rohrbach",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Rohrbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus Rohrbach"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "lation) are mined and processed from previously recognized plain text for diverse Natural Language Processing (NLP) tasks, such as Named Entity Recognition (NER) [24, 36, 48] and Question-Answer (QA) [1, 13, 57]. Note that in existing routines, the two stages are separately executed. It means that the former recognizes text from images without semantic supervision (e.g., entity name annotation) of IE stage, "
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2840197,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fddc15480d086629b960be5bff96232f967f2252",
            "isKey": false,
            "numCitedBy": 1086,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years. However, tasks such as visual question answering require combining these vector representations with each other. Approaches to multimodal pooling include element-wise product or sum, as well as concatenation of the visual and textual representations. We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors. As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multimodal features. We extensively evaluate MCB on the visual question answering and grounding tasks. We consistently show the benefit of MCB over ablations without MCB. For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation. This model outperforms the state-of-the-art on the Visual7W dataset and the VQA challenge."
            },
            "slug": "Multimodal-Compact-Bilinear-Pooling-for-Visual-and-Fukui-Park",
            "title": {
                "fragments": [],
                "text": "Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This work extensively evaluates Multimodal Compact Bilinear pooling (MCB) on the visual question answering and grounding tasks and consistently shows the benefit of MCB over ablations without MCB."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691993"
                        ],
                        "name": "E. Riloff",
                        "slug": "E.-Riloff",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Riloff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riloff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "elations between text reading and information extraction. 2.2 Information Extraction Information extraction has been studied for decades. Before the advent of learning based models, rule-based methods[8, 11, 21, 38, 44, 47] were proposed. Pattern matching were widely used in [21, 44] to extract one or multiple target values. Afterwards, Intellix[47] required predefined template with relevant fields annotated and SmartFi"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2257053,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bdc08721414c972ab451f8ef3ef39d63c741b324",
            "isKey": true,
            "numCitedBy": 555,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Knowledge-based natural language processing systems have achieved good success with certain tasks but they are often criticized because they depend on a domain-specific dictionary that requires a great deal of manual knowledge engineering. This knowledge engineering bottleneck makes knowledge-based NLP systems impractical for real-world applications because they cannot be easily scaled up or ported to new domains. In response to this problem, we developed a system called AutoSlog that automatically builds a domain-specific dictionary of concepts for extracting information from text. Using AutoSlog, we constructed a dictionary for the domain of terrorist event descriptions in only 5 person-hours. We then compared the AutoSlog dictionary with a hand-crafted dictionary that was built by two highly skilled graduate students and required approximately 1500 person-hours of effort. We evaluated the two dictionaries using two blind test sets of 100 texts each. Overall, the AutoSlog dictionary achieved 98% of the performance of the hand-crafted dictionary. On the first test set, the AutoSlog dictionary obtained 96.3% of the performance of the hand-crafted dictionary. On the second test set, the overall scores were virtually indistinguishable with the AutoSlog dictionary achieving 99.7% of the performance of the handcrafted dictionary."
            },
            "slug": "Automatically-Constructing-a-Dictionary-for-Tasks-Riloff",
            "title": {
                "fragments": [],
                "text": "Automatically Constructing a Dictionary for Information Extraction Tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Using AutoSlog, a system that automatically builds a domain-specific dictionary of concepts for extracting information from text, a dictionary for the domain of terrorist event descriptions was constructed in only 5 person-hours and the overall scores were virtually indistinguishable."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157221"
                        ],
                        "name": "E. T. K. Sang",
                        "slug": "E.-T.-K.-Sang",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Sang",
                            "middleNames": [
                                "Tjong",
                                "Kim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. T. K. Sang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143667674"
                        ],
                        "name": "J. Veenstra",
                        "slug": "J.-Veenstra",
                        "structuredName": {
                            "firstName": "Jorn",
                            "lastName": "Veenstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Veenstra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "which is followed by a fully connected network, projecting the output to the dimension of IOB [46] label space."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1845735,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "008a2291a257072f22764196a3acf0a394bf203a",
            "isKey": false,
            "numCitedBy": 392,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Dividing sentences in chunks of words is a useful preprocessing step for parsing, information extraction and information retrieval. (Ramshaw and Marcus, 1995) have introduced a \"convenient\" data representation for chunking by converting it to a tagging task. In this paper we will examine seven different data representations for the problem of recognizing noun phrase chunks. We will show that the the data representation choice has a minor influence on chunking performance. However, equipped with the most suitable data representation, our memory-based learning chunker was able to improve the best published chunking results for a standard data set."
            },
            "slug": "Representing-Text-Chunks-Sang-Veenstra",
            "title": {
                "fragments": [],
                "text": "Representing Text Chunks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that the the data representation choice has a minor influence on chunking performance, however, equipped with the most suitable data representation, the memory-based learning chunker was able to improve the best published chunking results for a standard data set."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143618944"
                        ],
                        "name": "Vikas Yadav",
                        "slug": "Vikas-Yadav",
                        "structuredName": {
                            "firstName": "Vikas",
                            "lastName": "Yadav",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vikas Yadav"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105138"
                        ],
                        "name": "Steven Bethard",
                        "slug": "Steven-Bethard",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Bethard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven Bethard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Learning-based networks [7, 10, 24, 36, 55, 56] were firstly proposed to work on plain text documents."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 49587276,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7245e15da2a0f09aeca6c1e7478237985f7e4819",
            "isKey": false,
            "numCitedBy": 332,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "Named Entity Recognition (NER) is a key component in NLP systems for question answering, information retrieval, relation extraction, etc. NER systems have been studied and developed widely for decades, but accurate systems using deep neural networks (NN) have only been introduced in the last few years. We present a comprehensive survey of deep neural network architectures for NER, and contrast them with previous approaches to NER based on feature engineering and other supervised or semi-supervised learning algorithms. Our results highlight the improvements achieved by neural networks, and show how incorporating some of the lessons learned from past work on feature-based NER systems can yield further improvements."
            },
            "slug": "A-Survey-on-Recent-Advances-in-Named-Entity-from-Yadav-Bethard",
            "title": {
                "fragments": [],
                "text": "A Survey on Recent Advances in Named Entity Recognition from Deep Learning models"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work presents a comprehensive survey of deep neural network architectures for NER, and contrast them with previous approaches to NER based on feature engineering and other supervised or semi-supervised learning algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8553015"
                        ],
                        "name": "Jiasen Lu",
                        "slug": "Jiasen-Lu",
                        "structuredName": {
                            "firstName": "Jiasen",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiasen Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746610"
                        ],
                        "name": "Dhruv Batra",
                        "slug": "Dhruv-Batra",
                        "structuredName": {
                            "firstName": "Dhruv",
                            "lastName": "Batra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dhruv Batra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153432684"
                        ],
                        "name": "Devi Parikh",
                        "slug": "Devi-Parikh",
                        "structuredName": {
                            "firstName": "Devi",
                            "lastName": "Parikh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Devi Parikh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2297229"
                        ],
                        "name": "Stefan Lee",
                        "slug": "Stefan-Lee",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefan Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 199453025,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "65a9c7b0800c86a196bc14e7621ff895cc6ab287",
            "isKey": false,
            "numCitedBy": 1266,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "We present ViLBERT (short for Vision-and-Language BERT), a model for learning task-agnostic joint representations of image content and natural language. We extend the popular BERT architecture to a multi-modal two-stream model, pro-cessing both visual and textual inputs in separate streams that interact through co-attentional transformer layers. We pretrain our model through two proxy tasks on the large, automatically collected Conceptual Captions dataset and then transfer it to multiple established vision-and-language tasks -- visual question answering, visual commonsense reasoning, referring expressions, and caption-based image retrieval -- by making only minor additions to the base architecture. We observe significant improvements across tasks compared to existing task-specific models -- achieving state-of-the-art on all four tasks. Our work represents a shift away from learning groundings between vision and language only as part of task training and towards treating visual grounding as a pretrainable and transferable capability."
            },
            "slug": "ViLBERT:-Pretraining-Task-Agnostic-Visiolinguistic-Lu-Batra",
            "title": {
                "fragments": [],
                "text": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "ViLBERT (short for Vision-and-Language BERT), a model for learning task-agnostic joint representations of image content and natural language, is presented, extending the popular BERT architecture to a multi-modal two-stream model, pro-cessing both visual and textual inputs in separate streams that interact through co-attentional transformer layers."
            },
            "venue": {
                "fragments": [],
                "text": "NeurIPS"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8387085"
                        ],
                        "name": "Zichao Yang",
                        "slug": "Zichao-Yang",
                        "structuredName": {
                            "firstName": "Zichao",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zichao Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144137069"
                        ],
                        "name": "Xiaodong He",
                        "slug": "Xiaodong-He",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800422"
                        ],
                        "name": "Jianfeng Gao",
                        "slug": "Jianfeng-Gao",
                        "structuredName": {
                            "firstName": "Jianfeng",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianfeng Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "lation) are mined and processed from previously recognized plain text for diverse Natural Language Processing (NLP) tasks, such as Named Entity Recognition (NER) [24, 36, 48] and Question-Answer (QA) [1, 13, 57]. Note that in existing routines, the two stages are separately executed. It means that the former recognizes text from images without semantic supervision (e.g., entity name annotation) of IE stage, "
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8849206,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c1890864c1c2b750f48316dc8b650ba4772adc5",
            "isKey": false,
            "numCitedBy": 1474,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents stacked attention networks (SANs) that learn to answer natural language questions from images. SANs use semantic representation of a question as query to search for the regions in an image that are related to the answer. We argue that image question answering (QA) often requires multiple steps of reasoning. Thus, we develop a multiple-layer SAN in which we query an image multiple times to infer the answer progressively. Experiments conducted on four image QA data sets demonstrate that the proposed SANs significantly outperform previous state-of-the-art approaches. The visualization of the attention layers illustrates the progress that the SAN locates the relevant visual clues that lead to the answer of the question layer-by-layer."
            },
            "slug": "Stacked-Attention-Networks-for-Image-Question-Yang-He",
            "title": {
                "fragments": [],
                "text": "Stacked Attention Networks for Image Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A multiple-layer SAN is developed in which an image is queried multiple times to infer the answer progressively, and the progress that the SAN locates the relevant visual clues that lead to the answer of the question layer-by-layer."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50521003"
                        ],
                        "name": "Chen-Yu Lee",
                        "slug": "Chen-Yu-Lee",
                        "structuredName": {
                            "firstName": "Chen-Yu",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chen-Yu Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Then, the attention mechanism replaced existing CTC decoder [50] and was applied to a stacked RNN on top of the recursive CNN [25], whose performance surpassed the state-of-the-art among diverse variations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8608310,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8c9d85147039ca54b0439cde05ef8c33efecf00",
            "isKey": false,
            "numCitedBy": 322,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "We present recursive recurrent neural networks with attention modeling (R2AM) for lexicon-free optical character recognition in natural scene images. The primary advantages of the proposed method are: (1) use of recursive convolutional neural networks (CNNs), which allow for parametrically efficient and effective image feature extraction, (2) an implicitly learned character-level language model, embodied in a recurrent neural network which avoids the need to use N-grams, and (3) the use of a soft-attention mechanism, allowing the model to selectively exploit image features in a coordinated way, and allowing for end-to-end training within a standard backpropagation framework. We validate our method with state-of-the-art performance on challenging benchmark datasets: Street View Text, IIIT5k, ICDAR and Synth90k."
            },
            "slug": "Recursive-Recurrent-Nets-with-Attention-Modeling-in-Lee-Osindero",
            "title": {
                "fragments": [],
                "text": "Recursive Recurrent Nets with Attention Modeling for OCR in the Wild"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "This work presents recursive recurrent neural networks with attention modeling (R2AM) for lexicon-free optical character recognition in natural scene images and validates the method with state-of-the-art performance on challenging benchmark datasets."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46447747"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7818229"
                        ],
                        "name": "J. Zhao",
                        "slug": "J.-Zhao",
                        "structuredName": {
                            "firstName": "Junbo",
                            "lastName": "Zhao",
                            "middleNames": [
                                "Jake"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We first introduce position embeddings to preserve layout information, Then, a ConvNet of multiple kernels similar to [60] is used to aggregate semantic character features in zi and outputs \u1e91i ,"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 368182,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "51a55df1f023571a7e07e338ee45a3e3d66ef73e",
            "isKey": false,
            "numCitedBy": 3477,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks."
            },
            "slug": "Character-level-Convolutional-Networks-for-Text-Zhang-Zhao",
            "title": {
                "fragments": [],
                "text": "Character-level Convolutional Networks for Text Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This article constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results in text classification."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32562635"
                        ],
                        "name": "Liunian Harold Li",
                        "slug": "Liunian-Harold-Li",
                        "structuredName": {
                            "firstName": "Liunian",
                            "lastName": "Li",
                            "middleNames": [
                                "Harold"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liunian Harold Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064210"
                        ],
                        "name": "Mark Yatskar",
                        "slug": "Mark-Yatskar",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Yatskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Yatskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144508458"
                        ],
                        "name": "Da Yin",
                        "slug": "Da-Yin",
                        "structuredName": {
                            "firstName": "Da",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Da Yin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793529"
                        ],
                        "name": "Cho-Jui Hsieh",
                        "slug": "Cho-Jui-Hsieh",
                        "structuredName": {
                            "firstName": "Cho-Jui",
                            "lastName": "Hsieh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cho-Jui Hsieh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2782886"
                        ],
                        "name": "Kai-Wei Chang",
                        "slug": "Kai-Wei-Chang",
                        "structuredName": {
                            "firstName": "Kai-Wei",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai-Wei Chang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Inspired by [10, 27, 34], we apply the self-attention mechanism to extract textual context features, supporting variable number of texts."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 199528533,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5aec474c31a2f4b74703c6f786c0a8ff85c450da",
            "isKey": false,
            "numCitedBy": 630,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose VisualBERT, a simple and flexible framework for modeling a broad range of vision-and-language tasks. VisualBERT consists of a stack of Transformer layers that implicitly align elements of an input text and regions in an associated input image with self-attention. We further propose two visually-grounded language model objectives for pre-training VisualBERT on image caption data. Experiments on four vision-and-language tasks including VQA, VCR, NLVR2, and Flickr30K show that VisualBERT outperforms or rivals with state-of-the-art models while being significantly simpler. Further analysis demonstrates that VisualBERT can ground elements of language to image regions without any explicit supervision and is even sensitive to syntactic relationships, tracking, for example, associations between verbs and image regions corresponding to their arguments."
            },
            "slug": "VisualBERT:-A-Simple-and-Performant-Baseline-for-Li-Yatskar",
            "title": {
                "fragments": [],
                "text": "VisualBERT: A Simple and Performant Baseline for Vision and Language"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Analysis demonstrates that VisualBERT can ground elements of language to image regions without any explicit supervision and is even sensitive to syntactic relationships, tracking, for example, associations between verbs and image regions corresponding to their arguments."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2032184078"
                        ],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Ldet is the loss of text detection branch, which consists of a classification loss and a regression loss, as defined in [43]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The text detection branch in text reading module adopts the Faster R-CNN [43] network and outputs the predicted bounding boxes of possible texts for later sequential recognition."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10328909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "424561d8585ff8ebce7d5d07de8dbf7aae5e7270",
            "isKey": false,
            "numCitedBy": 32561,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available"
            },
            "slug": "Faster-R-CNN:-Towards-Real-Time-Object-Detection-Ren-He",
            "title": {
                "fragments": [],
                "text": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work introduces a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals and further merge RPN and Fast R-CNN into a single network by sharing their convolutionAL features."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067170682"
                        ],
                        "name": "Rasmus Berg Palm",
                        "slug": "Rasmus-Berg-Palm",
                        "structuredName": {
                            "firstName": "Rasmus",
                            "lastName": "Palm",
                            "middleNames": [
                                "Berg"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rasmus Berg Palm"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724252"
                        ],
                        "name": "O. Winther",
                        "slug": "O.-Winther",
                        "structuredName": {
                            "firstName": "Ole",
                            "lastName": "Winther",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Winther"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805808"
                        ],
                        "name": "Florian Laws",
                        "slug": "Florian-Laws",
                        "structuredName": {
                            "firstName": "Florian",
                            "lastName": "Laws",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Florian Laws"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Under the traditional routines, most existing methods [9, 23, 30, 39, 40, 45, 54, 61] design frameworks in multiple stages of text reading (usually including detection and recognition) and information extraction independently in order."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[40, 45] adopted the idea from natural language processing and used recurrent neural networks to extract entities of interest from VRDs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In earlier explorations, the task is intrinsically downgraded into a traditional OCR procedure and the downstream IE from serialized plain text [40, 45], which completely discards the visual features and layout information from images, as shown in Fig."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 30210556,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01a50b9662a59070c4ff53873453d8854c15ade1",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We present CloudScan; an invoice analysis system that requires zero configuration or upfront annotation. In contrast to previous work, CloudScan does not rely on templates of invoice layout, instead it learns a single global model of invoices that naturally generalizes to unseen invoice layouts. The model is trained using data automatically extracted from end-user provided feedback. This automatic training data extraction removes the requirement for users to annotate the data precisely. We describe a recurrent neural network model that can capture long range context and compare it to a baseline logistic regression model corresponding to the current CloudScan production system. We train and evaluate the system on 8 important fields using a dataset of 326,471 invoices. The recurrent neural network and baseline model achieve 0.891 and 0.887 average F1 scores respectively on seen invoice layouts. For the harder task of unseen invoice layouts, the recurrent neural network model outperforms the baseline with 0.840 average F1 compared to 0.788."
            },
            "slug": "CloudScan-A-Configuration-Free-Invoice-Analysis-Palm-Winther",
            "title": {
                "fragments": [],
                "text": "CloudScan - A Configuration-Free Invoice Analysis System Using Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A recurrent neural network model that can capture long range context and compare it to a baseline logistic regression model corresponding to the current CloudScan production system are described."
            },
            "venue": {
                "fragments": [],
                "text": "2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3276863"
                        ],
                        "name": "Ion Muslea",
                        "slug": "Ion-Muslea",
                        "structuredName": {
                            "firstName": "Ion",
                            "lastName": "Muslea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ion Muslea"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "elations between text reading and information extraction. 2.2 Information Extraction Information extraction has been studied for decades. Before the advent of learning based models, rule-based methods[8, 11, 21, 38, 44, 47] were proposed. Pattern matching were widely used in [21, 44] to extract one or multiple target values. Afterwards, Intellix[47] required predefined template with relevant fields annotated and SmartFi"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18693692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f01732c9be8ccb70b41be008e043187d40be0fdf",
            "isKey": true,
            "numCitedBy": 165,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Information Extraction systems rely on a set of extraction patterns that they use in order to retrieve from each document the relevant information. In this paper we survey the various types of extraction patterns that are generated by machine learning algorithms. We identify three main categories of patterns, which cover a variety of application domains, and we compare and contrast the patterns from each category."
            },
            "slug": "Extraction-Patterns-for-Information-Extraction-A-Muslea",
            "title": {
                "fragments": [],
                "text": "Extraction Patterns for Information Extraction Tasks: A Survey"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper surveys the various types of extraction patterns that are generated by machine learning algorithms and identifies three main categories of patterns, which cover a variety of application domains, and compares and contrast the patterns from each category."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422912"
                        ],
                        "name": "Zihang Dai",
                        "slug": "Zihang-Dai",
                        "structuredName": {
                            "firstName": "Zihang",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zihang Dai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109512754"
                        ],
                        "name": "Zhilin Yang",
                        "slug": "Zhilin-Yang",
                        "structuredName": {
                            "firstName": "Zhilin",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhilin Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35729970"
                        ],
                        "name": "Yiming Yang",
                        "slug": "Yiming-Yang",
                        "structuredName": {
                            "firstName": "Yiming",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiming Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143712374"
                        ],
                        "name": "J. Carbonell",
                        "slug": "J.-Carbonell",
                        "structuredName": {
                            "firstName": "Jaime",
                            "lastName": "Carbonell",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Carbonell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 57759363,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6",
            "isKey": false,
            "numCitedBy": 1771,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch."
            },
            "slug": "Transformer-XL:-Attentive-Language-Models-beyond-a-Dai-Yang",
            "title": {
                "fragments": [],
                "text": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work proposes a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence, which consists of a segment-level recurrence mechanism and a novel positional encoding scheme."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33493200"
                        ],
                        "name": "Tsung-Yi Lin",
                        "slug": "Tsung-Yi-Lin",
                        "structuredName": {
                            "firstName": "Tsung-Yi",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tsung-Yi Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790580"
                        ],
                        "name": "Bharath Hariharan",
                        "slug": "Bharath-Hariharan",
                        "structuredName": {
                            "firstName": "Bharath",
                            "lastName": "Hariharan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bharath Hariharan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "1 Network Details: The backbone of our model is ResNet-50 [16], followed by the FPN [29] to further enhance features."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We adopt ResNet [16] and Feature Pyramid Network (FPN) [29] as our backbone to extract shared convolutional features."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10716717,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9b4e05faa194e5022edd9eb9dd07e3d675c2b36",
            "isKey": false,
            "numCitedBy": 9352,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But pyramid representations have been avoided in recent object detectors that are based on deep convolutional networks, partially because they are slow to compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available."
            },
            "slug": "Feature-Pyramid-Networks-for-Object-Detection-Lin-Doll\u00e1r",
            "title": {
                "fragments": [],
                "text": "Feature Pyramid Networks for Object Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper exploits the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost and achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39172707"
                        ],
                        "name": "Jacob Devlin",
                        "slug": "Jacob-Devlin",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Devlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacob Devlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744179"
                        ],
                        "name": "Ming-Wei Chang",
                        "slug": "Ming-Wei-Chang",
                        "structuredName": {
                            "firstName": "Ming-Wei",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Wei Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2544107"
                        ],
                        "name": "Kenton Lee",
                        "slug": "Kenton-Lee",
                        "structuredName": {
                            "firstName": "Kenton",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenton Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3259253"
                        ],
                        "name": "Kristina Toutanova",
                        "slug": "Kristina-Toutanova",
                        "structuredName": {
                            "firstName": "Kristina",
                            "lastName": "Toutanova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristina Toutanova"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "xt which focuses on local visual patterns, textual context models the fine-grained long distance dependencies and relationships between texts, providing complementary context information. Inspired by [10, 27, 34], we apply the self-attention mechanism to extract textual context features, supporting variable number of texts. Self-attention Recap. The input of popular scaled dot-product attention consists of qu"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "y heavily on the predefined rules, whose design and maintenance usually require deep expertise and large time cost. Besides, they can not generalize across document templates. Learning-based networks [7, 10, 24, 36, 55, 56] were firstly proposed to work on plain text documents. [40, 45] adopted the idea from natural language processing and used recurrent neural networks to extract entities of interest from VRDs. However"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "had also been utilized. [30] combined texts and their positions through a Graph Convolutional Network (GCN) and [54] further integrated position and image embeddings for pre-training inspired by BERT [10]. [37] presented a representation learning approach to extract structured information from templatic documents, which worked in pipeline of candidate generation, scoring and assignment. However, these"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 52967399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "isKey": false,
            "numCitedBy": 33744,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
            },
            "slug": "BERT:-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang",
            "title": {
                "fragments": [],
                "text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A new language representation model, BERT, designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers, which can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771551"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "nd information extraction modules. 3.2 Text Reading Module Text reading module commonly includes a shared backbone, a text detection branch as well as a sequential recognition branch. We adopt ResNet [16] and Feature Pyramid Network (FPN) [29] as our backbone to extract shared convolutional features. For an input image x, we denote I\u2208Rh\u00d7w\u00d7d as the shared feature maps, where h, w and d are the height, "
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "). As an owner can design his own resume template, this dataset has variable layouts and semi-structured text. 4.2 Implementation Details 4.2.1 Network Details: The backbone of our model is ResNet-50 [16], followed by the FPN [29] to further enhance features. The text detection branch in text reading module adopts the Faster R-CNN [43] network and outputs the predicted bounding boxes of possible texts"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206594692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "isKey": true,
            "numCitedBy": 95318,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
            },
            "slug": "Deep-Residual-Learning-for-Image-Recognition-He-Zhang",
            "title": {
                "fragments": [],
                "text": "Deep Residual Learning for Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work presents a residual learning framework to ease the training of networks that are substantially deeper than those used previously, and provides comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082991"
                        ],
                        "name": "Georgia Gkioxari",
                        "slug": "Georgia-Gkioxari",
                        "structuredName": {
                            "firstName": "Georgia",
                            "lastName": "Gkioxari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Georgia Gkioxari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "uts the predicted bounding boxes of possible texts for later sequential recognition. For each text region, its features of shape 32\u00d7256are extracted from the shared convolutional features by RoIAlign [13] and decoded by LSTM-based attention, where the number of hidden units is set to 256. In the information extraction module, we use convolutions of three kernel size [3,5,7]followed by max pooling to e"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "t detection methods. Here, B= (b1,b2,...,bm)is a set of m text bounding boxes and bi = (x0,y0, x1,y1)denotes the top-left and bottom-right positions of the i-th text. Given text positions B, RoIAlign [13] is applied on the shared convolutional features Ito get their text region features, denoted as C= (c1,c2,...,cm), where ci \u2208Rh \u2032\u00d7w \u00d7d, h\u2032and w\u2032are the spatial dimensions, and d is the vector dimensio"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 54465873,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "022dd244f2e25525eb37e9dda51abb9cd8ca8c30",
            "isKey": false,
            "numCitedBy": 9771,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron."
            },
            "slug": "Mask-R-CNN-He-Gkioxari",
            "title": {
                "fragments": [],
                "text": "Mask R-CNN"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This work presents a conceptually simple, flexible, and general framework for object instance segmentation that outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145279674"
                        ],
                        "name": "A. Dengel",
                        "slug": "A.-Dengel",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Dengel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dengel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34996245"
                        ],
                        "name": "B. Klein",
                        "slug": "B.-Klein",
                        "structuredName": {
                            "firstName": "Bertin",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Klein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 97
                            }
                        ],
                        "text": "Afterwards, Intellix[41] required predefined template with relevant fields annotated and SmartFix[6] employed specifically designed configuration rules for each template."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 62
                            }
                        ],
                        "text": "Before the advent of learning based models, rule-based methods[6, 9, 19, 32, 38, 41] were proposed."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7257921,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cb41491a9c4bcd09d83a29dc10304c9a8b602920",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Although the internet offers a wide-spread platform for information interchange, day-to-day work in large companies still means the processing of tens of thousands of printed documents every day. This paper presents the system smartFIX which is a document analysis and understanding system developed by the DFKI spin-off INSIDERS. It permits the processing of documents ranging from fixed format forms to unstructured letters of any format. Apart from the architecture, the main components and system characteristics, we also show some results when applying smartFIX to medical bills and prescriptions."
            },
            "slug": "smartFIX:-A-Requirements-Driven-System-for-Document-Dengel-Klein",
            "title": {
                "fragments": [],
                "text": "smartFIX: A Requirements-Driven System for Document Analysis and Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The system smartFIX which is a document analysis and understanding system developed by the DFKI spin-off INSIDERS permits the processing of documents ranging from fixed format forms to unstructured letters of any format."
            },
            "venue": {
                "fragments": [],
                "text": "Document Analysis Systems"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109512754"
                        ],
                        "name": "Zhilin Yang",
                        "slug": "Zhilin-Yang",
                        "structuredName": {
                            "firstName": "Zhilin",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhilin Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422912"
                        ],
                        "name": "Zihang Dai",
                        "slug": "Zihang-Dai",
                        "structuredName": {
                            "firstName": "Zihang",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zihang Dai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35729970"
                        ],
                        "name": "Yiming Yang",
                        "slug": "Yiming-Yang",
                        "structuredName": {
                            "firstName": "Yiming",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiming Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143712374"
                        ],
                        "name": "J. Carbonell",
                        "slug": "J.-Carbonell",
                        "structuredName": {
                            "firstName": "Jaime",
                            "lastName": "Carbonell",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Carbonell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Learning-based networks [7, 10, 24, 36, 55, 56] were firstly proposed to work on plain text documents."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 195069387,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
            "isKey": false,
            "numCitedBy": 4226,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking."
            },
            "slug": "XLNet:-Generalized-Autoregressive-Pretraining-for-Yang-Dai",
            "title": {
                "fragments": [],
                "text": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "XLNet is proposed, a generalized autoregressive pretraining method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and overcomes the limitations of BERT thanks to its autore progressive formulation."
            },
            "venue": {
                "fragments": [],
                "text": "NeurIPS"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308557"
                        ],
                        "name": "S. Hochreiter",
                        "slug": "S.-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hochreiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "(2) NER(TR): Pipeline of text reading and LSTM-CRF."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 135
                            }
                        ],
                        "text": "For each text region, its features of shape 32\u00d7256 are extracted from the shared convolutional features by RoIAlign [13] and decoded by LSTM-based attention, where the number of hidden units is set to 256."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "Character-wise BiLSTM is used to create word embeddings while word wise BiLSTM to classify each word into corresponding category."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "The input embedding of LSTM-CRF[31] is set to 256, followed by a BiLSTM with 128 hidden units and CRF. (3) GCN(TR): Pipeline of text Reading and GCN."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "The number of hidden units of BiLSTM used in entity extraction is set to 128."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 117
                            }
                        ],
                        "text": "In text recognition, the mainstreaming CRNN framework was indroduced by [44], using recurrent neural networks (RNNs) [4, 17] combined with CNN-based methods for better sequential recognition of text lines."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "In our experiment, LSTM[17] is adopted as RNN unit."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "Character-Word LSTM is similar to named entity recognition [22] and applys LSTM on character and word level sequentially."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 199
                            }
                        ],
                        "text": "We further combine the context vector ci and textual features zi by concatenating ci to each state vector sj in zi , Ui = (u1,1,ui,2, . . . ,ui,T ), where uj = si, j | |ci (13) Then, a Bidirectional-LSTM is applied to further model the dependencies within the characters,\nHi = (hi,1,hi,2, . . . ,hi,T ) = BiLSTM(Ui ), (14) which is followed by a fully connected network, projecting the output to the dimension of IOB [40] label space.\np inf o i, j = so f tmax(Linear (hi, j )) (15)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 1915014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44d2abe2175df8153f465f6c39b68b76a0d40ab9",
            "isKey": true,
            "numCitedBy": 51691,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms."
            },
            "slug": "Long-Short-Term-Memory-Hochreiter-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Long Short-Term Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel, efficient, gradient based method called long short-term memory (LSTM) is introduced, which can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8270717"
                        ],
                        "name": "Junyoung Chung",
                        "slug": "Junyoung-Chung",
                        "structuredName": {
                            "firstName": "Junyoung",
                            "lastName": "Chung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junyoung Chung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1854385"
                        ],
                        "name": "\u00c7aglar G\u00fcl\u00e7ehre",
                        "slug": "\u00c7aglar-G\u00fcl\u00e7ehre",
                        "structuredName": {
                            "firstName": "\u00c7aglar",
                            "lastName": "G\u00fcl\u00e7ehre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00c7aglar G\u00fcl\u00e7ehre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1979489"
                        ],
                        "name": "Kyunghyun Cho",
                        "slug": "Kyunghyun-Cho",
                        "structuredName": {
                            "firstName": "Kyunghyun",
                            "lastName": "Cho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyunghyun Cho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 117
                            }
                        ],
                        "text": "In text recognition, the mainstreaming CRNN framework was indroduced by [50], using recurrent neural networks (RNNs) [6, 19] combined with CNNbased methods for better sequential recognition of text lines."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5201925,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "adfcf065e15fd3bc9badf6145034c84dfb08f204",
            "isKey": false,
            "numCitedBy": 7376,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM."
            },
            "slug": "Empirical-Evaluation-of-Gated-Recurrent-Neural-on-Chung-G\u00fcl\u00e7ehre",
            "title": {
                "fragments": [],
                "text": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "These advanced recurrent units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU), are found to be comparable to LSTM."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48799969"
                        ],
                        "name": "Matthew D. Zeiler",
                        "slug": "Matthew-D.-Zeiler",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Zeiler",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew D. Zeiler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 33
                            }
                        ],
                        "text": "For our model, with the ADADELTA [52] optimization method, we set the learning rate to 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 24
                            }
                        ],
                        "text": "For our model, with the ADADELTA [52] optimization method, we set the learning rate to 1.0 at the beginning and decreased it to a tenth every 40 epoches."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7365802,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8729441d734782c3ed532a7d2d9611b438c0a09a",
            "isKey": true,
            "numCitedBy": 5464,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment."
            },
            "slug": "ADADELTA:-An-Adaptive-Learning-Rate-Method-Zeiler",
            "title": {
                "fragments": [],
                "text": "ADADELTA: An Adaptive Learning Rate Method"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A novel per-dimension learning rate method for gradient descent called ADADELTA that dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent is presented."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3407277"
                        ],
                        "name": "Adam Paszke",
                        "slug": "Adam-Paszke",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Paszke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Paszke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39793298"
                        ],
                        "name": "S. Gross",
                        "slug": "S.-Gross",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Gross",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gross"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403239967"
                        ],
                        "name": "Francisco Massa",
                        "slug": "Francisco-Massa",
                        "structuredName": {
                            "firstName": "Francisco",
                            "lastName": "Massa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Francisco Massa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1977806"
                        ],
                        "name": "Adam Lerer",
                        "slug": "Adam-Lerer",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Lerer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Lerer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065251344"
                        ],
                        "name": "James Bradbury",
                        "slug": "James-Bradbury",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bradbury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Bradbury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114250963"
                        ],
                        "name": "Gregory Chanan",
                        "slug": "Gregory-Chanan",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Chanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gregory Chanan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059271276"
                        ],
                        "name": "Trevor Killeen",
                        "slug": "Trevor-Killeen",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Killeen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Killeen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3370429"
                        ],
                        "name": "Zeming Lin",
                        "slug": "Zeming-Lin",
                        "structuredName": {
                            "firstName": "Zeming",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zeming Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3365851"
                        ],
                        "name": "N. Gimelshein",
                        "slug": "N.-Gimelshein",
                        "structuredName": {
                            "firstName": "Natalia",
                            "lastName": "Gimelshein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Gimelshein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3029482"
                        ],
                        "name": "L. Antiga",
                        "slug": "L.-Antiga",
                        "structuredName": {
                            "firstName": "Luca",
                            "lastName": "Antiga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Antiga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3050846"
                        ],
                        "name": "Alban Desmaison",
                        "slug": "Alban-Desmaison",
                        "structuredName": {
                            "firstName": "Alban",
                            "lastName": "Desmaison",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alban Desmaison"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1473151134"
                        ],
                        "name": "Andreas K\u00f6pf",
                        "slug": "Andreas-K\u00f6pf",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "K\u00f6pf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas K\u00f6pf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052812305"
                        ],
                        "name": "E. Yang",
                        "slug": "E.-Yang",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2375710"
                        ],
                        "name": "Zach DeVito",
                        "slug": "Zach-DeVito",
                        "structuredName": {
                            "firstName": "Zach",
                            "lastName": "DeVito",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zach DeVito"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10707709"
                        ],
                        "name": "Martin Raison",
                        "slug": "Martin-Raison",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Raison",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin Raison"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41203992"
                        ],
                        "name": "Alykhan Tejani",
                        "slug": "Alykhan-Tejani",
                        "structuredName": {
                            "firstName": "Alykhan",
                            "lastName": "Tejani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alykhan Tejani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "22236100"
                        ],
                        "name": "Sasank Chilamkurthy",
                        "slug": "Sasank-Chilamkurthy",
                        "structuredName": {
                            "firstName": "Sasank",
                            "lastName": "Chilamkurthy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sasank Chilamkurthy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32163737"
                        ],
                        "name": "Benoit Steiner",
                        "slug": "Benoit-Steiner",
                        "structuredName": {
                            "firstName": "Benoit",
                            "lastName": "Steiner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benoit Steiner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152599430"
                        ],
                        "name": "Lu Fang",
                        "slug": "Lu-Fang",
                        "structuredName": {
                            "firstName": "Lu",
                            "lastName": "Fang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lu Fang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113829116"
                        ],
                        "name": "Junjie Bai",
                        "slug": "Junjie-Bai",
                        "structuredName": {
                            "firstName": "Junjie",
                            "lastName": "Bai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junjie Bai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2127604"
                        ],
                        "name": "Soumith Chintala",
                        "slug": "Soumith-Chintala",
                        "structuredName": {
                            "firstName": "Soumith",
                            "lastName": "Chintala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Soumith Chintala"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "o 128. Hyper-parameters \u03bbreco\u0434 and \u03bbinf o in Equa. 16 are all set to 1 in our experiments. 4.2.2 Training: Our model and all the reimplemented counterparts are implemented under the PyTorch framework [35]. For our model, with the ADADELTA [52] optimization method, we set the learning rate to 1.0 at the beginning and decreased it to a tenth every 40 epoches. The batch size is set to 2 images per GPU an"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 202786778,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1",
            "isKey": true,
            "numCitedBy": 13414,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks."
            },
            "slug": "PyTorch:-An-Imperative-Style,-High-Performance-Deep-Paszke-Gross",
            "title": {
                "fragments": [],
                "text": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper details the principles that drove the implementation of PyTorch and how they are reflected in its architecture, and explains how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance."
            },
            "venue": {
                "fragments": [],
                "text": "NeurIPS"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830914"
                        ],
                        "name": "Guillaume Lample",
                        "slug": "Guillaume-Lample",
                        "structuredName": {
                            "firstName": "Guillaume",
                            "lastName": "Lample",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guillaume Lample"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143668305"
                        ],
                        "name": "Miguel Ballesteros",
                        "slug": "Miguel-Ballesteros",
                        "structuredName": {
                            "firstName": "Miguel",
                            "lastName": "Ballesteros",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Miguel Ballesteros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50324141"
                        ],
                        "name": "Sandeep Subramanian",
                        "slug": "Sandeep-Subramanian",
                        "structuredName": {
                            "firstName": "Sandeep",
                            "lastName": "Subramanian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sandeep Subramanian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2189948"
                        ],
                        "name": "Kazuya Kawakami",
                        "slug": "Kazuya-Kawakami",
                        "structuredName": {
                            "firstName": "Kazuya",
                            "lastName": "Kawakami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kazuya Kawakami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745899"
                        ],
                        "name": "Chris Dyer",
                        "slug": "Chris-Dyer",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Dyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Dyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ge, specific contents (e.g. entity, relation) are mined and processed from previously recognized plain text for diverse Natural Language Processing (NLP) tasks, such as Named Entity Recognition (NER) [24, 36, 48] and Question-Answer (QA) [1, 13, 57]. Note that in existing routines, the two stages are separately executed. It means that the former recognizes text from images without semantic supervision (e.g., "
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "ore Setting 1: Prediction of bboxes and transcript of texts Chargrid(TR) 78.24 NER(TR) 69.09 GCN(TR) 76.51 Our model 82.06 Setting 2: Groundtruth of bboxes and transcript of texts Character-Word LSTM [24] 90.85 LayoutLM[54] 95.24 PICK[58] 96.12 Our model 96.18 Character-Word LSTM is similar to NER [24], which applies LSTM on character and word level sequentially. LayoutLM [54] makes use of large pre-t"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "y heavily on the predefined rules, whose design and maintenance usually require deep expertise and large time cost. Besides, they can not generalize across document templates. Learning-based networks [7, 10, 24, 36, 55, 56] were firstly proposed to work on plain text documents. [40, 45] adopted the idea from natural language processing and used recurrent neural networks to extract entities of interest from VRDs. However"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6042994,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "24158c9fc293c8a998ac552b1188404a877da292",
            "isKey": true,
            "numCitedBy": 2899,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "Comunicacio presentada a la 2016 Conference of the North American Chapter of the Association for Computational Linguistics, celebrada a San Diego (CA, EUA) els dies 12 a 17 de juny 2016."
            },
            "slug": "Neural-Architectures-for-Named-Entity-Recognition-Lample-Ballesteros",
            "title": {
                "fragments": [],
                "text": "Neural Architectures for Named Entity Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "Comunicacio presentada a la 2016 Conference of the North American Chapter of the Association for Computational Linguistics, celebrada a San Diego (CA, EUA) els dies 12 a 17 of juny 2016."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 62
                            }
                        ],
                        "text": ", structured, and semi-structured) follows the conventions of [22, 51]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Apparatus and method for searching and retrieving structured, semi-structured and unstructured content"
            },
            "venue": {
                "fragments": [],
                "text": "US Patent App"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Denk and Christian Reisswig . 2019"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 24
                            }
                        ],
                        "text": "Learning-based networks [7, 10, 24, 36, 55, 56] were firstly proposed to work on plain text documents."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "XLNet: Generalized Autoregressive Pretraining for Poster Session A1: Deep Learning for Multimedia MM '20"
            },
            "venue": {
                "fragments": [],
                "text": "October 12\u201316,"
            },
            "year": 2019
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Katti , Christian Reisswig , Cordula Guder , Sebastian Brarda , Steffen Bickel , Johannes H\u00f6hne , and Jean Baptiste Faddoul . 2018 . Chargrid : Towards Understanding 2 D Documents"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tjong Kim Sang and Jorn Veenstra . 1999 . Representing Text Chunks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2019
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "V . Jawahar . 2019 . ICDAR 2019 Competition on Scanned Receipt OCR and Information Extraction"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Apparatus and method for searching and retrieving structured, semi-structured and unstructured content"
            },
            "venue": {
                "fragments": [],
                "text": "US Patent App."
            },
            "year": 2004
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 25,
            "methodology": 36,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 68,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/TRIE:-End-to-End-Text-Reading-and-Information-for-Zhang-Xu/617f5151f59848d24fe971cf1cf6bb0caec65ea4?sort=total-citations"
}